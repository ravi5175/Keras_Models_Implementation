{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Final level 1 code.ipynb","version":"0.3.2","provenance":[{"file_id":"1nIGcbv5nVvMX5j7-2QcpCH40sajfI33b","timestamp":1567197553049},{"file_id":"1j8arml3rmU58EM-LawVq9fEkcilu1aK6","timestamp":1567176458029},{"file_id":"1isPwterMsxfJSfDSj5Bwq-H8wefpA4xN","timestamp":1566814699351},{"file_id":"15owMBL2s_vB7DyRtIZz8WWGq14bkCL7Y","timestamp":1566814076343},{"file_id":"1uLAB5mxBP12d8CFiTVc0vTnWvR_0zsBk","timestamp":1566290405714},{"file_id":"12GgOGlZazByn1szUicEI9PzwwVghlZKX","timestamp":1566211956996}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"g4bvRfbKABth","colab_type":"code","outputId":"8e5ba03f-fa07-4cb4-de79-3ad55260e71d","executionInfo":{"status":"ok","timestamp":1567176727468,"user_tz":-330,"elapsed":56933,"user":{"displayName":"Ravi Pawar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBtV9qOP0yVjG8p6Z6OsNi6vPiuKPc2AKMnCJqR=s64","userId":"00073904877215451927"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["%cd\n","!rm -rf root/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10\n","\n","!rm -rf root/models\n","  \n","!git clone --quiet https://github.com/tensorflow/models.git\n","\n","!apt-get install -qq protobuf-compiler python-tk\n","\n","!pip install -q Cython contextlib2 pillow lxml matplotlib PyDrive\n","\n","!pip install -q pycocotools\n","\n","%cd ~/models/research\n","!protoc object_detection/protos/*.proto --python_out=.\n","\n","import os\n","os.environ['PYTHONPATH'] += ':/content/models/research/;:/content/models/research/slim/'\n","\n","!python setup.py build\n","!python setup.py install"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/root\n","\u001b[K     |████████████████████████████████| 993kB 3.4MB/s \n","\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n","/root/models/research\n","running build\n","running build_py\n","creating build\n","creating build/lib\n","creating build/lib/object_detection\n","copying object_detection/model_lib_v2.py -> build/lib/object_detection\n","copying object_detection/model_main.py -> build/lib/object_detection\n","copying object_detection/model_tpu_main.py -> build/lib/object_detection\n","copying object_detection/eval_util.py -> build/lib/object_detection\n","copying object_detection/model_hparams.py -> build/lib/object_detection\n","copying object_detection/__init__.py -> build/lib/object_detection\n","copying object_detection/exporter_test.py -> build/lib/object_detection\n","copying object_detection/export_tflite_ssd_graph_lib.py -> build/lib/object_detection\n","copying object_detection/model_lib_v2_test.py -> build/lib/object_detection\n","copying object_detection/model_lib_test.py -> build/lib/object_detection\n","copying object_detection/inputs_test.py -> build/lib/object_detection\n","copying object_detection/export_tflite_ssd_graph_lib_test.py -> build/lib/object_detection\n","copying object_detection/export_tflite_ssd_graph.py -> build/lib/object_detection\n","copying object_detection/inputs.py -> build/lib/object_detection\n","copying object_detection/export_inference_graph.py -> build/lib/object_detection\n","copying object_detection/eval_util_test.py -> build/lib/object_detection\n","copying object_detection/exporter.py -> build/lib/object_detection\n","copying object_detection/model_lib.py -> build/lib/object_detection\n","creating build/lib/object_detection/data_decoders\n","copying object_detection/data_decoders/__init__.py -> build/lib/object_detection/data_decoders\n","copying object_detection/data_decoders/tf_example_decoder_test.py -> build/lib/object_detection/data_decoders\n","copying object_detection/data_decoders/tf_example_decoder.py -> build/lib/object_detection/data_decoders\n","creating build/lib/object_detection/box_coders\n","copying object_detection/box_coders/mean_stddev_box_coder_test.py -> build/lib/object_detection/box_coders\n","copying object_detection/box_coders/faster_rcnn_box_coder.py -> build/lib/object_detection/box_coders\n","copying object_detection/box_coders/__init__.py -> build/lib/object_detection/box_coders\n","copying object_detection/box_coders/mean_stddev_box_coder.py -> build/lib/object_detection/box_coders\n","copying object_detection/box_coders/square_box_coder.py -> build/lib/object_detection/box_coders\n","copying object_detection/box_coders/square_box_coder_test.py -> build/lib/object_detection/box_coders\n","copying object_detection/box_coders/faster_rcnn_box_coder_test.py -> build/lib/object_detection/box_coders\n","copying object_detection/box_coders/keypoint_box_coder.py -> build/lib/object_detection/box_coders\n","copying object_detection/box_coders/keypoint_box_coder_test.py -> build/lib/object_detection/box_coders\n","creating build/lib/object_detection/utils\n","copying object_detection/utils/ops_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/visualization_utils.py -> build/lib/object_detection/utils\n","copying object_detection/utils/config_util.py -> build/lib/object_detection/utils\n","copying object_detection/utils/vrd_evaluation_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/metrics.py -> build/lib/object_detection/utils\n","copying object_detection/utils/category_util_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/variables_helper_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/variables_helper.py -> build/lib/object_detection/utils\n","copying object_detection/utils/ops.py -> build/lib/object_detection/utils\n","copying object_detection/utils/np_box_mask_list_ops_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/per_image_evaluation.py -> build/lib/object_detection/utils\n","copying object_detection/utils/np_box_list_ops.py -> build/lib/object_detection/utils\n","copying object_detection/utils/__init__.py -> build/lib/object_detection/utils\n","copying object_detection/utils/metrics_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/shape_utils_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/learning_schedules.py -> build/lib/object_detection/utils\n","copying object_detection/utils/np_box_list_ops_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/category_util.py -> build/lib/object_detection/utils\n","copying object_detection/utils/per_image_vrd_evaluation.py -> build/lib/object_detection/utils\n","copying object_detection/utils/np_mask_ops_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/np_box_mask_list_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/visualization_utils_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/np_box_ops_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/label_map_util.py -> build/lib/object_detection/utils\n","copying object_detection/utils/test_case.py -> build/lib/object_detection/utils\n","copying object_detection/utils/json_utils.py -> build/lib/object_detection/utils\n","copying object_detection/utils/label_map_util_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/vrd_evaluation.py -> build/lib/object_detection/utils\n","copying object_detection/utils/object_detection_evaluation_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/np_box_ops.py -> build/lib/object_detection/utils\n","copying object_detection/utils/shape_utils.py -> build/lib/object_detection/utils\n","copying object_detection/utils/model_util.py -> build/lib/object_detection/utils\n","copying object_detection/utils/static_shape_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/json_utils_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/autoaugment_utils.py -> build/lib/object_detection/utils\n","copying object_detection/utils/np_box_list_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/context_manager_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/spatial_transform_ops.py -> build/lib/object_detection/utils\n","copying object_detection/utils/per_image_evaluation_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/learning_schedules_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/config_util_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/np_mask_ops.py -> build/lib/object_detection/utils\n","copying object_detection/utils/static_shape.py -> build/lib/object_detection/utils\n","copying object_detection/utils/per_image_vrd_evaluation_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/dataset_util.py -> build/lib/object_detection/utils\n","copying object_detection/utils/np_box_mask_list_ops.py -> build/lib/object_detection/utils\n","copying object_detection/utils/object_detection_evaluation.py -> build/lib/object_detection/utils\n","copying object_detection/utils/np_box_mask_list.py -> build/lib/object_detection/utils\n","copying object_detection/utils/model_util_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/test_utils.py -> build/lib/object_detection/utils\n","copying object_detection/utils/spatial_transform_ops_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/context_manager.py -> build/lib/object_detection/utils\n","copying object_detection/utils/test_utils_test.py -> build/lib/object_detection/utils\n","copying object_detection/utils/np_box_list.py -> build/lib/object_detection/utils\n","copying object_detection/utils/dataset_util_test.py -> build/lib/object_detection/utils\n","creating build/lib/object_detection/protos\n","copying object_detection/protos/optimizer_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/string_int_label_map_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/ssd_anchor_generator_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/flexible_grid_anchor_generator_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/preprocessor_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/square_box_coder_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/image_resizer_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/anchor_generator_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/box_coder_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/grid_anchor_generator_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/mean_stddev_box_coder_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/__init__.py -> build/lib/object_detection/protos\n","copying object_detection/protos/model_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/hyperparams_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/region_similarity_calculator_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/eval_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/calibration_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/pipeline_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/faster_rcnn_box_coder_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/train_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/losses_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/graph_rewriter_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/input_reader_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/matcher_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/faster_rcnn_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/argmax_matcher_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/box_predictor_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/keypoint_box_coder_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/post_processing_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/ssd_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/bipartite_matcher_pb2.py -> build/lib/object_detection/protos\n","copying object_detection/protos/multiscale_anchor_generator_pb2.py -> build/lib/object_detection/protos\n","creating build/lib/object_detection/builders\n","copying object_detection/builders/optimizer_builder.py -> build/lib/object_detection/builders\n","copying object_detection/builders/input_reader_builder.py -> build/lib/object_detection/builders\n","copying object_detection/builders/hyperparams_builder_test.py -> build/lib/object_detection/builders\n","copying object_detection/builders/box_coder_builder.py -> build/lib/object_detection/builders\n","copying object_detection/builders/calibration_builder_test.py -> build/lib/object_detection/builders\n","copying object_detection/builders/dataset_builder_test.py -> build/lib/object_detection/builders\n","copying object_detection/builders/__init__.py -> build/lib/object_detection/builders\n","copying object_detection/builders/image_resizer_builder_test.py -> build/lib/object_detection/builders\n","copying object_detection/builders/calibration_builder.py -> build/lib/object_detection/builders\n","copying object_detection/builders/post_processing_builder_test.py -> build/lib/object_detection/builders\n","copying object_detection/builders/hyperparams_builder.py -> build/lib/object_detection/builders\n","copying object_detection/builders/region_similarity_calculator_builder_test.py -> build/lib/object_detection/builders\n","copying object_detection/builders/model_builder_test.py -> build/lib/object_detection/builders\n","copying object_detection/builders/preprocessor_builder_test.py -> build/lib/object_detection/builders\n","copying object_detection/builders/box_coder_builder_test.py -> build/lib/object_detection/builders\n","copying object_detection/builders/box_predictor_builder.py -> build/lib/object_detection/builders\n","copying object_detection/builders/post_processing_builder.py -> build/lib/object_detection/builders\n","copying object_detection/builders/matcher_builder.py -> build/lib/object_detection/builders\n","copying object_detection/builders/anchor_generator_builder_test.py -> build/lib/object_detection/builders\n","copying object_detection/builders/region_similarity_calculator_builder.py -> build/lib/object_detection/builders\n","copying object_detection/builders/input_reader_builder_test.py -> build/lib/object_detection/builders\n","copying object_detection/builders/anchor_generator_builder.py -> build/lib/object_detection/builders\n","copying object_detection/builders/model_builder.py -> build/lib/object_detection/builders\n","copying object_detection/builders/preprocessor_builder.py -> build/lib/object_detection/builders\n","copying object_detection/builders/matcher_builder_test.py -> build/lib/object_detection/builders\n","copying object_detection/builders/dataset_builder.py -> build/lib/object_detection/builders\n","copying object_detection/builders/image_resizer_builder.py -> build/lib/object_detection/builders\n","copying object_detection/builders/losses_builder.py -> build/lib/object_detection/builders\n","copying object_detection/builders/graph_rewriter_builder_test.py -> build/lib/object_detection/builders\n","copying object_detection/builders/optimizer_builder_test.py -> build/lib/object_detection/builders\n","copying object_detection/builders/box_predictor_builder_test.py -> build/lib/object_detection/builders\n","copying object_detection/builders/losses_builder_test.py -> build/lib/object_detection/builders\n","copying object_detection/builders/graph_rewriter_builder.py -> build/lib/object_detection/builders\n","creating build/lib/object_detection/inference\n","copying object_detection/inference/__init__.py -> build/lib/object_detection/inference\n","copying object_detection/inference/detection_inference_test.py -> build/lib/object_detection/inference\n","copying object_detection/inference/infer_detections.py -> build/lib/object_detection/inference\n","copying object_detection/inference/detection_inference.py -> build/lib/object_detection/inference\n","creating build/lib/object_detection/metrics\n","copying object_detection/metrics/offline_eval_map_corloc.py -> build/lib/object_detection/metrics\n","copying object_detection/metrics/oid_challenge_evaluation_utils_test.py -> build/lib/object_detection/metrics\n","copying object_detection/metrics/tf_example_parser_test.py -> build/lib/object_detection/metrics\n","copying object_detection/metrics/coco_tools_test.py -> build/lib/object_detection/metrics\n","copying object_detection/metrics/__init__.py -> build/lib/object_detection/metrics\n","copying object_detection/metrics/calibration_metrics.py -> build/lib/object_detection/metrics\n","copying object_detection/metrics/coco_evaluation_test.py -> build/lib/object_detection/metrics\n","copying object_detection/metrics/tf_example_parser.py -> build/lib/object_detection/metrics\n","copying object_detection/metrics/oid_challenge_evaluation.py -> build/lib/object_detection/metrics\n","copying object_detection/metrics/coco_tools.py -> build/lib/object_detection/metrics\n","copying object_detection/metrics/oid_vrd_challenge_evaluation_utils_test.py -> build/lib/object_detection/metrics\n","copying object_detection/metrics/oid_vrd_challenge_evaluation.py -> build/lib/object_detection/metrics\n","copying object_detection/metrics/calibration_evaluation.py -> build/lib/object_detection/metrics\n","copying object_detection/metrics/calibration_metrics_test.py -> build/lib/object_detection/metrics\n","copying object_detection/metrics/io_utils.py -> build/lib/object_detection/metrics\n","copying object_detection/metrics/oid_challenge_evaluation_utils.py -> build/lib/object_detection/metrics\n","copying object_detection/metrics/offline_eval_map_corloc_test.py -> build/lib/object_detection/metrics\n","copying object_detection/metrics/coco_evaluation.py -> build/lib/object_detection/metrics\n","copying object_detection/metrics/oid_vrd_challenge_evaluation_utils.py -> build/lib/object_detection/metrics\n","copying object_detection/metrics/calibration_evaluation_test.py -> build/lib/object_detection/metrics\n","creating build/lib/object_detection/anchor_generators\n","copying object_detection/anchor_generators/multiple_grid_anchor_generator_test.py -> build/lib/object_detection/anchor_generators\n","copying object_detection/anchor_generators/multiscale_grid_anchor_generator.py -> build/lib/object_detection/anchor_generators\n","copying object_detection/anchor_generators/__init__.py -> build/lib/object_detection/anchor_generators\n","copying object_detection/anchor_generators/grid_anchor_generator_test.py -> build/lib/object_detection/anchor_generators\n","copying object_detection/anchor_generators/flexible_grid_anchor_generator.py -> build/lib/object_detection/anchor_generators\n","copying object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py -> build/lib/object_detection/anchor_generators\n","copying object_detection/anchor_generators/multiple_grid_anchor_generator.py -> build/lib/object_detection/anchor_generators\n","copying object_detection/anchor_generators/flexible_grid_anchor_generator_test.py -> build/lib/object_detection/anchor_generators\n","copying object_detection/anchor_generators/grid_anchor_generator.py -> build/lib/object_detection/anchor_generators\n","creating build/lib/object_detection/predictors\n","copying object_detection/predictors/rfcn_box_predictor_test.py -> build/lib/object_detection/predictors\n","copying object_detection/predictors/convolutional_keras_box_predictor.py -> build/lib/object_detection/predictors\n","copying object_detection/predictors/__init__.py -> build/lib/object_detection/predictors\n","copying object_detection/predictors/rfcn_box_predictor.py -> build/lib/object_detection/predictors\n","copying object_detection/predictors/rfcn_keras_box_predictor_test.py -> build/lib/object_detection/predictors\n","copying object_detection/predictors/convolutional_box_predictor.py -> build/lib/object_detection/predictors\n","copying object_detection/predictors/mask_rcnn_keras_box_predictor.py -> build/lib/object_detection/predictors\n","copying object_detection/predictors/mask_rcnn_box_predictor_test.py -> build/lib/object_detection/predictors\n","copying object_detection/predictors/convolutional_box_predictor_test.py -> build/lib/object_detection/predictors\n","copying object_detection/predictors/convolutional_keras_box_predictor_test.py -> build/lib/object_detection/predictors\n","copying object_detection/predictors/mask_rcnn_box_predictor.py -> build/lib/object_detection/predictors\n","copying object_detection/predictors/mask_rcnn_keras_box_predictor_test.py -> build/lib/object_detection/predictors\n","copying object_detection/predictors/rfcn_keras_box_predictor.py -> build/lib/object_detection/predictors\n","creating build/lib/object_detection/core\n","copying object_detection/core/keypoint_ops_test.py -> build/lib/object_detection/core\n","copying object_detection/core/target_assigner_test.py -> build/lib/object_detection/core\n","copying object_detection/core/box_predictor.py -> build/lib/object_detection/core\n","copying object_detection/core/anchor_generator.py -> build/lib/object_detection/core\n","copying object_detection/core/region_similarity_calculator_test.py -> build/lib/object_detection/core\n","copying object_detection/core/matcher_test.py -> build/lib/object_detection/core\n","copying object_detection/core/region_similarity_calculator.py -> build/lib/object_detection/core\n","copying object_detection/core/__init__.py -> build/lib/object_detection/core\n","copying object_detection/core/model.py -> build/lib/object_detection/core\n","copying object_detection/core/losses.py -> build/lib/object_detection/core\n","copying object_detection/core/batcher.py -> build/lib/object_detection/core\n","copying object_detection/core/minibatch_sampler.py -> build/lib/object_detection/core\n","copying object_detection/core/minibatch_sampler_test.py -> build/lib/object_detection/core\n","copying object_detection/core/balanced_positive_negative_sampler_test.py -> build/lib/object_detection/core\n","copying object_detection/core/batch_multiclass_nms_test.py -> build/lib/object_detection/core\n","copying object_detection/core/standard_fields.py -> build/lib/object_detection/core\n","copying object_detection/core/matcher.py -> build/lib/object_detection/core\n","copying object_detection/core/box_coder.py -> build/lib/object_detection/core\n","copying object_detection/core/class_agnostic_nms_test.py -> build/lib/object_detection/core\n","copying object_detection/core/freezable_batch_norm.py -> build/lib/object_detection/core\n","copying object_detection/core/box_coder_test.py -> build/lib/object_detection/core\n","copying object_detection/core/target_assigner.py -> build/lib/object_detection/core\n","copying object_detection/core/box_list_ops_test.py -> build/lib/object_detection/core\n","copying object_detection/core/keypoint_ops.py -> build/lib/object_detection/core\n","copying object_detection/core/data_decoder.py -> build/lib/object_detection/core\n","copying object_detection/core/data_parser.py -> build/lib/object_detection/core\n","copying object_detection/core/batcher_test.py -> build/lib/object_detection/core\n","copying object_detection/core/losses_test.py -> build/lib/object_detection/core\n","copying object_detection/core/prefetcher_test.py -> build/lib/object_detection/core\n","copying object_detection/core/box_list_test.py -> build/lib/object_detection/core\n","copying object_detection/core/preprocessor_cache.py -> build/lib/object_detection/core\n","copying object_detection/core/post_processing.py -> build/lib/object_detection/core\n","copying object_detection/core/prefetcher.py -> build/lib/object_detection/core\n","copying object_detection/core/multiclass_nms_test.py -> build/lib/object_detection/core\n","copying object_detection/core/preprocessor_test.py -> build/lib/object_detection/core\n","copying object_detection/core/freezable_batch_norm_test.py -> build/lib/object_detection/core\n","copying object_detection/core/box_list.py -> build/lib/object_detection/core\n","copying object_detection/core/balanced_positive_negative_sampler.py -> build/lib/object_detection/core\n","copying object_detection/core/preprocessor.py -> build/lib/object_detection/core\n","copying object_detection/core/box_list_ops.py -> build/lib/object_detection/core\n","creating build/lib/object_detection/matchers\n","copying object_detection/matchers/bipartite_matcher.py -> build/lib/object_detection/matchers\n","copying object_detection/matchers/__init__.py -> build/lib/object_detection/matchers\n","copying object_detection/matchers/bipartite_matcher_test.py -> build/lib/object_detection/matchers\n","copying object_detection/matchers/argmax_matcher_test.py -> build/lib/object_detection/matchers\n","copying object_detection/matchers/argmax_matcher.py -> build/lib/object_detection/matchers\n","creating build/lib/object_detection/dataset_tools\n","copying object_detection/dataset_tools/create_pascal_tf_record_test.py -> build/lib/object_detection/dataset_tools\n","copying object_detection/dataset_tools/oid_tfrecord_creation_test.py -> build/lib/object_detection/dataset_tools\n","copying object_detection/dataset_tools/create_pascal_tf_record.py -> build/lib/object_detection/dataset_tools\n","copying object_detection/dataset_tools/create_kitti_tf_record.py -> build/lib/object_detection/dataset_tools\n","copying object_detection/dataset_tools/create_oid_tf_record.py -> build/lib/object_detection/dataset_tools\n","copying object_detection/dataset_tools/create_coco_tf_record_test.py -> build/lib/object_detection/dataset_tools\n","copying object_detection/dataset_tools/create_coco_tf_record.py -> build/lib/object_detection/dataset_tools\n","copying object_detection/dataset_tools/__init__.py -> build/lib/object_detection/dataset_tools\n","copying object_detection/dataset_tools/oid_tfrecord_creation.py -> build/lib/object_detection/dataset_tools\n","copying object_detection/dataset_tools/tf_record_creation_util_test.py -> build/lib/object_detection/dataset_tools\n","copying object_detection/dataset_tools/oid_hierarchical_labels_expansion_test.py -> build/lib/object_detection/dataset_tools\n","copying object_detection/dataset_tools/create_kitti_tf_record_test.py -> build/lib/object_detection/dataset_tools\n","copying object_detection/dataset_tools/tf_record_creation_util.py -> build/lib/object_detection/dataset_tools\n","copying object_detection/dataset_tools/oid_hierarchical_labels_expansion.py -> build/lib/object_detection/dataset_tools\n","copying object_detection/dataset_tools/create_pet_tf_record.py -> build/lib/object_detection/dataset_tools\n","creating build/lib/object_detection/tpu_exporters\n","copying object_detection/tpu_exporters/utils_test.py -> build/lib/object_detection/tpu_exporters\n","copying object_detection/tpu_exporters/export_saved_model_tpu_lib_test.py -> build/lib/object_detection/tpu_exporters\n","copying object_detection/tpu_exporters/__init__.py -> build/lib/object_detection/tpu_exporters\n","copying object_detection/tpu_exporters/export_saved_model_tpu.py -> build/lib/object_detection/tpu_exporters\n","copying object_detection/tpu_exporters/utils.py -> build/lib/object_detection/tpu_exporters\n","copying object_detection/tpu_exporters/faster_rcnn.py -> build/lib/object_detection/tpu_exporters\n","copying object_detection/tpu_exporters/ssd.py -> build/lib/object_detection/tpu_exporters\n","copying object_detection/tpu_exporters/export_saved_model_tpu_lib.py -> build/lib/object_detection/tpu_exporters\n","creating build/lib/object_detection/legacy\n","copying object_detection/legacy/__init__.py -> build/lib/object_detection/legacy\n","copying object_detection/legacy/trainer.py -> build/lib/object_detection/legacy\n","copying object_detection/legacy/evaluator.py -> build/lib/object_detection/legacy\n","copying object_detection/legacy/train.py -> build/lib/object_detection/legacy\n","copying object_detection/legacy/trainer_test.py -> build/lib/object_detection/legacy\n","copying object_detection/legacy/eval.py -> build/lib/object_detection/legacy\n","creating build/lib/object_detection/meta_architectures\n","copying object_detection/meta_architectures/ssd_meta_arch.py -> build/lib/object_detection/meta_architectures\n","copying object_detection/meta_architectures/__init__.py -> build/lib/object_detection/meta_architectures\n","copying object_detection/meta_architectures/rfcn_meta_arch_test.py -> build/lib/object_detection/meta_architectures\n","copying object_detection/meta_architectures/rfcn_meta_arch.py -> build/lib/object_detection/meta_architectures\n","copying object_detection/meta_architectures/ssd_meta_arch_test.py -> build/lib/object_detection/meta_architectures\n","copying object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py -> build/lib/object_detection/meta_architectures\n","copying object_detection/meta_architectures/faster_rcnn_meta_arch_test.py -> build/lib/object_detection/meta_architectures\n","copying object_detection/meta_architectures/faster_rcnn_meta_arch.py -> build/lib/object_detection/meta_architectures\n","copying object_detection/meta_architectures/ssd_meta_arch_test_lib.py -> build/lib/object_detection/meta_architectures\n","creating build/lib/object_detection/models\n","copying object_detection/models/ssd_inception_v2_feature_extractor_test.py -> build/lib/object_detection/models\n","copying object_detection/models/faster_rcnn_pnas_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_mobilenet_v2_feature_extractor_test.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_inception_v3_feature_extractor_test.py -> build/lib/object_detection/models\n","copying object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py -> build/lib/object_detection/models\n","copying object_detection/models/faster_rcnn_inception_v2_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_pnasnet_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/faster_rcnn_nas_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/feature_map_generators.py -> build/lib/object_detection/models\n","copying object_detection/models/faster_rcnn_nas_feature_extractor_test.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_inception_v3_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/__init__.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py -> build/lib/object_detection/models\n","copying object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_feature_extractor_test.py -> build/lib/object_detection/models\n","copying object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_inception_v2_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_pnasnet_feature_extractor_test.py -> build/lib/object_detection/models\n","copying object_detection/models/feature_map_generators_test.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_mobilenet_v1_keras_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/faster_rcnn_resnet_v1_feature_extractor_test.py -> build/lib/object_detection/models\n","copying object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor_test.py -> build/lib/object_detection/models\n","copying object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_resnet_v1_ppn_feature_extractor_test.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_resnet_v1_ppn_feature_extractor_testbase.py -> build/lib/object_detection/models\n","copying object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor_test.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor_test.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor_test.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor_test.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/faster_rcnn_inception_v2_feature_extractor_test.py -> build/lib/object_detection/models\n","copying object_detection/models/faster_rcnn_pnas_feature_extractor_test.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_mobilenet_v1_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_mobilenet_v2_keras_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_mobilenet_v2_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor.py -> build/lib/object_detection/models\n","copying object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor_test.py -> build/lib/object_detection/models\n","creating build/lib/object_detection/predictors/heads\n","copying object_detection/predictors/heads/mask_head.py -> build/lib/object_detection/predictors/heads\n","copying object_detection/predictors/heads/keras_box_head_test.py -> build/lib/object_detection/predictors/heads\n","copying object_detection/predictors/heads/__init__.py -> build/lib/object_detection/predictors/heads\n","copying object_detection/predictors/heads/keras_class_head.py -> build/lib/object_detection/predictors/heads\n","copying object_detection/predictors/heads/mask_head_test.py -> build/lib/object_detection/predictors/heads\n","copying object_detection/predictors/heads/keras_mask_head_test.py -> build/lib/object_detection/predictors/heads\n","copying object_detection/predictors/heads/head.py -> build/lib/object_detection/predictors/heads\n","copying object_detection/predictors/heads/class_head_test.py -> build/lib/object_detection/predictors/heads\n","copying object_detection/predictors/heads/keypoint_head.py -> build/lib/object_detection/predictors/heads\n","copying object_detection/predictors/heads/keras_class_head_test.py -> build/lib/object_detection/predictors/heads\n","copying object_detection/predictors/heads/keras_box_head.py -> build/lib/object_detection/predictors/heads\n","copying object_detection/predictors/heads/class_head.py -> build/lib/object_detection/predictors/heads\n","copying object_detection/predictors/heads/box_head.py -> build/lib/object_detection/predictors/heads\n","copying object_detection/predictors/heads/keras_mask_head.py -> build/lib/object_detection/predictors/heads\n","copying object_detection/predictors/heads/box_head_test.py -> build/lib/object_detection/predictors/heads\n","copying object_detection/predictors/heads/keypoint_head_test.py -> build/lib/object_detection/predictors/heads\n","creating build/lib/object_detection/tpu_exporters/testdata\n","copying object_detection/tpu_exporters/testdata/__init__.py -> build/lib/object_detection/tpu_exporters/testdata\n","creating build/lib/object_detection/models/keras_models\n","copying object_detection/models/keras_models/mobilenet_v1.py -> build/lib/object_detection/models/keras_models\n","copying object_detection/models/keras_models/__init__.py -> build/lib/object_detection/models/keras_models\n","copying object_detection/models/keras_models/model_utils.py -> build/lib/object_detection/models/keras_models\n","copying object_detection/models/keras_models/inception_resnet_v2.py -> build/lib/object_detection/models/keras_models\n","copying object_detection/models/keras_models/resnet_v1.py -> build/lib/object_detection/models/keras_models\n","copying object_detection/models/keras_models/mobilenet_v2_test.py -> build/lib/object_detection/models/keras_models\n","copying object_detection/models/keras_models/resnet_v1_test.py -> build/lib/object_detection/models/keras_models\n","copying object_detection/models/keras_models/mobilenet_v2.py -> build/lib/object_detection/models/keras_models\n","copying object_detection/models/keras_models/inception_resnet_v2_test.py -> build/lib/object_detection/models/keras_models\n","copying object_detection/models/keras_models/test_utils.py -> build/lib/object_detection/models/keras_models\n","copying object_detection/models/keras_models/mobilenet_v1_test.py -> build/lib/object_detection/models/keras_models\n","running egg_info\n","creating object_detection.egg-info\n","writing object_detection.egg-info/PKG-INFO\n","writing dependency_links to object_detection.egg-info/dependency_links.txt\n","writing requirements to object_detection.egg-info/requires.txt\n","writing top-level names to object_detection.egg-info/top_level.txt\n","writing manifest file 'object_detection.egg-info/SOURCES.txt'\n","writing manifest file 'object_detection.egg-info/SOURCES.txt'\n","copying object_detection/CONTRIBUTING.md -> build/lib/object_detection\n","copying object_detection/README.md -> build/lib/object_detection\n","copying object_detection/object_detection_tutorial.ipynb -> build/lib/object_detection\n","creating build/lib/object_detection/data\n","copying object_detection/data/ava_label_map_v2.1.pbtxt -> build/lib/object_detection/data\n","copying object_detection/data/face_label_map.pbtxt -> build/lib/object_detection/data\n","copying object_detection/data/fgvc_2854_classes_label_map.pbtxt -> build/lib/object_detection/data\n","copying object_detection/data/kitti_label_map.pbtxt -> build/lib/object_detection/data\n","copying object_detection/data/mscoco_complete_label_map.pbtxt -> build/lib/object_detection/data\n","copying object_detection/data/mscoco_label_map.pbtxt -> build/lib/object_detection/data\n","copying object_detection/data/mscoco_minival_ids.txt -> build/lib/object_detection/data\n","copying object_detection/data/oid_bbox_trainable_label_map.pbtxt -> build/lib/object_detection/data\n","copying object_detection/data/oid_object_detection_challenge_500_label_map.pbtxt -> build/lib/object_detection/data\n","copying object_detection/data/oid_v4_label_map.pbtxt -> build/lib/object_detection/data\n","copying object_detection/data/pascal_label_map.pbtxt -> build/lib/object_detection/data\n","copying object_detection/data/pet_label_map.pbtxt -> build/lib/object_detection/data\n","creating build/lib/object_detection/dockerfiles\n","creating build/lib/object_detection/dockerfiles/android\n","copying object_detection/dockerfiles/android/Dockerfile -> build/lib/object_detection/dockerfiles/android\n","copying object_detection/dockerfiles/android/README.md -> build/lib/object_detection/dockerfiles/android\n","creating build/lib/object_detection/g3doc\n","copying object_detection/g3doc/challenge_evaluation.md -> build/lib/object_detection/g3doc\n","copying object_detection/g3doc/configuring_jobs.md -> build/lib/object_detection/g3doc\n","copying object_detection/g3doc/defining_your_own_model.md -> build/lib/object_detection/g3doc\n","copying object_detection/g3doc/detection_model_zoo.md -> build/lib/object_detection/g3doc\n","copying object_detection/g3doc/evaluation_protocols.md -> build/lib/object_detection/g3doc\n","copying object_detection/g3doc/exporting_models.md -> build/lib/object_detection/g3doc\n","copying object_detection/g3doc/faq.md -> build/lib/object_detection/g3doc\n","copying object_detection/g3doc/installation.md -> build/lib/object_detection/g3doc\n","copying object_detection/g3doc/instance_segmentation.md -> build/lib/object_detection/g3doc\n","copying object_detection/g3doc/oid_inference_and_evaluation.md -> build/lib/object_detection/g3doc\n","copying object_detection/g3doc/preparing_inputs.md -> build/lib/object_detection/g3doc\n","copying object_detection/g3doc/running_locally.md -> build/lib/object_detection/g3doc\n","copying object_detection/g3doc/running_notebook.md -> build/lib/object_detection/g3doc\n","copying object_detection/g3doc/running_on_cloud.md -> build/lib/object_detection/g3doc\n","copying object_detection/g3doc/running_on_mobile_tensorflowlite.md -> build/lib/object_detection/g3doc\n","copying object_detection/g3doc/running_pets.md -> build/lib/object_detection/g3doc\n","copying object_detection/g3doc/tpu_compatibility.md -> build/lib/object_detection/g3doc\n","copying object_detection/g3doc/tpu_exporters.md -> build/lib/object_detection/g3doc\n","copying object_detection/g3doc/using_your_own_dataset.md -> build/lib/object_detection/g3doc\n","creating build/lib/object_detection/g3doc/img\n","copying object_detection/g3doc/img/dataset_explorer.png -> build/lib/object_detection/g3doc/img\n","copying object_detection/g3doc/img/dogs_detections_output.jpg -> build/lib/object_detection/g3doc/img\n","copying object_detection/g3doc/img/example_cat.jpg -> build/lib/object_detection/g3doc/img\n","copying object_detection/g3doc/img/groupof_case_eval.png -> build/lib/object_detection/g3doc/img\n","copying object_detection/g3doc/img/kites_detections_output.jpg -> build/lib/object_detection/g3doc/img\n","copying object_detection/g3doc/img/kites_with_segment_overlay.png -> build/lib/object_detection/g3doc/img\n","copying object_detection/g3doc/img/nongroupof_case_eval.png -> build/lib/object_detection/g3doc/img\n","copying object_detection/g3doc/img/oid_bus_72e19c28aac34ed8.jpg -> build/lib/object_detection/g3doc/img\n","copying object_detection/g3doc/img/oid_monkey_3b4168c89cecbc5b.jpg -> build/lib/object_detection/g3doc/img\n","copying object_detection/g3doc/img/oxford_pet.png -> build/lib/object_detection/g3doc/img\n","copying object_detection/g3doc/img/tensorboard.png -> build/lib/object_detection/g3doc/img\n","copying object_detection/g3doc/img/tensorboard2.png -> build/lib/object_detection/g3doc/img\n","copying object_detection/g3doc/img/tf-od-api-logo.png -> build/lib/object_detection/g3doc/img\n","creating build/lib/object_detection/samples\n","creating build/lib/object_detection/samples/cloud\n","copying object_detection/samples/cloud/cloud.yml -> build/lib/object_detection/samples/cloud\n","creating build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/embedded_ssd_mobilenet_v1_coco.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/facessd_mobilenet_v2_quantized_320x320_open_image_v4.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_coco.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_cosine_lr_coco.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_oid.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_oid_v4.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_pets.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/faster_rcnn_inception_v2_coco.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/faster_rcnn_inception_v2_pets.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/faster_rcnn_nas_coco.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/faster_rcnn_resnet101_atrous_coco.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/faster_rcnn_resnet101_ava_v2.1.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/faster_rcnn_resnet101_coco.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/faster_rcnn_resnet101_fgvc.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/faster_rcnn_resnet101_kitti.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/faster_rcnn_resnet101_pets.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/faster_rcnn_resnet101_voc07.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/faster_rcnn_resnet152_coco.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/faster_rcnn_resnet152_pets.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/faster_rcnn_resnet50_coco.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/faster_rcnn_resnet50_fgvc.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/faster_rcnn_resnet50_pets.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/mask_rcnn_inception_resnet_v2_atrous_coco.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/mask_rcnn_inception_v2_coco.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/mask_rcnn_resnet101_atrous_coco.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/mask_rcnn_resnet101_pets.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/mask_rcnn_resnet50_atrous_coco.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/rfcn_resnet101_coco.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/rfcn_resnet101_pets.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssd_inception_v2_coco.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssd_inception_v2_pets.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssd_inception_v3_pets.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_coco14_sync.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssd_mobilenet_v1_300x300_coco14_sync.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssd_mobilenet_v1_coco.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets_inference.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssd_mobilenet_v1_pets.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssd_mobilenet_v2_coco.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssd_mobilenet_v2_fpnlite_quantized_shared_box_predictor_256x256_depthmultiplier_75_coco14_sync.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssd_mobilenet_v2_fullyconv_coco.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssd_mobilenet_v2_oid_v4.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssd_mobilenet_v2_pets_keras.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssd_mobilenet_v2_quantized_300x300_coco.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssd_resnet101_v1_fpn_shared_box_predictor_oid_512x512_sync.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssdlite_mobilenet_v1_coco.config -> build/lib/object_detection/samples/configs\n","copying object_detection/samples/configs/ssdlite_mobilenet_v2_coco.config -> build/lib/object_detection/samples/configs\n","creating build/lib/object_detection/test_ckpt\n","copying object_detection/test_ckpt/ssd_inception_v2.pb -> build/lib/object_detection/test_ckpt\n","creating build/lib/object_detection/test_data\n","copying object_detection/test_data/pets_examples.record -> build/lib/object_detection/test_data\n","creating build/lib/object_detection/test_images\n","copying object_detection/test_images/image1.jpg -> build/lib/object_detection/test_images\n","copying object_detection/test_images/image2.jpg -> build/lib/object_detection/test_images\n","copying object_detection/test_images/image_info.txt -> build/lib/object_detection/test_images\n","copying object_detection/protos/anchor_generator.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/argmax_matcher.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/bipartite_matcher.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/box_coder.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/box_predictor.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/calibration.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/eval.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/faster_rcnn.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/faster_rcnn_box_coder.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/flexible_grid_anchor_generator.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/graph_rewriter.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/grid_anchor_generator.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/hyperparams.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/image_resizer.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/input_reader.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/keypoint_box_coder.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/losses.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/matcher.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/mean_stddev_box_coder.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/model.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/multiscale_anchor_generator.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/optimizer.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/pipeline.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/post_processing.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/preprocessor.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/region_similarity_calculator.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/square_box_coder.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/ssd.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/ssd_anchor_generator.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/string_int_label_map.proto -> build/lib/object_detection/protos\n","copying object_detection/protos/train.proto -> build/lib/object_detection/protos\n","copying object_detection/dataset_tools/create_pycocotools_package.sh -> build/lib/object_detection/dataset_tools\n","copying object_detection/dataset_tools/download_and_preprocess_mscoco.sh -> build/lib/object_detection/dataset_tools\n","creating build/lib/object_detection/tpu_exporters/testdata/faster_rcnn\n","copying object_detection/tpu_exporters/testdata/faster_rcnn/faster_rcnn_resnet101_atrous_coco.config -> build/lib/object_detection/tpu_exporters/testdata/faster_rcnn\n","creating build/lib/object_detection/tpu_exporters/testdata/ssd\n","copying object_detection/tpu_exporters/testdata/ssd/ssd_pipeline.config -> build/lib/object_detection/tpu_exporters/testdata/ssd\n","creating build/lib/object_detection/models/keras_models/base_models\n","copying object_detection/models/keras_models/base_models/original_mobilenet_v2.py -> build/lib/object_detection/models/keras_models/base_models\n","running install\n","running bdist_egg\n","running egg_info\n","writing object_detection.egg-info/PKG-INFO\n","writing dependency_links to object_detection.egg-info/dependency_links.txt\n","writing requirements to object_detection.egg-info/requires.txt\n","writing top-level names to object_detection.egg-info/top_level.txt\n","writing manifest file 'object_detection.egg-info/SOURCES.txt'\n","installing library code to build/bdist.linux-x86_64/egg\n","running install_lib\n","running build_py\n","creating build/bdist.linux-x86_64\n","creating build/bdist.linux-x86_64/egg\n","creating build/bdist.linux-x86_64/egg/object_detection\n","creating build/bdist.linux-x86_64/egg/object_detection/test_ckpt\n","copying build/lib/object_detection/test_ckpt/ssd_inception_v2.pb -> build/bdist.linux-x86_64/egg/object_detection/test_ckpt\n","creating build/bdist.linux-x86_64/egg/object_detection/data_decoders\n","copying build/lib/object_detection/data_decoders/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/data_decoders\n","copying build/lib/object_detection/data_decoders/tf_example_decoder_test.py -> build/bdist.linux-x86_64/egg/object_detection/data_decoders\n","copying build/lib/object_detection/data_decoders/tf_example_decoder.py -> build/bdist.linux-x86_64/egg/object_detection/data_decoders\n","copying build/lib/object_detection/model_lib_v2.py -> build/bdist.linux-x86_64/egg/object_detection\n","creating build/bdist.linux-x86_64/egg/object_detection/box_coders\n","copying build/lib/object_detection/box_coders/mean_stddev_box_coder_test.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n","copying build/lib/object_detection/box_coders/faster_rcnn_box_coder.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n","copying build/lib/object_detection/box_coders/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n","copying build/lib/object_detection/box_coders/mean_stddev_box_coder.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n","copying build/lib/object_detection/box_coders/square_box_coder.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n","copying build/lib/object_detection/box_coders/square_box_coder_test.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n","copying build/lib/object_detection/box_coders/faster_rcnn_box_coder_test.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n","copying build/lib/object_detection/box_coders/keypoint_box_coder.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n","copying build/lib/object_detection/box_coders/keypoint_box_coder_test.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n","copying build/lib/object_detection/model_main.py -> build/bdist.linux-x86_64/egg/object_detection\n","creating build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/visualization_utils.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/config_util.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/vrd_evaluation_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/metrics.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/category_util_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/variables_helper_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/variables_helper.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/ops.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/np_box_mask_list_ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/per_image_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/np_box_list_ops.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/metrics_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/shape_utils_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/learning_schedules.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/np_box_list_ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/category_util.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/per_image_vrd_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/np_mask_ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/np_box_mask_list_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/visualization_utils_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/np_box_ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/label_map_util.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/test_case.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/json_utils.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/label_map_util_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/vrd_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/object_detection_evaluation_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/np_box_ops.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/shape_utils.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/model_util.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/static_shape_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/json_utils_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/autoaugment_utils.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/np_box_list_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/context_manager_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/spatial_transform_ops.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/per_image_evaluation_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/learning_schedules_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/config_util_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/np_mask_ops.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/static_shape.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/per_image_vrd_evaluation_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/dataset_util.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/np_box_mask_list_ops.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/object_detection_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/np_box_mask_list.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/model_util_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/test_utils.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/spatial_transform_ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/context_manager.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/test_utils_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/np_box_list.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","copying build/lib/object_detection/utils/dataset_util_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n","creating build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/multiscale_anchor_generator.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/graph_rewriter.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/optimizer_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/string_int_label_map_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/losses.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/matcher.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/input_reader.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/ssd_anchor_generator_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/flexible_grid_anchor_generator_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/preprocessor_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/string_int_label_map.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/square_box_coder_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/image_resizer_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/hyperparams.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/anchor_generator_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/box_coder_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/grid_anchor_generator_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/square_box_coder.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/preprocessor.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/mean_stddev_box_coder_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/model_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/box_coder.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/box_predictor.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/hyperparams_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/region_similarity_calculator_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/grid_anchor_generator.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/eval_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/calibration_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/pipeline_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/faster_rcnn_box_coder_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/calibration.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/bipartite_matcher.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/argmax_matcher.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/train_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/model.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/losses_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/post_processing.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/mean_stddev_box_coder.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/keypoint_box_coder.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/graph_rewriter_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/train.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/region_similarity_calculator.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/input_reader_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/matcher_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/faster_rcnn_box_coder.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/faster_rcnn_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/anchor_generator.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/argmax_matcher_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/box_predictor_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/optimizer.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/ssd_anchor_generator.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/keypoint_box_coder_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/post_processing_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/ssd_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/bipartite_matcher_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/eval.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/pipeline.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/faster_rcnn.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/flexible_grid_anchor_generator.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/image_resizer.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/ssd.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n","copying build/lib/object_detection/protos/multiscale_anchor_generator_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n","creating build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/optimizer_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/input_reader_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/hyperparams_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/box_coder_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/calibration_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/dataset_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/image_resizer_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/calibration_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/post_processing_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/hyperparams_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/region_similarity_calculator_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/model_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/preprocessor_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/box_coder_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/box_predictor_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/post_processing_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/matcher_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/anchor_generator_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/region_similarity_calculator_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/input_reader_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/anchor_generator_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/model_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/preprocessor_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/matcher_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/dataset_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/image_resizer_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/losses_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/graph_rewriter_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/optimizer_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/box_predictor_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/losses_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","copying build/lib/object_detection/builders/graph_rewriter_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n","creating build/bdist.linux-x86_64/egg/object_detection/inference\n","copying build/lib/object_detection/inference/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/inference\n","copying build/lib/object_detection/inference/detection_inference_test.py -> build/bdist.linux-x86_64/egg/object_detection/inference\n","copying build/lib/object_detection/inference/infer_detections.py -> build/bdist.linux-x86_64/egg/object_detection/inference\n","copying build/lib/object_detection/inference/detection_inference.py -> build/bdist.linux-x86_64/egg/object_detection/inference\n","creating build/bdist.linux-x86_64/egg/object_detection/metrics\n","copying build/lib/object_detection/metrics/offline_eval_map_corloc.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n","copying build/lib/object_detection/metrics/oid_challenge_evaluation_utils_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n","copying build/lib/object_detection/metrics/tf_example_parser_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n","copying build/lib/object_detection/metrics/coco_tools_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n","copying build/lib/object_detection/metrics/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n","copying build/lib/object_detection/metrics/calibration_metrics.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n","copying build/lib/object_detection/metrics/coco_evaluation_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n","copying build/lib/object_detection/metrics/tf_example_parser.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n","copying build/lib/object_detection/metrics/oid_challenge_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n","copying build/lib/object_detection/metrics/coco_tools.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n","copying build/lib/object_detection/metrics/oid_vrd_challenge_evaluation_utils_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n","copying build/lib/object_detection/metrics/oid_vrd_challenge_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n","copying build/lib/object_detection/metrics/calibration_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n","copying build/lib/object_detection/metrics/calibration_metrics_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n","copying build/lib/object_detection/metrics/io_utils.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n","copying build/lib/object_detection/metrics/oid_challenge_evaluation_utils.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n","copying build/lib/object_detection/metrics/offline_eval_map_corloc_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n","copying build/lib/object_detection/metrics/coco_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n","copying build/lib/object_detection/metrics/oid_vrd_challenge_evaluation_utils.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n","copying build/lib/object_detection/metrics/calibration_evaluation_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n","copying build/lib/object_detection/model_tpu_main.py -> build/bdist.linux-x86_64/egg/object_detection\n","creating build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n","copying build/lib/object_detection/anchor_generators/multiple_grid_anchor_generator_test.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n","copying build/lib/object_detection/anchor_generators/multiscale_grid_anchor_generator.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n","copying build/lib/object_detection/anchor_generators/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n","copying build/lib/object_detection/anchor_generators/grid_anchor_generator_test.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n","copying build/lib/object_detection/anchor_generators/flexible_grid_anchor_generator.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n","copying build/lib/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n","copying build/lib/object_detection/anchor_generators/multiple_grid_anchor_generator.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n","copying build/lib/object_detection/anchor_generators/flexible_grid_anchor_generator_test.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n","copying build/lib/object_detection/anchor_generators/grid_anchor_generator.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n","copying build/lib/object_detection/eval_util.py -> build/bdist.linux-x86_64/egg/object_detection\n","copying build/lib/object_detection/model_hparams.py -> build/bdist.linux-x86_64/egg/object_detection\n","copying build/lib/object_detection/__init__.py -> build/bdist.linux-x86_64/egg/object_detection\n","copying build/lib/object_detection/exporter_test.py -> build/bdist.linux-x86_64/egg/object_detection\n","copying build/lib/object_detection/export_tflite_ssd_graph_lib.py -> build/bdist.linux-x86_64/egg/object_detection\n","creating build/bdist.linux-x86_64/egg/object_detection/g3doc\n","copying build/lib/object_detection/g3doc/running_locally.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n","copying build/lib/object_detection/g3doc/tpu_exporters.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n","copying build/lib/object_detection/g3doc/installation.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n","copying build/lib/object_detection/g3doc/challenge_evaluation.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n","copying build/lib/object_detection/g3doc/evaluation_protocols.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n","copying build/lib/object_detection/g3doc/tpu_compatibility.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n","copying build/lib/object_detection/g3doc/instance_segmentation.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n","copying build/lib/object_detection/g3doc/running_pets.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n","copying build/lib/object_detection/g3doc/running_on_mobile_tensorflowlite.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n","copying build/lib/object_detection/g3doc/detection_model_zoo.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n","copying build/lib/object_detection/g3doc/exporting_models.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n","copying build/lib/object_detection/g3doc/running_on_cloud.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n","copying build/lib/object_detection/g3doc/faq.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n","copying build/lib/object_detection/g3doc/defining_your_own_model.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n","creating build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n","copying build/lib/object_detection/g3doc/img/dogs_detections_output.jpg -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n","copying build/lib/object_detection/g3doc/img/tensorboard2.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n","copying build/lib/object_detection/g3doc/img/example_cat.jpg -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n","copying build/lib/object_detection/g3doc/img/oid_monkey_3b4168c89cecbc5b.jpg -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n","copying build/lib/object_detection/g3doc/img/kites_detections_output.jpg -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n","copying build/lib/object_detection/g3doc/img/groupof_case_eval.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n","copying build/lib/object_detection/g3doc/img/tf-od-api-logo.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n","copying build/lib/object_detection/g3doc/img/nongroupof_case_eval.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n","copying build/lib/object_detection/g3doc/img/oid_bus_72e19c28aac34ed8.jpg -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n","copying build/lib/object_detection/g3doc/img/tensorboard.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n","copying build/lib/object_detection/g3doc/img/oxford_pet.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n","copying build/lib/object_detection/g3doc/img/dataset_explorer.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n","copying build/lib/object_detection/g3doc/img/kites_with_segment_overlay.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n","copying build/lib/object_detection/g3doc/using_your_own_dataset.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n","copying build/lib/object_detection/g3doc/oid_inference_and_evaluation.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n","copying build/lib/object_detection/g3doc/preparing_inputs.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n","copying build/lib/object_detection/g3doc/configuring_jobs.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n","copying build/lib/object_detection/g3doc/running_notebook.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n","copying build/lib/object_detection/model_lib_v2_test.py -> build/bdist.linux-x86_64/egg/object_detection\n","creating build/bdist.linux-x86_64/egg/object_detection/dockerfiles\n","creating build/bdist.linux-x86_64/egg/object_detection/dockerfiles/android\n","copying build/lib/object_detection/dockerfiles/android/README.md -> build/bdist.linux-x86_64/egg/object_detection/dockerfiles/android\n","copying build/lib/object_detection/dockerfiles/android/Dockerfile -> build/bdist.linux-x86_64/egg/object_detection/dockerfiles/android\n","creating build/bdist.linux-x86_64/egg/object_detection/samples\n","creating build/bdist.linux-x86_64/egg/object_detection/samples/cloud\n","copying build/lib/object_detection/samples/cloud/cloud.yml -> build/bdist.linux-x86_64/egg/object_detection/samples/cloud\n","creating build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/rfcn_resnet101_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssd_mobilenet_v2_pets_keras.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssd_mobilenet_v2_fpnlite_quantized_shared_box_predictor_256x256_depthmultiplier_75_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/faster_rcnn_resnet101_voc07.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/faster_rcnn_inception_v2_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/faster_rcnn_resnet50_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/faster_rcnn_resnet101_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssd_mobilenet_v1_300x300_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/faster_rcnn_resnet50_fgvc.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/faster_rcnn_resnet101_fgvc.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/mask_rcnn_resnet101_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssd_mobilenet_v1_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssd_inception_v2_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssd_mobilenet_v2_quantized_300x300_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssd_inception_v3_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssd_mobilenet_v1_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/faster_rcnn_resnet101_atrous_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_oid_v4.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/mask_rcnn_resnet50_atrous_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/faster_rcnn_resnet101_ava_v2.1.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/faster_rcnn_resnet101_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssd_mobilenet_v2_oid_v4.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/faster_rcnn_resnet152_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/embedded_ssd_mobilenet_v1_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/faster_rcnn_inception_v2_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/faster_rcnn_nas_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/rfcn_resnet101_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssdlite_mobilenet_v2_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/faster_rcnn_resnet152_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/mask_rcnn_inception_resnet_v2_atrous_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_oid.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_cosine_lr_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/mask_rcnn_inception_v2_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssd_inception_v2_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/mask_rcnn_resnet101_atrous_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/faster_rcnn_resnet50_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssd_resnet101_v1_fpn_shared_box_predictor_oid_512x512_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets_inference.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/facessd_mobilenet_v2_quantized_320x320_open_image_v4.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/faster_rcnn_resnet101_kitti.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssd_mobilenet_v2_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssd_mobilenet_v2_fullyconv_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssdlite_mobilenet_v1_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n","copying build/lib/object_detection/model_lib_test.py -> build/bdist.linux-x86_64/egg/object_detection\n","copying build/lib/object_detection/inputs_test.py -> build/bdist.linux-x86_64/egg/object_detection\n","creating build/bdist.linux-x86_64/egg/object_detection/test_data\n","copying build/lib/object_detection/test_data/pets_examples.record -> build/bdist.linux-x86_64/egg/object_detection/test_data\n","copying build/lib/object_detection/export_tflite_ssd_graph_lib_test.py -> build/bdist.linux-x86_64/egg/object_detection\n","copying build/lib/object_detection/object_detection_tutorial.ipynb -> build/bdist.linux-x86_64/egg/object_detection\n","copying build/lib/object_detection/README.md -> build/bdist.linux-x86_64/egg/object_detection\n","creating build/bdist.linux-x86_64/egg/object_detection/predictors\n","copying build/lib/object_detection/predictors/rfcn_box_predictor_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n","copying build/lib/object_detection/predictors/convolutional_keras_box_predictor.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n","copying build/lib/object_detection/predictors/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n","creating build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n","copying build/lib/object_detection/predictors/heads/mask_head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n","copying build/lib/object_detection/predictors/heads/keras_box_head_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n","copying build/lib/object_detection/predictors/heads/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n","copying build/lib/object_detection/predictors/heads/keras_class_head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n","copying build/lib/object_detection/predictors/heads/mask_head_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n","copying build/lib/object_detection/predictors/heads/keras_mask_head_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n","copying build/lib/object_detection/predictors/heads/head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n","copying build/lib/object_detection/predictors/heads/class_head_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n","copying build/lib/object_detection/predictors/heads/keypoint_head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n","copying build/lib/object_detection/predictors/heads/keras_class_head_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n","copying build/lib/object_detection/predictors/heads/keras_box_head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n","copying build/lib/object_detection/predictors/heads/class_head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n","copying build/lib/object_detection/predictors/heads/box_head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n","copying build/lib/object_detection/predictors/heads/keras_mask_head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n","copying build/lib/object_detection/predictors/heads/box_head_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n","copying build/lib/object_detection/predictors/heads/keypoint_head_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n","copying build/lib/object_detection/predictors/rfcn_box_predictor.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n","copying build/lib/object_detection/predictors/rfcn_keras_box_predictor_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n","copying build/lib/object_detection/predictors/convolutional_box_predictor.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n","copying build/lib/object_detection/predictors/mask_rcnn_keras_box_predictor.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n","copying build/lib/object_detection/predictors/mask_rcnn_box_predictor_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n","copying build/lib/object_detection/predictors/convolutional_box_predictor_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n","copying build/lib/object_detection/predictors/convolutional_keras_box_predictor_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n","copying build/lib/object_detection/predictors/mask_rcnn_box_predictor.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n","copying build/lib/object_detection/predictors/mask_rcnn_keras_box_predictor_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n","copying build/lib/object_detection/predictors/rfcn_keras_box_predictor.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n","copying build/lib/object_detection/CONTRIBUTING.md -> build/bdist.linux-x86_64/egg/object_detection\n","creating build/bdist.linux-x86_64/egg/object_detection/test_images\n","copying build/lib/object_detection/test_images/image1.jpg -> build/bdist.linux-x86_64/egg/object_detection/test_images\n","copying build/lib/object_detection/test_images/image_info.txt -> build/bdist.linux-x86_64/egg/object_detection/test_images\n","copying build/lib/object_detection/test_images/image2.jpg -> build/bdist.linux-x86_64/egg/object_detection/test_images\n","creating build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/keypoint_ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/target_assigner_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/box_predictor.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/anchor_generator.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/region_similarity_calculator_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/matcher_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/region_similarity_calculator.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/model.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/losses.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/batcher.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/minibatch_sampler.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/minibatch_sampler_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/balanced_positive_negative_sampler_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/batch_multiclass_nms_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/standard_fields.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/matcher.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/box_coder.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/class_agnostic_nms_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/freezable_batch_norm.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/box_coder_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/target_assigner.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/box_list_ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/keypoint_ops.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/data_decoder.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/data_parser.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/batcher_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/losses_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/prefetcher_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/box_list_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/preprocessor_cache.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/post_processing.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/prefetcher.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/multiclass_nms_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/preprocessor_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/freezable_batch_norm_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/box_list.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/balanced_positive_negative_sampler.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/preprocessor.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","copying build/lib/object_detection/core/box_list_ops.py -> build/bdist.linux-x86_64/egg/object_detection/core\n","creating build/bdist.linux-x86_64/egg/object_detection/matchers\n","copying build/lib/object_detection/matchers/bipartite_matcher.py -> build/bdist.linux-x86_64/egg/object_detection/matchers\n","copying build/lib/object_detection/matchers/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/matchers\n","copying build/lib/object_detection/matchers/bipartite_matcher_test.py -> build/bdist.linux-x86_64/egg/object_detection/matchers\n","copying build/lib/object_detection/matchers/argmax_matcher_test.py -> build/bdist.linux-x86_64/egg/object_detection/matchers\n","copying build/lib/object_detection/matchers/argmax_matcher.py -> build/bdist.linux-x86_64/egg/object_detection/matchers\n","copying build/lib/object_detection/export_tflite_ssd_graph.py -> build/bdist.linux-x86_64/egg/object_detection\n","creating build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n","copying build/lib/object_detection/dataset_tools/create_pascal_tf_record_test.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n","copying build/lib/object_detection/dataset_tools/oid_tfrecord_creation_test.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n","copying build/lib/object_detection/dataset_tools/create_pascal_tf_record.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n","copying build/lib/object_detection/dataset_tools/create_kitti_tf_record.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n","copying build/lib/object_detection/dataset_tools/create_oid_tf_record.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n","copying build/lib/object_detection/dataset_tools/create_coco_tf_record_test.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n","copying build/lib/object_detection/dataset_tools/create_pycocotools_package.sh -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n","copying build/lib/object_detection/dataset_tools/create_coco_tf_record.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n","copying build/lib/object_detection/dataset_tools/download_and_preprocess_mscoco.sh -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n","copying build/lib/object_detection/dataset_tools/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n","copying build/lib/object_detection/dataset_tools/oid_tfrecord_creation.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n","copying build/lib/object_detection/dataset_tools/tf_record_creation_util_test.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n","copying build/lib/object_detection/dataset_tools/oid_hierarchical_labels_expansion_test.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n","copying build/lib/object_detection/dataset_tools/create_kitti_tf_record_test.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n","copying build/lib/object_detection/dataset_tools/tf_record_creation_util.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n","copying build/lib/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n","copying build/lib/object_detection/dataset_tools/create_pet_tf_record.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n","copying build/lib/object_detection/inputs.py -> build/bdist.linux-x86_64/egg/object_detection\n","copying build/lib/object_detection/export_inference_graph.py -> build/bdist.linux-x86_64/egg/object_detection\n","creating build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n","copying build/lib/object_detection/tpu_exporters/utils_test.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n","copying build/lib/object_detection/tpu_exporters/export_saved_model_tpu_lib_test.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n","copying build/lib/object_detection/tpu_exporters/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n","copying build/lib/object_detection/tpu_exporters/export_saved_model_tpu.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n","copying build/lib/object_detection/tpu_exporters/utils.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n","copying build/lib/object_detection/tpu_exporters/faster_rcnn.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n","creating build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/testdata\n","creating build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/testdata/faster_rcnn\n","copying build/lib/object_detection/tpu_exporters/testdata/faster_rcnn/faster_rcnn_resnet101_atrous_coco.config -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/testdata/faster_rcnn\n","copying build/lib/object_detection/tpu_exporters/testdata/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/testdata\n","creating build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/testdata/ssd\n","copying build/lib/object_detection/tpu_exporters/testdata/ssd/ssd_pipeline.config -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/testdata/ssd\n","copying build/lib/object_detection/tpu_exporters/ssd.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n","copying build/lib/object_detection/tpu_exporters/export_saved_model_tpu_lib.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n","copying build/lib/object_detection/eval_util_test.py -> build/bdist.linux-x86_64/egg/object_detection\n","creating build/bdist.linux-x86_64/egg/object_detection/data\n","copying build/lib/object_detection/data/kitti_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n","copying build/lib/object_detection/data/oid_v4_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n","copying build/lib/object_detection/data/mscoco_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n","copying build/lib/object_detection/data/mscoco_complete_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n","copying build/lib/object_detection/data/face_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n","copying build/lib/object_detection/data/fgvc_2854_classes_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n","copying build/lib/object_detection/data/pascal_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n","copying build/lib/object_detection/data/ava_label_map_v2.1.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n","copying build/lib/object_detection/data/oid_object_detection_challenge_500_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n","copying build/lib/object_detection/data/mscoco_minival_ids.txt -> build/bdist.linux-x86_64/egg/object_detection/data\n","copying build/lib/object_detection/data/pet_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n","copying build/lib/object_detection/data/oid_bbox_trainable_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n","creating build/bdist.linux-x86_64/egg/object_detection/legacy\n","copying build/lib/object_detection/legacy/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/legacy\n","copying build/lib/object_detection/legacy/trainer.py -> build/bdist.linux-x86_64/egg/object_detection/legacy\n","copying build/lib/object_detection/legacy/evaluator.py -> build/bdist.linux-x86_64/egg/object_detection/legacy\n","copying build/lib/object_detection/legacy/train.py -> build/bdist.linux-x86_64/egg/object_detection/legacy\n","copying build/lib/object_detection/legacy/trainer_test.py -> build/bdist.linux-x86_64/egg/object_detection/legacy\n","copying build/lib/object_detection/legacy/eval.py -> build/bdist.linux-x86_64/egg/object_detection/legacy\n","creating build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n","copying build/lib/object_detection/meta_architectures/ssd_meta_arch.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n","copying build/lib/object_detection/meta_architectures/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n","copying build/lib/object_detection/meta_architectures/rfcn_meta_arch_test.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n","copying build/lib/object_detection/meta_architectures/rfcn_meta_arch.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n","copying build/lib/object_detection/meta_architectures/ssd_meta_arch_test.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n","copying build/lib/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n","copying build/lib/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n","copying build/lib/object_detection/meta_architectures/faster_rcnn_meta_arch.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n","copying build/lib/object_detection/meta_architectures/ssd_meta_arch_test_lib.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n","copying build/lib/object_detection/exporter.py -> build/bdist.linux-x86_64/egg/object_detection\n","creating build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_inception_v2_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/faster_rcnn_pnas_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_mobilenet_v2_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_inception_v3_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/faster_rcnn_inception_v2_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_pnasnet_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/faster_rcnn_nas_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/feature_map_generators.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/faster_rcnn_nas_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_inception_v3_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","creating build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n","copying build/lib/object_detection/models/keras_models/mobilenet_v1.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n","copying build/lib/object_detection/models/keras_models/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n","copying build/lib/object_detection/models/keras_models/model_utils.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n","copying build/lib/object_detection/models/keras_models/inception_resnet_v2.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n","copying build/lib/object_detection/models/keras_models/resnet_v1.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n","copying build/lib/object_detection/models/keras_models/mobilenet_v2_test.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n","creating build/bdist.linux-x86_64/egg/object_detection/models/keras_models/base_models\n","copying build/lib/object_detection/models/keras_models/base_models/original_mobilenet_v2.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models/base_models\n","copying build/lib/object_detection/models/keras_models/resnet_v1_test.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n","copying build/lib/object_detection/models/keras_models/mobilenet_v2.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n","copying build/lib/object_detection/models/keras_models/inception_resnet_v2_test.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n","copying build/lib/object_detection/models/keras_models/test_utils.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n","copying build/lib/object_detection/models/keras_models/mobilenet_v1_test.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n","copying build/lib/object_detection/models/ssd_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_inception_v2_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_pnasnet_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/feature_map_generators_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_mobilenet_v1_keras_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/faster_rcnn_resnet_v1_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_testbase.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/faster_rcnn_inception_v2_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/faster_rcnn_pnas_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_mobilenet_v1_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_mobilenet_v2_keras_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_mobilenet_v2_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n","copying build/lib/object_detection/model_lib.py -> build/bdist.linux-x86_64/egg/object_detection\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/data_decoders/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/data_decoders/tf_example_decoder_test.py to tf_example_decoder_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/data_decoders/tf_example_decoder.py to tf_example_decoder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/model_lib_v2.py to model_lib_v2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/mean_stddev_box_coder_test.py to mean_stddev_box_coder_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/faster_rcnn_box_coder.py to faster_rcnn_box_coder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/mean_stddev_box_coder.py to mean_stddev_box_coder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/square_box_coder.py to square_box_coder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/square_box_coder_test.py to square_box_coder_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/faster_rcnn_box_coder_test.py to faster_rcnn_box_coder_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/keypoint_box_coder.py to keypoint_box_coder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/keypoint_box_coder_test.py to keypoint_box_coder_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/model_main.py to model_main.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/ops_test.py to ops_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/visualization_utils.py to visualization_utils.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/config_util.py to config_util.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/vrd_evaluation_test.py to vrd_evaluation_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/metrics.py to metrics.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/category_util_test.py to category_util_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/variables_helper_test.py to variables_helper_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/variables_helper.py to variables_helper.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/ops.py to ops.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_mask_list_ops_test.py to np_box_mask_list_ops_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/per_image_evaluation.py to per_image_evaluation.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_list_ops.py to np_box_list_ops.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/metrics_test.py to metrics_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/shape_utils_test.py to shape_utils_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/learning_schedules.py to learning_schedules.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_list_ops_test.py to np_box_list_ops_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/category_util.py to category_util.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/per_image_vrd_evaluation.py to per_image_vrd_evaluation.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_mask_ops_test.py to np_mask_ops_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_mask_list_test.py to np_box_mask_list_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/visualization_utils_test.py to visualization_utils_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_ops_test.py to np_box_ops_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/label_map_util.py to label_map_util.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/test_case.py to test_case.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/json_utils.py to json_utils.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/label_map_util_test.py to label_map_util_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/vrd_evaluation.py to vrd_evaluation.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/object_detection_evaluation_test.py to object_detection_evaluation_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_ops.py to np_box_ops.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/shape_utils.py to shape_utils.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/model_util.py to model_util.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/static_shape_test.py to static_shape_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/json_utils_test.py to json_utils_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/autoaugment_utils.py to autoaugment_utils.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_list_test.py to np_box_list_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/context_manager_test.py to context_manager_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/spatial_transform_ops.py to spatial_transform_ops.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/per_image_evaluation_test.py to per_image_evaluation_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/learning_schedules_test.py to learning_schedules_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/config_util_test.py to config_util_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_mask_ops.py to np_mask_ops.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/static_shape.py to static_shape.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/per_image_vrd_evaluation_test.py to per_image_vrd_evaluation_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/dataset_util.py to dataset_util.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_mask_list_ops.py to np_box_mask_list_ops.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/object_detection_evaluation.py to object_detection_evaluation.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_mask_list.py to np_box_mask_list.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/model_util_test.py to model_util_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/test_utils.py to test_utils.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/spatial_transform_ops_test.py to spatial_transform_ops_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/context_manager.py to context_manager.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/test_utils_test.py to test_utils_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_list.py to np_box_list.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/dataset_util_test.py to dataset_util_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/optimizer_pb2.py to optimizer_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/string_int_label_map_pb2.py to string_int_label_map_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/ssd_anchor_generator_pb2.py to ssd_anchor_generator_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/flexible_grid_anchor_generator_pb2.py to flexible_grid_anchor_generator_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/preprocessor_pb2.py to preprocessor_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/square_box_coder_pb2.py to square_box_coder_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/image_resizer_pb2.py to image_resizer_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/anchor_generator_pb2.py to anchor_generator_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/box_coder_pb2.py to box_coder_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/grid_anchor_generator_pb2.py to grid_anchor_generator_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/mean_stddev_box_coder_pb2.py to mean_stddev_box_coder_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/model_pb2.py to model_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/hyperparams_pb2.py to hyperparams_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/region_similarity_calculator_pb2.py to region_similarity_calculator_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/eval_pb2.py to eval_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/calibration_pb2.py to calibration_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/pipeline_pb2.py to pipeline_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/faster_rcnn_box_coder_pb2.py to faster_rcnn_box_coder_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/train_pb2.py to train_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/losses_pb2.py to losses_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/graph_rewriter_pb2.py to graph_rewriter_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/input_reader_pb2.py to input_reader_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/matcher_pb2.py to matcher_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/faster_rcnn_pb2.py to faster_rcnn_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/argmax_matcher_pb2.py to argmax_matcher_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/box_predictor_pb2.py to box_predictor_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/keypoint_box_coder_pb2.py to keypoint_box_coder_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/post_processing_pb2.py to post_processing_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/ssd_pb2.py to ssd_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/bipartite_matcher_pb2.py to bipartite_matcher_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/multiscale_anchor_generator_pb2.py to multiscale_anchor_generator_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/optimizer_builder.py to optimizer_builder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/input_reader_builder.py to input_reader_builder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/hyperparams_builder_test.py to hyperparams_builder_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/box_coder_builder.py to box_coder_builder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/calibration_builder_test.py to calibration_builder_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/dataset_builder_test.py to dataset_builder_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/image_resizer_builder_test.py to image_resizer_builder_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/calibration_builder.py to calibration_builder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/post_processing_builder_test.py to post_processing_builder_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/hyperparams_builder.py to hyperparams_builder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/region_similarity_calculator_builder_test.py to region_similarity_calculator_builder_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/model_builder_test.py to model_builder_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/preprocessor_builder_test.py to preprocessor_builder_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/box_coder_builder_test.py to box_coder_builder_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/box_predictor_builder.py to box_predictor_builder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/post_processing_builder.py to post_processing_builder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/matcher_builder.py to matcher_builder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/anchor_generator_builder_test.py to anchor_generator_builder_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/region_similarity_calculator_builder.py to region_similarity_calculator_builder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/input_reader_builder_test.py to input_reader_builder_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/anchor_generator_builder.py to anchor_generator_builder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/model_builder.py to model_builder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/preprocessor_builder.py to preprocessor_builder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/matcher_builder_test.py to matcher_builder_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/dataset_builder.py to dataset_builder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/image_resizer_builder.py to image_resizer_builder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/losses_builder.py to losses_builder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/graph_rewriter_builder_test.py to graph_rewriter_builder_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/optimizer_builder_test.py to optimizer_builder_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/box_predictor_builder_test.py to box_predictor_builder_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/losses_builder_test.py to losses_builder_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/graph_rewriter_builder.py to graph_rewriter_builder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/inference/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/inference/detection_inference_test.py to detection_inference_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/inference/infer_detections.py to infer_detections.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/inference/detection_inference.py to detection_inference.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/offline_eval_map_corloc.py to offline_eval_map_corloc.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/oid_challenge_evaluation_utils_test.py to oid_challenge_evaluation_utils_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/tf_example_parser_test.py to tf_example_parser_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/coco_tools_test.py to coco_tools_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/calibration_metrics.py to calibration_metrics.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/coco_evaluation_test.py to coco_evaluation_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/tf_example_parser.py to tf_example_parser.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/oid_challenge_evaluation.py to oid_challenge_evaluation.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/coco_tools.py to coco_tools.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/oid_vrd_challenge_evaluation_utils_test.py to oid_vrd_challenge_evaluation_utils_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/oid_vrd_challenge_evaluation.py to oid_vrd_challenge_evaluation.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/calibration_evaluation.py to calibration_evaluation.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/calibration_metrics_test.py to calibration_metrics_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/io_utils.py to io_utils.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/oid_challenge_evaluation_utils.py to oid_challenge_evaluation_utils.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/offline_eval_map_corloc_test.py to offline_eval_map_corloc_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/coco_evaluation.py to coco_evaluation.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/oid_vrd_challenge_evaluation_utils.py to oid_vrd_challenge_evaluation_utils.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/calibration_evaluation_test.py to calibration_evaluation_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/model_tpu_main.py to model_tpu_main.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/multiple_grid_anchor_generator_test.py to multiple_grid_anchor_generator_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/multiscale_grid_anchor_generator.py to multiscale_grid_anchor_generator.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/grid_anchor_generator_test.py to grid_anchor_generator_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/flexible_grid_anchor_generator.py to flexible_grid_anchor_generator.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py to multiscale_grid_anchor_generator_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/multiple_grid_anchor_generator.py to multiple_grid_anchor_generator.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/flexible_grid_anchor_generator_test.py to flexible_grid_anchor_generator_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/grid_anchor_generator.py to grid_anchor_generator.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/eval_util.py to eval_util.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/model_hparams.py to model_hparams.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/exporter_test.py to exporter_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/export_tflite_ssd_graph_lib.py to export_tflite_ssd_graph_lib.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/model_lib_v2_test.py to model_lib_v2_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/model_lib_test.py to model_lib_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/inputs_test.py to inputs_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/export_tflite_ssd_graph_lib_test.py to export_tflite_ssd_graph_lib_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/rfcn_box_predictor_test.py to rfcn_box_predictor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/convolutional_keras_box_predictor.py to convolutional_keras_box_predictor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/mask_head.py to mask_head.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keras_box_head_test.py to keras_box_head_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keras_class_head.py to keras_class_head.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/mask_head_test.py to mask_head_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keras_mask_head_test.py to keras_mask_head_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/head.py to head.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/class_head_test.py to class_head_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keypoint_head.py to keypoint_head.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keras_class_head_test.py to keras_class_head_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keras_box_head.py to keras_box_head.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/class_head.py to class_head.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/box_head.py to box_head.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keras_mask_head.py to keras_mask_head.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/box_head_test.py to box_head_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keypoint_head_test.py to keypoint_head_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/rfcn_box_predictor.py to rfcn_box_predictor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/rfcn_keras_box_predictor_test.py to rfcn_keras_box_predictor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/convolutional_box_predictor.py to convolutional_box_predictor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/mask_rcnn_keras_box_predictor.py to mask_rcnn_keras_box_predictor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/mask_rcnn_box_predictor_test.py to mask_rcnn_box_predictor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/convolutional_box_predictor_test.py to convolutional_box_predictor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/convolutional_keras_box_predictor_test.py to convolutional_keras_box_predictor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/mask_rcnn_box_predictor.py to mask_rcnn_box_predictor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/mask_rcnn_keras_box_predictor_test.py to mask_rcnn_keras_box_predictor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/rfcn_keras_box_predictor.py to rfcn_keras_box_predictor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/keypoint_ops_test.py to keypoint_ops_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/target_assigner_test.py to target_assigner_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/box_predictor.py to box_predictor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/anchor_generator.py to anchor_generator.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/region_similarity_calculator_test.py to region_similarity_calculator_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/matcher_test.py to matcher_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/region_similarity_calculator.py to region_similarity_calculator.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/model.py to model.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/losses.py to losses.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/batcher.py to batcher.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/minibatch_sampler.py to minibatch_sampler.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/minibatch_sampler_test.py to minibatch_sampler_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/balanced_positive_negative_sampler_test.py to balanced_positive_negative_sampler_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/batch_multiclass_nms_test.py to batch_multiclass_nms_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/standard_fields.py to standard_fields.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/matcher.py to matcher.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/box_coder.py to box_coder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/class_agnostic_nms_test.py to class_agnostic_nms_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/freezable_batch_norm.py to freezable_batch_norm.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/box_coder_test.py to box_coder_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/target_assigner.py to target_assigner.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/box_list_ops_test.py to box_list_ops_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/keypoint_ops.py to keypoint_ops.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/data_decoder.py to data_decoder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/data_parser.py to data_parser.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/batcher_test.py to batcher_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/losses_test.py to losses_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/prefetcher_test.py to prefetcher_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/box_list_test.py to box_list_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/preprocessor_cache.py to preprocessor_cache.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/post_processing.py to post_processing.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/prefetcher.py to prefetcher.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/multiclass_nms_test.py to multiclass_nms_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/preprocessor_test.py to preprocessor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/freezable_batch_norm_test.py to freezable_batch_norm_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/box_list.py to box_list.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/balanced_positive_negative_sampler.py to balanced_positive_negative_sampler.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/preprocessor.py to preprocessor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/box_list_ops.py to box_list_ops.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/matchers/bipartite_matcher.py to bipartite_matcher.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/matchers/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/matchers/bipartite_matcher_test.py to bipartite_matcher_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/matchers/argmax_matcher_test.py to argmax_matcher_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/matchers/argmax_matcher.py to argmax_matcher.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/export_tflite_ssd_graph.py to export_tflite_ssd_graph.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_pascal_tf_record_test.py to create_pascal_tf_record_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/oid_tfrecord_creation_test.py to oid_tfrecord_creation_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_pascal_tf_record.py to create_pascal_tf_record.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_kitti_tf_record.py to create_kitti_tf_record.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_oid_tf_record.py to create_oid_tf_record.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_coco_tf_record_test.py to create_coco_tf_record_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_coco_tf_record.py to create_coco_tf_record.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/oid_tfrecord_creation.py to oid_tfrecord_creation.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/tf_record_creation_util_test.py to tf_record_creation_util_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/oid_hierarchical_labels_expansion_test.py to oid_hierarchical_labels_expansion_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_kitti_tf_record_test.py to create_kitti_tf_record_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/tf_record_creation_util.py to tf_record_creation_util.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py to oid_hierarchical_labels_expansion.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_pet_tf_record.py to create_pet_tf_record.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/inputs.py to inputs.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/export_inference_graph.py to export_inference_graph.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/utils_test.py to utils_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/export_saved_model_tpu_lib_test.py to export_saved_model_tpu_lib_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/export_saved_model_tpu.py to export_saved_model_tpu.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/utils.py to utils.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/faster_rcnn.py to faster_rcnn.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/testdata/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/ssd.py to ssd.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/export_saved_model_tpu_lib.py to export_saved_model_tpu_lib.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/eval_util_test.py to eval_util_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/legacy/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/legacy/trainer.py to trainer.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/legacy/evaluator.py to evaluator.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/legacy/train.py to train.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/legacy/trainer_test.py to trainer_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/legacy/eval.py to eval.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/ssd_meta_arch.py to ssd_meta_arch.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/rfcn_meta_arch_test.py to rfcn_meta_arch_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/rfcn_meta_arch.py to rfcn_meta_arch.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/ssd_meta_arch_test.py to ssd_meta_arch_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py to faster_rcnn_meta_arch_test_lib.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py to faster_rcnn_meta_arch_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/faster_rcnn_meta_arch.py to faster_rcnn_meta_arch.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/ssd_meta_arch_test_lib.py to ssd_meta_arch_test_lib.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/exporter.py to exporter.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_inception_v2_feature_extractor_test.py to ssd_inception_v2_feature_extractor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_pnas_feature_extractor.py to faster_rcnn_pnas_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py to ssd_resnet_v1_fpn_feature_extractor_testbase.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v2_feature_extractor_test.py to ssd_mobilenet_v2_feature_extractor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_inception_v3_feature_extractor_test.py to ssd_inception_v3_feature_extractor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor.py to faster_rcnn_inception_resnet_v2_keras_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py to ssd_resnet_v1_fpn_feature_extractor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_inception_v2_feature_extractor.py to faster_rcnn_inception_v2_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_pnasnet_feature_extractor.py to ssd_pnasnet_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_nas_feature_extractor.py to faster_rcnn_nas_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/feature_map_generators.py to feature_map_generators.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_nas_feature_extractor_test.py to faster_rcnn_nas_feature_extractor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_inception_v3_feature_extractor.py to ssd_inception_v3_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py to ssd_mobilenet_v1_fpn_keras_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py to ssd_mobilenet_v1_fpn_feature_extractor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py to embedded_ssd_mobilenet_v1_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py to ssd_mobilenet_v1_fpn_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/mobilenet_v1.py to mobilenet_v1.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/model_utils.py to model_utils.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/inception_resnet_v2.py to inception_resnet_v2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/resnet_v1.py to resnet_v1.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/mobilenet_v2_test.py to mobilenet_v2_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/base_models/original_mobilenet_v2.py to original_mobilenet_v2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/resnet_v1_test.py to resnet_v1_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/mobilenet_v2.py to mobilenet_v2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/inception_resnet_v2_test.py to inception_resnet_v2_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/test_utils.py to test_utils.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/mobilenet_v1_test.py to mobilenet_v1_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_feature_extractor_test.py to ssd_feature_extractor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py to faster_rcnn_resnet_v1_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_inception_v2_feature_extractor.py to ssd_inception_v2_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor.py to faster_rcnn_mobilenet_v1_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_pnasnet_feature_extractor_test.py to ssd_pnasnet_feature_extractor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/feature_map_generators_test.py to feature_map_generators_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_keras_feature_extractor.py to ssd_mobilenet_v1_keras_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_resnet_v1_feature_extractor_test.py to faster_rcnn_resnet_v1_feature_extractor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor_test.py to faster_rcnn_inception_resnet_v2_keras_feature_extractor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py to faster_rcnn_inception_resnet_v2_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py to ssd_mobilenet_v1_ppn_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_test.py to ssd_resnet_v1_ppn_feature_extractor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_testbase.py to ssd_resnet_v1_ppn_feature_extractor_testbase.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor_test.py to faster_rcnn_inception_resnet_v2_feature_extractor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor_test.py to ssd_mobilenet_v2_fpn_feature_extractor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor_test.py to ssd_mobilenet_v1_ppn_feature_extractor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py to ssd_resnet_v1_fpn_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor_test.py to faster_rcnn_mobilenet_v1_feature_extractor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py to ssd_resnet_v1_ppn_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_inception_v2_feature_extractor_test.py to faster_rcnn_inception_v2_feature_extractor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_pnas_feature_extractor_test.py to faster_rcnn_pnas_feature_extractor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_feature_extractor.py to ssd_mobilenet_v1_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py to ssd_mobilenet_v1_feature_extractor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v2_keras_feature_extractor.py to ssd_mobilenet_v2_keras_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v2_feature_extractor.py to ssd_mobilenet_v2_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py to ssd_resnet_v1_fpn_keras_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py to ssd_mobilenet_v2_fpn_keras_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor.py to ssd_mobilenet_v2_fpn_feature_extractor.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor_test.py to embedded_ssd_mobilenet_v1_feature_extractor_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/object_detection/model_lib.py to model_lib.cpython-36.pyc\n","creating build/bdist.linux-x86_64/egg/EGG-INFO\n","copying object_detection.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying object_detection.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying object_detection.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying object_detection.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying object_detection.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","zip_safe flag not set; analyzing archive contents...\n","object_detection.core.__pycache__.preprocessor.cpython-36: module MAY be using inspect.stack\n","object_detection.utils.__pycache__.autoaugment_utils.cpython-36: module MAY be using inspect.stack\n","creating dist\n","creating 'dist/object_detection-0.1-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n","removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n","Processing object_detection-0.1-py3.6.egg\n","creating /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg\n","Extracting object_detection-0.1-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n","Adding object-detection 0.1 to easy-install.pth file\n","\n","Installed /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg\n","Processing dependencies for object-detection==0.1\n","Searching for Cython==0.29.13\n","Best match: Cython 0.29.13\n","Adding Cython 0.29.13 to easy-install.pth file\n","Installing cygdb script to /usr/local/bin\n","Installing cython script to /usr/local/bin\n","Installing cythonize script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.6/dist-packages\n","Searching for matplotlib==3.0.3\n","Best match: matplotlib 3.0.3\n","Adding matplotlib 3.0.3 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.6/dist-packages\n","Searching for Pillow==4.3.0\n","Best match: Pillow 4.3.0\n","Adding Pillow 4.3.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.6/dist-packages\n","Searching for kiwisolver==1.1.0\n","Best match: kiwisolver 1.1.0\n","Adding kiwisolver 1.1.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.6/dist-packages\n","Searching for python-dateutil==2.5.3\n","Best match: python-dateutil 2.5.3\n","Adding python-dateutil 2.5.3 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.6/dist-packages\n","Searching for numpy==1.16.4\n","Best match: numpy 1.16.4\n","Adding numpy 1.16.4 to easy-install.pth file\n","Installing f2py script to /usr/local/bin\n","Installing f2py3 script to /usr/local/bin\n","Installing f2py3.6 script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.6/dist-packages\n","Searching for cycler==0.10.0\n","Best match: cycler 0.10.0\n","Adding cycler 0.10.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.6/dist-packages\n","Searching for pyparsing==2.4.2\n","Best match: pyparsing 2.4.2\n","Adding pyparsing 2.4.2 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.6/dist-packages\n","Searching for olefile==0.46\n","Best match: olefile 0.46\n","Adding olefile 0.46 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.6/dist-packages\n","Searching for setuptools==41.2.0\n","Best match: setuptools 41.2.0\n","Adding setuptools 41.2.0 to easy-install.pth file\n","Installing easy_install script to /usr/local/bin\n","Installing easy_install-3.6 script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.6/dist-packages\n","Searching for six==1.12.0\n","Best match: six 1.12.0\n","Adding six 1.12.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.6/dist-packages\n","Finished processing dependencies for object-detection==0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5gQHHzD-AzT1","colab_type":"code","outputId":"c0b2fc78-f325-4432-c9c2-ab7fe837ebd0","executionInfo":{"status":"ok","timestamp":1567176892030,"user_tz":-330,"elapsed":12922,"user":{"displayName":"Ravi Pawar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBtV9qOP0yVjG8p6Z6OsNi6vPiuKPc2AKMnCJqR=s64","userId":"00073904877215451927"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["%cd slim\n","!python setup.py build\n","!python setup.py install\n","%cd ..\n","!python object_detection/builders/model_builder_test.py\n","%cd object_detection"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/root/models/research/slim\n","running build\n","running build_py\n","creating build\n","creating build/lib\n","creating build/lib/preprocessing\n","copying preprocessing/__init__.py -> build/lib/preprocessing\n","copying preprocessing/cifarnet_preprocessing.py -> build/lib/preprocessing\n","copying preprocessing/inception_preprocessing.py -> build/lib/preprocessing\n","copying preprocessing/preprocessing_factory.py -> build/lib/preprocessing\n","copying preprocessing/lenet_preprocessing.py -> build/lib/preprocessing\n","copying preprocessing/vgg_preprocessing.py -> build/lib/preprocessing\n","creating build/lib/nets\n","copying nets/inception_v4_test.py -> build/lib/nets\n","copying nets/inception_v1_test.py -> build/lib/nets\n","copying nets/i3d_utils.py -> build/lib/nets\n","copying nets/i3d.py -> build/lib/nets\n","copying nets/dcgan_test.py -> build/lib/nets\n","copying nets/mobilenet_v1.py -> build/lib/nets\n","copying nets/dcgan.py -> build/lib/nets\n","copying nets/__init__.py -> build/lib/nets\n","copying nets/mobilenet_v1_train.py -> build/lib/nets\n","copying nets/pix2pix.py -> build/lib/nets\n","copying nets/nets_factory_test.py -> build/lib/nets\n","copying nets/lenet.py -> build/lib/nets\n","copying nets/i3d_test.py -> build/lib/nets\n","copying nets/inception_v2.py -> build/lib/nets\n","copying nets/inception_v3_test.py -> build/lib/nets\n","copying nets/pix2pix_test.py -> build/lib/nets\n","copying nets/inception_v4.py -> build/lib/nets\n","copying nets/resnet_v2_test.py -> build/lib/nets\n","copying nets/inception_resnet_v2.py -> build/lib/nets\n","copying nets/resnet_v1.py -> build/lib/nets\n","copying nets/nets_factory.py -> build/lib/nets\n","copying nets/alexnet_test.py -> build/lib/nets\n","copying nets/inception_utils.py -> build/lib/nets\n","copying nets/mobilenet_v1_eval.py -> build/lib/nets\n","copying nets/inception_v2_test.py -> build/lib/nets\n","copying nets/s3dg.py -> build/lib/nets\n","copying nets/inception_v3.py -> build/lib/nets\n","copying nets/resnet_utils.py -> build/lib/nets\n","copying nets/resnet_v1_test.py -> build/lib/nets\n","copying nets/cyclegan_test.py -> build/lib/nets\n","copying nets/overfeat.py -> build/lib/nets\n","copying nets/cyclegan.py -> build/lib/nets\n","copying nets/vgg_test.py -> build/lib/nets\n","copying nets/resnet_v2.py -> build/lib/nets\n","copying nets/alexnet.py -> build/lib/nets\n","copying nets/inception_resnet_v2_test.py -> build/lib/nets\n","copying nets/s3dg_test.py -> build/lib/nets\n","copying nets/cifarnet.py -> build/lib/nets\n","copying nets/inception_v1.py -> build/lib/nets\n","copying nets/overfeat_test.py -> build/lib/nets\n","copying nets/vgg.py -> build/lib/nets\n","copying nets/mobilenet_v1_test.py -> build/lib/nets\n","copying nets/inception.py -> build/lib/nets\n","creating build/lib/datasets\n","copying datasets/mnist.py -> build/lib/datasets\n","copying datasets/__init__.py -> build/lib/datasets\n","copying datasets/flowers.py -> build/lib/datasets\n","copying datasets/download_and_convert_mnist.py -> build/lib/datasets\n","copying datasets/build_visualwakewords_data.py -> build/lib/datasets\n","copying datasets/cifar10.py -> build/lib/datasets\n","copying datasets/download_and_convert_cifar10.py -> build/lib/datasets\n","copying datasets/preprocess_imagenet_validation_data.py -> build/lib/datasets\n","copying datasets/visualwakewords.py -> build/lib/datasets\n","copying datasets/process_bounding_boxes.py -> build/lib/datasets\n","copying datasets/download_and_convert_flowers.py -> build/lib/datasets\n","copying datasets/build_imagenet_data.py -> build/lib/datasets\n","copying datasets/dataset_factory.py -> build/lib/datasets\n","copying datasets/dataset_utils.py -> build/lib/datasets\n","copying datasets/imagenet.py -> build/lib/datasets\n","copying datasets/build_visualwakewords_data_lib.py -> build/lib/datasets\n","creating build/lib/deployment\n","copying deployment/__init__.py -> build/lib/deployment\n","copying deployment/model_deploy_test.py -> build/lib/deployment\n","copying deployment/model_deploy.py -> build/lib/deployment\n","creating build/lib/nets/mobilenet\n","copying nets/mobilenet/mobilenet.py -> build/lib/nets/mobilenet\n","copying nets/mobilenet/__init__.py -> build/lib/nets/mobilenet\n","copying nets/mobilenet/mobilenet_v2_test.py -> build/lib/nets/mobilenet\n","copying nets/mobilenet/mobilenet_v2.py -> build/lib/nets/mobilenet\n","copying nets/mobilenet/conv_blocks.py -> build/lib/nets/mobilenet\n","creating build/lib/nets/nasnet\n","copying nets/nasnet/nasnet.py -> build/lib/nets/nasnet\n","copying nets/nasnet/nasnet_utils.py -> build/lib/nets/nasnet\n","copying nets/nasnet/__init__.py -> build/lib/nets/nasnet\n","copying nets/nasnet/nasnet_utils_test.py -> build/lib/nets/nasnet\n","copying nets/nasnet/nasnet_test.py -> build/lib/nets/nasnet\n","copying nets/nasnet/pnasnet.py -> build/lib/nets/nasnet\n","copying nets/nasnet/pnasnet_test.py -> build/lib/nets/nasnet\n","running egg_info\n","creating slim.egg-info\n","writing slim.egg-info/PKG-INFO\n","writing dependency_links to slim.egg-info/dependency_links.txt\n","writing top-level names to slim.egg-info/top_level.txt\n","writing manifest file 'slim.egg-info/SOURCES.txt'\n","writing manifest file 'slim.egg-info/SOURCES.txt'\n","copying nets/mobilenet_v1.md -> build/lib/nets\n","copying nets/mobilenet_v1.png -> build/lib/nets\n","copying datasets/download_and_convert_imagenet.sh -> build/lib/datasets\n","copying datasets/download_imagenet.sh -> build/lib/datasets\n","copying datasets/download_mscoco.sh -> build/lib/datasets\n","copying datasets/imagenet_2012_validation_synset_labels.txt -> build/lib/datasets\n","copying datasets/imagenet_lsvrc_2015_synsets.txt -> build/lib/datasets\n","copying datasets/imagenet_metadata.txt -> build/lib/datasets\n","copying nets/mobilenet/README.md -> build/lib/nets/mobilenet\n","copying nets/mobilenet/madds_top1_accuracy.png -> build/lib/nets/mobilenet\n","copying nets/mobilenet/mnet_v1_vs_v2_pixel1_latency.png -> build/lib/nets/mobilenet\n","copying nets/mobilenet/mobilenet_example.ipynb -> build/lib/nets/mobilenet\n","copying nets/nasnet/README.md -> build/lib/nets/nasnet\n","running install\n","running bdist_egg\n","running egg_info\n","writing slim.egg-info/PKG-INFO\n","writing dependency_links to slim.egg-info/dependency_links.txt\n","writing top-level names to slim.egg-info/top_level.txt\n","writing manifest file 'slim.egg-info/SOURCES.txt'\n","installing library code to build/bdist.linux-x86_64/egg\n","running install_lib\n","running build_py\n","creating build/bdist.linux-x86_64\n","creating build/bdist.linux-x86_64/egg\n","creating build/bdist.linux-x86_64/egg/preprocessing\n","copying build/lib/preprocessing/__init__.py -> build/bdist.linux-x86_64/egg/preprocessing\n","copying build/lib/preprocessing/cifarnet_preprocessing.py -> build/bdist.linux-x86_64/egg/preprocessing\n","copying build/lib/preprocessing/inception_preprocessing.py -> build/bdist.linux-x86_64/egg/preprocessing\n","copying build/lib/preprocessing/preprocessing_factory.py -> build/bdist.linux-x86_64/egg/preprocessing\n","copying build/lib/preprocessing/lenet_preprocessing.py -> build/bdist.linux-x86_64/egg/preprocessing\n","copying build/lib/preprocessing/vgg_preprocessing.py -> build/bdist.linux-x86_64/egg/preprocessing\n","creating build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception_v4_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception_v1_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/i3d_utils.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/i3d.py -> build/bdist.linux-x86_64/egg/nets\n","creating build/bdist.linux-x86_64/egg/nets/mobilenet\n","copying build/lib/nets/mobilenet/mobilenet_example.ipynb -> build/bdist.linux-x86_64/egg/nets/mobilenet\n","copying build/lib/nets/mobilenet/mobilenet.py -> build/bdist.linux-x86_64/egg/nets/mobilenet\n","copying build/lib/nets/mobilenet/__init__.py -> build/bdist.linux-x86_64/egg/nets/mobilenet\n","copying build/lib/nets/mobilenet/mobilenet_v2_test.py -> build/bdist.linux-x86_64/egg/nets/mobilenet\n","copying build/lib/nets/mobilenet/README.md -> build/bdist.linux-x86_64/egg/nets/mobilenet\n","copying build/lib/nets/mobilenet/mobilenet_v2.py -> build/bdist.linux-x86_64/egg/nets/mobilenet\n","copying build/lib/nets/mobilenet/madds_top1_accuracy.png -> build/bdist.linux-x86_64/egg/nets/mobilenet\n","copying build/lib/nets/mobilenet/conv_blocks.py -> build/bdist.linux-x86_64/egg/nets/mobilenet\n","copying build/lib/nets/mobilenet/mnet_v1_vs_v2_pixel1_latency.png -> build/bdist.linux-x86_64/egg/nets/mobilenet\n","copying build/lib/nets/dcgan_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/mobilenet_v1.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/dcgan.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/__init__.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/mobilenet_v1_train.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/pix2pix.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/nets_factory_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/lenet.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/i3d_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception_v2.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception_v3_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/pix2pix_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception_v4.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/resnet_v2_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception_resnet_v2.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/resnet_v1.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/nets_factory.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/alexnet_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception_utils.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/mobilenet_v1_eval.py -> build/bdist.linux-x86_64/egg/nets\n","creating build/bdist.linux-x86_64/egg/nets/nasnet\n","copying build/lib/nets/nasnet/nasnet.py -> build/bdist.linux-x86_64/egg/nets/nasnet\n","copying build/lib/nets/nasnet/nasnet_utils.py -> build/bdist.linux-x86_64/egg/nets/nasnet\n","copying build/lib/nets/nasnet/__init__.py -> build/bdist.linux-x86_64/egg/nets/nasnet\n","copying build/lib/nets/nasnet/README.md -> build/bdist.linux-x86_64/egg/nets/nasnet\n","copying build/lib/nets/nasnet/nasnet_utils_test.py -> build/bdist.linux-x86_64/egg/nets/nasnet\n","copying build/lib/nets/nasnet/nasnet_test.py -> build/bdist.linux-x86_64/egg/nets/nasnet\n","copying build/lib/nets/nasnet/pnasnet.py -> build/bdist.linux-x86_64/egg/nets/nasnet\n","copying build/lib/nets/nasnet/pnasnet_test.py -> build/bdist.linux-x86_64/egg/nets/nasnet\n","copying build/lib/nets/inception_v2_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/s3dg.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception_v3.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/resnet_utils.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/mobilenet_v1.png -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/resnet_v1_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/cyclegan_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/overfeat.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/cyclegan.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/vgg_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/resnet_v2.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/alexnet.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception_resnet_v2_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/mobilenet_v1.md -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/s3dg_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/cifarnet.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception_v1.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/overfeat_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/vgg.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/mobilenet_v1_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception.py -> build/bdist.linux-x86_64/egg/nets\n","creating build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/mnist.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/download_and_convert_imagenet.sh -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/imagenet_lsvrc_2015_synsets.txt -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/download_mscoco.sh -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/__init__.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/download_imagenet.sh -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/imagenet_2012_validation_synset_labels.txt -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/flowers.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/download_and_convert_mnist.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/build_visualwakewords_data.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/cifar10.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/download_and_convert_cifar10.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/preprocess_imagenet_validation_data.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/visualwakewords.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/process_bounding_boxes.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/download_and_convert_flowers.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/build_imagenet_data.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/imagenet_metadata.txt -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/dataset_factory.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/dataset_utils.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/imagenet.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/build_visualwakewords_data_lib.py -> build/bdist.linux-x86_64/egg/datasets\n","creating build/bdist.linux-x86_64/egg/deployment\n","copying build/lib/deployment/__init__.py -> build/bdist.linux-x86_64/egg/deployment\n","copying build/lib/deployment/model_deploy_test.py -> build/bdist.linux-x86_64/egg/deployment\n","copying build/lib/deployment/model_deploy.py -> build/bdist.linux-x86_64/egg/deployment\n","byte-compiling build/bdist.linux-x86_64/egg/preprocessing/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/preprocessing/cifarnet_preprocessing.py to cifarnet_preprocessing.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/preprocessing/inception_preprocessing.py to inception_preprocessing.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/preprocessing/preprocessing_factory.py to preprocessing_factory.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/preprocessing/lenet_preprocessing.py to lenet_preprocessing.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/preprocessing/vgg_preprocessing.py to vgg_preprocessing.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception_v4_test.py to inception_v4_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception_v1_test.py to inception_v1_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/i3d_utils.py to i3d_utils.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/i3d.py to i3d.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/mobilenet/mobilenet.py to mobilenet.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/mobilenet/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/mobilenet/mobilenet_v2_test.py to mobilenet_v2_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/mobilenet/mobilenet_v2.py to mobilenet_v2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/mobilenet/conv_blocks.py to conv_blocks.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/dcgan_test.py to dcgan_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/mobilenet_v1.py to mobilenet_v1.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/dcgan.py to dcgan.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/mobilenet_v1_train.py to mobilenet_v1_train.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/pix2pix.py to pix2pix.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/nets_factory_test.py to nets_factory_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/lenet.py to lenet.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/i3d_test.py to i3d_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception_v2.py to inception_v2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception_v3_test.py to inception_v3_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/pix2pix_test.py to pix2pix_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception_v4.py to inception_v4.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/resnet_v2_test.py to resnet_v2_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception_resnet_v2.py to inception_resnet_v2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/resnet_v1.py to resnet_v1.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/nets_factory.py to nets_factory.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/alexnet_test.py to alexnet_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception_utils.py to inception_utils.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/mobilenet_v1_eval.py to mobilenet_v1_eval.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/nasnet/nasnet.py to nasnet.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/nasnet/nasnet_utils.py to nasnet_utils.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/nasnet/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/nasnet/nasnet_utils_test.py to nasnet_utils_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/nasnet/nasnet_test.py to nasnet_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/nasnet/pnasnet.py to pnasnet.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/nasnet/pnasnet_test.py to pnasnet_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception_v2_test.py to inception_v2_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/s3dg.py to s3dg.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception_v3.py to inception_v3.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/resnet_utils.py to resnet_utils.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/resnet_v1_test.py to resnet_v1_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/cyclegan_test.py to cyclegan_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/overfeat.py to overfeat.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/cyclegan.py to cyclegan.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/vgg_test.py to vgg_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/resnet_v2.py to resnet_v2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/alexnet.py to alexnet.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception_resnet_v2_test.py to inception_resnet_v2_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/s3dg_test.py to s3dg_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/cifarnet.py to cifarnet.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception_v1.py to inception_v1.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/overfeat_test.py to overfeat_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/vgg.py to vgg.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/mobilenet_v1_test.py to mobilenet_v1_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception.py to inception.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/mnist.py to mnist.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/flowers.py to flowers.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/download_and_convert_mnist.py to download_and_convert_mnist.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/build_visualwakewords_data.py to build_visualwakewords_data.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/cifar10.py to cifar10.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/download_and_convert_cifar10.py to download_and_convert_cifar10.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/preprocess_imagenet_validation_data.py to preprocess_imagenet_validation_data.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/visualwakewords.py to visualwakewords.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/process_bounding_boxes.py to process_bounding_boxes.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/download_and_convert_flowers.py to download_and_convert_flowers.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/build_imagenet_data.py to build_imagenet_data.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/dataset_factory.py to dataset_factory.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/dataset_utils.py to dataset_utils.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/imagenet.py to imagenet.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/build_visualwakewords_data_lib.py to build_visualwakewords_data_lib.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/deployment/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/deployment/model_deploy_test.py to model_deploy_test.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/deployment/model_deploy.py to model_deploy.cpython-36.pyc\n","creating build/bdist.linux-x86_64/egg/EGG-INFO\n","copying slim.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying slim.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying slim.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying slim.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","zip_safe flag not set; analyzing archive contents...\n","creating dist\n","creating 'dist/slim-0.1-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n","removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n","Processing slim-0.1-py3.6.egg\n","Copying slim-0.1-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n","Adding slim 0.1 to easy-install.pth file\n","\n","Installed /usr/local/lib/python3.6/dist-packages/slim-0.1-py3.6.egg\n","Processing dependencies for slim==0.1\n","Finished processing dependencies for slim==0.1\n","/root/models/research\n","WARNING: Logging before flag parsing goes to stderr.\n","W0830 14:54:50.637747 140478974470016 lazy_loader.py:50] \n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","W0830 14:54:50.900791 140478974470016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/slim-0.1-py3.6.egg/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","W0830 14:54:50.946713 140478974470016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/slim-0.1-py3.6.egg/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n","\n","Running tests under Python 3.6.8: /usr/bin/python3\n","[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner\n","[       OK ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner\n","[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n","[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n","[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n","[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n","[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n","[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n","[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n","[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n","[ RUN      ] ModelBuilderTest.test_create_rfcn_model_from_config\n","[       OK ] ModelBuilderTest.test_create_rfcn_model_from_config\n","[ RUN      ] ModelBuilderTest.test_create_ssd_fpn_model_from_config\n","[       OK ] ModelBuilderTest.test_create_ssd_fpn_model_from_config\n","[ RUN      ] ModelBuilderTest.test_create_ssd_models_from_config\n","[       OK ] ModelBuilderTest.test_create_ssd_models_from_config\n","[ RUN      ] ModelBuilderTest.test_invalid_faster_rcnn_batchnorm_update\n","[       OK ] ModelBuilderTest.test_invalid_faster_rcnn_batchnorm_update\n","[ RUN      ] ModelBuilderTest.test_invalid_first_stage_nms_iou_threshold\n","[       OK ] ModelBuilderTest.test_invalid_first_stage_nms_iou_threshold\n","[ RUN      ] ModelBuilderTest.test_invalid_model_config_proto\n","[       OK ] ModelBuilderTest.test_invalid_model_config_proto\n","[ RUN      ] ModelBuilderTest.test_invalid_second_stage_batch_size\n","[       OK ] ModelBuilderTest.test_invalid_second_stage_batch_size\n","[ RUN      ] ModelBuilderTest.test_session\n","[  SKIPPED ] ModelBuilderTest.test_session\n","[ RUN      ] ModelBuilderTest.test_unknown_faster_rcnn_feature_extractor\n","[       OK ] ModelBuilderTest.test_unknown_faster_rcnn_feature_extractor\n","[ RUN      ] ModelBuilderTest.test_unknown_meta_architecture\n","[       OK ] ModelBuilderTest.test_unknown_meta_architecture\n","[ RUN      ] ModelBuilderTest.test_unknown_ssd_feature_extractor\n","[       OK ] ModelBuilderTest.test_unknown_ssd_feature_extractor\n","----------------------------------------------------------------------\n","Ran 16 tests in 0.152s\n","\n","OK (skipped=1)\n","/root/models/research/object_detection\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"v9BUosfwLvnY","colab_type":"code","outputId":"cbf425e7-f3b6-4f9a-f488-ccd18b822abe","executionInfo":{"status":"ok","timestamp":1567176900312,"user_tz":-330,"elapsed":3118,"user":{"displayName":"Ravi Pawar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBtV9qOP0yVjG8p6Z6OsNi6vPiuKPc2AKMnCJqR=s64","userId":"00073904877215451927"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd ~/models/research/object_detection\n","!rm -rf images"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/root/models/research/object_detection\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iB8ueM4ICLjf","colab_type":"code","outputId":"719bbd20-5094-425c-8ba1-39ca7b797a63","executionInfo":{"status":"ok","timestamp":1567177284416,"user_tz":-330,"elapsed":1452,"user":{"displayName":"Ravi Pawar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBtV9qOP0yVjG8p6Z6OsNi6vPiuKPc2AKMnCJqR=s64","userId":"00073904877215451927"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#images\n","fileId = '1SzVEs7Jlt7DndDy-ZGWD87v_lPYyd4hI'\n","\n","import os\n","from zipfile import ZipFile\n","from shutil import copy\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","fileName = fileId + '.zip'\n","downloaded = drive.CreateFile({'id': fileId})\n","downloaded.GetContentFile(fileName)\n","ds = ZipFile(fileName)\n","ds.extractall()\n","os.remove(fileName)\n","print('Extracted zip file ' + fileName)\n","\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Extracted zip file 1SzVEs7Jlt7DndDy-ZGWD87v_lPYyd4hI.zip\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZI5xQ2tMIKS4","colab_type":"code","outputId":"376fec34-fce4-4d59-a5ee-456ec68511bd","executionInfo":{"status":"ok","timestamp":1567177328173,"user_tz":-330,"elapsed":1269,"user":{"displayName":"Ravi Pawar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBtV9qOP0yVjG8p6Z6OsNi6vPiuKPc2AKMnCJqR=s64","userId":"00073904877215451927"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd \n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["/root\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MQTn-nI9IUip","colab_type":"code","colab":{}},"source":["!rm -rf TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10\n","!git clone --quiet https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10.git"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NS09bAJo7ft3","colab_type":"code","colab":{}},"source":["!mv ~/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/xml_to_csv.py /root/models/research/object_detection/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RbgslGulBIku","colab_type":"code","colab":{}},"source":["!mv ~/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/generate_tfrecord.py /root/models/research/object_detection/\n","!mv /root/models/research/object_detection/legacy/train.py /root/models/research/object_detection/\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LD6xs6YdMsH2","colab_type":"code","outputId":"8472b451-cb58-4547-ecb9-456bf93e1514","executionInfo":{"status":"ok","timestamp":1567177372828,"user_tz":-330,"elapsed":4128,"user":{"displayName":"Ravi Pawar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBtV9qOP0yVjG8p6Z6OsNi6vPiuKPc2AKMnCJqR=s64","userId":"00073904877215451927"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["#xml to csv\n","%cd ~/models/research/object_detection/\n","!python xml_to_csv.py"],"execution_count":12,"outputs":[{"output_type":"stream","text":["/root/models/research/object_detection\n","Successfully converted xml to csv.\n","Successfully converted xml to csv.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rluqJr3NSefH","colab_type":"code","colab":{}},"source":["!rm -rf training"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uvfpBE7bCX79","colab_type":"code","colab":{}},"source":["#training directory\n","!mkdir training"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PcMK-6RhCum1","colab_type":"code","colab":{}},"source":["%pycat generate_tfrecord.py\n","!rm generate_tfrecord.py"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fDBVDyrUDQ57","colab_type":"code","outputId":"f031a407-283d-4c89-a587-51a4139187cc","executionInfo":{"status":"ok","timestamp":1567177486453,"user_tz":-330,"elapsed":953,"user":{"displayName":"Ravi Pawar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBtV9qOP0yVjG8p6Z6OsNi6vPiuKPc2AKMnCJqR=s64","userId":"00073904877215451927"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%writefile generate_tfrecord.py\n","\"\"\"\n","Usage:\n","  # From tensorflow/models/\n","  # Create train data:\n","  python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record\n","\n","  # Create test data:\n","  python generate_tfrecord.py --csv_input=images/test_labels.csv  --image_dir=images/test --output_path=test.record\n","\"\"\"\n","from __future__ import division\n","from __future__ import print_function\n","from __future__ import absolute_import\n","\n","import os\n","import io\n","import pandas as pd\n","import tensorflow as tf\n","\n","from PIL import Image\n","from object_detection.utils import dataset_util\n","from collections import namedtuple, OrderedDict\n","\n","flags = tf.app.flags\n","flags.DEFINE_string('csv_input', '', 'Path to the CSV input')\n","flags.DEFINE_string('image_dir', '', 'Path to the image directory')\n","flags.DEFINE_string('output_path', '', 'Path to output TFRecord')\n","FLAGS = flags.FLAGS\n","\n","\n","# TO-DO replace this with label map\n","def class_text_to_int(row_label):\n","    if row_label == 'pdone':\n","        return 1\n","    elif row_label == 'pdtwo':\n","        return 2\n","    else:\n","        return 0\n","\n","\n","def split(df, group):\n","    data = namedtuple('data', ['filename', 'object'])\n","    gb = df.groupby(group)\n","    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n","\n","\n","def create_tf_example(group, path):\n","    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n","        encoded_jpg = fid.read()\n","    encoded_jpg_io = io.BytesIO(encoded_jpg)\n","    image = Image.open(encoded_jpg_io)\n","    width, height = image.size\n","\n","    filename = group.filename.encode('utf8')\n","    image_format = b'jpg'\n","    xmins = []\n","    xmaxs = []\n","    ymins = []\n","    ymaxs = []\n","    classes_text = []\n","    classes = []\n","\n","    for index, row in group.object.iterrows():\n","        xmins.append(row['xmin'] / width)\n","        xmaxs.append(row['xmax'] / width)\n","        ymins.append(row['ymin'] / height)\n","        ymaxs.append(row['ymax'] / height)\n","        classes_text.append(row['class'].encode('utf8'))\n","        classes.append(class_text_to_int(row['class']))\n","\n","    tf_example = tf.train.Example(features=tf.train.Features(feature={\n","        'image/height': dataset_util.int64_feature(height),\n","        'image/width': dataset_util.int64_feature(width),\n","        'image/filename': dataset_util.bytes_feature(filename),\n","        'image/source_id': dataset_util.bytes_feature(filename),\n","        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n","        'image/format': dataset_util.bytes_feature(image_format),\n","        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n","        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n","        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n","        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n","        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n","        'image/object/class/label': dataset_util.int64_list_feature(classes),\n","    }))\n","    return tf_example\n","\n","\n","def main(_):\n","    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\n","    path = os.path.join(os.getcwd(), FLAGS.image_dir)\n","    examples = pd.read_csv(FLAGS.csv_input)\n","    grouped = split(examples, 'filename')\n","    for group in grouped:\n","        tf_example = create_tf_example(group, path)\n","        writer.write(tf_example.SerializeToString())\n","\n","    writer.close()\n","    output_path = os.path.join(os.getcwd(), FLAGS.output_path)\n","    print('Successfully created the TFRecords: {}'.format(output_path))\n","\n","\n","if __name__ == '__main__':\n","    tf.app.run()"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Overwriting generate_tfrecord.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FSSsrMyfGTMy","colab_type":"code","outputId":"01e5dd40-ea56-4612-a997-473320593b72","executionInfo":{"status":"error","timestamp":1566833208985,"user_tz":-330,"elapsed":14883,"user":{"displayName":"Ravi Pawar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBtV9qOP0yVjG8p6Z6OsNi6vPiuKPc2AKMnCJqR=s64","userId":"00073904877215451927"}},"colab":{"base_uri":"https://localhost:8080/","height":417}},"source":["#Generate root password\n","import random, string\n","password = ''.join(random.choice(string.ascii_letters + string.digits) for i in range(20))\n","\n","#Download ngrok\n","! wget -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","! unzip -qq -n ngrok-stable-linux-amd64.zip\n","#Setup sshd\n","! apt-get install -qq -o=Dpkg::Use-Pty=0 openssh-server pwgen > /dev/null\n","#Set root password\n","! echo root:$password | chpasswd\n","! mkdir -p /var/run/sshd\n","! echo \"PermitRootLogin yes\" >> /etc/ssh/sshd_config\n","! echo \"PasswordAuthentication yes\" >> /etc/ssh/sshd_config\n","! echo \"LD_LIBRARY_PATH=/usr/lib64-nvidia\" >> /root/.bashrc\n","! echo \"export LD_LIBRARY_PATH\" >> /root/.bashrc\n","\n","#Run sshd\n","get_ipython().system_raw('/usr/sbin/sshd -D &')\n","\n","#Ask token\n","print(\"Copy authtoken from https://dashboard.ngrok.com/auth\")\n","import getpass\n","authtoken = getpass.getpass()\n","\n","#Create tunnel\n","get_ipython().system_raw('./ngrok authtoken $authtoken && ./ngrok tcp 22 &')\n","#Print root password\n","print(\"Root password: {}\".format(password))\n","#Get public address\n","! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["^C\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-5523c630bbbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' apt-get install -qq -o=Dpkg::Use-Pty=0 openssh-server pwgen > /dev/null'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#Set root password\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' echo root:$password | chpasswd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' mkdir -p /var/run/sshd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' echo \"PermitRootLogin yes\" >> /etc/ssh/sshd_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     with temporary_clearer(), _display_stdin_widget(\n\u001b[0;32m--> 181\u001b[0;31m         delay_millis=500) as update_stdin_widget:\n\u001b[0m\u001b[1;32m    182\u001b[0m       \u001b[0;31m# TODO(b/115531839): Ensure that subprocesses are terminated upon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0;31m# interrupt.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    339\u001b[0m   \u001b[0mshell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m   \u001b[0mdisplay_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_display_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'delayMillis'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdelay_millis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdisplay_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mecho_updater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_echo_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"fSaBBx_XDFtZ","colab_type":"code","outputId":"c5b67078-181a-4b90-a634-740d0346a09f","executionInfo":{"status":"ok","timestamp":1567177526339,"user_tz":-330,"elapsed":8978,"user":{"displayName":"Ravi Pawar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBtV9qOP0yVjG8p6Z6OsNi6vPiuKPc2AKMnCJqR=s64","userId":"00073904877215451927"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["!python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record\n","!python generate_tfrecord.py --csv_input=images/test_labels.csv --image_dir=images/test --output_path=test.record"],"execution_count":18,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0830 15:05:21.803023 140426659751808 deprecation_wrapper.py:119] From generate_tfrecord.py:102: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n","\n","W0830 15:05:21.803749 140426659751808 deprecation_wrapper.py:119] From generate_tfrecord.py:88: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n","\n","W0830 15:05:21.858285 140426659751808 deprecation_wrapper.py:119] From generate_tfrecord.py:47: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","Successfully created the TFRecords: /root/models/research/object_detection/train.record\n","WARNING: Logging before flag parsing goes to stderr.\n","W0830 15:05:25.670368 140391799867264 deprecation_wrapper.py:119] From generate_tfrecord.py:102: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n","\n","W0830 15:05:25.670992 140391799867264 deprecation_wrapper.py:119] From generate_tfrecord.py:88: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n","\n","W0830 15:05:25.686797 140391799867264 deprecation_wrapper.py:119] From generate_tfrecord.py:47: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","Successfully created the TFRecords: /root/models/research/object_detection/test.record\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ozvF9-xSDXlR","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Uu0NuCeFZdE","colab_type":"code","outputId":"d10ae214-7482-48eb-887f-17b54eec1d2a","executionInfo":{"status":"ok","timestamp":1567177532705,"user_tz":-330,"elapsed":937,"user":{"displayName":"Ravi Pawar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBtV9qOP0yVjG8p6Z6OsNi6vPiuKPc2AKMnCJqR=s64","userId":"00073904877215451927"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\n","%cd training"],"execution_count":19,"outputs":[{"output_type":"stream","text":["/root/models/research/object_detection/training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WZFEsLeFNIu8","colab_type":"code","outputId":"ce2652ee-04dc-49c9-8a7e-75182cbeca58","executionInfo":{"status":"ok","timestamp":1567177543034,"user_tz":-330,"elapsed":946,"user":{"displayName":"Ravi Pawar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBtV9qOP0yVjG8p6Z6OsNi6vPiuKPc2AKMnCJqR=s64","userId":"00073904877215451927"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%writefile labelmap.pbtxt\n","item {\n","  id: 1\n","  name: 'pdone'},\n","item {\n","  id: 2\n","  name: 'pdtwo'}"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Writing labelmap.pbtxt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4WWSzcS8NXlr","colab_type":"code","outputId":"fa86587f-78a9-44e3-d23d-6c8268696f43","executionInfo":{"status":"ok","timestamp":1567177551362,"user_tz":-330,"elapsed":3557,"user":{"displayName":"Ravi Pawar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBtV9qOP0yVjG8p6Z6OsNi6vPiuKPc2AKMnCJqR=s64","userId":"00073904877215451927"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["%pycat ssd_inception_v2_coco.config\n","!rm ssd_inception_v2_coco.config\n","\n"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Error: no such file, variable, URL, history range or macro\n","rm: cannot remove 'ssd_inception_v2_coco.config': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8xOHfClUcgdN","colab_type":"code","outputId":"c8aeb14f-ec3f-4a61-f3b8-866d662a5191","executionInfo":{"status":"ok","timestamp":1567197332800,"user_tz":-330,"elapsed":908,"user":{"displayName":"Ravi Pawar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBtV9qOP0yVjG8p6Z6OsNi6vPiuKPc2AKMnCJqR=s64","userId":"00073904877215451927"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd /root/models/research/object_detection"],"execution_count":47,"outputs":[{"output_type":"stream","text":["/root/models/research/object_detection\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8AxR2vIFMM0e","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":524},"outputId":"c8167304-3d99-420b-b942-fc7fac4a651c","executionInfo":{"status":"ok","timestamp":1567197344842,"user_tz":-330,"elapsed":8914,"user":{"displayName":"Ravi Pawar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBtV9qOP0yVjG8p6Z6OsNi6vPiuKPc2AKMnCJqR=s64","userId":"00073904877215451927"}}},"source":["#images\n","fileId = '1QbPObYUSb1kjpYckrDuY8djCDmkTRp_g'\n","\n","import os\n","from zipfile import ZipFile\n","from shutil import copy\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","fileName = fileId + '.zip'\n","downloaded = drive.CreateFile({'id': fileId})\n","downloaded.GetContentFile(fileName)\n","ds = ZipFile(fileName)\n","ds.extractall()\n","os.remove(fileName)\n","print('Extracted zip file ' + fileName)"],"execution_count":48,"outputs":[{"output_type":"stream","text":["W0830 20:35:37.524180 140041653114752 __init__.py:44] file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n","    from google.appengine.api import memcache\n","ModuleNotFoundError: No module named 'google.appengine'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n","    from oauth2client.contrib.locked_file import LockedFile\n","ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n","    from oauth2client.locked_file import LockedFile\n","ModuleNotFoundError: No module named 'oauth2client.locked_file'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n","    from . import file_cache\n","  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n","    'file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth')\n","ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n"],"name":"stderr"},{"output_type":"stream","text":["Extracted zip file 1QbPObYUSb1kjpYckrDuY8djCDmkTRp_g.zip\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dAKuaZrtdpS_","colab_type":"code","outputId":"acaa8def-1fcf-4d8a-b883-818554b52071","executionInfo":{"status":"ok","timestamp":1567197303399,"user_tz":-330,"elapsed":6496,"user":{"displayName":"Ravi Pawar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBtV9qOP0yVjG8p6Z6OsNi6vPiuKPc2AKMnCJqR=s64","userId":"00073904877215451927"}},"colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["!tar -xvf /root/models/research/object_detection/1RQZrRbFHsbGC-wX3xqiyMYCA0uAcjdk5.tar.gz"],"execution_count":46,"outputs":[{"output_type":"stream","text":["ssd_inception_v2_coco_2018_01_28/\n","ssd_inception_v2_coco_2018_01_28/model.ckpt.index\n","ssd_inception_v2_coco_2018_01_28/checkpoint\n","ssd_inception_v2_coco_2018_01_28/pipeline.config\n","ssd_inception_v2_coco_2018_01_28/model.ckpt.data-00000-of-00001\n","ssd_inception_v2_coco_2018_01_28/model.ckpt.meta\n","ssd_inception_v2_coco_2018_01_28/saved_model/\n","ssd_inception_v2_coco_2018_01_28/saved_model/saved_model.pb\n","ssd_inception_v2_coco_2018_01_28/saved_model/variables/\n","ssd_inception_v2_coco_2018_01_28/frozen_inference_graph.pb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ht_hu01neP8N","colab_type":"code","outputId":"ca72d484-743f-48fb-8472-83e617da86f2","executionInfo":{"status":"ok","timestamp":1567180699173,"user_tz":-330,"elapsed":963,"user":{"displayName":"Ravi Pawar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBtV9qOP0yVjG8p6Z6OsNi6vPiuKPc2AKMnCJqR=s64","userId":"00073904877215451927"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd /root/models/research/object_detection/training"],"execution_count":27,"outputs":[{"output_type":"stream","text":["/root/models/research/object_detection/training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PcVWb2HEOfoI","colab_type":"code","outputId":"3d1e36ce-64da-4a2f-98e4-92b9c8ae68ba","executionInfo":{"status":"ok","timestamp":1567180741251,"user_tz":-330,"elapsed":962,"user":{"displayName":"Ravi Pawar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBtV9qOP0yVjG8p6Z6OsNi6vPiuKPc2AKMnCJqR=s64","userId":"00073904877215451927"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%writefile ssd_inception_v2_coco.config\n","\n","# SSD with Inception v2 configuration for MSCOCO Dataset.\n","# Users should configure the fine_tune_checkpoint field in the train config as\n","# well as the label_map_path and input_path fields in the train_input_reader and\n","# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n","# should be configured.\n","\n","model {\n","  ssd {\n","    num_classes: 2\n","    box_coder {\n","      faster_rcnn_box_coder {\n","        y_scale: 10.0\n","        x_scale: 10.0\n","        height_scale: 5.0\n","        width_scale: 5.0\n","      }\n","    }\n","    matcher {\n","      argmax_matcher {\n","        matched_threshold: 0.5\n","        unmatched_threshold: 0.5\n","        ignore_thresholds: false\n","        negatives_lower_than_unmatched: true\n","        force_match_for_each_row: true\n","      }\n","    }\n","    similarity_calculator {\n","      iou_similarity {\n","      }\n","    }\n","    anchor_generator {\n","      ssd_anchor_generator {\n","        num_layers: 6\n","        min_scale: 0.2\n","        max_scale: 0.95\n","        aspect_ratios: 1.0\n","        aspect_ratios: 2.0\n","        aspect_ratios: 0.5\n","        aspect_ratios: 3.0\n","        aspect_ratios: 0.3333\n","        reduce_boxes_in_lowest_layer: true\n","      }\n","    }\n","    image_resizer {\n","      fixed_shape_resizer {\n","        height: 512\n","        width: 512\n","      }\n","    }\n","    box_predictor {\n","      convolutional_box_predictor {\n","        min_depth: 0\n","        max_depth: 0\n","        num_layers_before_predictor: 0\n","        use_dropout: false\n","        dropout_keep_probability: 0.8\n","        kernel_size: 3\n","        box_code_size: 4\n","        apply_sigmoid_to_scores: false\n","        conv_hyperparams {\n","          activation: RELU_6,\n","          regularizer {\n","            l2_regularizer {\n","              weight: 0.00004\n","            }\n","          }\n","          initializer {\n","            truncated_normal_initializer {\n","              stddev: 0.03\n","              mean: 0.0\n","            }\n","          }\n","        }\n","      }\n","    }\n","    feature_extractor {\n","      type: 'ssd_inception_v2'\n","      min_depth: 16\n","      depth_multiplier: 1.0\n","      conv_hyperparams {\n","        activation: RELU_6,\n","        regularizer {\n","          l2_regularizer {\n","            weight: 0.00004\n","          }\n","        }\n","        initializer {\n","          truncated_normal_initializer {\n","            stddev: 0.03\n","            mean: 0.0\n","          }\n","        }\n","        batch_norm {\n","          train: true,\n","          scale: true,\n","          center: true,\n","          decay: 0.9997,\n","          epsilon: 0.001,\n","        }\n","      }\n","      override_base_feature_extractor_hyperparams: true\n","    }\n","    loss {\n","      classification_loss {\n","        weighted_sigmoid {\n","        }\n","      }\n","      localization_loss {\n","        weighted_smooth_l1 {\n","        }\n","      }\n","      hard_example_miner {\n","        num_hard_examples: 3000\n","        iou_threshold: 0.99\n","        loss_type: CLASSIFICATION\n","        max_negatives_per_positive: 3\n","        min_negatives_per_image: 0\n","      }\n","      classification_weight: 1.0\n","      localization_weight: 1.0\n","    }\n","    normalize_loss_by_num_matches: true\n","    post_processing {\n","      batch_non_max_suppression {\n","        score_threshold: 1e-8\n","        iou_threshold: 0.6\n","        max_detections_per_class: 100\n","        max_total_detections: 100\n","      }\n","      score_converter: SIGMOID\n","    }\n","  }\n","}\n","\n","train_config: {\n","  batch_size: 8\n","  optimizer {\n","    rms_prop_optimizer: {\n","      learning_rate: {\n","        exponential_decay_learning_rate {\n","          initial_learning_rate: 0.0001\n","          decay_steps: 800720\n","          decay_factor: 0.95\n","        }\n","      }\n","      momentum_optimizer_value: 0.9\n","      decay: 0.9\n","      epsilon: 1.0\n","    }\n","  }\n","  fine_tune_checkpoint: \"/root/models/research/object_detection/ssd_inception_v2_coco_2018_01_28/model.ckpt\"\n","  from_detection_checkpoint: true\n","  # Note: The below line limits the training process to 200K steps, which we\n","  # empirically found to be sufficient enough to train the pets dataset. This\n","  # effectively bypasses the learning rate schedule (the learning rate will\n","  # never decay). Remove the below line to train indefinitely.\n","  num_steps: 10000\n","  data_augmentation_options {\n","    random_horizontal_flip {\n","    }\n","  }\n","  data_augmentation_options {\n","    ssd_random_crop {\n","    }\n","  }\n","}\n","\n","train_input_reader: {\n","  tf_record_input_reader {\n","    input_path: \"/root/models/research/object_detection/train.record\"\n","  }\n","  label_map_path: \"/root/models/research/object_detection/training/labelmap.pbtxt\"\n","}\n","\n","eval_config: {\n","  num_examples: 72\n","  # Note: The below line limits the evaluation process to 10 evaluations.\n","  # Remove the below line to evaluate indefinitely.\n","  max_evals: 10\n","}\n","\n","eval_input_reader: {\n","  tf_record_input_reader {\n","    input_path: \"/root/models/research/object_detection/test.record\"\n","  }\n","  label_map_path: \"/root/models/research/object_detection/training/labelmap.pbtxt\"\n","  shuffle: false\n","  num_readers: 1\n","}"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Writing ssd_inception_v2_coco.config\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WCdKZUbKIk2_","colab_type":"code","outputId":"d8ffb908-9698-4583-e3c1-b6ead62604c5","executionInfo":{"status":"ok","timestamp":1567193222059,"user_tz":-330,"elapsed":12116371,"user":{"displayName":"Ravi Pawar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBtV9qOP0yVjG8p6Z6OsNi6vPiuKPc2AKMnCJqR=s64","userId":"00073904877215451927"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#model training\n","%cd ~/models/research/object_detection\n","!python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_inception_v2_coco.config"],"execution_count":29,"outputs":[{"output_type":"stream","text":["/root/models/research/object_detection\n","WARNING: Logging before flag parsing goes to stderr.\n","W0830 15:59:14.310707 139622356395904 lazy_loader.py:50] \n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","W0830 15:59:14.563528 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/slim-0.1-py3.6.egg/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","W0830 15:59:14.610860 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/slim-0.1-py3.6.egg/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n","\n","W0830 15:59:14.631352 139622356395904 deprecation_wrapper.py:119] From train.py:55: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n","\n","W0830 15:59:14.631566 139622356395904 deprecation_wrapper.py:119] From train.py:55: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n","\n","W0830 15:59:14.632130 139622356395904 deprecation_wrapper.py:119] From train.py:184: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n","\n","W0830 15:59:14.632571 139622356395904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/absl/app.py:251: main (from __main__) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use object_detection/model_main.py.\n","W0830 15:59:14.632713 139622356395904 deprecation_wrapper.py:119] From train.py:90: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n","\n","W0830 15:59:14.633074 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","W0830 15:59:14.637588 139622356395904 deprecation_wrapper.py:119] From train.py:95: The name tf.gfile.Copy is deprecated. Please use tf.io.gfile.copy instead.\n","\n","W0830 15:59:14.640550 139622356395904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:266: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please switch to tf.train.create_global_step\n","W0830 15:59:14.645491 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n","\n","W0830 15:59:14.645753 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n","\n","W0830 15:59:14.661407 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n","\n","W0830 15:59:14.667799 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:71: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n","\n","W0830 15:59:14.667926 139622356395904 dataset_builder.py:72] num_readers has been reduced to 1 to match input file shards.\n","W0830 15:59:14.675045 139622356395904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.experimental.parallel_interleave(...)`.\n","W0830 15:59:14.675194 139622356395904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n","W0830 15:59:14.703876 139622356395904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.map()\n","W0830 15:59:14.931476 139622356395904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:43: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n","W0830 15:59:14.938683 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:44: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n","\n","W0830 15:59:14.945320 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:626: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","W0830 15:59:15.009705 139622356395904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:196: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n","W0830 15:59:15.022834 139622356395904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/box_list_ops.py:206: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","W0830 15:59:16.005654 139622356395904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/batcher.py:101: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n","W0830 15:59:16.010134 139622356395904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:753: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","To construct input pipelines, use the `tf.data` module.\n","W0830 15:59:16.011336 139622356395904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:753: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","To construct input pipelines, use the `tf.data` module.\n","W0830 15:59:16.021106 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/prefetcher.py:58: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n","\n","W0830 15:59:16.356292 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2660: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n","\n","W0830 15:59:16.482802 139622356395904 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","W0830 15:59:21.903886 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n","\n","I0830 15:59:21.904180 139622356395904 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n","I0830 15:59:21.946636 139622356395904 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n","I0830 15:59:21.988276 139622356395904 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n","I0830 15:59:22.028571 139622356395904 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n","I0830 15:59:22.069878 139622356395904 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n","I0830 15:59:22.117126 139622356395904 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n","W0830 15:59:23.906431 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n","\n","W0830 15:59:23.907973 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n","\n","W0830 15:59:24.219295 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:208: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.\n","\n","W0830 15:59:24.220198 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:95: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n","\n","W0830 15:59:24.220401 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/learning_schedules.py:66: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n","\n","W0830 15:59:24.231909 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:47: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n","\n","W0830 15:59:27.055743 139622356395904 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","W0830 15:59:30.166773 139622356395904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","W0830 15:59:36.760588 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:353: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n","\n","W0830 15:59:37.262126 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:355: The name tf.losses.get_losses is deprecated. Please use tf.compat.v1.losses.get_losses instead.\n","\n","W0830 15:59:37.264955 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:359: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.\n","\n","W0830 15:59:37.270234 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:368: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n","\n","W0830 15:59:37.281659 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:376: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n","\n","W0830 15:59:38.421723 139622356395904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/variables_helper.py:139: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n","\n","W0830 15:59:38.424746 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.424904 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.425045 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.425174 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.425258 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.425356 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.425461 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.425552 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/depthwise_weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.425652 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/depthwise_weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.425773 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.425886 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.425989 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.426095 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.426182 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.426265 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.426354 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.426451 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.426535 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.426634 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_2b_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.426719 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_2b_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.426802 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_2b_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.426892 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.426986 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.427080 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.427200 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.427295 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.427390 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.427491 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.427575 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.427655 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.427743 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.427825 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.427906 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.428003 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.428116 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.428201 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.428314 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.428432 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.428532 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.428633 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.428715 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.428812 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.428921 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.429005 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.429114 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.429219 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.429318 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.429402 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.429492 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.429575 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.429662 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.429750 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.429841 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.429931 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.430080 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.430167 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.430249 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.430339 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.430422 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.430505 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.430592 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.430674 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.430761 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.430858 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.430947 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.431043 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.431145 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.431225 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.431308 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.431393 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.431504 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.431615 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.431714 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.431797 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.431895 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.432019 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.432119 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.432201 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.432290 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.432371 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.432453 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.432551 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.432634 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.432714 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.432801 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.432883 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.508925 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.509190 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.509306 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.509414 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.509544 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.509665 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.509792 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.509971 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.510114 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.510227 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.510366 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.510478 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.510585 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.510715 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.510848 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.510984 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.511152 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.511278 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.511389 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.511522 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.511630 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.511737 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.511866 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.511988 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.512115 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.512253 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.512365 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.512476 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.512603 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.512726 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.512838 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.512980 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.513127 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.513265 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.513388 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.513498 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.513618 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.513733 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.513842 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.513961 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.514104 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.514216 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.514324 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.514441 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.514548 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.514655 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.514778 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.514888 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.515019 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.515185 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.515311 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.515486 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.515615 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.515742 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.515895 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.516033 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.516167 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.516279 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.516421 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.516531 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.516634 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.516755 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.516864 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.516981 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.517134 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.517261 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.517366 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.517509 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.517620 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.517725 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.517838 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.517952 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.518077 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.518234 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.518356 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.518510 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.518666 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.518773 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.518877 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.519013 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.519142 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.519246 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.519359 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.519500 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.519623 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.519766 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.519874 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.520006 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.520144 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.520254 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.520364 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.520522 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.520656 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.520773 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.520933 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.521044 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.521189 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.521306 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.521414 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.521538 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.521653 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.521761 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.521867 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.522003 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.522131 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.522241 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.522356 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.522463 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.522569 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.522686 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.522798 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.522902 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.523039 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.523184 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.523290 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.523404 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.523511 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.523618 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.523731 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.523837 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.524019 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.524171 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.524311 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.524458 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.524577 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.524689 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.524811 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.524933 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.525043 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.525171 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.525331 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.525441 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.525547 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.525662 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.525771 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.525885 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.526013 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.526141 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.526248 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.526376 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.526522 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.526631 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.526749 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.526864 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.526997 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.527135 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.527246 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.527355 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.527489 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.527598 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.527707 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.527825 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.527942 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.528069 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.528188 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.528297 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.528404 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.528532 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.528642 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.528756 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.528873 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.529005 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.529155 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.529287 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.529401 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.529540 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.529727 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.529854 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.529972 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.530126 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.530237 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.530344 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.530463 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.530592 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.530713 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.530841 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.530966 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.531107 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.531224 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.531349 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.531462 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.531581 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.531692 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.531800 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.531967 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.532112 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.532237 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.532358 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.532489 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.532596 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.532712 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.532824 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.532942 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.533104 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.533216 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.533324 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.533441 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.533549 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.533650 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.533766 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.533874 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.533991 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.534157 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.534271 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.534379 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.534496 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.534607 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.534717 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.534849 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.534984 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.535116 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.535263 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.535430 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.535560 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.535685 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.535793 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.535925 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.536045 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.536175 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.536282 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.536415 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.536556 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.536664 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.536780 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.536904 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.537035 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.537216 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.537356 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.537482 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.537613 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.537724 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.537833 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.537960 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.538127 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.538236 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.538355 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.538467 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.538576 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.538703 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.538831 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.538945 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.539101 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.539214 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.539325 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.539444 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.539554 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.539662 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.539791 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.539904 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.540023 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.540165 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.540277 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.540386 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.540504 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.540614 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.540724 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.540862 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.540984 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.541115 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.541236 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.541343 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.541452 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.541572 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.541685 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.541792 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.541970 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.542109 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.542218 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.542335 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.542446 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.542555 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.542672 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.542782 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.542975 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.543139 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.543277 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.543380 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.543516 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.543627 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.543735 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.543884 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.544006 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.544134 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.544264 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.544377 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.544497 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.544614 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.544722 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.544832 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.544960 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.545092 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.545202 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.545376 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.545484 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.545636 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.545759 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.545870 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.545990 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.546143 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.546285 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.546455 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.546585 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.546710 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.546814 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.546939 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.547044 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.547178 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.547340 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.547446 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.547549 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.547706 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.547819 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.547991 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.548157 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.548285 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.548424 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.548598 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.548737 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.548859 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.548994 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.549118 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.549253 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.549370 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.549478 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.549579 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.549724 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.549836 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.549989 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.550152 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.550264 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.550371 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.550486 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.550609 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.550734 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.550848 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.550965 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.551099 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.551226 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.551335 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.551447 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.551562 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.551668 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.551849 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.551975 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.552101 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.552210 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.552336 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.552445 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.552582 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.552723 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.552831 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.552968 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.553100 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.553236 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.553358 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.553488 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.553598 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.553705 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.553862 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.553997 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.554127 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.554245 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.554354 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.554465 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.554594 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.554709 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.554825 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.554958 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.555088 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.555200 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.555318 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.555430 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.555542 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.555671 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.555784 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.555893 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.556020 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.556152 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.556260 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.556378 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.556497 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.556605 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.556735 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.556846 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.556973 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.557124 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.557235 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.557342 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.557464 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.557574 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.557683 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.557809 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.557927 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.558037 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.558173 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.558281 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.558387 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.558515 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.558652 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.558773 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.558898 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.559021 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.559148 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.559276 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.559385 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.559489 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.559601 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.559705 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.559811 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.559989 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.560132 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.560238 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.560360 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.560467 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.560570 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.560682 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.560806 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.560924 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.561102 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.561213 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.561317 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.561426 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.561543 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.561648 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.561776 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.561878 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.561989 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.562130 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.562237 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.562451 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.562583 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.562692 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.562810 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.562932 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.563071 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.563224 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.563360 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.563497 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.563617 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.563741 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.563904 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.564087 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.564250 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.564359 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.564500 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.564671 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.564793 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.564894 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.565019 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.565195 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.565304 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.565422 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.565535 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.565647 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.565777 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.565927 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.566038 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.566179 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.566303 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.566473 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.566593 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.566715 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.566834 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.566967 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.567090 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.567198 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.567313 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.567421 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.567528 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.567642 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.567764 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.567885 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.568036 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.568184 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.568295 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.568428 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.568536 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.568643 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.568760 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.568868 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.568985 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.569146 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.569254 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.569358 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.569473 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.569580 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.569716 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.569827 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.569940 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.570063 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.570192 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.570298 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.570435 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.570587 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.570694 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.570801 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.570928 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.571036 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.571194 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.571319 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.571428 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.571535 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.571647 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.571784 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.571923 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.572042 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.572172 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.572280 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.572407 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.572520 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.572624 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.572739 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.572847 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.572962 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.573092 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.573217 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.573322 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.573443 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.573551 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.573656 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.573769 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.573874 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.574003 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.574172 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.574281 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.574400 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.574525 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.574633 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.574736 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.574856 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.574975 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.575096 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.575225 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.575361 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.575482 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.575606 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.575726 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.575833 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.575975 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_2_1x1_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.576118 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_2_1x1_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.576224 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_2_1x1_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.576341 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_2_1x1_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.576449 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_2_1x1_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.576565 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_2_1x1_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.576689 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_2_1x1_256/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.576799 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_2_1x1_256/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.576906 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_2_1x1_256/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.577044 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_3_1x1_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.577171 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_3_1x1_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.577277 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_3_1x1_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.577391 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_3_1x1_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.577496 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_3_1x1_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.577630 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_3_1x1_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.577757 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_3_1x1_128/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.577864 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_3_1x1_128/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.577978 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_3_1x1_128/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.578111 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_4_1x1_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.578250 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_4_1x1_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.578387 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_4_1x1_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.578532 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_4_1x1_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.578639 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_4_1x1_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.578743 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_4_1x1_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.578863 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_4_1x1_128/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.578979 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_4_1x1_128/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.579104 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_4_1x1_128/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.579220 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_5_1x1_64/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.579324 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_5_1x1_64/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.579429 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_5_1x1_64/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.579543 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_5_1x1_64/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.579648 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_5_1x1_64/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.579753 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_5_1x1_64/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.579878 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_5_1x1_64/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.579997 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_5_1x1_64/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.580121 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_5_1x1_64/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.580235 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.580342 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.580446 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.580556 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.580692 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.580787 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.580904 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_2_3x3_s2_512/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.581021 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_2_3x3_s2_512/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.581144 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_2_3x3_s2_512/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.581256 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.581376 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.581479 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.581588 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.581690 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.581810 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.581942 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_3_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.582077 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_3_3x3_s2_256/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.582185 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_3_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.582296 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.582400 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.582503 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.582629 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.582748 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.582849 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.582977 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_4_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.583116 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_4_3x3_s2_256/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.583236 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_4_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.583393 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.583531 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n","W0830 15:59:38.583651 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.583776 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.583876 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n","W0830 15:59:38.583987 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n","W0830 15:59:38.584196 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_5_3x3_s2_128/weights/ExponentialMovingAverage] is not available in checkpoint\n","W0830 15:59:38.584336 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_5_3x3_s2_128/weights/RMSProp] is not available in checkpoint\n","W0830 15:59:38.584457 139622356395904 variables_helper.py:157] Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_5_3x3_s2_128/weights/RMSProp_1] is not available in checkpoint\n","W0830 15:59:40.195269 139622356395904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:742: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please switch to tf.train.MonitoredTrainingSession\n","2019-08-30 15:59:41.691454: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n","2019-08-30 15:59:41.693159: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xfd401c0 executing computations on platform Host. Devices:\n","2019-08-30 15:59:41.693193: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n","2019-08-30 15:59:41.699953: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n","2019-08-30 15:59:41.855124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 15:59:41.856003: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xfd40380 executing computations on platform CUDA. Devices:\n","2019-08-30 15:59:41.856031: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n","2019-08-30 15:59:41.857297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 15:59:41.857968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n","pciBusID: 0000:00:04.0\n","2019-08-30 15:59:41.874281: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2019-08-30 15:59:42.043020: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2019-08-30 15:59:42.127766: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2019-08-30 15:59:42.156347: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2019-08-30 15:59:42.391551: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2019-08-30 15:59:42.508090: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2019-08-30 15:59:42.888665: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2019-08-30 15:59:42.888977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 15:59:42.889808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 15:59:42.890593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2019-08-30 15:59:42.895263: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2019-08-30 15:59:42.896939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2019-08-30 15:59:42.896973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2019-08-30 15:59:42.896990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2019-08-30 15:59:42.899329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 15:59:42.900223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 15:59:42.900968: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2019-08-30 15:59:42.901041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n","2019-08-30 15:59:46.887205: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n","W0830 15:59:47.654263 139622356395904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","I0830 15:59:47.655426 139622356395904 saver.py:1280] Restoring parameters from /root/models/research/object_detection/ssd_inception_v2_coco_2018_01_28/model.ckpt\n","I0830 15:59:48.329351 139622356395904 session_manager.py:500] Running local_init_op.\n","I0830 15:59:48.799148 139622356395904 session_manager.py:502] Done running local_init_op.\n","I0830 15:59:59.340376 139622356395904 learning.py:754] Starting Session.\n","I0830 15:59:59.645530 139619316770560 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n","I0830 15:59:59.652250 139622356395904 learning.py:768] Starting Queues.\n","I0830 16:00:10.503692 139619308377856 supervisor.py:1099] global_step/sec: 0\n","2019-08-30 16:00:13.375230: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","I0830 16:00:19.814451 139619299985152 supervisor.py:1050] Recording summary at step 0.\n","I0830 16:00:22.428138 139622356395904 learning.py:507] global step 1: loss = 12.1596 (22.447 sec/step)\n","I0830 16:00:23.960500 139622356395904 learning.py:507] global step 2: loss = 10.6802 (1.213 sec/step)\n","I0830 16:00:25.176795 139622356395904 learning.py:507] global step 3: loss = 10.9515 (1.214 sec/step)\n","I0830 16:00:26.402977 139622356395904 learning.py:507] global step 4: loss = 13.1568 (1.224 sec/step)\n","I0830 16:00:27.581806 139622356395904 learning.py:507] global step 5: loss = 10.5727 (1.177 sec/step)\n","I0830 16:00:28.785096 139622356395904 learning.py:507] global step 6: loss = 12.4286 (1.201 sec/step)\n","I0830 16:00:30.024933 139622356395904 learning.py:507] global step 7: loss = 11.6453 (1.238 sec/step)\n","I0830 16:00:31.262462 139622356395904 learning.py:507] global step 8: loss = 11.6249 (1.235 sec/step)\n","I0830 16:00:32.500231 139622356395904 learning.py:507] global step 9: loss = 12.9048 (1.236 sec/step)\n","I0830 16:00:33.700778 139622356395904 learning.py:507] global step 10: loss = 11.1250 (1.199 sec/step)\n","I0830 16:00:34.910706 139622356395904 learning.py:507] global step 11: loss = 12.2715 (1.208 sec/step)\n","I0830 16:00:36.113545 139622356395904 learning.py:507] global step 12: loss = 10.6768 (1.201 sec/step)\n","I0830 16:00:37.320213 139622356395904 learning.py:507] global step 13: loss = 12.4908 (1.205 sec/step)\n","I0830 16:00:38.547605 139622356395904 learning.py:507] global step 14: loss = 12.1250 (1.225 sec/step)\n","I0830 16:00:39.738934 139622356395904 learning.py:507] global step 15: loss = 12.5825 (1.189 sec/step)\n","I0830 16:00:40.972451 139622356395904 learning.py:507] global step 16: loss = 11.1946 (1.232 sec/step)\n","I0830 16:00:42.173931 139622356395904 learning.py:507] global step 17: loss = 10.5419 (1.199 sec/step)\n","I0830 16:00:43.413501 139622356395904 learning.py:507] global step 18: loss = 11.4395 (1.237 sec/step)\n","I0830 16:00:44.658807 139622356395904 learning.py:507] global step 19: loss = 11.2273 (1.243 sec/step)\n","I0830 16:00:45.878684 139622356395904 learning.py:507] global step 20: loss = 11.2697 (1.218 sec/step)\n","I0830 16:00:47.097800 139622356395904 learning.py:507] global step 21: loss = 12.3021 (1.217 sec/step)\n","I0830 16:00:48.319826 139622356395904 learning.py:507] global step 22: loss = 11.0539 (1.220 sec/step)\n","I0830 16:00:49.521406 139622356395904 learning.py:507] global step 23: loss = 10.7519 (1.200 sec/step)\n","I0830 16:00:50.709190 139622356395904 learning.py:507] global step 24: loss = 13.2793 (1.185 sec/step)\n","I0830 16:00:51.956628 139622356395904 learning.py:507] global step 25: loss = 11.6975 (1.245 sec/step)\n","I0830 16:00:53.179130 139622356395904 learning.py:507] global step 26: loss = 11.4995 (1.221 sec/step)\n","I0830 16:00:54.390368 139622356395904 learning.py:507] global step 27: loss = 12.8101 (1.209 sec/step)\n","I0830 16:00:55.617285 139622356395904 learning.py:507] global step 28: loss = 12.2679 (1.225 sec/step)\n","I0830 16:00:56.867814 139622356395904 learning.py:507] global step 29: loss = 11.6391 (1.248 sec/step)\n","I0830 16:00:58.078548 139622356395904 learning.py:507] global step 30: loss = 11.4850 (1.209 sec/step)\n","I0830 16:00:59.311918 139622356395904 learning.py:507] global step 31: loss = 11.4737 (1.232 sec/step)\n","I0830 16:01:00.528934 139622356395904 learning.py:507] global step 32: loss = 11.5771 (1.215 sec/step)\n","I0830 16:01:01.757482 139622356395904 learning.py:507] global step 33: loss = 11.1086 (1.227 sec/step)\n","I0830 16:01:03.000687 139622356395904 learning.py:507] global step 34: loss = 11.6282 (1.241 sec/step)\n","I0830 16:01:04.200735 139622356395904 learning.py:507] global step 35: loss = 11.8757 (1.198 sec/step)\n","I0830 16:01:05.437217 139622356395904 learning.py:507] global step 36: loss = 12.2228 (1.234 sec/step)\n","I0830 16:01:06.677437 139622356395904 learning.py:507] global step 37: loss = 11.1833 (1.238 sec/step)\n","I0830 16:01:07.895796 139622356395904 learning.py:507] global step 38: loss = 10.9755 (1.217 sec/step)\n","I0830 16:01:09.101653 139622356395904 learning.py:507] global step 39: loss = 11.5546 (1.204 sec/step)\n","I0830 16:01:10.331001 139622356395904 learning.py:507] global step 40: loss = 12.8098 (1.228 sec/step)\n","I0830 16:01:11.558275 139622356395904 learning.py:507] global step 41: loss = 12.2928 (1.225 sec/step)\n","I0830 16:01:12.786081 139622356395904 learning.py:507] global step 42: loss = 12.7786 (1.225 sec/step)\n","I0830 16:01:13.998178 139622356395904 learning.py:507] global step 43: loss = 11.7807 (1.210 sec/step)\n","I0830 16:01:15.210399 139622356395904 learning.py:507] global step 44: loss = 11.3017 (1.210 sec/step)\n","I0830 16:01:16.442154 139622356395904 learning.py:507] global step 45: loss = 10.8582 (1.230 sec/step)\n","I0830 16:01:17.644970 139622356395904 learning.py:507] global step 46: loss = 11.2320 (1.201 sec/step)\n","I0830 16:01:18.891631 139622356395904 learning.py:507] global step 47: loss = 11.3880 (1.245 sec/step)\n","I0830 16:01:20.081101 139622356395904 learning.py:507] global step 48: loss = 11.8117 (1.188 sec/step)\n","I0830 16:01:21.286178 139622356395904 learning.py:507] global step 49: loss = 11.4789 (1.203 sec/step)\n","I0830 16:01:22.499884 139622356395904 learning.py:507] global step 50: loss = 12.3575 (1.211 sec/step)\n","I0830 16:01:23.738928 139622356395904 learning.py:507] global step 51: loss = 11.6080 (1.237 sec/step)\n","I0830 16:01:24.970428 139622356395904 learning.py:507] global step 52: loss = 11.7808 (1.229 sec/step)\n","I0830 16:01:26.217593 139622356395904 learning.py:507] global step 53: loss = 11.7016 (1.245 sec/step)\n","I0830 16:01:27.417893 139622356395904 learning.py:507] global step 54: loss = 12.1084 (1.199 sec/step)\n","I0830 16:01:28.630786 139622356395904 learning.py:507] global step 55: loss = 11.9023 (1.211 sec/step)\n","I0830 16:01:29.844681 139622356395904 learning.py:507] global step 56: loss = 11.8569 (1.212 sec/step)\n","I0830 16:01:31.094479 139622356395904 learning.py:507] global step 57: loss = 12.1271 (1.248 sec/step)\n","I0830 16:01:32.328158 139622356395904 learning.py:507] global step 58: loss = 12.1071 (1.232 sec/step)\n","I0830 16:01:33.552942 139622356395904 learning.py:507] global step 59: loss = 12.5231 (1.223 sec/step)\n","I0830 16:01:34.788863 139622356395904 learning.py:507] global step 60: loss = 12.9491 (1.234 sec/step)\n","I0830 16:01:36.013861 139622356395904 learning.py:507] global step 61: loss = 12.7503 (1.223 sec/step)\n","I0830 16:01:37.261042 139622356395904 learning.py:507] global step 62: loss = 12.5554 (1.245 sec/step)\n","I0830 16:01:38.530536 139622356395904 learning.py:507] global step 63: loss = 12.6218 (1.268 sec/step)\n","I0830 16:01:39.756300 139622356395904 learning.py:507] global step 64: loss = 12.9492 (1.224 sec/step)\n","I0830 16:01:40.961505 139622356395904 learning.py:507] global step 65: loss = 11.9180 (1.203 sec/step)\n","I0830 16:01:42.181773 139622356395904 learning.py:507] global step 66: loss = 12.8950 (1.218 sec/step)\n","I0830 16:01:43.382438 139622356395904 learning.py:507] global step 67: loss = 12.6758 (1.199 sec/step)\n","I0830 16:01:44.597581 139622356395904 learning.py:507] global step 68: loss = 12.4109 (1.213 sec/step)\n","I0830 16:01:45.860321 139622356395904 learning.py:507] global step 69: loss = 12.6598 (1.261 sec/step)\n","I0830 16:01:47.080340 139622356395904 learning.py:507] global step 70: loss = 12.4428 (1.218 sec/step)\n","I0830 16:01:48.320156 139622356395904 learning.py:507] global step 71: loss = 12.6373 (1.238 sec/step)\n","I0830 16:01:49.553961 139622356395904 learning.py:507] global step 72: loss = 13.1789 (1.232 sec/step)\n","I0830 16:01:50.786750 139622356395904 learning.py:507] global step 73: loss = 12.1556 (1.231 sec/step)\n","I0830 16:01:51.999942 139622356395904 learning.py:507] global step 74: loss = 12.3087 (1.211 sec/step)\n","I0830 16:01:53.253116 139622356395904 learning.py:507] global step 75: loss = 13.0169 (1.251 sec/step)\n","I0830 16:01:54.496246 139622356395904 learning.py:507] global step 76: loss = 12.5905 (1.239 sec/step)\n","I0830 16:01:55.726852 139622356395904 learning.py:507] global step 77: loss = 12.9448 (1.228 sec/step)\n","I0830 16:01:56.926579 139622356395904 learning.py:507] global step 78: loss = 12.7365 (1.198 sec/step)\n","I0830 16:01:58.193097 139622356395904 learning.py:507] global step 79: loss = 13.3372 (1.265 sec/step)\n","I0830 16:01:59.423265 139622356395904 learning.py:507] global step 80: loss = 13.1057 (1.228 sec/step)\n","I0830 16:02:01.670534 139619299985152 supervisor.py:1050] Recording summary at step 81.\n","I0830 16:02:01.702486 139622356395904 learning.py:507] global step 81: loss = 12.5765 (2.277 sec/step)\n","I0830 16:02:02.438861 139619308377856 supervisor.py:1099] global_step/sec: 0.723633\n","I0830 16:02:02.920368 139622356395904 learning.py:507] global step 82: loss = 12.4734 (1.216 sec/step)\n","I0830 16:02:04.158777 139622356395904 learning.py:507] global step 83: loss = 12.6630 (1.236 sec/step)\n","I0830 16:02:05.387461 139622356395904 learning.py:507] global step 84: loss = 12.8306 (1.227 sec/step)\n","I0830 16:02:06.608364 139622356395904 learning.py:507] global step 85: loss = 12.9510 (1.219 sec/step)\n","I0830 16:02:07.814822 139622356395904 learning.py:507] global step 86: loss = 12.9114 (1.205 sec/step)\n","I0830 16:02:09.067817 139622356395904 learning.py:507] global step 87: loss = 12.5007 (1.251 sec/step)\n","I0830 16:02:10.286407 139622356395904 learning.py:507] global step 88: loss = 12.9814 (1.217 sec/step)\n","I0830 16:02:11.522668 139622356395904 learning.py:507] global step 89: loss = 12.3520 (1.235 sec/step)\n","I0830 16:02:12.753882 139622356395904 learning.py:507] global step 90: loss = 11.9825 (1.229 sec/step)\n","I0830 16:02:13.966398 139622356395904 learning.py:507] global step 91: loss = 13.1758 (1.211 sec/step)\n","I0830 16:02:15.237033 139622356395904 learning.py:507] global step 92: loss = 12.2330 (1.269 sec/step)\n","I0830 16:02:16.460980 139622356395904 learning.py:507] global step 93: loss = 12.3661 (1.222 sec/step)\n","I0830 16:02:17.684763 139622356395904 learning.py:507] global step 94: loss = 13.2065 (1.222 sec/step)\n","I0830 16:02:18.969118 139622356395904 learning.py:507] global step 95: loss = 13.6644 (1.282 sec/step)\n","I0830 16:02:20.211572 139622356395904 learning.py:507] global step 96: loss = 12.5705 (1.241 sec/step)\n","I0830 16:02:21.461514 139622356395904 learning.py:507] global step 97: loss = 12.4189 (1.248 sec/step)\n","I0830 16:02:22.666139 139622356395904 learning.py:507] global step 98: loss = 12.5628 (1.202 sec/step)\n","I0830 16:02:23.860570 139622356395904 learning.py:507] global step 99: loss = 12.5824 (1.192 sec/step)\n","I0830 16:02:25.069601 139622356395904 learning.py:507] global step 100: loss = 12.1583 (1.207 sec/step)\n","I0830 16:02:26.276745 139622356395904 learning.py:507] global step 101: loss = 12.4789 (1.205 sec/step)\n","I0830 16:02:27.513376 139622356395904 learning.py:507] global step 102: loss = 12.6667 (1.235 sec/step)\n","I0830 16:02:28.718724 139622356395904 learning.py:507] global step 103: loss = 12.8270 (1.203 sec/step)\n","I0830 16:02:29.915881 139622356395904 learning.py:507] global step 104: loss = 12.1444 (1.195 sec/step)\n","I0830 16:02:31.135902 139622356395904 learning.py:507] global step 105: loss = 12.9112 (1.218 sec/step)\n","I0830 16:02:32.341547 139622356395904 learning.py:507] global step 106: loss = 11.7805 (1.204 sec/step)\n","I0830 16:02:33.548115 139622356395904 learning.py:507] global step 107: loss = 12.0213 (1.205 sec/step)\n","I0830 16:02:34.753561 139622356395904 learning.py:507] global step 108: loss = 12.1805 (1.204 sec/step)\n","I0830 16:02:35.985929 139622356395904 learning.py:507] global step 109: loss = 12.0486 (1.231 sec/step)\n","I0830 16:02:37.204118 139622356395904 learning.py:507] global step 110: loss = 12.1158 (1.216 sec/step)\n","I0830 16:02:38.444216 139622356395904 learning.py:507] global step 111: loss = 11.8810 (1.238 sec/step)\n","I0830 16:02:39.698188 139622356395904 learning.py:507] global step 112: loss = 11.9195 (1.252 sec/step)\n","I0830 16:02:40.946011 139622356395904 learning.py:507] global step 113: loss = 11.9743 (1.246 sec/step)\n","I0830 16:02:42.150562 139622356395904 learning.py:507] global step 114: loss = 12.6983 (1.202 sec/step)\n","I0830 16:02:43.376988 139622356395904 learning.py:507] global step 115: loss = 12.2543 (1.224 sec/step)\n","I0830 16:02:44.612526 139622356395904 learning.py:507] global step 116: loss = 12.5437 (1.234 sec/step)\n","I0830 16:02:45.847718 139622356395904 learning.py:507] global step 117: loss = 12.7922 (1.233 sec/step)\n","I0830 16:02:47.069701 139622356395904 learning.py:507] global step 118: loss = 13.1525 (1.220 sec/step)\n","I0830 16:02:48.273691 139622356395904 learning.py:507] global step 119: loss = 13.2859 (1.202 sec/step)\n","I0830 16:02:49.511005 139622356395904 learning.py:507] global step 120: loss = 12.1906 (1.235 sec/step)\n","I0830 16:02:50.717871 139622356395904 learning.py:507] global step 121: loss = 12.7316 (1.205 sec/step)\n","I0830 16:02:51.944653 139622356395904 learning.py:507] global step 122: loss = 12.1840 (1.225 sec/step)\n","I0830 16:02:53.157935 139622356395904 learning.py:507] global step 123: loss = 12.2375 (1.212 sec/step)\n","I0830 16:02:54.371813 139622356395904 learning.py:507] global step 124: loss = 12.0352 (1.212 sec/step)\n","I0830 16:02:55.590845 139622356395904 learning.py:507] global step 125: loss = 12.1321 (1.217 sec/step)\n","I0830 16:02:56.805595 139622356395904 learning.py:507] global step 126: loss = 11.8789 (1.213 sec/step)\n","I0830 16:02:58.026568 139622356395904 learning.py:507] global step 127: loss = 11.4450 (1.219 sec/step)\n","I0830 16:02:59.259687 139622356395904 learning.py:507] global step 128: loss = 11.5920 (1.231 sec/step)\n","I0830 16:03:00.539679 139622356395904 learning.py:507] global step 129: loss = 11.5291 (1.278 sec/step)\n","I0830 16:03:01.772930 139622356395904 learning.py:507] global step 130: loss = 11.9179 (1.231 sec/step)\n","I0830 16:03:03.032680 139622356395904 learning.py:507] global step 131: loss = 11.7572 (1.258 sec/step)\n","I0830 16:03:04.232109 139622356395904 learning.py:507] global step 132: loss = 12.1984 (1.197 sec/step)\n","I0830 16:03:05.502429 139622356395904 learning.py:507] global step 133: loss = 11.8051 (1.269 sec/step)\n","I0830 16:03:06.740908 139622356395904 learning.py:507] global step 134: loss = 12.8471 (1.236 sec/step)\n","I0830 16:03:07.955241 139622356395904 learning.py:507] global step 135: loss = 11.7482 (1.212 sec/step)\n","I0830 16:03:09.206187 139622356395904 learning.py:507] global step 136: loss = 11.9861 (1.249 sec/step)\n","I0830 16:03:10.402914 139622356395904 learning.py:507] global step 137: loss = 11.6774 (1.195 sec/step)\n","I0830 16:03:11.601501 139622356395904 learning.py:507] global step 138: loss = 11.5462 (1.197 sec/step)\n","I0830 16:03:12.818569 139622356395904 learning.py:507] global step 139: loss = 11.7232 (1.215 sec/step)\n","I0830 16:03:14.088577 139622356395904 learning.py:507] global step 140: loss = 11.6618 (1.268 sec/step)\n","I0830 16:03:15.302414 139622356395904 learning.py:507] global step 141: loss = 11.2084 (1.212 sec/step)\n","I0830 16:03:16.516608 139622356395904 learning.py:507] global step 142: loss = 11.1532 (1.213 sec/step)\n","I0830 16:03:17.772168 139622356395904 learning.py:507] global step 143: loss = 11.4795 (1.254 sec/step)\n","I0830 16:03:19.091687 139622356395904 learning.py:507] global step 144: loss = 11.6989 (1.318 sec/step)\n","I0830 16:03:20.352836 139622356395904 learning.py:507] global step 145: loss = 11.8075 (1.256 sec/step)\n","I0830 16:03:21.545717 139622356395904 learning.py:507] global step 146: loss = 11.5263 (1.191 sec/step)\n","I0830 16:03:22.774390 139622356395904 learning.py:507] global step 147: loss = 11.6185 (1.227 sec/step)\n","I0830 16:03:23.963542 139622356395904 learning.py:507] global step 148: loss = 12.6073 (1.187 sec/step)\n","I0830 16:03:25.172913 139622356395904 learning.py:507] global step 149: loss = 11.1245 (1.208 sec/step)\n","I0830 16:03:26.400991 139622356395904 learning.py:507] global step 150: loss = 11.2884 (1.223 sec/step)\n","I0830 16:03:27.612760 139622356395904 learning.py:507] global step 151: loss = 11.2483 (1.210 sec/step)\n","I0830 16:03:28.853930 139622356395904 learning.py:507] global step 152: loss = 11.6993 (1.239 sec/step)\n","I0830 16:03:30.058217 139622356395904 learning.py:507] global step 153: loss = 11.3468 (1.202 sec/step)\n","I0830 16:03:31.290152 139622356395904 learning.py:507] global step 154: loss = 11.2658 (1.230 sec/step)\n","I0830 16:03:32.507946 139622356395904 learning.py:507] global step 155: loss = 11.7920 (1.216 sec/step)\n","I0830 16:03:33.751801 139622356395904 learning.py:507] global step 156: loss = 11.7551 (1.242 sec/step)\n","I0830 16:03:34.986573 139622356395904 learning.py:507] global step 157: loss = 11.6991 (1.233 sec/step)\n","I0830 16:03:36.187354 139622356395904 learning.py:507] global step 158: loss = 11.1703 (1.199 sec/step)\n","I0830 16:03:37.429501 139622356395904 learning.py:507] global step 159: loss = 11.3766 (1.240 sec/step)\n","I0830 16:03:38.678658 139622356395904 learning.py:507] global step 160: loss = 10.9062 (1.247 sec/step)\n","I0830 16:03:39.911970 139622356395904 learning.py:507] global step 161: loss = 11.0813 (1.231 sec/step)\n","I0830 16:03:41.097084 139622356395904 learning.py:507] global step 162: loss = 11.0122 (1.183 sec/step)\n","I0830 16:03:42.351422 139622356395904 learning.py:507] global step 163: loss = 11.5966 (1.252 sec/step)\n","I0830 16:03:43.608689 139622356395904 learning.py:507] global step 164: loss = 11.7148 (1.255 sec/step)\n","I0830 16:03:44.849962 139622356395904 learning.py:507] global step 165: loss = 11.1670 (1.239 sec/step)\n","I0830 16:03:46.070845 139622356395904 learning.py:507] global step 166: loss = 11.4170 (1.219 sec/step)\n","I0830 16:03:47.290953 139622356395904 learning.py:507] global step 167: loss = 10.9064 (1.218 sec/step)\n","I0830 16:03:48.477711 139622356395904 learning.py:507] global step 168: loss = 11.1089 (1.185 sec/step)\n","I0830 16:03:49.702425 139622356395904 learning.py:507] global step 169: loss = 10.8791 (1.223 sec/step)\n","I0830 16:03:50.952997 139622356395904 learning.py:507] global step 170: loss = 11.1571 (1.248 sec/step)\n","I0830 16:03:52.151413 139622356395904 learning.py:507] global step 171: loss = 11.1578 (1.196 sec/step)\n","I0830 16:03:53.380077 139622356395904 learning.py:507] global step 172: loss = 11.1394 (1.227 sec/step)\n","I0830 16:03:54.591413 139622356395904 learning.py:507] global step 173: loss = 11.7466 (1.209 sec/step)\n","I0830 16:03:55.843976 139622356395904 learning.py:507] global step 174: loss = 11.2118 (1.251 sec/step)\n","I0830 16:03:57.086423 139622356395904 learning.py:507] global step 175: loss = 11.2081 (1.240 sec/step)\n","I0830 16:03:58.307419 139622356395904 learning.py:507] global step 176: loss = 11.1331 (1.219 sec/step)\n","I0830 16:03:59.512899 139622356395904 learning.py:507] global step 177: loss = 11.1912 (1.204 sec/step)\n","I0830 16:04:01.537575 139622356395904 learning.py:507] global step 178: loss = 10.7263 (1.745 sec/step)\n","I0830 16:04:02.302284 139619299985152 supervisor.py:1050] Recording summary at step 178.\n","I0830 16:04:02.406630 139619308377856 supervisor.py:1099] global_step/sec: 0.808551\n","I0830 16:04:02.877831 139622356395904 learning.py:507] global step 179: loss = 10.8570 (1.338 sec/step)\n","I0830 16:04:04.123438 139622356395904 learning.py:507] global step 180: loss = 11.3274 (1.244 sec/step)\n","I0830 16:04:05.376145 139622356395904 learning.py:507] global step 181: loss = 10.9261 (1.251 sec/step)\n","I0830 16:04:06.577019 139622356395904 learning.py:507] global step 182: loss = 10.6385 (1.199 sec/step)\n","I0830 16:04:07.783742 139622356395904 learning.py:507] global step 183: loss = 11.5724 (1.205 sec/step)\n","I0830 16:04:09.021632 139622356395904 learning.py:507] global step 184: loss = 10.7367 (1.236 sec/step)\n","I0830 16:04:10.219244 139622356395904 learning.py:507] global step 185: loss = 10.8349 (1.196 sec/step)\n","I0830 16:04:11.434787 139622356395904 learning.py:507] global step 186: loss = 10.3511 (1.213 sec/step)\n","I0830 16:04:12.670423 139622356395904 learning.py:507] global step 187: loss = 11.6743 (1.234 sec/step)\n","I0830 16:04:13.877550 139622356395904 learning.py:507] global step 188: loss = 11.1912 (1.205 sec/step)\n","I0830 16:04:15.123434 139622356395904 learning.py:507] global step 189: loss = 10.6694 (1.244 sec/step)\n","I0830 16:04:16.349094 139622356395904 learning.py:507] global step 190: loss = 10.8559 (1.224 sec/step)\n","I0830 16:04:17.562596 139622356395904 learning.py:507] global step 191: loss = 11.1289 (1.212 sec/step)\n","I0830 16:04:18.754737 139622356395904 learning.py:507] global step 192: loss = 11.0755 (1.191 sec/step)\n","I0830 16:04:20.003190 139622356395904 learning.py:507] global step 193: loss = 10.9596 (1.246 sec/step)\n","I0830 16:04:21.227998 139622356395904 learning.py:507] global step 194: loss = 10.4497 (1.223 sec/step)\n","I0830 16:04:22.467969 139622356395904 learning.py:507] global step 195: loss = 10.7697 (1.238 sec/step)\n","I0830 16:04:23.695780 139622356395904 learning.py:507] global step 196: loss = 11.4373 (1.226 sec/step)\n","I0830 16:04:24.934605 139622356395904 learning.py:507] global step 197: loss = 10.4920 (1.237 sec/step)\n","I0830 16:04:26.155247 139622356395904 learning.py:507] global step 198: loss = 10.7484 (1.218 sec/step)\n","I0830 16:04:27.363481 139622356395904 learning.py:507] global step 199: loss = 11.2028 (1.206 sec/step)\n","I0830 16:04:28.581996 139622356395904 learning.py:507] global step 200: loss = 10.9945 (1.216 sec/step)\n","I0830 16:04:29.802227 139622356395904 learning.py:507] global step 201: loss = 10.4798 (1.218 sec/step)\n","I0830 16:04:31.079135 139622356395904 learning.py:507] global step 202: loss = 9.9962 (1.275 sec/step)\n","I0830 16:04:32.262907 139622356395904 learning.py:507] global step 203: loss = 10.2666 (1.182 sec/step)\n","I0830 16:04:33.478313 139622356395904 learning.py:507] global step 204: loss = 10.4073 (1.214 sec/step)\n","I0830 16:04:34.683631 139622356395904 learning.py:507] global step 205: loss = 10.1986 (1.204 sec/step)\n","I0830 16:04:35.957562 139622356395904 learning.py:507] global step 206: loss = 10.5970 (1.272 sec/step)\n","I0830 16:04:37.215570 139622356395904 learning.py:507] global step 207: loss = 11.0623 (1.256 sec/step)\n","I0830 16:04:38.428622 139622356395904 learning.py:507] global step 208: loss = 10.1165 (1.211 sec/step)\n","I0830 16:04:39.631215 139622356395904 learning.py:507] global step 209: loss = 10.8454 (1.201 sec/step)\n","I0830 16:04:40.849163 139622356395904 learning.py:507] global step 210: loss = 11.3021 (1.216 sec/step)\n","I0830 16:04:42.066957 139622356395904 learning.py:507] global step 211: loss = 11.0484 (1.216 sec/step)\n","I0830 16:04:43.304206 139622356395904 learning.py:507] global step 212: loss = 10.2051 (1.236 sec/step)\n","I0830 16:04:44.526847 139622356395904 learning.py:507] global step 213: loss = 10.5593 (1.221 sec/step)\n","I0830 16:04:45.720579 139622356395904 learning.py:507] global step 214: loss = 10.0096 (1.192 sec/step)\n","I0830 16:04:46.919148 139622356395904 learning.py:507] global step 215: loss = 9.9764 (1.196 sec/step)\n","I0830 16:04:48.122537 139622356395904 learning.py:507] global step 216: loss = 11.0692 (1.202 sec/step)\n","I0830 16:04:49.358300 139622356395904 learning.py:507] global step 217: loss = 10.3738 (1.234 sec/step)\n","I0830 16:04:50.597926 139622356395904 learning.py:507] global step 218: loss = 10.6143 (1.238 sec/step)\n","I0830 16:04:51.827708 139622356395904 learning.py:507] global step 219: loss = 11.2239 (1.228 sec/step)\n","I0830 16:04:53.068677 139622356395904 learning.py:507] global step 220: loss = 9.9098 (1.239 sec/step)\n","I0830 16:04:54.254585 139622356395904 learning.py:507] global step 221: loss = 11.1615 (1.184 sec/step)\n","I0830 16:04:55.454134 139622356395904 learning.py:507] global step 222: loss = 10.2366 (1.197 sec/step)\n","I0830 16:04:56.723934 139622356395904 learning.py:507] global step 223: loss = 10.7126 (1.268 sec/step)\n","I0830 16:04:57.941455 139622356395904 learning.py:507] global step 224: loss = 9.9912 (1.216 sec/step)\n","I0830 16:04:59.142473 139622356395904 learning.py:507] global step 225: loss = 10.4911 (1.199 sec/step)\n","I0830 16:05:00.346166 139622356395904 learning.py:507] global step 226: loss = 10.1302 (1.202 sec/step)\n","I0830 16:05:01.574309 139622356395904 learning.py:507] global step 227: loss = 10.2948 (1.226 sec/step)\n","I0830 16:05:02.822720 139622356395904 learning.py:507] global step 228: loss = 10.7043 (1.247 sec/step)\n","I0830 16:05:04.055350 139622356395904 learning.py:507] global step 229: loss = 10.0286 (1.231 sec/step)\n","I0830 16:05:05.284181 139622356395904 learning.py:507] global step 230: loss = 10.2591 (1.227 sec/step)\n","I0830 16:05:06.593548 139622356395904 learning.py:507] global step 231: loss = 9.9698 (1.307 sec/step)\n","I0830 16:05:07.849382 139622356395904 learning.py:507] global step 232: loss = 10.4445 (1.254 sec/step)\n","I0830 16:05:09.085698 139622356395904 learning.py:507] global step 233: loss = 10.2749 (1.234 sec/step)\n","I0830 16:05:10.335218 139622356395904 learning.py:507] global step 234: loss = 10.9054 (1.248 sec/step)\n","I0830 16:05:11.590604 139622356395904 learning.py:507] global step 235: loss = 11.1229 (1.253 sec/step)\n","I0830 16:05:12.781876 139622356395904 learning.py:507] global step 236: loss = 9.9236 (1.189 sec/step)\n","I0830 16:05:14.038380 139622356395904 learning.py:507] global step 237: loss = 10.6150 (1.255 sec/step)\n","I0830 16:05:15.232890 139622356395904 learning.py:507] global step 238: loss = 9.9747 (1.192 sec/step)\n","I0830 16:05:16.459368 139622356395904 learning.py:507] global step 239: loss = 10.1760 (1.225 sec/step)\n","I0830 16:05:17.671652 139622356395904 learning.py:507] global step 240: loss = 9.7559 (1.210 sec/step)\n","I0830 16:05:18.871849 139622356395904 learning.py:507] global step 241: loss = 10.7768 (1.199 sec/step)\n","I0830 16:05:20.057467 139622356395904 learning.py:507] global step 242: loss = 9.8856 (1.184 sec/step)\n","I0830 16:05:21.261143 139622356395904 learning.py:507] global step 243: loss = 10.0009 (1.202 sec/step)\n","I0830 16:05:22.490463 139622356395904 learning.py:507] global step 244: loss = 10.4860 (1.228 sec/step)\n","I0830 16:05:23.722170 139622356395904 learning.py:507] global step 245: loss = 10.3959 (1.230 sec/step)\n","I0830 16:05:24.959465 139622356395904 learning.py:507] global step 246: loss = 10.2740 (1.235 sec/step)\n","I0830 16:05:26.184476 139622356395904 learning.py:507] global step 247: loss = 10.5077 (1.223 sec/step)\n","I0830 16:05:27.420430 139622356395904 learning.py:507] global step 248: loss = 10.2476 (1.234 sec/step)\n","I0830 16:05:28.668251 139622356395904 learning.py:507] global step 249: loss = 10.2617 (1.246 sec/step)\n","I0830 16:05:29.915483 139622356395904 learning.py:507] global step 250: loss = 10.2223 (1.245 sec/step)\n","I0830 16:05:31.125543 139622356395904 learning.py:507] global step 251: loss = 9.8727 (1.208 sec/step)\n","I0830 16:05:32.342191 139622356395904 learning.py:507] global step 252: loss = 10.1499 (1.215 sec/step)\n","I0830 16:05:33.602781 139622356395904 learning.py:507] global step 253: loss = 10.7387 (1.259 sec/step)\n","I0830 16:05:34.851469 139622356395904 learning.py:507] global step 254: loss = 10.0261 (1.247 sec/step)\n","I0830 16:05:36.115689 139622356395904 learning.py:507] global step 255: loss = 10.9374 (1.262 sec/step)\n","I0830 16:05:37.343264 139622356395904 learning.py:507] global step 256: loss = 9.8188 (1.226 sec/step)\n","I0830 16:05:38.579551 139622356395904 learning.py:507] global step 257: loss = 10.1992 (1.234 sec/step)\n","I0830 16:05:39.817867 139622356395904 learning.py:507] global step 258: loss = 9.9601 (1.237 sec/step)\n","I0830 16:05:41.048768 139622356395904 learning.py:507] global step 259: loss = 9.6121 (1.229 sec/step)\n","I0830 16:05:42.268400 139622356395904 learning.py:507] global step 260: loss = 9.6993 (1.218 sec/step)\n","I0830 16:05:43.531896 139622356395904 learning.py:507] global step 261: loss = 10.1177 (1.261 sec/step)\n","I0830 16:05:44.747811 139622356395904 learning.py:507] global step 262: loss = 10.0852 (1.214 sec/step)\n","I0830 16:05:45.952868 139622356395904 learning.py:507] global step 263: loss = 9.7504 (1.203 sec/step)\n","I0830 16:05:47.172113 139622356395904 learning.py:507] global step 264: loss = 9.7087 (1.217 sec/step)\n","I0830 16:05:48.406483 139622356395904 learning.py:507] global step 265: loss = 10.5127 (1.233 sec/step)\n","I0830 16:05:49.654118 139622356395904 learning.py:507] global step 266: loss = 9.9691 (1.246 sec/step)\n","I0830 16:05:50.870712 139622356395904 learning.py:507] global step 267: loss = 9.8019 (1.215 sec/step)\n","I0830 16:05:52.096844 139622356395904 learning.py:507] global step 268: loss = 9.7549 (1.224 sec/step)\n","I0830 16:05:53.315448 139622356395904 learning.py:507] global step 269: loss = 9.5232 (1.217 sec/step)\n","I0830 16:05:54.555662 139622356395904 learning.py:507] global step 270: loss = 10.4641 (1.238 sec/step)\n","I0830 16:05:55.781626 139622356395904 learning.py:507] global step 271: loss = 9.6918 (1.224 sec/step)\n","I0830 16:05:56.954510 139622356395904 learning.py:507] global step 272: loss = 9.6188 (1.171 sec/step)\n","I0830 16:05:58.189023 139622356395904 learning.py:507] global step 273: loss = 9.3167 (1.233 sec/step)\n","I0830 16:05:59.453603 139622356395904 learning.py:507] global step 274: loss = 9.7557 (1.263 sec/step)\n","I0830 16:06:01.583844 139622356395904 learning.py:507] global step 275: loss = 9.5828 (2.128 sec/step)\n","I0830 16:06:01.665345 139619299985152 supervisor.py:1050] Recording summary at step 275.\n","I0830 16:06:02.395934 139619308377856 supervisor.py:1099] global_step/sec: 0.808406\n","I0830 16:06:02.830220 139622356395904 learning.py:507] global step 276: loss = 9.6314 (1.244 sec/step)\n","I0830 16:06:04.056698 139622356395904 learning.py:507] global step 277: loss = 9.7157 (1.224 sec/step)\n","I0830 16:06:05.269164 139622356395904 learning.py:507] global step 278: loss = 9.7602 (1.211 sec/step)\n","I0830 16:06:06.529239 139622356395904 learning.py:507] global step 279: loss = 10.1188 (1.258 sec/step)\n","I0830 16:06:07.778318 139622356395904 learning.py:507] global step 280: loss = 9.3492 (1.247 sec/step)\n","I0830 16:06:09.017636 139622356395904 learning.py:507] global step 281: loss = 10.0933 (1.237 sec/step)\n","I0830 16:06:10.241668 139622356395904 learning.py:507] global step 282: loss = 9.9418 (1.221 sec/step)\n","I0830 16:06:11.476009 139622356395904 learning.py:507] global step 283: loss = 9.4874 (1.232 sec/step)\n","I0830 16:06:12.716628 139622356395904 learning.py:507] global step 284: loss = 9.8682 (1.239 sec/step)\n","I0830 16:06:13.931534 139622356395904 learning.py:507] global step 285: loss = 9.7404 (1.213 sec/step)\n","I0830 16:06:15.146836 139622356395904 learning.py:507] global step 286: loss = 9.5572 (1.214 sec/step)\n","I0830 16:06:16.387402 139622356395904 learning.py:507] global step 287: loss = 9.8621 (1.238 sec/step)\n","I0830 16:06:17.615369 139622356395904 learning.py:507] global step 288: loss = 9.7290 (1.226 sec/step)\n","I0830 16:06:18.836800 139622356395904 learning.py:507] global step 289: loss = 9.7487 (1.220 sec/step)\n","I0830 16:06:20.078132 139622356395904 learning.py:507] global step 290: loss = 9.4598 (1.239 sec/step)\n","I0830 16:06:21.301347 139622356395904 learning.py:507] global step 291: loss = 9.0729 (1.221 sec/step)\n","I0830 16:06:22.514499 139622356395904 learning.py:507] global step 292: loss = 9.5041 (1.211 sec/step)\n","I0830 16:06:23.738900 139622356395904 learning.py:507] global step 293: loss = 9.6369 (1.222 sec/step)\n","I0830 16:06:24.989448 139622356395904 learning.py:507] global step 294: loss = 9.6221 (1.249 sec/step)\n","I0830 16:06:26.187692 139622356395904 learning.py:507] global step 295: loss = 9.8881 (1.196 sec/step)\n","I0830 16:06:27.472210 139622356395904 learning.py:507] global step 296: loss = 9.5777 (1.283 sec/step)\n","I0830 16:06:28.688916 139622356395904 learning.py:507] global step 297: loss = 9.9458 (1.215 sec/step)\n","I0830 16:06:29.938757 139622356395904 learning.py:507] global step 298: loss = 10.6526 (1.248 sec/step)\n","I0830 16:06:31.174677 139622356395904 learning.py:507] global step 299: loss = 9.5222 (1.234 sec/step)\n","I0830 16:06:32.405662 139622356395904 learning.py:507] global step 300: loss = 9.7755 (1.229 sec/step)\n","I0830 16:06:33.618184 139622356395904 learning.py:507] global step 301: loss = 9.4755 (1.211 sec/step)\n","I0830 16:06:34.819749 139622356395904 learning.py:507] global step 302: loss = 9.7926 (1.200 sec/step)\n","I0830 16:06:36.066203 139622356395904 learning.py:507] global step 303: loss = 9.6587 (1.245 sec/step)\n","I0830 16:06:37.269899 139622356395904 learning.py:507] global step 304: loss = 9.3921 (1.202 sec/step)\n","I0830 16:06:38.496824 139622356395904 learning.py:507] global step 305: loss = 9.8687 (1.225 sec/step)\n","I0830 16:06:39.713718 139622356395904 learning.py:507] global step 306: loss = 10.0559 (1.215 sec/step)\n","I0830 16:06:40.931082 139622356395904 learning.py:507] global step 307: loss = 9.7534 (1.215 sec/step)\n","I0830 16:06:42.137794 139622356395904 learning.py:507] global step 308: loss = 9.4206 (1.205 sec/step)\n","I0830 16:06:43.367169 139622356395904 learning.py:507] global step 309: loss = 9.6243 (1.228 sec/step)\n","I0830 16:06:44.611440 139622356395904 learning.py:507] global step 310: loss = 9.0950 (1.243 sec/step)\n","I0830 16:06:45.830006 139622356395904 learning.py:507] global step 311: loss = 9.0323 (1.217 sec/step)\n","I0830 16:06:47.050416 139622356395904 learning.py:507] global step 312: loss = 9.8816 (1.219 sec/step)\n","I0830 16:06:48.259034 139622356395904 learning.py:507] global step 313: loss = 9.2211 (1.207 sec/step)\n","I0830 16:06:49.498728 139622356395904 learning.py:507] global step 314: loss = 9.0679 (1.238 sec/step)\n","I0830 16:06:50.748560 139622356395904 learning.py:507] global step 315: loss = 9.4311 (1.248 sec/step)\n","I0830 16:06:52.040198 139622356395904 learning.py:507] global step 316: loss = 9.7557 (1.290 sec/step)\n","I0830 16:06:53.279146 139622356395904 learning.py:507] global step 317: loss = 10.1896 (1.237 sec/step)\n","I0830 16:06:54.528917 139622356395904 learning.py:507] global step 318: loss = 8.9570 (1.247 sec/step)\n","I0830 16:06:55.724836 139622356395904 learning.py:507] global step 319: loss = 9.9260 (1.194 sec/step)\n","I0830 16:06:56.964501 139622356395904 learning.py:507] global step 320: loss = 9.6110 (1.238 sec/step)\n","I0830 16:06:58.215645 139622356395904 learning.py:507] global step 321: loss = 9.6681 (1.249 sec/step)\n","I0830 16:06:59.455096 139622356395904 learning.py:507] global step 322: loss = 10.0737 (1.238 sec/step)\n","I0830 16:07:00.686453 139622356395904 learning.py:507] global step 323: loss = 9.6326 (1.230 sec/step)\n","I0830 16:07:01.921602 139622356395904 learning.py:507] global step 324: loss = 9.3160 (1.233 sec/step)\n","I0830 16:07:03.143571 139622356395904 learning.py:507] global step 325: loss = 8.7907 (1.220 sec/step)\n","I0830 16:07:04.358032 139622356395904 learning.py:507] global step 326: loss = 8.9379 (1.213 sec/step)\n","I0830 16:07:05.577576 139622356395904 learning.py:507] global step 327: loss = 9.2714 (1.218 sec/step)\n","I0830 16:07:06.796121 139622356395904 learning.py:507] global step 328: loss = 9.2268 (1.217 sec/step)\n","I0830 16:07:08.000110 139622356395904 learning.py:507] global step 329: loss = 9.1094 (1.202 sec/step)\n","I0830 16:07:09.219192 139622356395904 learning.py:507] global step 330: loss = 9.6223 (1.217 sec/step)\n","I0830 16:07:10.455170 139622356395904 learning.py:507] global step 331: loss = 9.6445 (1.234 sec/step)\n","I0830 16:07:11.678520 139622356395904 learning.py:507] global step 332: loss = 9.6427 (1.221 sec/step)\n","I0830 16:07:12.920877 139622356395904 learning.py:507] global step 333: loss = 9.0368 (1.241 sec/step)\n","I0830 16:07:14.134854 139622356395904 learning.py:507] global step 334: loss = 9.1179 (1.212 sec/step)\n","I0830 16:07:15.365278 139622356395904 learning.py:507] global step 335: loss = 9.0726 (1.229 sec/step)\n","I0830 16:07:16.540996 139622356395904 learning.py:507] global step 336: loss = 9.2356 (1.174 sec/step)\n","I0830 16:07:17.746536 139622356395904 learning.py:507] global step 337: loss = 9.4028 (1.204 sec/step)\n","I0830 16:07:18.999146 139622356395904 learning.py:507] global step 338: loss = 9.2523 (1.251 sec/step)\n","I0830 16:07:20.183195 139622356395904 learning.py:507] global step 339: loss = 9.1549 (1.182 sec/step)\n","I0830 16:07:21.392320 139622356395904 learning.py:507] global step 340: loss = 8.8755 (1.207 sec/step)\n","I0830 16:07:22.621196 139622356395904 learning.py:507] global step 341: loss = 9.9895 (1.227 sec/step)\n","I0830 16:07:23.850095 139622356395904 learning.py:507] global step 342: loss = 8.8766 (1.227 sec/step)\n","I0830 16:07:25.099633 139622356395904 learning.py:507] global step 343: loss = 9.0872 (1.248 sec/step)\n","I0830 16:07:26.340112 139622356395904 learning.py:507] global step 344: loss = 8.7256 (1.238 sec/step)\n","I0830 16:07:27.580762 139622356395904 learning.py:507] global step 345: loss = 9.7276 (1.239 sec/step)\n","I0830 16:07:28.831738 139622356395904 learning.py:507] global step 346: loss = 8.7440 (1.249 sec/step)\n","I0830 16:07:30.127715 139622356395904 learning.py:507] global step 347: loss = 9.3367 (1.294 sec/step)\n","I0830 16:07:31.353852 139622356395904 learning.py:507] global step 348: loss = 9.2175 (1.224 sec/step)\n","I0830 16:07:32.543921 139622356395904 learning.py:507] global step 349: loss = 8.9371 (1.188 sec/step)\n","I0830 16:07:33.785415 139622356395904 learning.py:507] global step 350: loss = 8.9470 (1.239 sec/step)\n","I0830 16:07:35.018333 139622356395904 learning.py:507] global step 351: loss = 9.1629 (1.231 sec/step)\n","I0830 16:07:36.227210 139622356395904 learning.py:507] global step 352: loss = 9.2990 (1.207 sec/step)\n","I0830 16:07:37.479032 139622356395904 learning.py:507] global step 353: loss = 8.7633 (1.249 sec/step)\n","I0830 16:07:38.687831 139622356395904 learning.py:507] global step 354: loss = 9.2178 (1.207 sec/step)\n","I0830 16:07:39.913169 139622356395904 learning.py:507] global step 355: loss = 9.2191 (1.223 sec/step)\n","I0830 16:07:41.138797 139622356395904 learning.py:507] global step 356: loss = 8.7802 (1.224 sec/step)\n","I0830 16:07:42.338428 139622356395904 learning.py:507] global step 357: loss = 9.7365 (1.198 sec/step)\n","I0830 16:07:43.568682 139622356395904 learning.py:507] global step 358: loss = 8.9472 (1.228 sec/step)\n","I0830 16:07:44.772576 139622356395904 learning.py:507] global step 359: loss = 8.8200 (1.202 sec/step)\n","I0830 16:07:46.007425 139622356395904 learning.py:507] global step 360: loss = 9.1228 (1.233 sec/step)\n","I0830 16:07:47.240464 139622356395904 learning.py:507] global step 361: loss = 8.4022 (1.231 sec/step)\n","I0830 16:07:48.500147 139622356395904 learning.py:507] global step 362: loss = 9.5404 (1.258 sec/step)\n","I0830 16:07:49.719925 139622356395904 learning.py:507] global step 363: loss = 8.5425 (1.218 sec/step)\n","I0830 16:07:50.944700 139622356395904 learning.py:507] global step 364: loss = 8.8959 (1.223 sec/step)\n","I0830 16:07:52.186391 139622356395904 learning.py:507] global step 365: loss = 9.2515 (1.240 sec/step)\n","I0830 16:07:53.419951 139622356395904 learning.py:507] global step 366: loss = 8.8271 (1.232 sec/step)\n","I0830 16:07:54.635904 139622356395904 learning.py:507] global step 367: loss = 8.6973 (1.214 sec/step)\n","I0830 16:07:55.861213 139622356395904 learning.py:507] global step 368: loss = 9.2773 (1.224 sec/step)\n","I0830 16:07:57.062173 139622356395904 learning.py:507] global step 369: loss = 8.5904 (1.199 sec/step)\n","I0830 16:07:58.262788 139622356395904 learning.py:507] global step 370: loss = 8.8853 (1.199 sec/step)\n","I0830 16:07:59.476622 139622356395904 learning.py:507] global step 371: loss = 9.0342 (1.212 sec/step)\n","I0830 16:08:01.321337 139622356395904 learning.py:507] global step 372: loss = 8.4575 (1.762 sec/step)\n","I0830 16:08:02.006732 139619299985152 supervisor.py:1050] Recording summary at step 372.\n","I0830 16:08:02.507397 139619308377856 supervisor.py:1099] global_step/sec: 0.807583\n","I0830 16:08:02.974839 139622356395904 learning.py:507] global step 373: loss = 9.3304 (1.648 sec/step)\n","I0830 16:08:04.221568 139622356395904 learning.py:507] global step 374: loss = 9.1367 (1.245 sec/step)\n","I0830 16:08:05.430938 139622356395904 learning.py:507] global step 375: loss = 8.7543 (1.207 sec/step)\n","I0830 16:08:06.677121 139622356395904 learning.py:507] global step 376: loss = 8.7983 (1.244 sec/step)\n","I0830 16:08:07.943158 139622356395904 learning.py:507] global step 377: loss = 8.7430 (1.264 sec/step)\n","I0830 16:08:09.159266 139622356395904 learning.py:507] global step 378: loss = 8.4777 (1.214 sec/step)\n","I0830 16:08:10.408715 139622356395904 learning.py:507] global step 379: loss = 8.7585 (1.248 sec/step)\n","I0830 16:08:11.627001 139622356395904 learning.py:507] global step 380: loss = 8.5773 (1.216 sec/step)\n","I0830 16:08:12.853709 139622356395904 learning.py:507] global step 381: loss = 8.9558 (1.224 sec/step)\n","I0830 16:08:14.078570 139622356395904 learning.py:507] global step 382: loss = 9.0406 (1.223 sec/step)\n","I0830 16:08:15.298947 139622356395904 learning.py:507] global step 383: loss = 9.1067 (1.218 sec/step)\n","I0830 16:08:16.550713 139622356395904 learning.py:507] global step 384: loss = 8.6591 (1.250 sec/step)\n","I0830 16:08:17.782088 139622356395904 learning.py:507] global step 385: loss = 9.1105 (1.230 sec/step)\n","I0830 16:08:19.021362 139622356395904 learning.py:507] global step 386: loss = 8.4282 (1.238 sec/step)\n","I0830 16:08:20.263173 139622356395904 learning.py:507] global step 387: loss = 8.6307 (1.240 sec/step)\n","I0830 16:08:21.480983 139622356395904 learning.py:507] global step 388: loss = 9.0723 (1.216 sec/step)\n","I0830 16:08:22.710998 139622356395904 learning.py:507] global step 389: loss = 8.5936 (1.228 sec/step)\n","I0830 16:08:23.954352 139622356395904 learning.py:507] global step 390: loss = 8.8451 (1.241 sec/step)\n","I0830 16:08:25.192322 139622356395904 learning.py:507] global step 391: loss = 8.7743 (1.236 sec/step)\n","I0830 16:08:26.425632 139622356395904 learning.py:507] global step 392: loss = 8.5149 (1.232 sec/step)\n","I0830 16:08:27.683463 139622356395904 learning.py:507] global step 393: loss = 9.4369 (1.256 sec/step)\n","I0830 16:08:28.907424 139622356395904 learning.py:507] global step 394: loss = 9.4519 (1.222 sec/step)\n","I0830 16:08:30.126512 139622356395904 learning.py:507] global step 395: loss = 8.4493 (1.217 sec/step)\n","I0830 16:08:31.348371 139622356395904 learning.py:507] global step 396: loss = 8.7129 (1.220 sec/step)\n","I0830 16:08:32.573142 139622356395904 learning.py:507] global step 397: loss = 8.8295 (1.223 sec/step)\n","I0830 16:08:33.824010 139622356395904 learning.py:507] global step 398: loss = 8.8132 (1.248 sec/step)\n","I0830 16:08:35.081627 139622356395904 learning.py:507] global step 399: loss = 8.6667 (1.256 sec/step)\n","I0830 16:08:36.368822 139622356395904 learning.py:507] global step 400: loss = 9.2422 (1.285 sec/step)\n","I0830 16:08:37.603835 139622356395904 learning.py:507] global step 401: loss = 8.5572 (1.233 sec/step)\n","I0830 16:08:38.813611 139622356395904 learning.py:507] global step 402: loss = 8.7891 (1.208 sec/step)\n","I0830 16:08:40.025983 139622356395904 learning.py:507] global step 403: loss = 8.2555 (1.211 sec/step)\n","I0830 16:08:41.278634 139622356395904 learning.py:507] global step 404: loss = 9.3330 (1.251 sec/step)\n","I0830 16:08:42.516988 139622356395904 learning.py:507] global step 405: loss = 8.9156 (1.236 sec/step)\n","I0830 16:08:43.755376 139622356395904 learning.py:507] global step 406: loss = 8.5317 (1.237 sec/step)\n","I0830 16:08:44.978477 139622356395904 learning.py:507] global step 407: loss = 9.0501 (1.221 sec/step)\n","I0830 16:08:46.212437 139622356395904 learning.py:507] global step 408: loss = 9.0305 (1.232 sec/step)\n","I0830 16:08:47.439485 139622356395904 learning.py:507] global step 409: loss = 8.9506 (1.225 sec/step)\n","I0830 16:08:48.659109 139622356395904 learning.py:507] global step 410: loss = 8.8741 (1.218 sec/step)\n","I0830 16:08:49.899778 139622356395904 learning.py:507] global step 411: loss = 8.2292 (1.239 sec/step)\n","I0830 16:08:51.152129 139622356395904 learning.py:507] global step 412: loss = 8.6419 (1.250 sec/step)\n","I0830 16:08:52.362104 139622356395904 learning.py:507] global step 413: loss = 8.2045 (1.208 sec/step)\n","I0830 16:08:53.579509 139622356395904 learning.py:507] global step 414: loss = 8.3747 (1.215 sec/step)\n","I0830 16:08:54.793807 139622356395904 learning.py:507] global step 415: loss = 8.8883 (1.212 sec/step)\n","I0830 16:08:56.009716 139622356395904 learning.py:507] global step 416: loss = 8.9742 (1.214 sec/step)\n","I0830 16:08:57.209465 139622356395904 learning.py:507] global step 417: loss = 9.0388 (1.198 sec/step)\n","I0830 16:08:58.469165 139622356395904 learning.py:507] global step 418: loss = 8.9257 (1.258 sec/step)\n","I0830 16:08:59.694198 139622356395904 learning.py:507] global step 419: loss = 8.7793 (1.222 sec/step)\n","I0830 16:09:00.959699 139622356395904 learning.py:507] global step 420: loss = 8.9714 (1.264 sec/step)\n","I0830 16:09:02.178442 139622356395904 learning.py:507] global step 421: loss = 8.6791 (1.217 sec/step)\n","I0830 16:09:03.422204 139622356395904 learning.py:507] global step 422: loss = 8.2282 (1.242 sec/step)\n","I0830 16:09:04.654033 139622356395904 learning.py:507] global step 423: loss = 9.2385 (1.230 sec/step)\n","I0830 16:09:05.867350 139622356395904 learning.py:507] global step 424: loss = 8.2745 (1.211 sec/step)\n","I0830 16:09:07.088443 139622356395904 learning.py:507] global step 425: loss = 8.7324 (1.219 sec/step)\n","I0830 16:09:08.285532 139622356395904 learning.py:507] global step 426: loss = 8.4379 (1.195 sec/step)\n","I0830 16:09:09.543179 139622356395904 learning.py:507] global step 427: loss = 8.6882 (1.256 sec/step)\n","I0830 16:09:10.741851 139622356395904 learning.py:507] global step 428: loss = 8.2893 (1.197 sec/step)\n","I0830 16:09:11.984538 139622356395904 learning.py:507] global step 429: loss = 8.9385 (1.241 sec/step)\n","I0830 16:09:13.190071 139622356395904 learning.py:507] global step 430: loss = 8.3221 (1.204 sec/step)\n","I0830 16:09:14.398713 139622356395904 learning.py:507] global step 431: loss = 8.5816 (1.207 sec/step)\n","I0830 16:09:15.607388 139622356395904 learning.py:507] global step 432: loss = 9.4880 (1.207 sec/step)\n","I0830 16:09:16.845843 139622356395904 learning.py:507] global step 433: loss = 8.8110 (1.237 sec/step)\n","I0830 16:09:18.089842 139622356395904 learning.py:507] global step 434: loss = 8.0876 (1.242 sec/step)\n","I0830 16:09:19.357115 139622356395904 learning.py:507] global step 435: loss = 8.2187 (1.265 sec/step)\n","I0830 16:09:20.571847 139622356395904 learning.py:507] global step 436: loss = 8.5990 (1.213 sec/step)\n","I0830 16:09:21.820022 139622356395904 learning.py:507] global step 437: loss = 9.1490 (1.246 sec/step)\n","I0830 16:09:23.060658 139622356395904 learning.py:507] global step 438: loss = 8.9897 (1.239 sec/step)\n","I0830 16:09:24.269658 139622356395904 learning.py:507] global step 439: loss = 8.3270 (1.207 sec/step)\n","I0830 16:09:25.516473 139622356395904 learning.py:507] global step 440: loss = 8.4236 (1.245 sec/step)\n","I0830 16:09:26.750523 139622356395904 learning.py:507] global step 441: loss = 8.2096 (1.232 sec/step)\n","I0830 16:09:27.948736 139622356395904 learning.py:507] global step 442: loss = 8.3865 (1.196 sec/step)\n","I0830 16:09:29.194614 139622356395904 learning.py:507] global step 443: loss = 8.4343 (1.244 sec/step)\n","I0830 16:09:30.420568 139622356395904 learning.py:507] global step 444: loss = 8.6648 (1.224 sec/step)\n","I0830 16:09:31.635122 139622356395904 learning.py:507] global step 445: loss = 8.2490 (1.213 sec/step)\n","I0830 16:09:32.893159 139622356395904 learning.py:507] global step 446: loss = 8.2054 (1.256 sec/step)\n","I0830 16:09:34.135353 139622356395904 learning.py:507] global step 447: loss = 8.5667 (1.240 sec/step)\n","I0830 16:09:35.395196 139622356395904 learning.py:507] global step 448: loss = 8.2125 (1.258 sec/step)\n","I0830 16:09:36.612787 139622356395904 learning.py:507] global step 449: loss = 8.0661 (1.216 sec/step)\n","I0830 16:09:37.853293 139622356395904 learning.py:507] global step 450: loss = 8.6951 (1.239 sec/step)\n","I0830 16:09:39.092452 139622356395904 learning.py:507] global step 451: loss = 8.5217 (1.238 sec/step)\n","I0830 16:09:40.341325 139622356395904 learning.py:507] global step 452: loss = 8.9667 (1.247 sec/step)\n","I0830 16:09:41.539262 139622356395904 learning.py:507] global step 453: loss = 8.7527 (1.196 sec/step)\n","I0830 16:09:42.771339 139622356395904 learning.py:507] global step 454: loss = 8.7062 (1.230 sec/step)\n","I0830 16:09:44.025691 139622356395904 learning.py:507] global step 455: loss = 8.3532 (1.253 sec/step)\n","I0830 16:09:45.211591 139622356395904 learning.py:507] global step 456: loss = 8.3594 (1.184 sec/step)\n","I0830 16:09:46.464184 139622356395904 learning.py:507] global step 457: loss = 8.1270 (1.250 sec/step)\n","I0830 16:09:47.726926 139622356395904 learning.py:507] global step 458: loss = 8.3950 (1.261 sec/step)\n","I0830 16:09:48.992816 139622356395904 learning.py:507] global step 459: loss = 8.5649 (1.264 sec/step)\n","I0830 16:09:50.221440 139622356395904 learning.py:507] global step 460: loss = 8.9349 (1.227 sec/step)\n","I0830 16:09:51.452619 139622356395904 learning.py:507] global step 461: loss = 8.5549 (1.229 sec/step)\n","I0830 16:09:52.680067 139622356395904 learning.py:507] global step 462: loss = 8.5822 (1.226 sec/step)\n","I0830 16:09:53.916139 139622356395904 learning.py:507] global step 463: loss = 8.9944 (1.234 sec/step)\n","I0830 16:09:55.159177 139622356395904 learning.py:507] global step 464: loss = 8.2096 (1.241 sec/step)\n","I0830 16:09:56.357035 139622356395904 learning.py:507] global step 465: loss = 8.3277 (1.196 sec/step)\n","I0830 16:09:57.596305 139622356395904 learning.py:507] global step 466: loss = 8.4991 (1.237 sec/step)\n","I0830 16:09:58.834533 139622356395904 learning.py:507] global step 467: loss = 9.0185 (1.236 sec/step)\n","I0830 16:09:59.646191 139619316770560 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n","I0830 16:10:00.826566 139622356395904 learning.py:507] global step 468: loss = 8.6797 (1.842 sec/step)\n","I0830 16:10:02.438796 139619308377856 supervisor.py:1099] global_step/sec: 0.800457\n","I0830 16:10:02.490083 139619299985152 supervisor.py:1050] Recording summary at step 468.\n","I0830 16:10:03.103972 139622356395904 learning.py:507] global step 469: loss = 9.1795 (2.263 sec/step)\n","I0830 16:10:04.717857 139622356395904 learning.py:507] global step 470: loss = 7.9067 (1.601 sec/step)\n","I0830 16:10:05.915580 139622356395904 learning.py:507] global step 471: loss = 8.3474 (1.196 sec/step)\n","I0830 16:10:07.154457 139622356395904 learning.py:507] global step 472: loss = 8.0649 (1.237 sec/step)\n","I0830 16:10:08.409489 139622356395904 learning.py:507] global step 473: loss = 8.3148 (1.253 sec/step)\n","I0830 16:10:09.611170 139622356395904 learning.py:507] global step 474: loss = 8.8208 (1.200 sec/step)\n","I0830 16:10:10.847978 139622356395904 learning.py:507] global step 475: loss = 8.5572 (1.235 sec/step)\n","I0830 16:10:12.080661 139622356395904 learning.py:507] global step 476: loss = 9.0710 (1.231 sec/step)\n","I0830 16:10:13.281779 139622356395904 learning.py:507] global step 477: loss = 8.0404 (1.199 sec/step)\n","I0830 16:10:14.545074 139622356395904 learning.py:507] global step 478: loss = 8.1625 (1.261 sec/step)\n","I0830 16:10:15.789564 139622356395904 learning.py:507] global step 479: loss = 8.3976 (1.242 sec/step)\n","I0830 16:10:17.038009 139622356395904 learning.py:507] global step 480: loss = 8.3041 (1.241 sec/step)\n","I0830 16:10:18.266852 139622356395904 learning.py:507] global step 481: loss = 8.6371 (1.227 sec/step)\n","I0830 16:10:19.531572 139622356395904 learning.py:507] global step 482: loss = 8.6497 (1.263 sec/step)\n","I0830 16:10:20.749218 139622356395904 learning.py:507] global step 483: loss = 8.2823 (1.216 sec/step)\n","I0830 16:10:21.984413 139622356395904 learning.py:507] global step 484: loss = 7.7265 (1.233 sec/step)\n","I0830 16:10:23.197535 139622356395904 learning.py:507] global step 485: loss = 8.1390 (1.211 sec/step)\n","I0830 16:10:24.443185 139622356395904 learning.py:507] global step 486: loss = 7.8036 (1.244 sec/step)\n","I0830 16:10:25.631946 139622356395904 learning.py:507] global step 487: loss = 8.5713 (1.187 sec/step)\n","I0830 16:10:26.857403 139622356395904 learning.py:507] global step 488: loss = 8.4624 (1.224 sec/step)\n","I0830 16:10:28.071853 139622356395904 learning.py:507] global step 489: loss = 8.1398 (1.213 sec/step)\n","I0830 16:10:29.324340 139622356395904 learning.py:507] global step 490: loss = 8.4139 (1.251 sec/step)\n","I0830 16:10:30.547925 139622356395904 learning.py:507] global step 491: loss = 8.4381 (1.222 sec/step)\n","I0830 16:10:31.769724 139622356395904 learning.py:507] global step 492: loss = 8.5708 (1.217 sec/step)\n","I0830 16:10:33.031598 139622356395904 learning.py:507] global step 493: loss = 8.0935 (1.260 sec/step)\n","I0830 16:10:34.244709 139622356395904 learning.py:507] global step 494: loss = 8.0966 (1.211 sec/step)\n","I0830 16:10:35.459069 139622356395904 learning.py:507] global step 495: loss = 8.5942 (1.212 sec/step)\n","I0830 16:10:36.678342 139622356395904 learning.py:507] global step 496: loss = 8.0542 (1.218 sec/step)\n","I0830 16:10:37.883817 139622356395904 learning.py:507] global step 497: loss = 8.2685 (1.204 sec/step)\n","I0830 16:10:39.077994 139622356395904 learning.py:507] global step 498: loss = 7.7070 (1.192 sec/step)\n","I0830 16:10:40.299507 139622356395904 learning.py:507] global step 499: loss = 8.0508 (1.220 sec/step)\n","I0830 16:10:41.527107 139622356395904 learning.py:507] global step 500: loss = 8.1854 (1.224 sec/step)\n","I0830 16:10:42.759440 139622356395904 learning.py:507] global step 501: loss = 7.9949 (1.230 sec/step)\n","I0830 16:10:43.978209 139622356395904 learning.py:507] global step 502: loss = 7.8508 (1.217 sec/step)\n","I0830 16:10:45.240161 139622356395904 learning.py:507] global step 503: loss = 8.0611 (1.260 sec/step)\n","I0830 16:10:46.487618 139622356395904 learning.py:507] global step 504: loss = 8.0674 (1.245 sec/step)\n","I0830 16:10:47.704542 139622356395904 learning.py:507] global step 505: loss = 8.0809 (1.215 sec/step)\n","I0830 16:10:48.939033 139622356395904 learning.py:507] global step 506: loss = 7.8246 (1.233 sec/step)\n","I0830 16:10:50.149250 139622356395904 learning.py:507] global step 507: loss = 8.0685 (1.208 sec/step)\n","I0830 16:10:51.384275 139622356395904 learning.py:507] global step 508: loss = 8.4702 (1.233 sec/step)\n","I0830 16:10:52.632084 139622356395904 learning.py:507] global step 509: loss = 8.6163 (1.246 sec/step)\n","I0830 16:10:53.889799 139622356395904 learning.py:507] global step 510: loss = 9.1951 (1.256 sec/step)\n","I0830 16:10:55.072872 139622356395904 learning.py:507] global step 511: loss = 7.6384 (1.181 sec/step)\n","I0830 16:10:56.341011 139622356395904 learning.py:507] global step 512: loss = 7.9255 (1.266 sec/step)\n","I0830 16:10:57.563569 139622356395904 learning.py:507] global step 513: loss = 8.1282 (1.221 sec/step)\n","I0830 16:10:58.805010 139622356395904 learning.py:507] global step 514: loss = 8.3834 (1.239 sec/step)\n","I0830 16:11:00.033533 139622356395904 learning.py:507] global step 515: loss = 8.1240 (1.227 sec/step)\n","I0830 16:11:01.266366 139622356395904 learning.py:507] global step 516: loss = 7.8744 (1.231 sec/step)\n","I0830 16:11:02.474535 139622356395904 learning.py:507] global step 517: loss = 8.2856 (1.206 sec/step)\n","I0830 16:11:03.737910 139622356395904 learning.py:507] global step 518: loss = 8.2458 (1.262 sec/step)\n","I0830 16:11:04.966813 139622356395904 learning.py:507] global step 519: loss = 8.4326 (1.226 sec/step)\n","I0830 16:11:06.156595 139622356395904 learning.py:507] global step 520: loss = 7.6124 (1.188 sec/step)\n","I0830 16:11:07.391300 139622356395904 learning.py:507] global step 521: loss = 7.8555 (1.232 sec/step)\n","I0830 16:11:08.603379 139622356395904 learning.py:507] global step 522: loss = 7.5064 (1.210 sec/step)\n","I0830 16:11:09.828449 139622356395904 learning.py:507] global step 523: loss = 7.4320 (1.223 sec/step)\n","I0830 16:11:11.029346 139622356395904 learning.py:507] global step 524: loss = 7.7462 (1.199 sec/step)\n","I0830 16:11:12.287734 139622356395904 learning.py:507] global step 525: loss = 8.0995 (1.256 sec/step)\n","I0830 16:11:13.485254 139622356395904 learning.py:507] global step 526: loss = 7.7614 (1.196 sec/step)\n","I0830 16:11:14.691201 139622356395904 learning.py:507] global step 527: loss = 7.5162 (1.204 sec/step)\n","I0830 16:11:15.954876 139622356395904 learning.py:507] global step 528: loss = 8.7540 (1.262 sec/step)\n","I0830 16:11:17.187432 139622356395904 learning.py:507] global step 529: loss = 7.6529 (1.231 sec/step)\n","I0830 16:11:18.464368 139622356395904 learning.py:507] global step 530: loss = 7.6108 (1.275 sec/step)\n","I0830 16:11:19.682021 139622356395904 learning.py:507] global step 531: loss = 7.9891 (1.216 sec/step)\n","I0830 16:11:20.911825 139622356395904 learning.py:507] global step 532: loss = 7.7408 (1.228 sec/step)\n","I0830 16:11:22.154191 139622356395904 learning.py:507] global step 533: loss = 8.5241 (1.241 sec/step)\n","I0830 16:11:23.416669 139622356395904 learning.py:507] global step 534: loss = 7.7388 (1.260 sec/step)\n","I0830 16:11:24.651692 139622356395904 learning.py:507] global step 535: loss = 8.1080 (1.233 sec/step)\n","I0830 16:11:25.887164 139622356395904 learning.py:507] global step 536: loss = 8.1312 (1.234 sec/step)\n","I0830 16:11:27.103026 139622356395904 learning.py:507] global step 537: loss = 8.0230 (1.214 sec/step)\n","I0830 16:11:28.355648 139622356395904 learning.py:507] global step 538: loss = 8.5131 (1.251 sec/step)\n","I0830 16:11:29.577582 139622356395904 learning.py:507] global step 539: loss = 7.6028 (1.220 sec/step)\n","I0830 16:11:30.816610 139622356395904 learning.py:507] global step 540: loss = 8.0786 (1.237 sec/step)\n","I0830 16:11:32.053799 139622356395904 learning.py:507] global step 541: loss = 7.4855 (1.235 sec/step)\n","I0830 16:11:33.249148 139622356395904 learning.py:507] global step 542: loss = 7.7489 (1.193 sec/step)\n","I0830 16:11:34.468365 139622356395904 learning.py:507] global step 543: loss = 8.1402 (1.217 sec/step)\n","I0830 16:11:35.686273 139622356395904 learning.py:507] global step 544: loss = 9.3613 (1.216 sec/step)\n","I0830 16:11:36.960102 139622356395904 learning.py:507] global step 545: loss = 8.0114 (1.272 sec/step)\n","I0830 16:11:38.161882 139622356395904 learning.py:507] global step 546: loss = 8.0888 (1.199 sec/step)\n","I0830 16:11:39.381644 139622356395904 learning.py:507] global step 547: loss = 8.1995 (1.218 sec/step)\n","I0830 16:11:40.606959 139622356395904 learning.py:507] global step 548: loss = 7.9142 (1.223 sec/step)\n","I0830 16:11:41.845493 139622356395904 learning.py:507] global step 549: loss = 8.0205 (1.237 sec/step)\n","I0830 16:11:43.040012 139622356395904 learning.py:507] global step 550: loss = 7.7153 (1.193 sec/step)\n","I0830 16:11:44.260169 139622356395904 learning.py:507] global step 551: loss = 8.9952 (1.218 sec/step)\n","I0830 16:11:45.492905 139622356395904 learning.py:507] global step 552: loss = 8.3203 (1.231 sec/step)\n","I0830 16:11:46.712343 139622356395904 learning.py:507] global step 553: loss = 8.0874 (1.218 sec/step)\n","I0830 16:11:47.980627 139622356395904 learning.py:507] global step 554: loss = 7.7693 (1.266 sec/step)\n","I0830 16:11:49.213811 139622356395904 learning.py:507] global step 555: loss = 7.6041 (1.231 sec/step)\n","I0830 16:11:50.411896 139622356395904 learning.py:507] global step 556: loss = 7.1729 (1.196 sec/step)\n","I0830 16:11:51.668119 139622356395904 learning.py:507] global step 557: loss = 8.0074 (1.254 sec/step)\n","I0830 16:11:52.864233 139622356395904 learning.py:507] global step 558: loss = 8.6203 (1.194 sec/step)\n","I0830 16:11:54.102168 139622356395904 learning.py:507] global step 559: loss = 7.5291 (1.236 sec/step)\n","I0830 16:11:55.349429 139622356395904 learning.py:507] global step 560: loss = 8.4216 (1.245 sec/step)\n","I0830 16:11:56.605572 139622356395904 learning.py:507] global step 561: loss = 7.6546 (1.254 sec/step)\n","I0830 16:11:57.815279 139622356395904 learning.py:507] global step 562: loss = 8.5186 (1.208 sec/step)\n","I0830 16:11:59.038601 139622356395904 learning.py:507] global step 563: loss = 7.8937 (1.222 sec/step)\n","I0830 16:12:00.468854 139622356395904 learning.py:507] global step 564: loss = 8.2911 (1.369 sec/step)\n","I0830 16:12:02.330573 139622356395904 learning.py:507] global step 565: loss = 7.9716 (1.779 sec/step)\n","I0830 16:12:02.379013 139619299985152 supervisor.py:1050] Recording summary at step 565.\n","I0830 16:12:02.429329 139619308377856 supervisor.py:1099] global_step/sec: 0.808397\n","I0830 16:12:03.585660 139622356395904 learning.py:507] global step 566: loss = 8.2009 (1.253 sec/step)\n","I0830 16:12:04.785672 139622356395904 learning.py:507] global step 567: loss = 7.6385 (1.198 sec/step)\n","I0830 16:12:06.008707 139622356395904 learning.py:507] global step 568: loss = 8.1441 (1.221 sec/step)\n","I0830 16:12:07.239659 139622356395904 learning.py:507] global step 569: loss = 7.6983 (1.228 sec/step)\n","I0830 16:12:08.483206 139622356395904 learning.py:507] global step 570: loss = 7.5248 (1.242 sec/step)\n","I0830 16:12:09.719987 139622356395904 learning.py:507] global step 571: loss = 7.8874 (1.235 sec/step)\n","I0830 16:12:10.953368 139622356395904 learning.py:507] global step 572: loss = 7.7365 (1.232 sec/step)\n","I0830 16:12:12.154974 139622356395904 learning.py:507] global step 573: loss = 7.8421 (1.199 sec/step)\n","I0830 16:12:13.370863 139622356395904 learning.py:507] global step 574: loss = 7.4132 (1.214 sec/step)\n","I0830 16:12:14.603097 139622356395904 learning.py:507] global step 575: loss = 7.4568 (1.230 sec/step)\n","I0830 16:12:15.818428 139622356395904 learning.py:507] global step 576: loss = 7.6570 (1.213 sec/step)\n","I0830 16:12:17.045261 139622356395904 learning.py:507] global step 577: loss = 7.9405 (1.225 sec/step)\n","I0830 16:12:18.297228 139622356395904 learning.py:507] global step 578: loss = 7.9885 (1.250 sec/step)\n","I0830 16:12:19.522225 139622356395904 learning.py:507] global step 579: loss = 7.3804 (1.223 sec/step)\n","I0830 16:12:20.738675 139622356395904 learning.py:507] global step 580: loss = 7.6345 (1.215 sec/step)\n","I0830 16:12:21.967982 139622356395904 learning.py:507] global step 581: loss = 7.5694 (1.227 sec/step)\n","I0830 16:12:23.183605 139622356395904 learning.py:507] global step 582: loss = 7.6052 (1.214 sec/step)\n","I0830 16:12:24.427870 139622356395904 learning.py:507] global step 583: loss = 7.4934 (1.242 sec/step)\n","I0830 16:12:25.658633 139622356395904 learning.py:507] global step 584: loss = 7.7195 (1.229 sec/step)\n","I0830 16:12:26.890254 139622356395904 learning.py:507] global step 585: loss = 7.7209 (1.229 sec/step)\n","I0830 16:12:28.125038 139622356395904 learning.py:507] global step 586: loss = 7.7846 (1.233 sec/step)\n","I0830 16:12:29.385630 139622356395904 learning.py:507] global step 587: loss = 8.4518 (1.259 sec/step)\n","I0830 16:12:30.623507 139622356395904 learning.py:507] global step 588: loss = 7.8192 (1.236 sec/step)\n","I0830 16:12:31.827090 139622356395904 learning.py:507] global step 589: loss = 7.9504 (1.202 sec/step)\n","I0830 16:12:33.082487 139622356395904 learning.py:507] global step 590: loss = 7.5083 (1.254 sec/step)\n","I0830 16:12:34.297282 139622356395904 learning.py:507] global step 591: loss = 7.9533 (1.213 sec/step)\n","I0830 16:12:35.510703 139622356395904 learning.py:507] global step 592: loss = 7.4381 (1.211 sec/step)\n","I0830 16:12:36.735940 139622356395904 learning.py:507] global step 593: loss = 7.6474 (1.223 sec/step)\n","I0830 16:12:37.953974 139622356395904 learning.py:507] global step 594: loss = 7.2728 (1.216 sec/step)\n","I0830 16:12:39.179248 139622356395904 learning.py:507] global step 595: loss = 7.2621 (1.223 sec/step)\n","I0830 16:12:40.398926 139622356395904 learning.py:507] global step 596: loss = 7.6076 (1.218 sec/step)\n","I0830 16:12:41.645070 139622356395904 learning.py:507] global step 597: loss = 7.7110 (1.244 sec/step)\n","I0830 16:12:42.835624 139622356395904 learning.py:507] global step 598: loss = 7.7790 (1.189 sec/step)\n","I0830 16:12:44.077138 139622356395904 learning.py:507] global step 599: loss = 7.8956 (1.239 sec/step)\n","I0830 16:12:45.282912 139622356395904 learning.py:507] global step 600: loss = 7.0575 (1.204 sec/step)\n","I0830 16:12:46.517616 139622356395904 learning.py:507] global step 601: loss = 7.5108 (1.233 sec/step)\n","I0830 16:12:47.758164 139622356395904 learning.py:507] global step 602: loss = 7.4080 (1.238 sec/step)\n","I0830 16:12:49.004329 139622356395904 learning.py:507] global step 603: loss = 7.5304 (1.244 sec/step)\n","I0830 16:12:50.213976 139622356395904 learning.py:507] global step 604: loss = 7.4240 (1.208 sec/step)\n","I0830 16:12:51.455293 139622356395904 learning.py:507] global step 605: loss = 7.7067 (1.240 sec/step)\n","I0830 16:12:52.703830 139622356395904 learning.py:507] global step 606: loss = 7.6648 (1.247 sec/step)\n","I0830 16:12:53.967906 139622356395904 learning.py:507] global step 607: loss = 8.3541 (1.262 sec/step)\n","I0830 16:12:55.168591 139622356395904 learning.py:507] global step 608: loss = 7.5969 (1.199 sec/step)\n","I0830 16:12:56.419414 139622356395904 learning.py:507] global step 609: loss = 7.6280 (1.249 sec/step)\n","I0830 16:12:57.650192 139622356395904 learning.py:507] global step 610: loss = 7.3404 (1.229 sec/step)\n","I0830 16:12:58.884759 139622356395904 learning.py:507] global step 611: loss = 7.7336 (1.233 sec/step)\n","I0830 16:13:00.103708 139622356395904 learning.py:507] global step 612: loss = 7.8854 (1.217 sec/step)\n","I0830 16:13:01.312614 139622356395904 learning.py:507] global step 613: loss = 7.5111 (1.207 sec/step)\n","I0830 16:13:02.559965 139622356395904 learning.py:507] global step 614: loss = 8.0035 (1.246 sec/step)\n","I0830 16:13:03.781809 139622356395904 learning.py:507] global step 615: loss = 8.2257 (1.220 sec/step)\n","I0830 16:13:05.001867 139622356395904 learning.py:507] global step 616: loss = 7.5672 (1.218 sec/step)\n","I0830 16:13:06.243412 139622356395904 learning.py:507] global step 617: loss = 7.2707 (1.240 sec/step)\n","I0830 16:13:07.461719 139622356395904 learning.py:507] global step 618: loss = 7.4864 (1.216 sec/step)\n","I0830 16:13:08.712430 139622356395904 learning.py:507] global step 619: loss = 7.3772 (1.249 sec/step)\n","I0830 16:13:09.914402 139622356395904 learning.py:507] global step 620: loss = 7.3980 (1.200 sec/step)\n","I0830 16:13:11.132505 139622356395904 learning.py:507] global step 621: loss = 7.4676 (1.216 sec/step)\n","I0830 16:13:12.371136 139622356395904 learning.py:507] global step 622: loss = 7.4071 (1.237 sec/step)\n","I0830 16:13:13.620712 139622356395904 learning.py:507] global step 623: loss = 7.9824 (1.248 sec/step)\n","I0830 16:13:14.866332 139622356395904 learning.py:507] global step 624: loss = 8.4190 (1.244 sec/step)\n","I0830 16:13:16.095436 139622356395904 learning.py:507] global step 625: loss = 7.5928 (1.227 sec/step)\n","I0830 16:13:17.330983 139622356395904 learning.py:507] global step 626: loss = 7.6760 (1.234 sec/step)\n","I0830 16:13:18.575175 139622356395904 learning.py:507] global step 627: loss = 7.3517 (1.242 sec/step)\n","I0830 16:13:19.798673 139622356395904 learning.py:507] global step 628: loss = 6.7515 (1.221 sec/step)\n","I0830 16:13:21.057698 139622356395904 learning.py:507] global step 629: loss = 7.9015 (1.257 sec/step)\n","I0830 16:13:22.286243 139622356395904 learning.py:507] global step 630: loss = 7.2556 (1.226 sec/step)\n","I0830 16:13:23.534428 139622356395904 learning.py:507] global step 631: loss = 7.9572 (1.246 sec/step)\n","I0830 16:13:24.747784 139622356395904 learning.py:507] global step 632: loss = 7.8629 (1.211 sec/step)\n","I0830 16:13:25.941211 139622356395904 learning.py:507] global step 633: loss = 7.8235 (1.192 sec/step)\n","I0830 16:13:27.165166 139622356395904 learning.py:507] global step 634: loss = 7.2199 (1.222 sec/step)\n","I0830 16:13:28.426005 139622356395904 learning.py:507] global step 635: loss = 7.7437 (1.259 sec/step)\n","I0830 16:13:29.683814 139622356395904 learning.py:507] global step 636: loss = 7.7555 (1.256 sec/step)\n","I0830 16:13:30.919962 139622356395904 learning.py:507] global step 637: loss = 7.0653 (1.235 sec/step)\n","I0830 16:13:32.128184 139622356395904 learning.py:507] global step 638: loss = 7.9281 (1.206 sec/step)\n","I0830 16:13:33.378465 139622356395904 learning.py:507] global step 639: loss = 7.9284 (1.249 sec/step)\n","I0830 16:13:34.620319 139622356395904 learning.py:507] global step 640: loss = 7.6738 (1.240 sec/step)\n","I0830 16:13:35.834158 139622356395904 learning.py:507] global step 641: loss = 7.5771 (1.211 sec/step)\n","I0830 16:13:37.087369 139622356395904 learning.py:507] global step 642: loss = 6.9232 (1.251 sec/step)\n","I0830 16:13:38.281974 139622356395904 learning.py:507] global step 643: loss = 7.6365 (1.193 sec/step)\n","I0830 16:13:39.497622 139622356395904 learning.py:507] global step 644: loss = 7.1332 (1.214 sec/step)\n","I0830 16:13:40.712796 139622356395904 learning.py:507] global step 645: loss = 7.6667 (1.213 sec/step)\n","I0830 16:13:41.953007 139622356395904 learning.py:507] global step 646: loss = 7.3121 (1.238 sec/step)\n","I0830 16:13:43.151141 139622356395904 learning.py:507] global step 647: loss = 7.2926 (1.196 sec/step)\n","I0830 16:13:44.338135 139622356395904 learning.py:507] global step 648: loss = 7.5937 (1.185 sec/step)\n","I0830 16:13:45.559629 139622356395904 learning.py:507] global step 649: loss = 7.5970 (1.220 sec/step)\n","I0830 16:13:46.805333 139622356395904 learning.py:507] global step 650: loss = 7.6742 (1.244 sec/step)\n","I0830 16:13:48.029893 139622356395904 learning.py:507] global step 651: loss = 7.2528 (1.223 sec/step)\n","I0830 16:13:49.244131 139622356395904 learning.py:507] global step 652: loss = 8.0772 (1.212 sec/step)\n","I0830 16:13:50.464604 139622356395904 learning.py:507] global step 653: loss = 7.7876 (1.218 sec/step)\n","I0830 16:13:51.693418 139622356395904 learning.py:507] global step 654: loss = 8.1016 (1.226 sec/step)\n","I0830 16:13:52.945663 139622356395904 learning.py:507] global step 655: loss = 7.8641 (1.250 sec/step)\n","I0830 16:13:54.182636 139622356395904 learning.py:507] global step 656: loss = 7.3343 (1.235 sec/step)\n","I0830 16:13:55.409430 139622356395904 learning.py:507] global step 657: loss = 7.9262 (1.225 sec/step)\n","I0830 16:13:56.666399 139622356395904 learning.py:507] global step 658: loss = 7.5489 (1.255 sec/step)\n","I0830 16:13:57.874473 139622356395904 learning.py:507] global step 659: loss = 7.5531 (1.206 sec/step)\n","I0830 16:13:59.093309 139622356395904 learning.py:507] global step 660: loss = 7.6616 (1.217 sec/step)\n","I0830 16:14:00.498145 139622356395904 learning.py:507] global step 661: loss = 7.2395 (1.390 sec/step)\n","I0830 16:14:02.210667 139619299985152 supervisor.py:1050] Recording summary at step 661.\n","I0830 16:14:02.254667 139622356395904 learning.py:507] global step 662: loss = 7.5549 (1.755 sec/step)\n","I0830 16:14:02.393419 139619308377856 supervisor.py:1099] global_step/sec: 0.808575\n","I0830 16:14:03.473531 139622356395904 learning.py:507] global step 663: loss = 7.5469 (1.217 sec/step)\n","I0830 16:14:04.727161 139622356395904 learning.py:507] global step 664: loss = 9.0410 (1.252 sec/step)\n","I0830 16:14:05.945749 139622356395904 learning.py:507] global step 665: loss = 7.5381 (1.217 sec/step)\n","I0830 16:14:07.159554 139622356395904 learning.py:507] global step 666: loss = 8.0367 (1.212 sec/step)\n","I0830 16:14:08.411563 139622356395904 learning.py:507] global step 667: loss = 6.9812 (1.250 sec/step)\n","I0830 16:14:09.657519 139622356395904 learning.py:507] global step 668: loss = 7.6820 (1.244 sec/step)\n","I0830 16:14:10.911165 139622356395904 learning.py:507] global step 669: loss = 7.3916 (1.252 sec/step)\n","I0830 16:14:12.106899 139622356395904 learning.py:507] global step 670: loss = 8.3261 (1.194 sec/step)\n","I0830 16:14:13.301846 139622356395904 learning.py:507] global step 671: loss = 8.4322 (1.193 sec/step)\n","I0830 16:14:14.514659 139622356395904 learning.py:507] global step 672: loss = 7.3644 (1.211 sec/step)\n","I0830 16:14:15.737239 139622356395904 learning.py:507] global step 673: loss = 7.2149 (1.221 sec/step)\n","I0830 16:14:16.953098 139622356395904 learning.py:507] global step 674: loss = 7.4505 (1.212 sec/step)\n","I0830 16:14:18.192822 139622356395904 learning.py:507] global step 675: loss = 7.5141 (1.238 sec/step)\n","I0830 16:14:19.431866 139622356395904 learning.py:507] global step 676: loss = 8.5186 (1.237 sec/step)\n","I0830 16:14:20.650160 139622356395904 learning.py:507] global step 677: loss = 6.9800 (1.216 sec/step)\n","I0830 16:14:21.884599 139622356395904 learning.py:507] global step 678: loss = 7.4671 (1.233 sec/step)\n","I0830 16:14:23.099882 139622356395904 learning.py:507] global step 679: loss = 7.0919 (1.213 sec/step)\n","I0830 16:14:24.345296 139622356395904 learning.py:507] global step 680: loss = 7.2695 (1.243 sec/step)\n","I0830 16:14:25.586736 139622356395904 learning.py:507] global step 681: loss = 7.3435 (1.239 sec/step)\n","I0830 16:14:26.818389 139622356395904 learning.py:507] global step 682: loss = 6.6952 (1.230 sec/step)\n","I0830 16:14:28.038467 139622356395904 learning.py:507] global step 683: loss = 7.4325 (1.218 sec/step)\n","I0830 16:14:29.302165 139622356395904 learning.py:507] global step 684: loss = 6.6441 (1.262 sec/step)\n","I0830 16:14:30.525886 139622356395904 learning.py:507] global step 685: loss = 7.0739 (1.222 sec/step)\n","I0830 16:14:31.729591 139622356395904 learning.py:507] global step 686: loss = 7.2937 (1.202 sec/step)\n","I0830 16:14:32.940573 139622356395904 learning.py:507] global step 687: loss = 7.1434 (1.209 sec/step)\n","I0830 16:14:34.138883 139622356395904 learning.py:507] global step 688: loss = 7.7921 (1.197 sec/step)\n","I0830 16:14:35.359492 139622356395904 learning.py:507] global step 689: loss = 7.1101 (1.219 sec/step)\n","I0830 16:14:36.587008 139622356395904 learning.py:507] global step 690: loss = 7.1755 (1.226 sec/step)\n","I0830 16:14:37.790650 139622356395904 learning.py:507] global step 691: loss = 7.7101 (1.202 sec/step)\n","I0830 16:14:39.031626 139622356395904 learning.py:507] global step 692: loss = 7.6308 (1.239 sec/step)\n","I0830 16:14:40.225239 139622356395904 learning.py:507] global step 693: loss = 7.4917 (1.192 sec/step)\n","I0830 16:14:41.460512 139622356395904 learning.py:507] global step 694: loss = 7.2309 (1.233 sec/step)\n","I0830 16:14:42.700160 139622356395904 learning.py:507] global step 695: loss = 7.6012 (1.238 sec/step)\n","I0830 16:14:43.910950 139622356395904 learning.py:507] global step 696: loss = 7.0037 (1.209 sec/step)\n","I0830 16:14:45.152117 139622356395904 learning.py:507] global step 697: loss = 7.2705 (1.239 sec/step)\n","I0830 16:14:46.387312 139622356395904 learning.py:507] global step 698: loss = 6.6402 (1.233 sec/step)\n","I0830 16:14:47.619438 139622356395904 learning.py:507] global step 699: loss = 7.5219 (1.230 sec/step)\n","I0830 16:14:48.843021 139622356395904 learning.py:507] global step 700: loss = 7.3913 (1.222 sec/step)\n","I0830 16:14:50.089931 139622356395904 learning.py:507] global step 701: loss = 6.6260 (1.245 sec/step)\n","I0830 16:14:51.326890 139622356395904 learning.py:507] global step 702: loss = 7.0982 (1.235 sec/step)\n","I0830 16:14:52.555747 139622356395904 learning.py:507] global step 703: loss = 6.9040 (1.227 sec/step)\n","I0830 16:14:53.786926 139622356395904 learning.py:507] global step 704: loss = 7.3002 (1.229 sec/step)\n","I0830 16:14:55.016929 139622356395904 learning.py:507] global step 705: loss = 6.8511 (1.228 sec/step)\n","I0830 16:14:56.257533 139622356395904 learning.py:507] global step 706: loss = 7.0055 (1.239 sec/step)\n","I0830 16:14:57.479654 139622356395904 learning.py:507] global step 707: loss = 7.1154 (1.220 sec/step)\n","I0830 16:14:58.698536 139622356395904 learning.py:507] global step 708: loss = 6.9588 (1.217 sec/step)\n","I0830 16:14:59.907608 139622356395904 learning.py:507] global step 709: loss = 7.1921 (1.207 sec/step)\n","I0830 16:15:01.134623 139622356395904 learning.py:507] global step 710: loss = 7.6495 (1.225 sec/step)\n","I0830 16:15:02.331591 139622356395904 learning.py:507] global step 711: loss = 7.9428 (1.195 sec/step)\n","I0830 16:15:03.556600 139622356395904 learning.py:507] global step 712: loss = 6.9811 (1.223 sec/step)\n","I0830 16:15:04.745180 139622356395904 learning.py:507] global step 713: loss = 6.6906 (1.187 sec/step)\n","I0830 16:15:05.962209 139622356395904 learning.py:507] global step 714: loss = 7.9654 (1.215 sec/step)\n","I0830 16:15:07.214471 139622356395904 learning.py:507] global step 715: loss = 7.2213 (1.251 sec/step)\n","I0830 16:15:08.410028 139622356395904 learning.py:507] global step 716: loss = 6.8254 (1.194 sec/step)\n","I0830 16:15:09.620907 139622356395904 learning.py:507] global step 717: loss = 7.4024 (1.206 sec/step)\n","I0830 16:15:10.863440 139622356395904 learning.py:507] global step 718: loss = 6.5216 (1.241 sec/step)\n","I0830 16:15:12.104131 139622356395904 learning.py:507] global step 719: loss = 7.5619 (1.239 sec/step)\n","I0830 16:15:13.326764 139622356395904 learning.py:507] global step 720: loss = 6.9231 (1.221 sec/step)\n","I0830 16:15:14.559415 139622356395904 learning.py:507] global step 721: loss = 6.9376 (1.231 sec/step)\n","I0830 16:15:15.753037 139622356395904 learning.py:507] global step 722: loss = 6.9319 (1.192 sec/step)\n","I0830 16:15:16.994792 139622356395904 learning.py:507] global step 723: loss = 7.4275 (1.240 sec/step)\n","I0830 16:15:18.247205 139622356395904 learning.py:507] global step 724: loss = 6.6959 (1.251 sec/step)\n","I0830 16:15:19.443087 139622356395904 learning.py:507] global step 725: loss = 6.9571 (1.194 sec/step)\n","I0830 16:15:20.624387 139622356395904 learning.py:507] global step 726: loss = 7.6930 (1.179 sec/step)\n","I0830 16:15:21.825598 139622356395904 learning.py:507] global step 727: loss = 7.3242 (1.199 sec/step)\n","I0830 16:15:23.077861 139622356395904 learning.py:507] global step 728: loss = 7.2942 (1.250 sec/step)\n","I0830 16:15:24.344259 139622356395904 learning.py:507] global step 729: loss = 7.1734 (1.264 sec/step)\n","I0830 16:15:25.588903 139622356395904 learning.py:507] global step 730: loss = 7.5126 (1.241 sec/step)\n","I0830 16:15:26.802835 139622356395904 learning.py:507] global step 731: loss = 7.3362 (1.212 sec/step)\n","I0830 16:15:28.060872 139622356395904 learning.py:507] global step 732: loss = 7.2738 (1.256 sec/step)\n","I0830 16:15:29.290007 139622356395904 learning.py:507] global step 733: loss = 7.3695 (1.227 sec/step)\n","I0830 16:15:30.512826 139622356395904 learning.py:507] global step 734: loss = 6.5783 (1.221 sec/step)\n","I0830 16:15:31.711268 139622356395904 learning.py:507] global step 735: loss = 7.4628 (1.197 sec/step)\n","I0830 16:15:32.923556 139622356395904 learning.py:507] global step 736: loss = 7.1507 (1.211 sec/step)\n","I0830 16:15:34.151206 139622356395904 learning.py:507] global step 737: loss = 7.2946 (1.226 sec/step)\n","I0830 16:15:35.345121 139622356395904 learning.py:507] global step 738: loss = 7.5553 (1.192 sec/step)\n","I0830 16:15:36.581822 139622356395904 learning.py:507] global step 739: loss = 7.7118 (1.235 sec/step)\n","I0830 16:15:37.799292 139622356395904 learning.py:507] global step 740: loss = 6.8133 (1.216 sec/step)\n","I0830 16:15:38.986697 139622356395904 learning.py:507] global step 741: loss = 6.8537 (1.185 sec/step)\n","I0830 16:15:40.249614 139622356395904 learning.py:507] global step 742: loss = 7.0891 (1.261 sec/step)\n","I0830 16:15:41.511466 139622356395904 learning.py:507] global step 743: loss = 7.5144 (1.260 sec/step)\n","I0830 16:15:42.752174 139622356395904 learning.py:507] global step 744: loss = 7.7107 (1.239 sec/step)\n","I0830 16:15:43.939214 139622356395904 learning.py:507] global step 745: loss = 6.5858 (1.185 sec/step)\n","I0830 16:15:45.178649 139622356395904 learning.py:507] global step 746: loss = 6.9667 (1.237 sec/step)\n","I0830 16:15:46.382132 139622356395904 learning.py:507] global step 747: loss = 6.8666 (1.201 sec/step)\n","I0830 16:15:47.607154 139622356395904 learning.py:507] global step 748: loss = 6.8451 (1.223 sec/step)\n","I0830 16:15:48.839891 139622356395904 learning.py:507] global step 749: loss = 7.4923 (1.231 sec/step)\n","I0830 16:15:50.039898 139622356395904 learning.py:507] global step 750: loss = 7.1010 (1.198 sec/step)\n","I0830 16:15:51.291692 139622356395904 learning.py:507] global step 751: loss = 7.3815 (1.250 sec/step)\n","I0830 16:15:52.513925 139622356395904 learning.py:507] global step 752: loss = 7.5383 (1.220 sec/step)\n","I0830 16:15:53.735871 139622356395904 learning.py:507] global step 753: loss = 7.2434 (1.220 sec/step)\n","I0830 16:15:54.965659 139622356395904 learning.py:507] global step 754: loss = 7.4802 (1.228 sec/step)\n","I0830 16:15:56.236515 139622356395904 learning.py:507] global step 755: loss = 6.9763 (1.269 sec/step)\n","I0830 16:15:57.459681 139622356395904 learning.py:507] global step 756: loss = 6.6261 (1.221 sec/step)\n","I0830 16:15:58.659853 139622356395904 learning.py:507] global step 757: loss = 7.9730 (1.198 sec/step)\n","I0830 16:15:59.969811 139622356395904 learning.py:507] global step 758: loss = 7.0241 (1.304 sec/step)\n","I0830 16:16:01.956707 139619299985152 supervisor.py:1050] Recording summary at step 759.\n","I0830 16:16:01.993160 139622356395904 learning.py:507] global step 759: loss = 7.0788 (2.022 sec/step)\n","I0830 16:16:02.629562 139619308377856 supervisor.py:1099] global_step/sec: 0.806746\n","I0830 16:16:03.245198 139622356395904 learning.py:507] global step 760: loss = 7.2721 (1.250 sec/step)\n","I0830 16:16:04.481198 139622356395904 learning.py:507] global step 761: loss = 6.8750 (1.234 sec/step)\n","I0830 16:16:05.715372 139622356395904 learning.py:507] global step 762: loss = 6.6919 (1.232 sec/step)\n","I0830 16:16:06.911224 139622356395904 learning.py:507] global step 763: loss = 6.6238 (1.194 sec/step)\n","I0830 16:16:08.124831 139622356395904 learning.py:507] global step 764: loss = 7.5488 (1.212 sec/step)\n","I0830 16:16:09.365451 139622356395904 learning.py:507] global step 765: loss = 6.8663 (1.239 sec/step)\n","I0830 16:16:10.594844 139622356395904 learning.py:507] global step 766: loss = 6.6026 (1.227 sec/step)\n","I0830 16:16:11.831314 139622356395904 learning.py:507] global step 767: loss = 6.7271 (1.232 sec/step)\n","I0830 16:16:13.046903 139622356395904 learning.py:507] global step 768: loss = 6.6450 (1.214 sec/step)\n","I0830 16:16:14.262309 139622356395904 learning.py:507] global step 769: loss = 6.6233 (1.213 sec/step)\n","I0830 16:16:15.504451 139622356395904 learning.py:507] global step 770: loss = 7.3904 (1.240 sec/step)\n","I0830 16:16:16.705899 139622356395904 learning.py:507] global step 771: loss = 7.0210 (1.196 sec/step)\n","I0830 16:16:17.941972 139622356395904 learning.py:507] global step 772: loss = 6.5245 (1.234 sec/step)\n","I0830 16:16:19.190672 139622356395904 learning.py:507] global step 773: loss = 7.2677 (1.247 sec/step)\n","I0830 16:16:20.398498 139622356395904 learning.py:507] global step 774: loss = 7.0709 (1.206 sec/step)\n","I0830 16:16:21.631875 139622356395904 learning.py:507] global step 775: loss = 6.8204 (1.231 sec/step)\n","I0830 16:16:22.867345 139622356395904 learning.py:507] global step 776: loss = 8.0626 (1.234 sec/step)\n","I0830 16:16:24.140769 139622356395904 learning.py:507] global step 777: loss = 6.8200 (1.271 sec/step)\n","I0830 16:16:25.385744 139622356395904 learning.py:507] global step 778: loss = 7.1130 (1.243 sec/step)\n","I0830 16:16:26.620391 139622356395904 learning.py:507] global step 779: loss = 7.1047 (1.233 sec/step)\n","I0830 16:16:27.810552 139622356395904 learning.py:507] global step 780: loss = 6.7539 (1.188 sec/step)\n","I0830 16:16:29.058203 139622356395904 learning.py:507] global step 781: loss = 7.1426 (1.246 sec/step)\n","I0830 16:16:30.267694 139622356395904 learning.py:507] global step 782: loss = 7.1747 (1.208 sec/step)\n","I0830 16:16:31.503191 139622356395904 learning.py:507] global step 783: loss = 7.7445 (1.234 sec/step)\n","I0830 16:16:32.735093 139622356395904 learning.py:507] global step 784: loss = 6.8013 (1.230 sec/step)\n","I0830 16:16:33.982494 139622356395904 learning.py:507] global step 785: loss = 7.1490 (1.246 sec/step)\n","I0830 16:16:35.196557 139622356395904 learning.py:507] global step 786: loss = 6.5252 (1.212 sec/step)\n","I0830 16:16:36.417595 139622356395904 learning.py:507] global step 787: loss = 7.1654 (1.219 sec/step)\n","I0830 16:16:37.639312 139622356395904 learning.py:507] global step 788: loss = 7.1866 (1.220 sec/step)\n","I0830 16:16:38.857890 139622356395904 learning.py:507] global step 789: loss = 6.5870 (1.217 sec/step)\n","I0830 16:16:40.097314 139622356395904 learning.py:507] global step 790: loss = 6.8708 (1.237 sec/step)\n","I0830 16:16:41.335578 139622356395904 learning.py:507] global step 791: loss = 6.8802 (1.236 sec/step)\n","I0830 16:16:42.591995 139622356395904 learning.py:507] global step 792: loss = 7.3063 (1.255 sec/step)\n","I0830 16:16:43.806110 139622356395904 learning.py:507] global step 793: loss = 6.3974 (1.212 sec/step)\n","I0830 16:16:45.057782 139622356395904 learning.py:507] global step 794: loss = 6.8428 (1.250 sec/step)\n","I0830 16:16:46.285571 139622356395904 learning.py:507] global step 795: loss = 6.3844 (1.226 sec/step)\n","I0830 16:16:47.504461 139622356395904 learning.py:507] global step 796: loss = 7.7801 (1.217 sec/step)\n","I0830 16:16:48.755126 139622356395904 learning.py:507] global step 797: loss = 6.6936 (1.249 sec/step)\n","I0830 16:16:49.946425 139622356395904 learning.py:507] global step 798: loss = 6.8906 (1.189 sec/step)\n","I0830 16:16:51.184436 139622356395904 learning.py:507] global step 799: loss = 7.7832 (1.236 sec/step)\n","I0830 16:16:52.434551 139622356395904 learning.py:507] global step 800: loss = 6.4552 (1.248 sec/step)\n","I0830 16:16:53.642739 139622356395904 learning.py:507] global step 801: loss = 7.5795 (1.206 sec/step)\n","I0830 16:16:54.873026 139622356395904 learning.py:507] global step 802: loss = 7.0748 (1.229 sec/step)\n","I0830 16:16:56.129440 139622356395904 learning.py:507] global step 803: loss = 7.1285 (1.255 sec/step)\n","I0830 16:16:57.327557 139622356395904 learning.py:507] global step 804: loss = 7.3227 (1.196 sec/step)\n","I0830 16:16:58.547436 139622356395904 learning.py:507] global step 805: loss = 6.6525 (1.218 sec/step)\n","I0830 16:16:59.795320 139622356395904 learning.py:507] global step 806: loss = 7.3710 (1.246 sec/step)\n","I0830 16:17:00.986137 139622356395904 learning.py:507] global step 807: loss = 6.4345 (1.189 sec/step)\n","I0830 16:17:02.227885 139622356395904 learning.py:507] global step 808: loss = 7.0598 (1.240 sec/step)\n","I0830 16:17:03.453606 139622356395904 learning.py:507] global step 809: loss = 7.1394 (1.224 sec/step)\n","I0830 16:17:04.702519 139622356395904 learning.py:507] global step 810: loss = 7.4748 (1.247 sec/step)\n","I0830 16:17:05.933258 139622356395904 learning.py:507] global step 811: loss = 7.3253 (1.229 sec/step)\n","I0830 16:17:07.141134 139622356395904 learning.py:507] global step 812: loss = 7.3697 (1.206 sec/step)\n","I0830 16:17:08.378598 139622356395904 learning.py:507] global step 813: loss = 6.5263 (1.236 sec/step)\n","I0830 16:17:09.617829 139622356395904 learning.py:507] global step 814: loss = 7.3684 (1.237 sec/step)\n","I0830 16:17:10.859660 139622356395904 learning.py:507] global step 815: loss = 6.3541 (1.240 sec/step)\n","I0830 16:17:12.080843 139622356395904 learning.py:507] global step 816: loss = 6.7001 (1.219 sec/step)\n","I0830 16:17:13.314766 139622356395904 learning.py:507] global step 817: loss = 6.6835 (1.232 sec/step)\n","I0830 16:17:14.537718 139622356395904 learning.py:507] global step 818: loss = 7.6463 (1.221 sec/step)\n","I0830 16:17:15.748856 139622356395904 learning.py:507] global step 819: loss = 7.7283 (1.209 sec/step)\n","I0830 16:17:16.976547 139622356395904 learning.py:507] global step 820: loss = 5.9760 (1.226 sec/step)\n","I0830 16:17:18.236290 139622356395904 learning.py:507] global step 821: loss = 7.9228 (1.258 sec/step)\n","I0830 16:17:19.454415 139622356395904 learning.py:507] global step 822: loss = 7.0661 (1.216 sec/step)\n","I0830 16:17:20.692066 139622356395904 learning.py:507] global step 823: loss = 6.7268 (1.236 sec/step)\n","I0830 16:17:21.932513 139622356395904 learning.py:507] global step 824: loss = 7.2728 (1.239 sec/step)\n","I0830 16:17:23.178404 139622356395904 learning.py:507] global step 825: loss = 6.8152 (1.244 sec/step)\n","I0830 16:17:24.403292 139622356395904 learning.py:507] global step 826: loss = 6.7712 (1.223 sec/step)\n","I0830 16:17:25.629097 139622356395904 learning.py:507] global step 827: loss = 6.4819 (1.224 sec/step)\n","I0830 16:17:26.905044 139622356395904 learning.py:507] global step 828: loss = 6.8933 (1.270 sec/step)\n","I0830 16:17:28.116947 139622356395904 learning.py:507] global step 829: loss = 7.3188 (1.210 sec/step)\n","I0830 16:17:29.348625 139622356395904 learning.py:507] global step 830: loss = 6.2291 (1.230 sec/step)\n","I0830 16:17:30.588561 139622356395904 learning.py:507] global step 831: loss = 6.6821 (1.238 sec/step)\n","I0830 16:17:31.773235 139622356395904 learning.py:507] global step 832: loss = 6.9663 (1.183 sec/step)\n","I0830 16:17:33.020128 139622356395904 learning.py:507] global step 833: loss = 7.3127 (1.245 sec/step)\n","I0830 16:17:34.236615 139622356395904 learning.py:507] global step 834: loss = 7.2427 (1.215 sec/step)\n","I0830 16:17:35.467513 139622356395904 learning.py:507] global step 835: loss = 6.2767 (1.229 sec/step)\n","I0830 16:17:36.710787 139622356395904 learning.py:507] global step 836: loss = 6.8100 (1.241 sec/step)\n","I0830 16:17:37.920297 139622356395904 learning.py:507] global step 837: loss = 6.9112 (1.208 sec/step)\n","I0830 16:17:39.117194 139622356395904 learning.py:507] global step 838: loss = 6.3342 (1.195 sec/step)\n","I0830 16:17:40.332365 139622356395904 learning.py:507] global step 839: loss = 6.6464 (1.213 sec/step)\n","I0830 16:17:41.552120 139622356395904 learning.py:507] global step 840: loss = 7.1045 (1.218 sec/step)\n","I0830 16:17:42.741960 139622356395904 learning.py:507] global step 841: loss = 6.9711 (1.188 sec/step)\n","I0830 16:17:44.000330 139622356395904 learning.py:507] global step 842: loss = 6.6916 (1.256 sec/step)\n","I0830 16:17:45.207112 139622356395904 learning.py:507] global step 843: loss = 7.4616 (1.205 sec/step)\n","I0830 16:17:46.407248 139622356395904 learning.py:507] global step 844: loss = 6.7436 (1.198 sec/step)\n","I0830 16:17:47.652987 139622356395904 learning.py:507] global step 845: loss = 7.2373 (1.244 sec/step)\n","I0830 16:17:48.882282 139622356395904 learning.py:507] global step 846: loss = 6.4226 (1.227 sec/step)\n","I0830 16:17:50.103778 139622356395904 learning.py:507] global step 847: loss = 6.7626 (1.219 sec/step)\n","I0830 16:17:51.336517 139622356395904 learning.py:507] global step 848: loss = 6.6510 (1.231 sec/step)\n","I0830 16:17:52.585365 139622356395904 learning.py:507] global step 849: loss = 6.5915 (1.247 sec/step)\n","I0830 16:17:53.797549 139622356395904 learning.py:507] global step 850: loss = 7.2604 (1.210 sec/step)\n","I0830 16:17:55.041044 139622356395904 learning.py:507] global step 851: loss = 7.6005 (1.242 sec/step)\n","I0830 16:17:56.299418 139622356395904 learning.py:507] global step 852: loss = 6.8561 (1.257 sec/step)\n","I0830 16:17:57.509146 139622356395904 learning.py:507] global step 853: loss = 6.9827 (1.208 sec/step)\n","I0830 16:17:58.743803 139622356395904 learning.py:507] global step 854: loss = 6.6784 (1.233 sec/step)\n","I0830 16:18:00.052796 139622356395904 learning.py:507] global step 855: loss = 6.9258 (1.234 sec/step)\n","I0830 16:18:02.126756 139619299985152 supervisor.py:1050] Recording summary at step 856.\n","I0830 16:18:02.152497 139622356395904 learning.py:507] global step 856: loss = 7.1584 (2.096 sec/step)\n","I0830 16:18:02.397219 139619308377856 supervisor.py:1099] global_step/sec: 0.809902\n","I0830 16:18:03.387687 139622356395904 learning.py:507] global step 857: loss = 6.4530 (1.233 sec/step)\n","I0830 16:18:04.598273 139622356395904 learning.py:507] global step 858: loss = 6.9659 (1.209 sec/step)\n","I0830 16:18:05.810888 139622356395904 learning.py:507] global step 859: loss = 7.1668 (1.211 sec/step)\n","I0830 16:18:07.071378 139622356395904 learning.py:507] global step 860: loss = 7.0189 (1.259 sec/step)\n","I0830 16:18:08.267827 139622356395904 learning.py:507] global step 861: loss = 6.8620 (1.195 sec/step)\n","I0830 16:18:09.531828 139622356395904 learning.py:507] global step 862: loss = 6.6430 (1.262 sec/step)\n","I0830 16:18:10.754411 139622356395904 learning.py:507] global step 863: loss = 6.5923 (1.221 sec/step)\n","I0830 16:18:11.956603 139622356395904 learning.py:507] global step 864: loss = 8.0389 (1.200 sec/step)\n","I0830 16:18:13.163296 139622356395904 learning.py:507] global step 865: loss = 6.6701 (1.205 sec/step)\n","I0830 16:18:14.349809 139622356395904 learning.py:507] global step 866: loss = 7.0541 (1.184 sec/step)\n","I0830 16:18:15.566691 139622356395904 learning.py:507] global step 867: loss = 6.7069 (1.215 sec/step)\n","I0830 16:18:16.794577 139622356395904 learning.py:507] global step 868: loss = 6.5933 (1.226 sec/step)\n","I0830 16:18:18.015001 139622356395904 learning.py:507] global step 869: loss = 6.5589 (1.218 sec/step)\n","I0830 16:18:19.221170 139622356395904 learning.py:507] global step 870: loss = 7.0412 (1.204 sec/step)\n","I0830 16:18:20.447774 139622356395904 learning.py:507] global step 871: loss = 7.0287 (1.223 sec/step)\n","I0830 16:18:21.691739 139622356395904 learning.py:507] global step 872: loss = 6.3093 (1.242 sec/step)\n","I0830 16:18:22.887263 139622356395904 learning.py:507] global step 873: loss = 7.1736 (1.193 sec/step)\n","I0830 16:18:24.125312 139622356395904 learning.py:507] global step 874: loss = 6.2474 (1.236 sec/step)\n","I0830 16:18:25.325487 139622356395904 learning.py:507] global step 875: loss = 6.9259 (1.198 sec/step)\n","I0830 16:18:26.587347 139622356395904 learning.py:507] global step 876: loss = 6.2961 (1.260 sec/step)\n","I0830 16:18:27.792965 139622356395904 learning.py:507] global step 877: loss = 6.9353 (1.204 sec/step)\n","I0830 16:18:29.028214 139622356395904 learning.py:507] global step 878: loss = 6.2168 (1.234 sec/step)\n","I0830 16:18:30.277161 139622356395904 learning.py:507] global step 879: loss = 6.5706 (1.247 sec/step)\n","I0830 16:18:31.491125 139622356395904 learning.py:507] global step 880: loss = 6.1462 (1.212 sec/step)\n","I0830 16:18:32.729662 139622356395904 learning.py:507] global step 881: loss = 6.3712 (1.237 sec/step)\n","I0830 16:18:33.948627 139622356395904 learning.py:507] global step 882: loss = 6.7557 (1.217 sec/step)\n","I0830 16:18:35.186429 139622356395904 learning.py:507] global step 883: loss = 6.2541 (1.236 sec/step)\n","I0830 16:18:36.399808 139622356395904 learning.py:507] global step 884: loss = 7.1920 (1.212 sec/step)\n","I0830 16:18:37.652308 139622356395904 learning.py:507] global step 885: loss = 6.4375 (1.250 sec/step)\n","I0830 16:18:38.905449 139622356395904 learning.py:507] global step 886: loss = 6.1636 (1.251 sec/step)\n","I0830 16:18:40.116566 139622356395904 learning.py:507] global step 887: loss = 6.7112 (1.209 sec/step)\n","I0830 16:18:41.334703 139622356395904 learning.py:507] global step 888: loss = 6.4611 (1.216 sec/step)\n","I0830 16:18:42.554923 139622356395904 learning.py:507] global step 889: loss = 6.7731 (1.218 sec/step)\n","I0830 16:18:43.799909 139622356395904 learning.py:507] global step 890: loss = 6.8018 (1.242 sec/step)\n","I0830 16:18:45.035506 139622356395904 learning.py:507] global step 891: loss = 5.9028 (1.234 sec/step)\n","I0830 16:18:46.276514 139622356395904 learning.py:507] global step 892: loss = 6.9443 (1.239 sec/step)\n","I0830 16:18:47.503830 139622356395904 learning.py:507] global step 893: loss = 6.5660 (1.226 sec/step)\n","I0830 16:18:48.738757 139622356395904 learning.py:507] global step 894: loss = 7.2956 (1.233 sec/step)\n","I0830 16:18:49.966831 139622356395904 learning.py:507] global step 895: loss = 6.4427 (1.226 sec/step)\n","I0830 16:18:51.179747 139622356395904 learning.py:507] global step 896: loss = 6.3203 (1.211 sec/step)\n","I0830 16:18:52.412708 139622356395904 learning.py:507] global step 897: loss = 7.5794 (1.231 sec/step)\n","I0830 16:18:53.655442 139622356395904 learning.py:507] global step 898: loss = 6.7194 (1.241 sec/step)\n","I0830 16:18:54.893022 139622356395904 learning.py:507] global step 899: loss = 6.4395 (1.236 sec/step)\n","I0830 16:18:56.128951 139622356395904 learning.py:507] global step 900: loss = 7.6039 (1.234 sec/step)\n","I0830 16:18:57.327880 139622356395904 learning.py:507] global step 901: loss = 6.4447 (1.197 sec/step)\n","I0830 16:18:58.546404 139622356395904 learning.py:507] global step 902: loss = 6.8541 (1.217 sec/step)\n","I0830 16:18:59.757032 139622356395904 learning.py:507] global step 903: loss = 7.5013 (1.209 sec/step)\n","I0830 16:19:01.018087 139622356395904 learning.py:507] global step 904: loss = 7.3554 (1.259 sec/step)\n","I0830 16:19:02.220232 139622356395904 learning.py:507] global step 905: loss = 6.5286 (1.200 sec/step)\n","I0830 16:19:03.425177 139622356395904 learning.py:507] global step 906: loss = 6.9230 (1.203 sec/step)\n","I0830 16:19:04.656713 139622356395904 learning.py:507] global step 907: loss = 6.6108 (1.227 sec/step)\n","I0830 16:19:05.883256 139622356395904 learning.py:507] global step 908: loss = 6.6251 (1.225 sec/step)\n","I0830 16:19:07.143833 139622356395904 learning.py:507] global step 909: loss = 6.8461 (1.259 sec/step)\n","I0830 16:19:08.357164 139622356395904 learning.py:507] global step 910: loss = 5.9191 (1.211 sec/step)\n","I0830 16:19:09.571007 139622356395904 learning.py:507] global step 911: loss = 6.6708 (1.212 sec/step)\n","I0830 16:19:10.809703 139622356395904 learning.py:507] global step 912: loss = 7.3354 (1.237 sec/step)\n","I0830 16:19:12.044784 139622356395904 learning.py:507] global step 913: loss = 6.7587 (1.233 sec/step)\n","I0830 16:19:13.283099 139622356395904 learning.py:507] global step 914: loss = 6.7841 (1.237 sec/step)\n","I0830 16:19:14.505631 139622356395904 learning.py:507] global step 915: loss = 6.5402 (1.221 sec/step)\n","I0830 16:19:15.746524 139622356395904 learning.py:507] global step 916: loss = 6.9620 (1.239 sec/step)\n","I0830 16:19:16.976408 139622356395904 learning.py:507] global step 917: loss = 6.0260 (1.228 sec/step)\n","I0830 16:19:18.168323 139622356395904 learning.py:507] global step 918: loss = 6.0562 (1.190 sec/step)\n","I0830 16:19:19.370101 139622356395904 learning.py:507] global step 919: loss = 6.1975 (1.200 sec/step)\n","I0830 16:19:20.616700 139622356395904 learning.py:507] global step 920: loss = 7.0587 (1.245 sec/step)\n","I0830 16:19:21.827084 139622356395904 learning.py:507] global step 921: loss = 5.9824 (1.208 sec/step)\n","I0830 16:19:23.031321 139622356395904 learning.py:507] global step 922: loss = 6.1213 (1.202 sec/step)\n","I0830 16:19:24.256951 139622356395904 learning.py:507] global step 923: loss = 6.4178 (1.224 sec/step)\n","I0830 16:19:25.477814 139622356395904 learning.py:507] global step 924: loss = 6.5316 (1.219 sec/step)\n","I0830 16:19:26.707604 139622356395904 learning.py:507] global step 925: loss = 7.1035 (1.228 sec/step)\n","I0830 16:19:27.942124 139622356395904 learning.py:507] global step 926: loss = 6.6120 (1.233 sec/step)\n","I0830 16:19:29.174526 139622356395904 learning.py:507] global step 927: loss = 6.4393 (1.230 sec/step)\n","I0830 16:19:30.412535 139622356395904 learning.py:507] global step 928: loss = 6.6159 (1.236 sec/step)\n","I0830 16:19:31.658321 139622356395904 learning.py:507] global step 929: loss = 6.6594 (1.244 sec/step)\n","I0830 16:19:32.863370 139622356395904 learning.py:507] global step 930: loss = 6.3766 (1.203 sec/step)\n","I0830 16:19:34.101613 139622356395904 learning.py:507] global step 931: loss = 6.4259 (1.237 sec/step)\n","I0830 16:19:35.324808 139622356395904 learning.py:507] global step 932: loss = 6.7093 (1.221 sec/step)\n","I0830 16:19:36.543588 139622356395904 learning.py:507] global step 933: loss = 7.0527 (1.217 sec/step)\n","I0830 16:19:37.747713 139622356395904 learning.py:507] global step 934: loss = 6.3454 (1.202 sec/step)\n","I0830 16:19:38.948637 139622356395904 learning.py:507] global step 935: loss = 6.1905 (1.199 sec/step)\n","I0830 16:19:40.189985 139622356395904 learning.py:507] global step 936: loss = 6.8692 (1.239 sec/step)\n","I0830 16:19:41.413075 139622356395904 learning.py:507] global step 937: loss = 6.3855 (1.221 sec/step)\n","I0830 16:19:42.663777 139622356395904 learning.py:507] global step 938: loss = 6.4345 (1.249 sec/step)\n","I0830 16:19:43.909069 139622356395904 learning.py:507] global step 939: loss = 6.3949 (1.243 sec/step)\n","I0830 16:19:45.107569 139622356395904 learning.py:507] global step 940: loss = 7.2656 (1.197 sec/step)\n","I0830 16:19:46.306691 139622356395904 learning.py:507] global step 941: loss = 6.2325 (1.197 sec/step)\n","I0830 16:19:47.557898 139622356395904 learning.py:507] global step 942: loss = 6.6511 (1.249 sec/step)\n","I0830 16:19:48.779814 139622356395904 learning.py:507] global step 943: loss = 7.1103 (1.220 sec/step)\n","I0830 16:19:50.021996 139622356395904 learning.py:507] global step 944: loss = 6.6014 (1.241 sec/step)\n","I0830 16:19:51.230273 139622356395904 learning.py:507] global step 945: loss = 6.6844 (1.206 sec/step)\n","I0830 16:19:52.472862 139622356395904 learning.py:507] global step 946: loss = 7.1274 (1.241 sec/step)\n","I0830 16:19:53.694812 139622356395904 learning.py:507] global step 947: loss = 6.5225 (1.220 sec/step)\n","I0830 16:19:54.913896 139622356395904 learning.py:507] global step 948: loss = 6.8909 (1.217 sec/step)\n","I0830 16:19:56.170434 139622356395904 learning.py:507] global step 949: loss = 6.4920 (1.254 sec/step)\n","I0830 16:19:57.397674 139622356395904 learning.py:507] global step 950: loss = 5.9247 (1.225 sec/step)\n","I0830 16:19:58.661268 139622356395904 learning.py:507] global step 951: loss = 6.2587 (1.261 sec/step)\n","I0830 16:19:59.645950 139619316770560 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n","I0830 16:20:00.231531 139622356395904 learning.py:507] global step 952: loss = 7.3399 (1.567 sec/step)\n","I0830 16:20:03.449545 139619299985152 supervisor.py:1050] Recording summary at step 953.\n","I0830 16:20:03.456905 139622356395904 learning.py:507] global step 953: loss = 6.3710 (3.034 sec/step)\n","I0830 16:20:04.840300 139622356395904 learning.py:507] global step 954: loss = 6.3847 (1.382 sec/step)\n","I0830 16:20:06.069701 139622356395904 learning.py:507] global step 955: loss = 6.8469 (1.227 sec/step)\n","I0830 16:20:07.274004 139622356395904 learning.py:507] global step 956: loss = 6.8332 (1.202 sec/step)\n","I0830 16:20:08.511558 139622356395904 learning.py:507] global step 957: loss = 6.3479 (1.234 sec/step)\n","I0830 16:20:09.728216 139622356395904 learning.py:507] global step 958: loss = 6.4641 (1.215 sec/step)\n","I0830 16:20:10.959131 139622356395904 learning.py:507] global step 959: loss = 7.2960 (1.229 sec/step)\n","I0830 16:20:12.205636 139622356395904 learning.py:507] global step 960: loss = 6.6021 (1.245 sec/step)\n","I0830 16:20:13.454812 139622356395904 learning.py:507] global step 961: loss = 6.8285 (1.247 sec/step)\n","I0830 16:20:14.634921 139622356395904 learning.py:507] global step 962: loss = 6.8657 (1.178 sec/step)\n","I0830 16:20:15.860216 139622356395904 learning.py:507] global step 963: loss = 6.5065 (1.223 sec/step)\n","I0830 16:20:17.091016 139622356395904 learning.py:507] global step 964: loss = 6.2935 (1.229 sec/step)\n","I0830 16:20:18.349558 139622356395904 learning.py:507] global step 965: loss = 6.5988 (1.256 sec/step)\n","I0830 16:20:19.584815 139622356395904 learning.py:507] global step 966: loss = 5.7412 (1.233 sec/step)\n","I0830 16:20:20.794640 139622356395904 learning.py:507] global step 967: loss = 7.0291 (1.207 sec/step)\n","I0830 16:20:22.016188 139622356395904 learning.py:507] global step 968: loss = 6.4084 (1.219 sec/step)\n","I0830 16:20:23.209679 139622356395904 learning.py:507] global step 969: loss = 7.1709 (1.192 sec/step)\n","I0830 16:20:24.458850 139622356395904 learning.py:507] global step 970: loss = 6.4264 (1.247 sec/step)\n","I0830 16:20:25.692069 139622356395904 learning.py:507] global step 971: loss = 6.3717 (1.231 sec/step)\n","I0830 16:20:26.907993 139622356395904 learning.py:507] global step 972: loss = 6.5704 (1.214 sec/step)\n","I0830 16:20:28.093080 139622356395904 learning.py:507] global step 973: loss = 6.4861 (1.183 sec/step)\n","I0830 16:20:29.302088 139622356395904 learning.py:507] global step 974: loss = 6.2413 (1.207 sec/step)\n","I0830 16:20:30.522250 139622356395904 learning.py:507] global step 975: loss = 6.9957 (1.218 sec/step)\n","I0830 16:20:31.713364 139622356395904 learning.py:507] global step 976: loss = 6.4946 (1.189 sec/step)\n","I0830 16:20:32.955866 139622356395904 learning.py:507] global step 977: loss = 6.4167 (1.240 sec/step)\n","I0830 16:20:34.199965 139622356395904 learning.py:507] global step 978: loss = 6.3964 (1.241 sec/step)\n","I0830 16:20:35.414499 139622356395904 learning.py:507] global step 979: loss = 6.0061 (1.212 sec/step)\n","I0830 16:20:36.595339 139622356395904 learning.py:507] global step 980: loss = 6.8162 (1.179 sec/step)\n","I0830 16:20:37.834457 139622356395904 learning.py:507] global step 981: loss = 7.0037 (1.236 sec/step)\n","I0830 16:20:39.043581 139622356395904 learning.py:507] global step 982: loss = 7.3312 (1.207 sec/step)\n","I0830 16:20:40.227531 139622356395904 learning.py:507] global step 983: loss = 6.0940 (1.182 sec/step)\n","I0830 16:20:41.418196 139622356395904 learning.py:507] global step 984: loss = 6.1663 (1.189 sec/step)\n","I0830 16:20:42.632453 139622356395904 learning.py:507] global step 985: loss = 6.2388 (1.212 sec/step)\n","I0830 16:20:43.822135 139622356395904 learning.py:507] global step 986: loss = 6.9055 (1.188 sec/step)\n","I0830 16:20:45.067845 139622356395904 learning.py:507] global step 987: loss = 6.9224 (1.244 sec/step)\n","I0830 16:20:46.275669 139622356395904 learning.py:507] global step 988: loss = 6.0999 (1.206 sec/step)\n","I0830 16:20:47.475300 139622356395904 learning.py:507] global step 989: loss = 6.4776 (1.198 sec/step)\n","I0830 16:20:48.679421 139622356395904 learning.py:507] global step 990: loss = 7.0165 (1.202 sec/step)\n","I0830 16:20:49.880632 139622356395904 learning.py:507] global step 991: loss = 6.3096 (1.199 sec/step)\n","I0830 16:20:51.058522 139622356395904 learning.py:507] global step 992: loss = 6.4427 (1.176 sec/step)\n","I0830 16:20:52.270139 139622356395904 learning.py:507] global step 993: loss = 6.5247 (1.210 sec/step)\n","I0830 16:20:53.511490 139622356395904 learning.py:507] global step 994: loss = 6.0692 (1.239 sec/step)\n","I0830 16:20:54.723128 139622356395904 learning.py:507] global step 995: loss = 7.2893 (1.209 sec/step)\n","I0830 16:20:55.928965 139622356395904 learning.py:507] global step 996: loss = 6.3318 (1.204 sec/step)\n","I0830 16:20:57.150153 139622356395904 learning.py:507] global step 997: loss = 6.8504 (1.219 sec/step)\n","I0830 16:20:58.386027 139622356395904 learning.py:507] global step 998: loss = 7.8815 (1.234 sec/step)\n","I0830 16:20:59.634798 139622356395904 learning.py:507] global step 999: loss = 6.4312 (1.247 sec/step)\n","I0830 16:21:00.856592 139622356395904 learning.py:507] global step 1000: loss = 6.1455 (1.220 sec/step)\n","I0830 16:21:02.085336 139622356395904 learning.py:507] global step 1001: loss = 5.8368 (1.227 sec/step)\n","I0830 16:21:03.294645 139622356395904 learning.py:507] global step 1002: loss = 7.0830 (1.207 sec/step)\n","I0830 16:21:04.499607 139622356395904 learning.py:507] global step 1003: loss = 6.4110 (1.203 sec/step)\n","I0830 16:21:05.712168 139622356395904 learning.py:507] global step 1004: loss = 6.2848 (1.211 sec/step)\n","I0830 16:21:06.899322 139622356395904 learning.py:507] global step 1005: loss = 5.5724 (1.185 sec/step)\n","I0830 16:21:08.097005 139622356395904 learning.py:507] global step 1006: loss = 6.2166 (1.196 sec/step)\n","I0830 16:21:09.341198 139622356395904 learning.py:507] global step 1007: loss = 6.8410 (1.242 sec/step)\n","I0830 16:21:10.560875 139622356395904 learning.py:507] global step 1008: loss = 6.4028 (1.218 sec/step)\n","I0830 16:21:11.763606 139622356395904 learning.py:507] global step 1009: loss = 6.1134 (1.201 sec/step)\n","I0830 16:21:12.978536 139622356395904 learning.py:507] global step 1010: loss = 6.5358 (1.213 sec/step)\n","I0830 16:21:14.204527 139622356395904 learning.py:507] global step 1011: loss = 6.8420 (1.224 sec/step)\n","I0830 16:21:15.424208 139622356395904 learning.py:507] global step 1012: loss = 6.2479 (1.218 sec/step)\n","I0830 16:21:16.636591 139622356395904 learning.py:507] global step 1013: loss = 7.2509 (1.211 sec/step)\n","I0830 16:21:17.903116 139622356395904 learning.py:507] global step 1014: loss = 6.6765 (1.265 sec/step)\n","I0830 16:21:19.149711 139622356395904 learning.py:507] global step 1015: loss = 6.4529 (1.245 sec/step)\n","I0830 16:21:20.376296 139622356395904 learning.py:507] global step 1016: loss = 6.5774 (1.225 sec/step)\n","I0830 16:21:21.604026 139622356395904 learning.py:507] global step 1017: loss = 6.1404 (1.226 sec/step)\n","I0830 16:21:22.853681 139622356395904 learning.py:507] global step 1018: loss = 6.2375 (1.248 sec/step)\n","I0830 16:21:24.065643 139622356395904 learning.py:507] global step 1019: loss = 6.2722 (1.210 sec/step)\n","I0830 16:21:25.298332 139622356395904 learning.py:507] global step 1020: loss = 6.2183 (1.231 sec/step)\n","I0830 16:21:26.559042 139622356395904 learning.py:507] global step 1021: loss = 6.9682 (1.259 sec/step)\n","I0830 16:21:27.774986 139622356395904 learning.py:507] global step 1022: loss = 6.0253 (1.214 sec/step)\n","I0830 16:21:28.997827 139622356395904 learning.py:507] global step 1023: loss = 6.3559 (1.221 sec/step)\n","I0830 16:21:30.241093 139622356395904 learning.py:507] global step 1024: loss = 6.7080 (1.241 sec/step)\n","I0830 16:21:31.494041 139622356395904 learning.py:507] global step 1025: loss = 5.9327 (1.251 sec/step)\n","I0830 16:21:32.718376 139622356395904 learning.py:507] global step 1026: loss = 6.6175 (1.222 sec/step)\n","I0830 16:21:33.940812 139622356395904 learning.py:507] global step 1027: loss = 6.5610 (1.221 sec/step)\n","I0830 16:21:35.184495 139622356395904 learning.py:507] global step 1028: loss = 5.8530 (1.242 sec/step)\n","I0830 16:21:36.427720 139622356395904 learning.py:507] global step 1029: loss = 5.8716 (1.241 sec/step)\n","I0830 16:21:37.653439 139622356395904 learning.py:507] global step 1030: loss = 6.4379 (1.224 sec/step)\n","I0830 16:21:38.863418 139622356395904 learning.py:507] global step 1031: loss = 5.9347 (1.208 sec/step)\n","I0830 16:21:40.093009 139622356395904 learning.py:507] global step 1032: loss = 7.4779 (1.228 sec/step)\n","I0830 16:21:41.397524 139622356395904 learning.py:507] global step 1033: loss = 6.1685 (1.303 sec/step)\n","I0830 16:21:42.614706 139622356395904 learning.py:507] global step 1034: loss = 6.3817 (1.215 sec/step)\n","I0830 16:21:43.824735 139622356395904 learning.py:507] global step 1035: loss = 6.6958 (1.208 sec/step)\n","I0830 16:21:45.029574 139622356395904 learning.py:507] global step 1036: loss = 6.4807 (1.203 sec/step)\n","I0830 16:21:46.250340 139622356395904 learning.py:507] global step 1037: loss = 6.0062 (1.219 sec/step)\n","I0830 16:21:47.479940 139622356395904 learning.py:507] global step 1038: loss = 6.4380 (1.228 sec/step)\n","I0830 16:21:48.737167 139622356395904 learning.py:507] global step 1039: loss = 7.0700 (1.255 sec/step)\n","I0830 16:21:49.952188 139622356395904 learning.py:507] global step 1040: loss = 5.8440 (1.213 sec/step)\n","I0830 16:21:51.179582 139622356395904 learning.py:507] global step 1041: loss = 6.5056 (1.225 sec/step)\n","I0830 16:21:52.417450 139622356395904 learning.py:507] global step 1042: loss = 6.5798 (1.236 sec/step)\n","I0830 16:21:53.665998 139622356395904 learning.py:507] global step 1043: loss = 7.0378 (1.247 sec/step)\n","I0830 16:21:54.868432 139622356395904 learning.py:507] global step 1044: loss = 6.1963 (1.201 sec/step)\n","I0830 16:21:56.114566 139622356395904 learning.py:507] global step 1045: loss = 6.3788 (1.244 sec/step)\n","I0830 16:21:57.318874 139622356395904 learning.py:507] global step 1046: loss = 6.1248 (1.202 sec/step)\n","I0830 16:21:58.528247 139622356395904 learning.py:507] global step 1047: loss = 6.7835 (1.206 sec/step)\n","I0830 16:21:59.797955 139622356395904 learning.py:507] global step 1048: loss = 7.0784 (1.261 sec/step)\n","I0830 16:22:01.963952 139619299985152 supervisor.py:1050] Recording summary at step 1049.\n","I0830 16:22:01.994752 139622356395904 learning.py:507] global step 1049: loss = 6.1865 (2.194 sec/step)\n","I0830 16:22:03.213753 139622356395904 learning.py:507] global step 1050: loss = 6.5994 (1.217 sec/step)\n","I0830 16:22:04.445225 139622356395904 learning.py:507] global step 1051: loss = 6.2195 (1.229 sec/step)\n","I0830 16:22:05.705458 139622356395904 learning.py:507] global step 1052: loss = 6.4578 (1.258 sec/step)\n","I0830 16:22:06.941498 139622356395904 learning.py:507] global step 1053: loss = 6.4105 (1.234 sec/step)\n","I0830 16:22:08.163219 139622356395904 learning.py:507] global step 1054: loss = 7.4724 (1.220 sec/step)\n","I0830 16:22:09.411842 139622356395904 learning.py:507] global step 1055: loss = 6.8614 (1.246 sec/step)\n","I0830 16:22:10.655081 139622356395904 learning.py:507] global step 1056: loss = 6.3223 (1.241 sec/step)\n","I0830 16:22:11.864862 139622356395904 learning.py:507] global step 1057: loss = 6.1779 (1.208 sec/step)\n","I0830 16:22:13.084702 139622356395904 learning.py:507] global step 1058: loss = 6.7900 (1.218 sec/step)\n","I0830 16:22:14.305876 139622356395904 learning.py:507] global step 1059: loss = 6.0366 (1.220 sec/step)\n","I0830 16:22:15.517166 139622356395904 learning.py:507] global step 1060: loss = 7.8895 (1.209 sec/step)\n","I0830 16:22:16.766505 139622356395904 learning.py:507] global step 1061: loss = 5.7536 (1.247 sec/step)\n","I0830 16:22:18.019802 139622356395904 learning.py:507] global step 1062: loss = 6.2093 (1.251 sec/step)\n","I0830 16:22:19.257005 139622356395904 learning.py:507] global step 1063: loss = 6.1006 (1.235 sec/step)\n","I0830 16:22:20.471546 139622356395904 learning.py:507] global step 1064: loss = 6.6575 (1.213 sec/step)\n","I0830 16:22:21.688505 139622356395904 learning.py:507] global step 1065: loss = 6.5490 (1.215 sec/step)\n","I0830 16:22:22.879804 139622356395904 learning.py:507] global step 1066: loss = 6.5231 (1.189 sec/step)\n","I0830 16:22:24.080186 139622356395904 learning.py:507] global step 1067: loss = 5.9029 (1.198 sec/step)\n","I0830 16:22:25.285986 139622356395904 learning.py:507] global step 1068: loss = 6.6030 (1.204 sec/step)\n","I0830 16:22:26.521121 139622356395904 learning.py:507] global step 1069: loss = 6.5752 (1.233 sec/step)\n","I0830 16:22:27.728275 139622356395904 learning.py:507] global step 1070: loss = 5.9242 (1.205 sec/step)\n","I0830 16:22:28.955023 139622356395904 learning.py:507] global step 1071: loss = 6.8724 (1.225 sec/step)\n","I0830 16:22:30.191994 139622356395904 learning.py:507] global step 1072: loss = 6.8313 (1.235 sec/step)\n","I0830 16:22:31.428649 139622356395904 learning.py:507] global step 1073: loss = 5.9851 (1.235 sec/step)\n","I0830 16:22:32.629225 139622356395904 learning.py:507] global step 1074: loss = 6.1175 (1.198 sec/step)\n","I0830 16:22:33.878675 139622356395904 learning.py:507] global step 1075: loss = 6.0604 (1.248 sec/step)\n","I0830 16:22:35.142272 139622356395904 learning.py:507] global step 1076: loss = 5.8298 (1.262 sec/step)\n","I0830 16:22:36.353631 139622356395904 learning.py:507] global step 1077: loss = 6.3246 (1.209 sec/step)\n","I0830 16:22:37.597122 139622356395904 learning.py:507] global step 1078: loss = 7.2625 (1.241 sec/step)\n","I0830 16:22:38.814822 139622356395904 learning.py:507] global step 1079: loss = 6.2682 (1.216 sec/step)\n","I0830 16:22:40.088296 139622356395904 learning.py:507] global step 1080: loss = 5.8892 (1.271 sec/step)\n","I0830 16:22:41.304785 139622356395904 learning.py:507] global step 1081: loss = 5.7202 (1.214 sec/step)\n","I0830 16:22:42.540361 139622356395904 learning.py:507] global step 1082: loss = 6.0052 (1.234 sec/step)\n","I0830 16:22:43.803042 139622356395904 learning.py:507] global step 1083: loss = 6.4635 (1.261 sec/step)\n","I0830 16:22:45.000232 139622356395904 learning.py:507] global step 1084: loss = 5.9147 (1.195 sec/step)\n","I0830 16:22:46.222785 139622356395904 learning.py:507] global step 1085: loss = 6.4747 (1.220 sec/step)\n","I0830 16:22:47.447911 139622356395904 learning.py:507] global step 1086: loss = 6.1877 (1.223 sec/step)\n","I0830 16:22:48.681131 139622356395904 learning.py:507] global step 1087: loss = 5.8472 (1.231 sec/step)\n","I0830 16:22:49.889716 139622356395904 learning.py:507] global step 1088: loss = 6.9851 (1.207 sec/step)\n","I0830 16:22:51.120216 139622356395904 learning.py:507] global step 1089: loss = 7.3175 (1.229 sec/step)\n","I0830 16:22:52.340118 139622356395904 learning.py:507] global step 1090: loss = 7.1369 (1.218 sec/step)\n","I0830 16:22:53.554000 139622356395904 learning.py:507] global step 1091: loss = 6.1198 (1.212 sec/step)\n","I0830 16:22:54.753837 139622356395904 learning.py:507] global step 1092: loss = 6.4912 (1.198 sec/step)\n","I0830 16:22:55.962493 139622356395904 learning.py:507] global step 1093: loss = 6.5021 (1.207 sec/step)\n","I0830 16:22:57.176506 139622356395904 learning.py:507] global step 1094: loss = 6.4979 (1.212 sec/step)\n","I0830 16:22:58.409207 139622356395904 learning.py:507] global step 1095: loss = 6.1499 (1.231 sec/step)\n","I0830 16:22:59.633031 139622356395904 learning.py:507] global step 1096: loss = 6.4414 (1.222 sec/step)\n","I0830 16:23:00.833401 139622356395904 learning.py:507] global step 1097: loss = 6.9064 (1.199 sec/step)\n","I0830 16:23:02.057234 139622356395904 learning.py:507] global step 1098: loss = 5.9006 (1.222 sec/step)\n","I0830 16:23:03.263436 139622356395904 learning.py:507] global step 1099: loss = 5.6767 (1.204 sec/step)\n","I0830 16:23:04.469429 139622356395904 learning.py:507] global step 1100: loss = 7.2971 (1.204 sec/step)\n","I0830 16:23:05.648194 139622356395904 learning.py:507] global step 1101: loss = 6.5019 (1.177 sec/step)\n","I0830 16:23:06.885392 139622356395904 learning.py:507] global step 1102: loss = 6.2526 (1.235 sec/step)\n","I0830 16:23:08.111225 139622356395904 learning.py:507] global step 1103: loss = 5.8944 (1.224 sec/step)\n","I0830 16:23:09.360985 139622356395904 learning.py:507] global step 1104: loss = 6.6507 (1.247 sec/step)\n","I0830 16:23:10.599281 139622356395904 learning.py:507] global step 1105: loss = 5.7793 (1.236 sec/step)\n","I0830 16:23:11.814924 139622356395904 learning.py:507] global step 1106: loss = 6.2078 (1.214 sec/step)\n","I0830 16:23:13.035231 139622356395904 learning.py:507] global step 1107: loss = 5.7838 (1.218 sec/step)\n","I0830 16:23:14.293716 139622356395904 learning.py:507] global step 1108: loss = 6.2240 (1.257 sec/step)\n","I0830 16:23:15.551120 139622356395904 learning.py:507] global step 1109: loss = 7.3208 (1.256 sec/step)\n","I0830 16:23:16.772961 139622356395904 learning.py:507] global step 1110: loss = 5.6647 (1.220 sec/step)\n","I0830 16:23:18.001495 139622356395904 learning.py:507] global step 1111: loss = 6.0761 (1.227 sec/step)\n","I0830 16:23:19.262735 139622356395904 learning.py:507] global step 1112: loss = 5.6456 (1.259 sec/step)\n","I0830 16:23:20.503322 139622356395904 learning.py:507] global step 1113: loss = 6.3869 (1.239 sec/step)\n","I0830 16:23:21.722948 139622356395904 learning.py:507] global step 1114: loss = 6.4293 (1.217 sec/step)\n","I0830 16:23:22.924028 139622356395904 learning.py:507] global step 1115: loss = 6.3223 (1.199 sec/step)\n","I0830 16:23:24.127892 139622356395904 learning.py:507] global step 1116: loss = 6.6729 (1.202 sec/step)\n","I0830 16:23:25.368351 139622356395904 learning.py:507] global step 1117: loss = 5.7735 (1.239 sec/step)\n","I0830 16:23:26.580131 139622356395904 learning.py:507] global step 1118: loss = 5.8855 (1.210 sec/step)\n","I0830 16:23:27.823139 139622356395904 learning.py:507] global step 1119: loss = 6.7883 (1.241 sec/step)\n","I0830 16:23:29.042026 139622356395904 learning.py:507] global step 1120: loss = 6.0479 (1.217 sec/step)\n","I0830 16:23:30.263333 139622356395904 learning.py:507] global step 1121: loss = 5.7181 (1.219 sec/step)\n","I0830 16:23:31.519520 139622356395904 learning.py:507] global step 1122: loss = 6.6688 (1.254 sec/step)\n","I0830 16:23:32.758746 139622356395904 learning.py:507] global step 1123: loss = 5.8117 (1.237 sec/step)\n","I0830 16:23:34.001365 139622356395904 learning.py:507] global step 1124: loss = 5.7291 (1.241 sec/step)\n","I0830 16:23:35.247044 139622356395904 learning.py:507] global step 1125: loss = 5.8410 (1.244 sec/step)\n","I0830 16:23:36.487506 139622356395904 learning.py:507] global step 1126: loss = 6.0172 (1.239 sec/step)\n","I0830 16:23:37.702044 139622356395904 learning.py:507] global step 1127: loss = 6.1451 (1.212 sec/step)\n","I0830 16:23:38.945525 139622356395904 learning.py:507] global step 1128: loss = 5.9298 (1.242 sec/step)\n","I0830 16:23:40.170951 139622356395904 learning.py:507] global step 1129: loss = 5.9391 (1.223 sec/step)\n","I0830 16:23:41.419150 139622356395904 learning.py:507] global step 1130: loss = 6.7245 (1.246 sec/step)\n","I0830 16:23:42.622542 139622356395904 learning.py:507] global step 1131: loss = 6.1687 (1.202 sec/step)\n","I0830 16:23:43.876720 139622356395904 learning.py:507] global step 1132: loss = 5.8047 (1.252 sec/step)\n","I0830 16:23:45.099114 139622356395904 learning.py:507] global step 1133: loss = 5.8269 (1.221 sec/step)\n","I0830 16:23:46.350210 139622356395904 learning.py:507] global step 1134: loss = 5.4642 (1.249 sec/step)\n","I0830 16:23:47.548127 139622356395904 learning.py:507] global step 1135: loss = 6.6713 (1.196 sec/step)\n","I0830 16:23:48.778493 139622356395904 learning.py:507] global step 1136: loss = 6.0370 (1.229 sec/step)\n","I0830 16:23:49.998027 139622356395904 learning.py:507] global step 1137: loss = 6.6149 (1.218 sec/step)\n","I0830 16:23:51.234918 139622356395904 learning.py:507] global step 1138: loss = 6.4744 (1.235 sec/step)\n","I0830 16:23:52.450523 139622356395904 learning.py:507] global step 1139: loss = 6.3091 (1.214 sec/step)\n","I0830 16:23:53.724844 139622356395904 learning.py:507] global step 1140: loss = 6.3069 (1.272 sec/step)\n","I0830 16:23:54.948542 139622356395904 learning.py:507] global step 1141: loss = 5.7950 (1.222 sec/step)\n","I0830 16:23:56.208400 139622356395904 learning.py:507] global step 1142: loss = 5.7347 (1.258 sec/step)\n","I0830 16:23:57.419073 139622356395904 learning.py:507] global step 1143: loss = 6.1851 (1.209 sec/step)\n","I0830 16:23:58.653362 139622356395904 learning.py:507] global step 1144: loss = 5.9417 (1.233 sec/step)\n","I0830 16:23:59.905399 139622356395904 learning.py:507] global step 1145: loss = 7.2912 (1.247 sec/step)\n","I0830 16:24:02.129521 139619299985152 supervisor.py:1050] Recording summary at step 1146.\n","I0830 16:24:02.162436 139622356395904 learning.py:507] global step 1146: loss = 5.9540 (2.255 sec/step)\n","I0830 16:24:03.375244 139622356395904 learning.py:507] global step 1147: loss = 7.2179 (1.211 sec/step)\n","I0830 16:24:04.610042 139622356395904 learning.py:507] global step 1148: loss = 5.7355 (1.233 sec/step)\n","I0830 16:24:05.828466 139622356395904 learning.py:507] global step 1149: loss = 6.5866 (1.216 sec/step)\n","I0830 16:24:07.056279 139622356395904 learning.py:507] global step 1150: loss = 5.6728 (1.226 sec/step)\n","I0830 16:24:08.288405 139622356395904 learning.py:507] global step 1151: loss = 5.8757 (1.230 sec/step)\n","I0830 16:24:09.531328 139622356395904 learning.py:507] global step 1152: loss = 5.7038 (1.241 sec/step)\n","I0830 16:24:10.772575 139622356395904 learning.py:507] global step 1153: loss = 6.7550 (1.239 sec/step)\n","I0830 16:24:11.974865 139622356395904 learning.py:507] global step 1154: loss = 5.7877 (1.201 sec/step)\n","I0830 16:24:13.213561 139622356395904 learning.py:507] global step 1155: loss = 5.8613 (1.237 sec/step)\n","I0830 16:24:14.414514 139622356395904 learning.py:507] global step 1156: loss = 6.0803 (1.199 sec/step)\n","I0830 16:24:15.622765 139622356395904 learning.py:507] global step 1157: loss = 6.4680 (1.206 sec/step)\n","I0830 16:24:16.852976 139622356395904 learning.py:507] global step 1158: loss = 5.6452 (1.228 sec/step)\n","I0830 16:24:18.089964 139622356395904 learning.py:507] global step 1159: loss = 5.5872 (1.235 sec/step)\n","I0830 16:24:19.298789 139622356395904 learning.py:507] global step 1160: loss = 5.7788 (1.206 sec/step)\n","I0830 16:24:20.533401 139622356395904 learning.py:507] global step 1161: loss = 6.4817 (1.233 sec/step)\n","I0830 16:24:21.756322 139622356395904 learning.py:507] global step 1162: loss = 5.8748 (1.221 sec/step)\n","I0830 16:24:22.975923 139622356395904 learning.py:507] global step 1163: loss = 6.8193 (1.218 sec/step)\n","I0830 16:24:24.230481 139622356395904 learning.py:507] global step 1164: loss = 6.1420 (1.253 sec/step)\n","I0830 16:24:25.497272 139622356395904 learning.py:507] global step 1165: loss = 6.8865 (1.265 sec/step)\n","I0830 16:24:26.682272 139622356395904 learning.py:507] global step 1166: loss = 5.6150 (1.183 sec/step)\n","I0830 16:24:27.890194 139622356395904 learning.py:507] global step 1167: loss = 7.0623 (1.206 sec/step)\n","I0830 16:24:29.118976 139622356395904 learning.py:507] global step 1168: loss = 5.4869 (1.226 sec/step)\n","I0830 16:24:30.331610 139622356395904 learning.py:507] global step 1169: loss = 5.7069 (1.211 sec/step)\n","I0830 16:24:31.561207 139622356395904 learning.py:507] global step 1170: loss = 6.3026 (1.227 sec/step)\n","I0830 16:24:32.792041 139622356395904 learning.py:507] global step 1171: loss = 5.6721 (1.229 sec/step)\n","I0830 16:24:33.994335 139622356395904 learning.py:507] global step 1172: loss = 6.9503 (1.200 sec/step)\n","I0830 16:24:35.209926 139622356395904 learning.py:507] global step 1173: loss = 5.5569 (1.214 sec/step)\n","I0830 16:24:36.452797 139622356395904 learning.py:507] global step 1174: loss = 6.1084 (1.241 sec/step)\n","I0830 16:24:37.666951 139622356395904 learning.py:507] global step 1175: loss = 5.4878 (1.212 sec/step)\n","I0830 16:24:38.868247 139622356395904 learning.py:507] global step 1176: loss = 5.8622 (1.199 sec/step)\n","I0830 16:24:40.079412 139622356395904 learning.py:507] global step 1177: loss = 5.7982 (1.209 sec/step)\n","I0830 16:24:41.325032 139622356395904 learning.py:507] global step 1178: loss = 6.2481 (1.244 sec/step)\n","I0830 16:24:42.570429 139622356395904 learning.py:507] global step 1179: loss = 6.7899 (1.244 sec/step)\n","I0830 16:24:43.784998 139622356395904 learning.py:507] global step 1180: loss = 5.9911 (1.212 sec/step)\n","I0830 16:24:45.010420 139622356395904 learning.py:507] global step 1181: loss = 5.4803 (1.223 sec/step)\n","I0830 16:24:46.237029 139622356395904 learning.py:507] global step 1182: loss = 6.5381 (1.225 sec/step)\n","I0830 16:24:47.494884 139622356395904 learning.py:507] global step 1183: loss = 6.0727 (1.256 sec/step)\n","I0830 16:24:48.774854 139622356395904 learning.py:507] global step 1184: loss = 5.6754 (1.278 sec/step)\n","I0830 16:24:50.022766 139622356395904 learning.py:507] global step 1185: loss = 5.7338 (1.246 sec/step)\n","I0830 16:24:51.259497 139622356395904 learning.py:507] global step 1186: loss = 6.4091 (1.235 sec/step)\n","I0830 16:24:52.507346 139622356395904 learning.py:507] global step 1187: loss = 5.8845 (1.246 sec/step)\n","I0830 16:24:53.697122 139622356395904 learning.py:507] global step 1188: loss = 5.5523 (1.188 sec/step)\n","I0830 16:24:54.914783 139622356395904 learning.py:507] global step 1189: loss = 6.7271 (1.216 sec/step)\n","I0830 16:24:56.143555 139622356395904 learning.py:507] global step 1190: loss = 6.5910 (1.227 sec/step)\n","I0830 16:24:57.366503 139622356395904 learning.py:507] global step 1191: loss = 6.2735 (1.221 sec/step)\n","I0830 16:24:58.598080 139622356395904 learning.py:507] global step 1192: loss = 6.3035 (1.230 sec/step)\n","I0830 16:24:59.852571 139622356395904 learning.py:507] global step 1193: loss = 6.5500 (1.253 sec/step)\n","I0830 16:25:01.068387 139622356395904 learning.py:507] global step 1194: loss = 7.5592 (1.214 sec/step)\n","I0830 16:25:02.315388 139622356395904 learning.py:507] global step 1195: loss = 6.2240 (1.244 sec/step)\n","I0830 16:25:03.533759 139622356395904 learning.py:507] global step 1196: loss = 6.1764 (1.217 sec/step)\n","I0830 16:25:04.782917 139622356395904 learning.py:507] global step 1197: loss = 6.3344 (1.247 sec/step)\n","I0830 16:25:05.999478 139622356395904 learning.py:507] global step 1198: loss = 5.5658 (1.215 sec/step)\n","I0830 16:25:07.232274 139622356395904 learning.py:507] global step 1199: loss = 5.5530 (1.231 sec/step)\n","I0830 16:25:08.454634 139622356395904 learning.py:507] global step 1200: loss = 6.1073 (1.221 sec/step)\n","I0830 16:25:09.683583 139622356395904 learning.py:507] global step 1201: loss = 5.4037 (1.227 sec/step)\n","I0830 16:25:10.908818 139622356395904 learning.py:507] global step 1202: loss = 5.6490 (1.223 sec/step)\n","I0830 16:25:12.134104 139622356395904 learning.py:507] global step 1203: loss = 5.6012 (1.224 sec/step)\n","I0830 16:25:13.376187 139622356395904 learning.py:507] global step 1204: loss = 6.0514 (1.240 sec/step)\n","I0830 16:25:14.633123 139622356395904 learning.py:507] global step 1205: loss = 6.3113 (1.255 sec/step)\n","I0830 16:25:15.855582 139622356395904 learning.py:507] global step 1206: loss = 5.6099 (1.221 sec/step)\n","I0830 16:25:17.076290 139622356395904 learning.py:507] global step 1207: loss = 6.0611 (1.217 sec/step)\n","I0830 16:25:18.325685 139622356395904 learning.py:507] global step 1208: loss = 5.5522 (1.247 sec/step)\n","I0830 16:25:19.575739 139622356395904 learning.py:507] global step 1209: loss = 5.8864 (1.248 sec/step)\n","I0830 16:25:20.802248 139622356395904 learning.py:507] global step 1210: loss = 5.8613 (1.224 sec/step)\n","I0830 16:25:22.057499 139622356395904 learning.py:507] global step 1211: loss = 5.9956 (1.253 sec/step)\n","I0830 16:25:23.276748 139622356395904 learning.py:507] global step 1212: loss = 5.9165 (1.217 sec/step)\n","I0830 16:25:24.491499 139622356395904 learning.py:507] global step 1213: loss = 5.9024 (1.213 sec/step)\n","I0830 16:25:25.751872 139622356395904 learning.py:507] global step 1214: loss = 5.9240 (1.258 sec/step)\n","I0830 16:25:26.980785 139622356395904 learning.py:507] global step 1215: loss = 5.8370 (1.227 sec/step)\n","I0830 16:25:28.236912 139622356395904 learning.py:507] global step 1216: loss = 5.8640 (1.254 sec/step)\n","I0830 16:25:29.473173 139622356395904 learning.py:507] global step 1217: loss = 5.6079 (1.234 sec/step)\n","I0830 16:25:30.713609 139622356395904 learning.py:507] global step 1218: loss = 5.1267 (1.239 sec/step)\n","I0830 16:25:31.936352 139622356395904 learning.py:507] global step 1219: loss = 5.7102 (1.221 sec/step)\n","I0830 16:25:33.152991 139622356395904 learning.py:507] global step 1220: loss = 5.8085 (1.215 sec/step)\n","I0830 16:25:34.372689 139622356395904 learning.py:507] global step 1221: loss = 5.8399 (1.218 sec/step)\n","I0830 16:25:35.615786 139622356395904 learning.py:507] global step 1222: loss = 6.8497 (1.241 sec/step)\n","I0830 16:25:36.799318 139622356395904 learning.py:507] global step 1223: loss = 5.6186 (1.182 sec/step)\n","I0830 16:25:38.009275 139622356395904 learning.py:507] global step 1224: loss = 5.9998 (1.208 sec/step)\n","I0830 16:25:39.235736 139622356395904 learning.py:507] global step 1225: loss = 6.8539 (1.225 sec/step)\n","I0830 16:25:40.459185 139622356395904 learning.py:507] global step 1226: loss = 6.2536 (1.222 sec/step)\n","I0830 16:25:41.676825 139622356395904 learning.py:507] global step 1227: loss = 6.2967 (1.216 sec/step)\n","I0830 16:25:42.887110 139622356395904 learning.py:507] global step 1228: loss = 5.8851 (1.209 sec/step)\n","I0830 16:25:44.094326 139622356395904 learning.py:507] global step 1229: loss = 5.8305 (1.205 sec/step)\n","I0830 16:25:45.331640 139622356395904 learning.py:507] global step 1230: loss = 6.1999 (1.235 sec/step)\n","I0830 16:25:46.552221 139622356395904 learning.py:507] global step 1231: loss = 6.7556 (1.218 sec/step)\n","I0830 16:25:47.772951 139622356395904 learning.py:507] global step 1232: loss = 5.9150 (1.219 sec/step)\n","I0830 16:25:49.008757 139622356395904 learning.py:507] global step 1233: loss = 5.8681 (1.234 sec/step)\n","I0830 16:25:50.238438 139622356395904 learning.py:507] global step 1234: loss = 6.8529 (1.228 sec/step)\n","I0830 16:25:51.461697 139622356395904 learning.py:507] global step 1235: loss = 5.5294 (1.221 sec/step)\n","I0830 16:25:52.674470 139622356395904 learning.py:507] global step 1236: loss = 5.7034 (1.211 sec/step)\n","I0830 16:25:53.885747 139622356395904 learning.py:507] global step 1237: loss = 5.6231 (1.209 sec/step)\n","I0830 16:25:55.126382 139622356395904 learning.py:507] global step 1238: loss = 5.2388 (1.239 sec/step)\n","I0830 16:25:56.412442 139622356395904 learning.py:507] global step 1239: loss = 7.2861 (1.284 sec/step)\n","I0830 16:25:57.669345 139622356395904 learning.py:507] global step 1240: loss = 5.7325 (1.255 sec/step)\n","I0830 16:25:58.891613 139622356395904 learning.py:507] global step 1241: loss = 6.3327 (1.220 sec/step)\n","I0830 16:26:00.256993 139622356395904 learning.py:507] global step 1242: loss = 5.8478 (1.340 sec/step)\n","I0830 16:26:02.105707 139619299985152 supervisor.py:1050] Recording summary at step 1243.\n","I0830 16:26:02.126014 139622356395904 learning.py:507] global step 1243: loss = 6.1983 (1.864 sec/step)\n","I0830 16:26:03.353005 139622356395904 learning.py:507] global step 1244: loss = 5.4551 (1.225 sec/step)\n","I0830 16:26:04.595948 139622356395904 learning.py:507] global step 1245: loss = 6.1566 (1.241 sec/step)\n","I0830 16:26:05.822957 139622356395904 learning.py:507] global step 1246: loss = 5.6753 (1.225 sec/step)\n","I0830 16:26:07.027788 139622356395904 learning.py:507] global step 1247: loss = 5.9468 (1.203 sec/step)\n","I0830 16:26:08.223652 139622356395904 learning.py:507] global step 1248: loss = 5.6725 (1.194 sec/step)\n","I0830 16:26:09.449553 139622356395904 learning.py:507] global step 1249: loss = 5.6218 (1.223 sec/step)\n","I0830 16:26:10.679649 139622356395904 learning.py:507] global step 1250: loss = 6.7928 (1.228 sec/step)\n","I0830 16:26:11.887195 139622356395904 learning.py:507] global step 1251: loss = 5.4568 (1.206 sec/step)\n","I0830 16:26:13.123558 139622356395904 learning.py:507] global step 1252: loss = 6.4019 (1.235 sec/step)\n","I0830 16:26:14.322755 139622356395904 learning.py:507] global step 1253: loss = 5.8157 (1.197 sec/step)\n","I0830 16:26:15.543950 139622356395904 learning.py:507] global step 1254: loss = 6.5059 (1.219 sec/step)\n","I0830 16:26:16.757761 139622356395904 learning.py:507] global step 1255: loss = 6.0236 (1.212 sec/step)\n","I0830 16:26:17.958932 139622356395904 learning.py:507] global step 1256: loss = 5.7681 (1.199 sec/step)\n","I0830 16:26:19.184827 139622356395904 learning.py:507] global step 1257: loss = 6.5521 (1.224 sec/step)\n","I0830 16:26:20.413643 139622356395904 learning.py:507] global step 1258: loss = 5.7899 (1.227 sec/step)\n","I0830 16:26:21.627538 139622356395904 learning.py:507] global step 1259: loss = 6.0174 (1.212 sec/step)\n","I0830 16:26:22.849457 139622356395904 learning.py:507] global step 1260: loss = 6.1634 (1.219 sec/step)\n","I0830 16:26:24.081687 139622356395904 learning.py:507] global step 1261: loss = 5.6697 (1.230 sec/step)\n","I0830 16:26:25.326827 139622356395904 learning.py:507] global step 1262: loss = 5.4539 (1.243 sec/step)\n","I0830 16:26:26.584110 139622356395904 learning.py:507] global step 1263: loss = 5.8678 (1.255 sec/step)\n","I0830 16:26:27.817724 139622356395904 learning.py:507] global step 1264: loss = 6.1377 (1.232 sec/step)\n","I0830 16:26:29.014397 139622356395904 learning.py:507] global step 1265: loss = 5.4535 (1.195 sec/step)\n","I0830 16:26:30.266241 139622356395904 learning.py:507] global step 1266: loss = 5.6546 (1.249 sec/step)\n","I0830 16:26:31.465751 139622356395904 learning.py:507] global step 1267: loss = 6.1999 (1.198 sec/step)\n","I0830 16:26:32.721221 139622356395904 learning.py:507] global step 1268: loss = 6.1083 (1.254 sec/step)\n","I0830 16:26:33.953177 139622356395904 learning.py:507] global step 1269: loss = 5.7618 (1.230 sec/step)\n","I0830 16:26:35.152899 139622356395904 learning.py:507] global step 1270: loss = 6.1780 (1.198 sec/step)\n","I0830 16:26:36.363861 139622356395904 learning.py:507] global step 1271: loss = 5.7992 (1.209 sec/step)\n","I0830 16:26:37.604318 139622356395904 learning.py:507] global step 1272: loss = 5.5001 (1.239 sec/step)\n","I0830 16:26:38.799240 139622356395904 learning.py:507] global step 1273: loss = 6.0557 (1.193 sec/step)\n","I0830 16:26:40.031260 139622356395904 learning.py:507] global step 1274: loss = 5.6807 (1.230 sec/step)\n","I0830 16:26:41.271693 139622356395904 learning.py:507] global step 1275: loss = 6.5908 (1.239 sec/step)\n","I0830 16:26:42.467777 139622356395904 learning.py:507] global step 1276: loss = 6.0556 (1.194 sec/step)\n","I0830 16:26:43.699448 139622356395904 learning.py:507] global step 1277: loss = 5.6833 (1.230 sec/step)\n","I0830 16:26:44.914433 139622356395904 learning.py:507] global step 1278: loss = 5.5929 (1.213 sec/step)\n","I0830 16:26:46.121860 139622356395904 learning.py:507] global step 1279: loss = 5.8428 (1.206 sec/step)\n","I0830 16:26:47.319483 139622356395904 learning.py:507] global step 1280: loss = 5.7187 (1.196 sec/step)\n","I0830 16:26:48.569411 139622356395904 learning.py:507] global step 1281: loss = 5.6007 (1.248 sec/step)\n","I0830 16:26:49.779215 139622356395904 learning.py:507] global step 1282: loss = 5.6393 (1.208 sec/step)\n","I0830 16:26:51.002256 139622356395904 learning.py:507] global step 1283: loss = 6.0757 (1.221 sec/step)\n","I0830 16:26:52.244402 139622356395904 learning.py:507] global step 1284: loss = 5.8996 (1.240 sec/step)\n","I0830 16:26:53.486501 139622356395904 learning.py:507] global step 1285: loss = 6.5352 (1.240 sec/step)\n","I0830 16:26:54.720976 139622356395904 learning.py:507] global step 1286: loss = 5.8456 (1.233 sec/step)\n","I0830 16:26:55.959483 139622356395904 learning.py:507] global step 1287: loss = 5.8382 (1.237 sec/step)\n","I0830 16:26:57.217026 139622356395904 learning.py:507] global step 1288: loss = 5.5173 (1.256 sec/step)\n","I0830 16:26:58.448830 139622356395904 learning.py:507] global step 1289: loss = 7.2237 (1.230 sec/step)\n","I0830 16:26:59.680666 139622356395904 learning.py:507] global step 1290: loss = 5.8627 (1.230 sec/step)\n","I0830 16:27:00.925943 139622356395904 learning.py:507] global step 1291: loss = 5.3769 (1.242 sec/step)\n","I0830 16:27:02.157107 139622356395904 learning.py:507] global step 1292: loss = 6.1942 (1.229 sec/step)\n","I0830 16:27:03.385666 139622356395904 learning.py:507] global step 1293: loss = 5.5886 (1.227 sec/step)\n","I0830 16:27:04.639301 139622356395904 learning.py:507] global step 1294: loss = 5.7791 (1.252 sec/step)\n","I0830 16:27:05.858758 139622356395904 learning.py:507] global step 1295: loss = 5.3741 (1.218 sec/step)\n","I0830 16:27:07.080495 139622356395904 learning.py:507] global step 1296: loss = 5.6538 (1.220 sec/step)\n","I0830 16:27:08.312468 139622356395904 learning.py:507] global step 1297: loss = 5.8287 (1.230 sec/step)\n","I0830 16:27:09.554127 139622356395904 learning.py:507] global step 1298: loss = 5.6605 (1.240 sec/step)\n","I0830 16:27:10.770401 139622356395904 learning.py:507] global step 1299: loss = 5.3878 (1.214 sec/step)\n","I0830 16:27:12.006474 139622356395904 learning.py:507] global step 1300: loss = 6.0457 (1.234 sec/step)\n","I0830 16:27:13.222729 139622356395904 learning.py:507] global step 1301: loss = 5.5239 (1.214 sec/step)\n","I0830 16:27:14.466092 139622356395904 learning.py:507] global step 1302: loss = 5.5406 (1.242 sec/step)\n","I0830 16:27:15.695113 139622356395904 learning.py:507] global step 1303: loss = 6.6812 (1.227 sec/step)\n","I0830 16:27:16.930951 139622356395904 learning.py:507] global step 1304: loss = 7.2091 (1.234 sec/step)\n","I0830 16:27:18.159297 139622356395904 learning.py:507] global step 1305: loss = 5.2345 (1.227 sec/step)\n","I0830 16:27:19.371868 139622356395904 learning.py:507] global step 1306: loss = 5.5095 (1.211 sec/step)\n","I0830 16:27:20.594293 139622356395904 learning.py:507] global step 1307: loss = 5.6515 (1.220 sec/step)\n","I0830 16:27:21.810350 139622356395904 learning.py:507] global step 1308: loss = 5.8532 (1.214 sec/step)\n","I0830 16:27:23.000726 139622356395904 learning.py:507] global step 1309: loss = 6.4664 (1.188 sec/step)\n","I0830 16:27:24.232613 139622356395904 learning.py:507] global step 1310: loss = 5.5974 (1.230 sec/step)\n","I0830 16:27:25.480696 139622356395904 learning.py:507] global step 1311: loss = 5.3964 (1.246 sec/step)\n","I0830 16:27:26.706269 139622356395904 learning.py:507] global step 1312: loss = 5.5780 (1.224 sec/step)\n","I0830 16:27:27.919747 139622356395904 learning.py:507] global step 1313: loss = 5.3646 (1.212 sec/step)\n","I0830 16:27:29.163548 139622356395904 learning.py:507] global step 1314: loss = 6.2999 (1.242 sec/step)\n","I0830 16:27:30.386011 139622356395904 learning.py:507] global step 1315: loss = 5.7975 (1.220 sec/step)\n","I0830 16:27:31.619381 139622356395904 learning.py:507] global step 1316: loss = 6.0961 (1.232 sec/step)\n","I0830 16:27:32.857911 139622356395904 learning.py:507] global step 1317: loss = 5.9255 (1.237 sec/step)\n","I0830 16:27:34.101341 139622356395904 learning.py:507] global step 1318: loss = 5.5763 (1.239 sec/step)\n","I0830 16:27:35.337199 139622356395904 learning.py:507] global step 1319: loss = 4.9623 (1.234 sec/step)\n","I0830 16:27:36.563476 139622356395904 learning.py:507] global step 1320: loss = 5.7619 (1.224 sec/step)\n","I0830 16:27:37.802275 139622356395904 learning.py:507] global step 1321: loss = 5.6426 (1.237 sec/step)\n","I0830 16:27:39.013723 139622356395904 learning.py:507] global step 1322: loss = 5.9580 (1.210 sec/step)\n","I0830 16:27:40.237354 139622356395904 learning.py:507] global step 1323: loss = 5.6522 (1.222 sec/step)\n","I0830 16:27:41.474843 139622356395904 learning.py:507] global step 1324: loss = 6.6347 (1.236 sec/step)\n","I0830 16:27:42.703949 139622356395904 learning.py:507] global step 1325: loss = 5.2926 (1.227 sec/step)\n","I0830 16:27:43.936269 139622356395904 learning.py:507] global step 1326: loss = 5.2652 (1.230 sec/step)\n","I0830 16:27:45.161384 139622356395904 learning.py:507] global step 1327: loss = 5.7333 (1.223 sec/step)\n","I0830 16:27:46.350090 139622356395904 learning.py:507] global step 1328: loss = 6.0626 (1.187 sec/step)\n","I0830 16:27:47.589694 139622356395904 learning.py:507] global step 1329: loss = 5.6640 (1.238 sec/step)\n","I0830 16:27:48.785000 139622356395904 learning.py:507] global step 1330: loss = 5.3313 (1.193 sec/step)\n","I0830 16:27:49.999431 139622356395904 learning.py:507] global step 1331: loss = 5.9095 (1.213 sec/step)\n","I0830 16:27:51.194164 139622356395904 learning.py:507] global step 1332: loss = 5.6339 (1.193 sec/step)\n","I0830 16:27:52.405598 139622356395904 learning.py:507] global step 1333: loss = 5.2726 (1.209 sec/step)\n","I0830 16:27:53.628819 139622356395904 learning.py:507] global step 1334: loss = 6.2889 (1.221 sec/step)\n","I0830 16:27:54.868582 139622356395904 learning.py:507] global step 1335: loss = 5.9253 (1.238 sec/step)\n","I0830 16:27:56.132507 139622356395904 learning.py:507] global step 1336: loss = 5.8205 (1.262 sec/step)\n","I0830 16:27:57.355719 139622356395904 learning.py:507] global step 1337: loss = 5.7101 (1.221 sec/step)\n","I0830 16:27:58.589319 139622356395904 learning.py:507] global step 1338: loss = 5.9573 (1.232 sec/step)\n","I0830 16:27:59.856809 139622356395904 learning.py:507] global step 1339: loss = 6.2499 (1.265 sec/step)\n","I0830 16:28:01.993090 139622356395904 learning.py:507] global step 1340: loss = 5.3854 (2.122 sec/step)\n","I0830 16:28:02.232744 139619299985152 supervisor.py:1050] Recording summary at step 1340.\n","I0830 16:28:03.226634 139622356395904 learning.py:507] global step 1341: loss = 5.6055 (1.232 sec/step)\n","I0830 16:28:04.444965 139622356395904 learning.py:507] global step 1342: loss = 5.8266 (1.216 sec/step)\n","I0830 16:28:05.684301 139622356395904 learning.py:507] global step 1343: loss = 6.4274 (1.238 sec/step)\n","I0830 16:28:06.886645 139622356395904 learning.py:507] global step 1344: loss = 5.9798 (1.200 sec/step)\n","I0830 16:28:08.089506 139622356395904 learning.py:507] global step 1345: loss = 7.0079 (1.201 sec/step)\n","I0830 16:28:09.296482 139622356395904 learning.py:507] global step 1346: loss = 5.5839 (1.205 sec/step)\n","I0830 16:28:10.526507 139622356395904 learning.py:507] global step 1347: loss = 5.8388 (1.228 sec/step)\n","I0830 16:28:11.768071 139622356395904 learning.py:507] global step 1348: loss = 5.8966 (1.240 sec/step)\n","I0830 16:28:12.969491 139622356395904 learning.py:507] global step 1349: loss = 6.1541 (1.199 sec/step)\n","I0830 16:28:14.186615 139622356395904 learning.py:507] global step 1350: loss = 6.1016 (1.215 sec/step)\n","I0830 16:28:15.405712 139622356395904 learning.py:507] global step 1351: loss = 5.5430 (1.217 sec/step)\n","I0830 16:28:16.661623 139622356395904 learning.py:507] global step 1352: loss = 6.1626 (1.254 sec/step)\n","I0830 16:28:17.891698 139622356395904 learning.py:507] global step 1353: loss = 6.4235 (1.228 sec/step)\n","I0830 16:28:19.098499 139622356395904 learning.py:507] global step 1354: loss = 5.8612 (1.205 sec/step)\n","I0830 16:28:20.335711 139622356395904 learning.py:507] global step 1355: loss = 5.9116 (1.235 sec/step)\n","I0830 16:28:21.544173 139622356395904 learning.py:507] global step 1356: loss = 7.0722 (1.207 sec/step)\n","I0830 16:28:22.779025 139622356395904 learning.py:507] global step 1357: loss = 6.0285 (1.233 sec/step)\n","I0830 16:28:24.010223 139622356395904 learning.py:507] global step 1358: loss = 6.3495 (1.229 sec/step)\n","I0830 16:28:25.236247 139622356395904 learning.py:507] global step 1359: loss = 6.5800 (1.224 sec/step)\n","I0830 16:28:26.445947 139622356395904 learning.py:507] global step 1360: loss = 5.6580 (1.208 sec/step)\n","I0830 16:28:27.665148 139622356395904 learning.py:507] global step 1361: loss = 6.5017 (1.217 sec/step)\n","I0830 16:28:28.896949 139622356395904 learning.py:507] global step 1362: loss = 6.6320 (1.230 sec/step)\n","I0830 16:28:30.117105 139622356395904 learning.py:507] global step 1363: loss = 7.1655 (1.218 sec/step)\n","I0830 16:28:31.400182 139622356395904 learning.py:507] global step 1364: loss = 5.9870 (1.281 sec/step)\n","I0830 16:28:32.633521 139622356395904 learning.py:507] global step 1365: loss = 5.9502 (1.231 sec/step)\n","I0830 16:28:33.838976 139622356395904 learning.py:507] global step 1366: loss = 5.8913 (1.203 sec/step)\n","I0830 16:28:35.059412 139622356395904 learning.py:507] global step 1367: loss = 5.8929 (1.218 sec/step)\n","I0830 16:28:36.291519 139622356395904 learning.py:507] global step 1368: loss = 6.4108 (1.230 sec/step)\n","I0830 16:28:37.550823 139622356395904 learning.py:507] global step 1369: loss = 6.1561 (1.257 sec/step)\n","I0830 16:28:38.788393 139622356395904 learning.py:507] global step 1370: loss = 5.8325 (1.236 sec/step)\n","I0830 16:28:40.042518 139622356395904 learning.py:507] global step 1371: loss = 5.4785 (1.252 sec/step)\n","I0830 16:28:41.270702 139622356395904 learning.py:507] global step 1372: loss = 5.3764 (1.226 sec/step)\n","I0830 16:28:42.518473 139622356395904 learning.py:507] global step 1373: loss = 5.8491 (1.246 sec/step)\n","I0830 16:28:43.742759 139622356395904 learning.py:507] global step 1374: loss = 5.6064 (1.223 sec/step)\n","I0830 16:28:44.997750 139622356395904 learning.py:507] global step 1375: loss = 5.2102 (1.253 sec/step)\n","I0830 16:28:46.213310 139622356395904 learning.py:507] global step 1376: loss = 5.6617 (1.214 sec/step)\n","I0830 16:28:47.434207 139622356395904 learning.py:507] global step 1377: loss = 5.4250 (1.219 sec/step)\n","I0830 16:28:48.657560 139622356395904 learning.py:507] global step 1378: loss = 6.0462 (1.221 sec/step)\n","I0830 16:28:49.855666 139622356395904 learning.py:507] global step 1379: loss = 5.8297 (1.196 sec/step)\n","I0830 16:28:51.079818 139622356395904 learning.py:507] global step 1380: loss = 5.1972 (1.222 sec/step)\n","I0830 16:28:52.255638 139622356395904 learning.py:507] global step 1381: loss = 5.5425 (1.174 sec/step)\n","I0830 16:28:53.498863 139622356395904 learning.py:507] global step 1382: loss = 6.2284 (1.241 sec/step)\n","I0830 16:28:54.719540 139622356395904 learning.py:507] global step 1383: loss = 5.6112 (1.219 sec/step)\n","I0830 16:28:55.987472 139622356395904 learning.py:507] global step 1384: loss = 6.0531 (1.266 sec/step)\n","I0830 16:28:57.198932 139622356395904 learning.py:507] global step 1385: loss = 6.5308 (1.210 sec/step)\n","I0830 16:28:58.425573 139622356395904 learning.py:507] global step 1386: loss = 5.5766 (1.225 sec/step)\n","I0830 16:28:59.684264 139622356395904 learning.py:507] global step 1387: loss = 6.1013 (1.257 sec/step)\n","I0830 16:29:00.926194 139622356395904 learning.py:507] global step 1388: loss = 5.8295 (1.240 sec/step)\n","I0830 16:29:02.164291 139622356395904 learning.py:507] global step 1389: loss = 6.1696 (1.236 sec/step)\n","I0830 16:29:03.371526 139622356395904 learning.py:507] global step 1390: loss = 5.6598 (1.205 sec/step)\n","I0830 16:29:04.580812 139622356395904 learning.py:507] global step 1391: loss = 5.2116 (1.207 sec/step)\n","I0830 16:29:05.801859 139622356395904 learning.py:507] global step 1392: loss = 5.3645 (1.219 sec/step)\n","I0830 16:29:07.039150 139622356395904 learning.py:507] global step 1393: loss = 5.5412 (1.236 sec/step)\n","I0830 16:29:08.268641 139622356395904 learning.py:507] global step 1394: loss = 5.5644 (1.228 sec/step)\n","I0830 16:29:09.519505 139622356395904 learning.py:507] global step 1395: loss = 5.6338 (1.249 sec/step)\n","I0830 16:29:10.701355 139622356395904 learning.py:507] global step 1396: loss = 5.4667 (1.180 sec/step)\n","I0830 16:29:11.905687 139622356395904 learning.py:507] global step 1397: loss = 5.4107 (1.203 sec/step)\n","I0830 16:29:13.144198 139622356395904 learning.py:507] global step 1398: loss = 5.1687 (1.237 sec/step)\n","I0830 16:29:14.359482 139622356395904 learning.py:507] global step 1399: loss = 5.6270 (1.213 sec/step)\n","I0830 16:29:15.572176 139622356395904 learning.py:507] global step 1400: loss = 6.3411 (1.211 sec/step)\n","I0830 16:29:16.803041 139622356395904 learning.py:507] global step 1401: loss = 5.1297 (1.229 sec/step)\n","I0830 16:29:17.994523 139622356395904 learning.py:507] global step 1402: loss = 6.1570 (1.189 sec/step)\n","I0830 16:29:19.198037 139622356395904 learning.py:507] global step 1403: loss = 5.5783 (1.202 sec/step)\n","I0830 16:29:20.454585 139622356395904 learning.py:507] global step 1404: loss = 5.1062 (1.255 sec/step)\n","I0830 16:29:21.673937 139622356395904 learning.py:507] global step 1405: loss = 5.1453 (1.218 sec/step)\n","I0830 16:29:22.882841 139622356395904 learning.py:507] global step 1406: loss = 5.6191 (1.207 sec/step)\n","I0830 16:29:24.131748 139622356395904 learning.py:507] global step 1407: loss = 5.4630 (1.247 sec/step)\n","I0830 16:29:25.349632 139622356395904 learning.py:507] global step 1408: loss = 6.2786 (1.216 sec/step)\n","I0830 16:29:26.582071 139622356395904 learning.py:507] global step 1409: loss = 5.8329 (1.231 sec/step)\n","I0830 16:29:27.821203 139622356395904 learning.py:507] global step 1410: loss = 5.6890 (1.238 sec/step)\n","I0830 16:29:29.069163 139622356395904 learning.py:507] global step 1411: loss = 5.2774 (1.246 sec/step)\n","I0830 16:29:30.303590 139622356395904 learning.py:507] global step 1412: loss = 6.8388 (1.232 sec/step)\n","I0830 16:29:31.560300 139622356395904 learning.py:507] global step 1413: loss = 5.2328 (1.255 sec/step)\n","I0830 16:29:32.802133 139622356395904 learning.py:507] global step 1414: loss = 5.7705 (1.240 sec/step)\n","I0830 16:29:34.023144 139622356395904 learning.py:507] global step 1415: loss = 4.8878 (1.219 sec/step)\n","I0830 16:29:35.279670 139622356395904 learning.py:507] global step 1416: loss = 5.3444 (1.255 sec/step)\n","I0830 16:29:36.502308 139622356395904 learning.py:507] global step 1417: loss = 5.2886 (1.221 sec/step)\n","I0830 16:29:37.731897 139622356395904 learning.py:507] global step 1418: loss = 6.3802 (1.228 sec/step)\n","I0830 16:29:38.975885 139622356395904 learning.py:507] global step 1419: loss = 5.4758 (1.242 sec/step)\n","I0830 16:29:40.218868 139622356395904 learning.py:507] global step 1420: loss = 5.5624 (1.241 sec/step)\n","I0830 16:29:41.477520 139622356395904 learning.py:507] global step 1421: loss = 5.4999 (1.256 sec/step)\n","I0830 16:29:42.709517 139622356395904 learning.py:507] global step 1422: loss = 5.6970 (1.230 sec/step)\n","I0830 16:29:43.951391 139622356395904 learning.py:507] global step 1423: loss = 6.2359 (1.240 sec/step)\n","I0830 16:29:45.149615 139622356395904 learning.py:507] global step 1424: loss = 6.1532 (1.196 sec/step)\n","I0830 16:29:46.393294 139622356395904 learning.py:507] global step 1425: loss = 5.9785 (1.242 sec/step)\n","I0830 16:29:47.602900 139622356395904 learning.py:507] global step 1426: loss = 5.1795 (1.208 sec/step)\n","I0830 16:29:48.817517 139622356395904 learning.py:507] global step 1427: loss = 5.1973 (1.213 sec/step)\n","I0830 16:29:50.069792 139622356395904 learning.py:507] global step 1428: loss = 5.2866 (1.250 sec/step)\n","I0830 16:29:51.290158 139622356395904 learning.py:507] global step 1429: loss = 6.5611 (1.219 sec/step)\n","I0830 16:29:52.549546 139622356395904 learning.py:507] global step 1430: loss = 5.7042 (1.258 sec/step)\n","I0830 16:29:53.791834 139622356395904 learning.py:507] global step 1431: loss = 4.7946 (1.240 sec/step)\n","I0830 16:29:55.026467 139622356395904 learning.py:507] global step 1432: loss = 6.4645 (1.233 sec/step)\n","I0830 16:29:56.228479 139622356395904 learning.py:507] global step 1433: loss = 6.7336 (1.200 sec/step)\n","I0830 16:29:57.488946 139622356395904 learning.py:507] global step 1434: loss = 5.0001 (1.259 sec/step)\n","I0830 16:29:58.718594 139622356395904 learning.py:507] global step 1435: loss = 5.5244 (1.228 sec/step)\n","I0830 16:29:59.646164 139619316770560 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n","I0830 16:30:00.270389 139622356395904 learning.py:507] global step 1436: loss = 5.8578 (1.544 sec/step)\n","I0830 16:30:03.568317 139619299985152 supervisor.py:1050] Recording summary at step 1437.\n","I0830 16:30:03.580204 139622356395904 learning.py:507] global step 1437: loss = 5.2887 (3.084 sec/step)\n","I0830 16:30:04.984806 139622356395904 learning.py:507] global step 1438: loss = 5.3056 (1.403 sec/step)\n","I0830 16:30:06.228103 139622356395904 learning.py:507] global step 1439: loss = 6.2733 (1.241 sec/step)\n","I0830 16:30:07.510213 139622356395904 learning.py:507] global step 1440: loss = 5.9523 (1.280 sec/step)\n","I0830 16:30:08.739222 139622356395904 learning.py:507] global step 1441: loss = 7.3202 (1.227 sec/step)\n","I0830 16:30:09.960429 139622356395904 learning.py:507] global step 1442: loss = 6.3439 (1.219 sec/step)\n","I0830 16:30:11.186678 139622356395904 learning.py:507] global step 1443: loss = 5.2796 (1.224 sec/step)\n","I0830 16:30:12.430920 139622356395904 learning.py:507] global step 1444: loss = 5.6182 (1.242 sec/step)\n","I0830 16:30:13.645730 139622356395904 learning.py:507] global step 1445: loss = 5.9306 (1.213 sec/step)\n","I0830 16:30:14.863334 139622356395904 learning.py:507] global step 1446: loss = 6.4613 (1.215 sec/step)\n","I0830 16:30:16.117911 139622356395904 learning.py:507] global step 1447: loss = 5.2693 (1.252 sec/step)\n","I0830 16:30:17.321560 139622356395904 learning.py:507] global step 1448: loss = 6.8147 (1.202 sec/step)\n","I0830 16:30:18.578506 139622356395904 learning.py:507] global step 1449: loss = 5.7622 (1.255 sec/step)\n","I0830 16:30:19.807731 139622356395904 learning.py:507] global step 1450: loss = 5.3093 (1.227 sec/step)\n","I0830 16:30:21.013525 139622356395904 learning.py:507] global step 1451: loss = 5.9923 (1.204 sec/step)\n","I0830 16:30:22.244698 139622356395904 learning.py:507] global step 1452: loss = 5.9356 (1.229 sec/step)\n","I0830 16:30:23.463034 139622356395904 learning.py:507] global step 1453: loss = 5.4320 (1.216 sec/step)\n","I0830 16:30:24.651658 139622356395904 learning.py:507] global step 1454: loss = 5.4018 (1.187 sec/step)\n","I0830 16:30:25.837864 139622356395904 learning.py:507] global step 1455: loss = 5.1657 (1.184 sec/step)\n","I0830 16:30:27.031777 139622356395904 learning.py:507] global step 1456: loss = 5.4528 (1.192 sec/step)\n","I0830 16:30:28.266824 139622356395904 learning.py:507] global step 1457: loss = 5.4463 (1.233 sec/step)\n","I0830 16:30:29.500304 139622356395904 learning.py:507] global step 1458: loss = 5.7946 (1.231 sec/step)\n","I0830 16:30:30.744012 139622356395904 learning.py:507] global step 1459: loss = 4.9990 (1.241 sec/step)\n","I0830 16:30:31.996837 139622356395904 learning.py:507] global step 1460: loss = 5.2883 (1.250 sec/step)\n","I0830 16:30:33.227697 139622356395904 learning.py:507] global step 1461: loss = 4.7855 (1.229 sec/step)\n","I0830 16:30:34.434013 139622356395904 learning.py:507] global step 1462: loss = 5.1602 (1.204 sec/step)\n","I0830 16:30:35.647437 139622356395904 learning.py:507] global step 1463: loss = 5.6696 (1.211 sec/step)\n","I0830 16:30:36.917309 139622356395904 learning.py:507] global step 1464: loss = 6.5678 (1.268 sec/step)\n","I0830 16:30:38.143375 139622356395904 learning.py:507] global step 1465: loss = 6.4569 (1.224 sec/step)\n","I0830 16:30:39.392474 139622356395904 learning.py:507] global step 1466: loss = 5.4204 (1.247 sec/step)\n","I0830 16:30:40.636220 139622356395904 learning.py:507] global step 1467: loss = 5.5774 (1.242 sec/step)\n","I0830 16:30:41.836357 139622356395904 learning.py:507] global step 1468: loss = 5.2175 (1.198 sec/step)\n","I0830 16:30:43.039724 139622356395904 learning.py:507] global step 1469: loss = 5.0578 (1.202 sec/step)\n","I0830 16:30:44.272861 139622356395904 learning.py:507] global step 1470: loss = 5.3264 (1.231 sec/step)\n","I0830 16:30:45.477571 139622356395904 learning.py:507] global step 1471: loss = 5.5649 (1.203 sec/step)\n","I0830 16:30:46.683810 139622356395904 learning.py:507] global step 1472: loss = 5.7081 (1.204 sec/step)\n","I0830 16:30:47.942722 139622356395904 learning.py:507] global step 1473: loss = 5.3020 (1.256 sec/step)\n","I0830 16:30:49.184536 139622356395904 learning.py:507] global step 1474: loss = 5.8240 (1.240 sec/step)\n","I0830 16:30:50.469082 139622356395904 learning.py:507] global step 1475: loss = 5.9085 (1.283 sec/step)\n","I0830 16:30:51.684396 139622356395904 learning.py:507] global step 1476: loss = 5.6542 (1.214 sec/step)\n","I0830 16:30:52.880228 139622356395904 learning.py:507] global step 1477: loss = 6.1890 (1.194 sec/step)\n","I0830 16:30:54.118223 139622356395904 learning.py:507] global step 1478: loss = 6.4338 (1.236 sec/step)\n","I0830 16:30:55.377537 139622356395904 learning.py:507] global step 1479: loss = 5.6381 (1.258 sec/step)\n","I0830 16:30:56.628211 139622356395904 learning.py:507] global step 1480: loss = 5.1757 (1.249 sec/step)\n","I0830 16:30:57.872076 139622356395904 learning.py:507] global step 1481: loss = 5.2882 (1.242 sec/step)\n","I0830 16:30:59.072203 139622356395904 learning.py:507] global step 1482: loss = 5.5379 (1.198 sec/step)\n","I0830 16:31:00.307210 139622356395904 learning.py:507] global step 1483: loss = 5.7741 (1.233 sec/step)\n","I0830 16:31:01.530529 139622356395904 learning.py:507] global step 1484: loss = 6.3864 (1.221 sec/step)\n","I0830 16:31:02.751494 139622356395904 learning.py:507] global step 1485: loss = 5.1832 (1.219 sec/step)\n","I0830 16:31:04.002106 139622356395904 learning.py:507] global step 1486: loss = 5.6775 (1.249 sec/step)\n","I0830 16:31:05.184369 139622356395904 learning.py:507] global step 1487: loss = 5.5695 (1.180 sec/step)\n","I0830 16:31:06.429291 139622356395904 learning.py:507] global step 1488: loss = 5.0697 (1.243 sec/step)\n","I0830 16:31:07.668095 139622356395904 learning.py:507] global step 1489: loss = 5.4903 (1.237 sec/step)\n","I0830 16:31:08.914845 139622356395904 learning.py:507] global step 1490: loss = 5.7687 (1.245 sec/step)\n","I0830 16:31:10.108170 139622356395904 learning.py:507] global step 1491: loss = 5.8018 (1.191 sec/step)\n","I0830 16:31:11.346778 139622356395904 learning.py:507] global step 1492: loss = 4.9827 (1.237 sec/step)\n","I0830 16:31:12.577299 139622356395904 learning.py:507] global step 1493: loss = 6.7671 (1.229 sec/step)\n","I0830 16:31:13.804665 139622356395904 learning.py:507] global step 1494: loss = 5.2019 (1.225 sec/step)\n","I0830 16:31:15.030304 139622356395904 learning.py:507] global step 1495: loss = 5.4504 (1.224 sec/step)\n","I0830 16:31:16.272313 139622356395904 learning.py:507] global step 1496: loss = 5.5771 (1.240 sec/step)\n","I0830 16:31:17.508608 139622356395904 learning.py:507] global step 1497: loss = 5.2322 (1.235 sec/step)\n","I0830 16:31:18.731205 139622356395904 learning.py:507] global step 1498: loss = 5.2538 (1.221 sec/step)\n","I0830 16:31:19.986626 139622356395904 learning.py:507] global step 1499: loss = 5.2546 (1.254 sec/step)\n","I0830 16:31:21.190456 139622356395904 learning.py:507] global step 1500: loss = 5.7041 (1.202 sec/step)\n","I0830 16:31:22.397846 139622356395904 learning.py:507] global step 1501: loss = 4.9250 (1.205 sec/step)\n","I0830 16:31:23.631235 139622356395904 learning.py:507] global step 1502: loss = 5.4886 (1.231 sec/step)\n","I0830 16:31:24.843919 139622356395904 learning.py:507] global step 1503: loss = 5.2464 (1.211 sec/step)\n","I0830 16:31:26.085095 139622356395904 learning.py:507] global step 1504: loss = 5.4747 (1.239 sec/step)\n","I0830 16:31:27.329130 139622356395904 learning.py:507] global step 1505: loss = 5.5246 (1.242 sec/step)\n","I0830 16:31:28.528557 139622356395904 learning.py:507] global step 1506: loss = 5.8359 (1.198 sec/step)\n","I0830 16:31:29.739540 139622356395904 learning.py:507] global step 1507: loss = 6.0310 (1.209 sec/step)\n","I0830 16:31:30.980125 139622356395904 learning.py:507] global step 1508: loss = 6.1518 (1.238 sec/step)\n","I0830 16:31:32.239884 139622356395904 learning.py:507] global step 1509: loss = 5.7813 (1.258 sec/step)\n","I0830 16:31:33.481949 139622356395904 learning.py:507] global step 1510: loss = 6.0042 (1.239 sec/step)\n","I0830 16:31:34.733306 139622356395904 learning.py:507] global step 1511: loss = 5.7873 (1.249 sec/step)\n","I0830 16:31:35.921715 139622356395904 learning.py:507] global step 1512: loss = 6.6661 (1.187 sec/step)\n","I0830 16:31:37.162942 139622356395904 learning.py:507] global step 1513: loss = 5.7114 (1.239 sec/step)\n","I0830 16:31:38.370764 139622356395904 learning.py:507] global step 1514: loss = 5.3781 (1.206 sec/step)\n","I0830 16:31:39.586935 139622356395904 learning.py:507] global step 1515: loss = 5.4692 (1.215 sec/step)\n","I0830 16:31:40.805075 139622356395904 learning.py:507] global step 1516: loss = 5.4734 (1.216 sec/step)\n","I0830 16:31:42.006531 139622356395904 learning.py:507] global step 1517: loss = 5.2300 (1.199 sec/step)\n","I0830 16:31:43.263242 139622356395904 learning.py:507] global step 1518: loss = 5.7346 (1.255 sec/step)\n","I0830 16:31:44.459201 139622356395904 learning.py:507] global step 1519: loss = 6.0123 (1.194 sec/step)\n","I0830 16:31:45.693548 139622356395904 learning.py:507] global step 1520: loss = 6.4388 (1.233 sec/step)\n","I0830 16:31:46.931989 139622356395904 learning.py:507] global step 1521: loss = 5.3505 (1.237 sec/step)\n","I0830 16:31:48.170803 139622356395904 learning.py:507] global step 1522: loss = 5.0318 (1.237 sec/step)\n","I0830 16:31:49.398989 139622356395904 learning.py:507] global step 1523: loss = 5.3042 (1.226 sec/step)\n","I0830 16:31:50.653368 139622356395904 learning.py:507] global step 1524: loss = 5.6117 (1.253 sec/step)\n","I0830 16:31:51.875554 139622356395904 learning.py:507] global step 1525: loss = 5.0312 (1.220 sec/step)\n","I0830 16:31:53.112318 139622356395904 learning.py:507] global step 1526: loss = 5.0071 (1.235 sec/step)\n","I0830 16:31:54.358776 139622356395904 learning.py:507] global step 1527: loss = 5.2264 (1.245 sec/step)\n","I0830 16:31:55.599802 139622356395904 learning.py:507] global step 1528: loss = 5.0764 (1.239 sec/step)\n","I0830 16:31:56.855010 139622356395904 learning.py:507] global step 1529: loss = 5.0510 (1.253 sec/step)\n","I0830 16:31:58.102657 139622356395904 learning.py:507] global step 1530: loss = 5.6754 (1.246 sec/step)\n","I0830 16:31:59.332667 139622356395904 learning.py:507] global step 1531: loss = 5.6580 (1.227 sec/step)\n","I0830 16:32:00.724525 139622356395904 learning.py:507] global step 1532: loss = 6.1575 (1.326 sec/step)\n","I0830 16:32:02.340740 139619299985152 supervisor.py:1050] Recording summary at step 1532.\n","I0830 16:32:02.816159 139622356395904 learning.py:507] global step 1533: loss = 5.1528 (2.089 sec/step)\n","I0830 16:32:04.037319 139622356395904 learning.py:507] global step 1534: loss = 5.4867 (1.219 sec/step)\n","I0830 16:32:05.297567 139622356395904 learning.py:507] global step 1535: loss = 5.1443 (1.258 sec/step)\n","I0830 16:32:06.528870 139622356395904 learning.py:507] global step 1536: loss = 5.8696 (1.229 sec/step)\n","I0830 16:32:07.751127 139622356395904 learning.py:507] global step 1537: loss = 5.9225 (1.221 sec/step)\n","I0830 16:32:08.973224 139622356395904 learning.py:507] global step 1538: loss = 6.1449 (1.220 sec/step)\n","I0830 16:32:10.215374 139622356395904 learning.py:507] global step 1539: loss = 5.9892 (1.240 sec/step)\n","I0830 16:32:11.445176 139622356395904 learning.py:507] global step 1540: loss = 5.3944 (1.228 sec/step)\n","I0830 16:32:12.641164 139622356395904 learning.py:507] global step 1541: loss = 5.5371 (1.194 sec/step)\n","I0830 16:32:13.858474 139622356395904 learning.py:507] global step 1542: loss = 5.7876 (1.215 sec/step)\n","I0830 16:32:15.112242 139622356395904 learning.py:507] global step 1543: loss = 5.6707 (1.252 sec/step)\n","I0830 16:32:16.340595 139622356395904 learning.py:507] global step 1544: loss = 6.0469 (1.226 sec/step)\n","I0830 16:32:17.571303 139622356395904 learning.py:507] global step 1545: loss = 5.1019 (1.229 sec/step)\n","I0830 16:32:18.791499 139622356395904 learning.py:507] global step 1546: loss = 5.1399 (1.218 sec/step)\n","I0830 16:32:20.013115 139622356395904 learning.py:507] global step 1547: loss = 5.4039 (1.220 sec/step)\n","I0830 16:32:21.235274 139622356395904 learning.py:507] global step 1548: loss = 5.1102 (1.220 sec/step)\n","I0830 16:32:22.496110 139622356395904 learning.py:507] global step 1549: loss = 6.5834 (1.259 sec/step)\n","I0830 16:32:23.692352 139622356395904 learning.py:507] global step 1550: loss = 5.8614 (1.194 sec/step)\n","I0830 16:32:24.915947 139622356395904 learning.py:507] global step 1551: loss = 5.2451 (1.222 sec/step)\n","I0830 16:32:26.125499 139622356395904 learning.py:507] global step 1552: loss = 5.5352 (1.207 sec/step)\n","I0830 16:32:27.371283 139622356395904 learning.py:507] global step 1553: loss = 5.7802 (1.244 sec/step)\n","I0830 16:32:28.631204 139622356395904 learning.py:507] global step 1554: loss = 5.0355 (1.258 sec/step)\n","I0830 16:32:29.837717 139622356395904 learning.py:507] global step 1555: loss = 5.4898 (1.205 sec/step)\n","I0830 16:32:31.050417 139622356395904 learning.py:507] global step 1556: loss = 5.5375 (1.211 sec/step)\n","I0830 16:32:32.317822 139622356395904 learning.py:507] global step 1557: loss = 5.1218 (1.266 sec/step)\n","I0830 16:32:33.543253 139622356395904 learning.py:507] global step 1558: loss = 5.4200 (1.223 sec/step)\n","I0830 16:32:34.748745 139622356395904 learning.py:507] global step 1559: loss = 5.0429 (1.204 sec/step)\n","I0830 16:32:35.998959 139622356395904 learning.py:507] global step 1560: loss = 5.0884 (1.248 sec/step)\n","I0830 16:32:37.221276 139622356395904 learning.py:507] global step 1561: loss = 5.5123 (1.220 sec/step)\n","I0830 16:32:38.454924 139622356395904 learning.py:507] global step 1562: loss = 5.6086 (1.232 sec/step)\n","I0830 16:32:39.658945 139622356395904 learning.py:507] global step 1563: loss = 4.8892 (1.202 sec/step)\n","I0830 16:32:40.868710 139622356395904 learning.py:507] global step 1564: loss = 5.1894 (1.208 sec/step)\n","I0830 16:32:42.116381 139622356395904 learning.py:507] global step 1565: loss = 4.8304 (1.246 sec/step)\n","I0830 16:32:43.344830 139622356395904 learning.py:507] global step 1566: loss = 5.4085 (1.227 sec/step)\n","I0830 16:32:44.570780 139622356395904 learning.py:507] global step 1567: loss = 5.4586 (1.224 sec/step)\n","I0830 16:32:45.825268 139622356395904 learning.py:507] global step 1568: loss = 5.2880 (1.253 sec/step)\n","I0830 16:32:47.039554 139622356395904 learning.py:507] global step 1569: loss = 5.4386 (1.213 sec/step)\n","I0830 16:32:48.289810 139622356395904 learning.py:507] global step 1570: loss = 6.4207 (1.248 sec/step)\n","I0830 16:32:49.507715 139622356395904 learning.py:507] global step 1571: loss = 5.6297 (1.216 sec/step)\n","I0830 16:32:50.725942 139622356395904 learning.py:507] global step 1572: loss = 5.5833 (1.216 sec/step)\n","I0830 16:32:51.913838 139622356395904 learning.py:507] global step 1573: loss = 5.8649 (1.183 sec/step)\n","I0830 16:32:53.142548 139622356395904 learning.py:507] global step 1574: loss = 5.0300 (1.227 sec/step)\n","I0830 16:32:54.348924 139622356395904 learning.py:507] global step 1575: loss = 4.8373 (1.204 sec/step)\n","I0830 16:32:55.552655 139622356395904 learning.py:507] global step 1576: loss = 5.4637 (1.202 sec/step)\n","I0830 16:32:56.777426 139622356395904 learning.py:507] global step 1577: loss = 5.3223 (1.223 sec/step)\n","I0830 16:32:58.014977 139622356395904 learning.py:507] global step 1578: loss = 4.9258 (1.236 sec/step)\n","I0830 16:32:59.266602 139622356395904 learning.py:507] global step 1579: loss = 5.3234 (1.250 sec/step)\n","I0830 16:33:00.458789 139622356395904 learning.py:507] global step 1580: loss = 4.8844 (1.190 sec/step)\n","I0830 16:33:01.665704 139622356395904 learning.py:507] global step 1581: loss = 5.0367 (1.205 sec/step)\n","I0830 16:33:02.926768 139622356395904 learning.py:507] global step 1582: loss = 5.2193 (1.259 sec/step)\n","I0830 16:33:04.136264 139622356395904 learning.py:507] global step 1583: loss = 5.3346 (1.208 sec/step)\n","I0830 16:33:05.328042 139622356395904 learning.py:507] global step 1584: loss = 5.0471 (1.190 sec/step)\n","I0830 16:33:06.535339 139622356395904 learning.py:507] global step 1585: loss = 5.5370 (1.205 sec/step)\n","I0830 16:33:07.801203 139622356395904 learning.py:507] global step 1586: loss = 6.2744 (1.264 sec/step)\n","I0830 16:33:09.008032 139622356395904 learning.py:507] global step 1587: loss = 6.0016 (1.205 sec/step)\n","I0830 16:33:10.190320 139622356395904 learning.py:507] global step 1588: loss = 5.6407 (1.181 sec/step)\n","I0830 16:33:11.447100 139622356395904 learning.py:507] global step 1589: loss = 6.1608 (1.255 sec/step)\n","I0830 16:33:12.693961 139622356395904 learning.py:507] global step 1590: loss = 5.0969 (1.245 sec/step)\n","I0830 16:33:13.906773 139622356395904 learning.py:507] global step 1591: loss = 4.5433 (1.211 sec/step)\n","I0830 16:33:15.151992 139622356395904 learning.py:507] global step 1592: loss = 6.7136 (1.243 sec/step)\n","I0830 16:33:16.403871 139622356395904 learning.py:507] global step 1593: loss = 5.8966 (1.249 sec/step)\n","I0830 16:33:17.623111 139622356395904 learning.py:507] global step 1594: loss = 5.7277 (1.217 sec/step)\n","I0830 16:33:18.869952 139622356395904 learning.py:507] global step 1595: loss = 5.4622 (1.245 sec/step)\n","I0830 16:33:20.081277 139622356395904 learning.py:507] global step 1596: loss = 4.8646 (1.209 sec/step)\n","I0830 16:33:21.310242 139622356395904 learning.py:507] global step 1597: loss = 5.2127 (1.227 sec/step)\n","I0830 16:33:22.532025 139622356395904 learning.py:507] global step 1598: loss = 5.5755 (1.220 sec/step)\n","I0830 16:33:23.741274 139622356395904 learning.py:507] global step 1599: loss = 5.3611 (1.207 sec/step)\n","I0830 16:33:24.972495 139622356395904 learning.py:507] global step 1600: loss = 5.1812 (1.229 sec/step)\n","I0830 16:33:26.232193 139622356395904 learning.py:507] global step 1601: loss = 5.8011 (1.258 sec/step)\n","I0830 16:33:27.432350 139622356395904 learning.py:507] global step 1602: loss = 5.1812 (1.198 sec/step)\n","I0830 16:33:28.669131 139622356395904 learning.py:507] global step 1603: loss = 5.3464 (1.235 sec/step)\n","I0830 16:33:29.887911 139622356395904 learning.py:507] global step 1604: loss = 6.2205 (1.217 sec/step)\n","I0830 16:33:31.141274 139622356395904 learning.py:507] global step 1605: loss = 5.7698 (1.252 sec/step)\n","I0830 16:33:32.360810 139622356395904 learning.py:507] global step 1606: loss = 5.2334 (1.218 sec/step)\n","I0830 16:33:33.574999 139622356395904 learning.py:507] global step 1607: loss = 5.6974 (1.212 sec/step)\n","I0830 16:33:34.795632 139622356395904 learning.py:507] global step 1608: loss = 5.2234 (1.219 sec/step)\n","I0830 16:33:36.043956 139622356395904 learning.py:507] global step 1609: loss = 5.7494 (1.246 sec/step)\n","I0830 16:33:37.294643 139622356395904 learning.py:507] global step 1610: loss = 5.8332 (1.249 sec/step)\n","I0830 16:33:38.513605 139622356395904 learning.py:507] global step 1611: loss = 6.4400 (1.217 sec/step)\n","I0830 16:33:39.732517 139622356395904 learning.py:507] global step 1612: loss = 5.1397 (1.217 sec/step)\n","I0830 16:33:40.931014 139622356395904 learning.py:507] global step 1613: loss = 5.1946 (1.196 sec/step)\n","I0830 16:33:42.149230 139622356395904 learning.py:507] global step 1614: loss = 5.7753 (1.216 sec/step)\n","I0830 16:33:43.387506 139622356395904 learning.py:507] global step 1615: loss = 5.0383 (1.236 sec/step)\n","I0830 16:33:44.603407 139622356395904 learning.py:507] global step 1616: loss = 4.8585 (1.214 sec/step)\n","I0830 16:33:45.839753 139622356395904 learning.py:507] global step 1617: loss = 4.9870 (1.234 sec/step)\n","I0830 16:33:47.021822 139622356395904 learning.py:507] global step 1618: loss = 4.9314 (1.180 sec/step)\n","I0830 16:33:48.244691 139622356395904 learning.py:507] global step 1619: loss = 5.9754 (1.221 sec/step)\n","I0830 16:33:49.491252 139622356395904 learning.py:507] global step 1620: loss = 5.3713 (1.244 sec/step)\n","I0830 16:33:50.685391 139622356395904 learning.py:507] global step 1621: loss = 5.5345 (1.192 sec/step)\n","I0830 16:33:51.911899 139622356395904 learning.py:507] global step 1622: loss = 5.4126 (1.225 sec/step)\n","I0830 16:33:53.100668 139622356395904 learning.py:507] global step 1623: loss = 5.1731 (1.187 sec/step)\n","I0830 16:33:54.328160 139622356395904 learning.py:507] global step 1624: loss = 5.3108 (1.226 sec/step)\n","I0830 16:33:55.544443 139622356395904 learning.py:507] global step 1625: loss = 5.0351 (1.214 sec/step)\n","I0830 16:33:56.746595 139622356395904 learning.py:507] global step 1626: loss = 5.0256 (1.200 sec/step)\n","I0830 16:33:57.979845 139622356395904 learning.py:507] global step 1627: loss = 5.2511 (1.231 sec/step)\n","I0830 16:33:59.198968 139622356395904 learning.py:507] global step 1628: loss = 5.7776 (1.217 sec/step)\n","I0830 16:34:00.671236 139622356395904 learning.py:507] global step 1629: loss = 5.6805 (1.359 sec/step)\n","I0830 16:34:02.659470 139619299985152 supervisor.py:1050] Recording summary at step 1630.\n","I0830 16:34:02.674601 139622356395904 learning.py:507] global step 1630: loss = 4.9241 (1.997 sec/step)\n","I0830 16:34:03.898262 139622356395904 learning.py:507] global step 1631: loss = 4.9802 (1.222 sec/step)\n","I0830 16:34:05.086918 139622356395904 learning.py:507] global step 1632: loss = 5.1640 (1.187 sec/step)\n","I0830 16:34:06.315725 139622356395904 learning.py:507] global step 1633: loss = 4.8051 (1.227 sec/step)\n","I0830 16:34:07.518662 139622356395904 learning.py:507] global step 1634: loss = 6.7548 (1.201 sec/step)\n","I0830 16:34:08.758486 139622356395904 learning.py:507] global step 1635: loss = 5.8802 (1.238 sec/step)\n","I0830 16:34:09.950007 139622356395904 learning.py:507] global step 1636: loss = 4.8900 (1.190 sec/step)\n","I0830 16:34:11.168176 139622356395904 learning.py:507] global step 1637: loss = 5.4743 (1.216 sec/step)\n","I0830 16:34:12.410893 139622356395904 learning.py:507] global step 1638: loss = 5.5977 (1.241 sec/step)\n","I0830 16:34:13.619738 139622356395904 learning.py:507] global step 1639: loss = 5.2399 (1.207 sec/step)\n","I0830 16:34:14.853661 139622356395904 learning.py:507] global step 1640: loss = 6.3013 (1.232 sec/step)\n","I0830 16:34:16.057079 139622356395904 learning.py:507] global step 1641: loss = 5.1320 (1.201 sec/step)\n","I0830 16:34:17.287853 139622356395904 learning.py:507] global step 1642: loss = 5.4912 (1.229 sec/step)\n","I0830 16:34:18.515541 139622356395904 learning.py:507] global step 1643: loss = 5.5606 (1.226 sec/step)\n","I0830 16:34:19.719117 139622356395904 learning.py:507] global step 1644: loss = 4.8052 (1.202 sec/step)\n","I0830 16:34:20.959704 139622356395904 learning.py:507] global step 1645: loss = 5.1455 (1.239 sec/step)\n","I0830 16:34:22.200637 139622356395904 learning.py:507] global step 1646: loss = 6.1830 (1.239 sec/step)\n","I0830 16:34:23.428378 139622356395904 learning.py:507] global step 1647: loss = 5.3085 (1.225 sec/step)\n","I0830 16:34:24.643991 139622356395904 learning.py:507] global step 1648: loss = 6.2396 (1.214 sec/step)\n","I0830 16:34:25.869109 139622356395904 learning.py:507] global step 1649: loss = 5.5832 (1.223 sec/step)\n","I0830 16:34:27.126033 139622356395904 learning.py:507] global step 1650: loss = 6.0873 (1.255 sec/step)\n","I0830 16:34:28.357962 139622356395904 learning.py:507] global step 1651: loss = 5.5091 (1.230 sec/step)\n","I0830 16:34:29.581164 139622356395904 learning.py:507] global step 1652: loss = 5.8726 (1.221 sec/step)\n","I0830 16:34:30.830390 139622356395904 learning.py:507] global step 1653: loss = 4.8974 (1.247 sec/step)\n","I0830 16:34:32.080091 139622356395904 learning.py:507] global step 1654: loss = 6.4317 (1.248 sec/step)\n","I0830 16:34:33.270023 139622356395904 learning.py:507] global step 1655: loss = 5.1032 (1.188 sec/step)\n","I0830 16:34:34.512618 139622356395904 learning.py:507] global step 1656: loss = 5.2023 (1.241 sec/step)\n","I0830 16:34:35.720518 139622356395904 learning.py:507] global step 1657: loss = 5.1735 (1.206 sec/step)\n","I0830 16:34:36.935222 139622356395904 learning.py:507] global step 1658: loss = 5.4694 (1.213 sec/step)\n","I0830 16:34:38.216909 139622356395904 learning.py:507] global step 1659: loss = 5.3570 (1.280 sec/step)\n","I0830 16:34:39.457280 139622356395904 learning.py:507] global step 1660: loss = 5.0240 (1.238 sec/step)\n","I0830 16:34:40.689747 139622356395904 learning.py:507] global step 1661: loss = 5.4539 (1.230 sec/step)\n","I0830 16:34:41.922605 139622356395904 learning.py:507] global step 1662: loss = 4.6493 (1.231 sec/step)\n","I0830 16:34:43.186713 139622356395904 learning.py:507] global step 1663: loss = 5.2702 (1.262 sec/step)\n","I0830 16:34:44.434913 139622356395904 learning.py:507] global step 1664: loss = 5.8503 (1.246 sec/step)\n","I0830 16:34:45.688510 139622356395904 learning.py:507] global step 1665: loss = 5.9552 (1.252 sec/step)\n","I0830 16:34:46.910412 139622356395904 learning.py:507] global step 1666: loss = 5.8274 (1.220 sec/step)\n","I0830 16:34:48.141522 139622356395904 learning.py:507] global step 1667: loss = 5.2686 (1.226 sec/step)\n","I0830 16:34:49.345599 139622356395904 learning.py:507] global step 1668: loss = 5.4223 (1.202 sec/step)\n","I0830 16:34:50.570451 139622356395904 learning.py:507] global step 1669: loss = 5.6748 (1.223 sec/step)\n","I0830 16:34:51.805970 139622356395904 learning.py:507] global step 1670: loss = 4.7890 (1.234 sec/step)\n","I0830 16:34:53.038082 139622356395904 learning.py:507] global step 1671: loss = 5.9519 (1.230 sec/step)\n","I0830 16:34:54.270962 139622356395904 learning.py:507] global step 1672: loss = 5.1778 (1.231 sec/step)\n","I0830 16:34:55.527152 139622356395904 learning.py:507] global step 1673: loss = 5.7078 (1.254 sec/step)\n","I0830 16:34:56.738080 139622356395904 learning.py:507] global step 1674: loss = 5.1850 (1.209 sec/step)\n","I0830 16:34:57.961230 139622356395904 learning.py:507] global step 1675: loss = 5.2368 (1.221 sec/step)\n","I0830 16:34:59.173244 139622356395904 learning.py:507] global step 1676: loss = 5.1447 (1.210 sec/step)\n","I0830 16:35:00.398855 139622356395904 learning.py:507] global step 1677: loss = 5.8637 (1.224 sec/step)\n","I0830 16:35:01.607655 139622356395904 learning.py:507] global step 1678: loss = 5.1236 (1.207 sec/step)\n","I0830 16:35:02.821912 139622356395904 learning.py:507] global step 1679: loss = 5.2063 (1.212 sec/step)\n","I0830 16:35:04.036178 139622356395904 learning.py:507] global step 1680: loss = 5.1824 (1.212 sec/step)\n","I0830 16:35:05.280602 139622356395904 learning.py:507] global step 1681: loss = 5.2965 (1.242 sec/step)\n","I0830 16:35:06.495095 139622356395904 learning.py:507] global step 1682: loss = 5.1914 (1.213 sec/step)\n","I0830 16:35:07.693629 139622356395904 learning.py:507] global step 1683: loss = 5.1065 (1.196 sec/step)\n","I0830 16:35:08.924032 139622356395904 learning.py:507] global step 1684: loss = 5.1029 (1.229 sec/step)\n","I0830 16:35:10.151362 139622356395904 learning.py:507] global step 1685: loss = 5.0975 (1.226 sec/step)\n","I0830 16:35:11.385384 139622356395904 learning.py:507] global step 1686: loss = 5.0323 (1.232 sec/step)\n","I0830 16:35:12.605099 139622356395904 learning.py:507] global step 1687: loss = 5.4213 (1.218 sec/step)\n","I0830 16:35:13.856035 139622356395904 learning.py:507] global step 1688: loss = 5.5552 (1.249 sec/step)\n","I0830 16:35:15.075545 139622356395904 learning.py:507] global step 1689: loss = 5.4174 (1.218 sec/step)\n","I0830 16:35:16.300237 139622356395904 learning.py:507] global step 1690: loss = 6.0045 (1.223 sec/step)\n","I0830 16:35:17.534226 139622356395904 learning.py:507] global step 1691: loss = 4.9365 (1.232 sec/step)\n","I0830 16:35:18.802479 139622356395904 learning.py:507] global step 1692: loss = 4.7732 (1.266 sec/step)\n","I0830 16:35:20.008789 139622356395904 learning.py:507] global step 1693: loss = 5.2452 (1.204 sec/step)\n","I0830 16:35:21.228113 139622356395904 learning.py:507] global step 1694: loss = 4.8165 (1.217 sec/step)\n","I0830 16:35:22.443840 139622356395904 learning.py:507] global step 1695: loss = 5.9730 (1.214 sec/step)\n","I0830 16:35:23.648010 139622356395904 learning.py:507] global step 1696: loss = 4.8191 (1.202 sec/step)\n","I0830 16:35:24.883705 139622356395904 learning.py:507] global step 1697: loss = 5.1201 (1.234 sec/step)\n","I0830 16:35:26.106843 139622356395904 learning.py:507] global step 1698: loss = 5.5653 (1.221 sec/step)\n","I0830 16:35:27.367154 139622356395904 learning.py:507] global step 1699: loss = 5.6624 (1.258 sec/step)\n","I0830 16:35:28.573270 139622356395904 learning.py:507] global step 1700: loss = 4.9771 (1.204 sec/step)\n","I0830 16:35:29.810269 139622356395904 learning.py:507] global step 1701: loss = 4.8259 (1.235 sec/step)\n","I0830 16:35:31.073951 139622356395904 learning.py:507] global step 1702: loss = 5.3656 (1.261 sec/step)\n","I0830 16:35:32.334771 139622356395904 learning.py:507] global step 1703: loss = 5.4180 (1.259 sec/step)\n","I0830 16:35:33.569513 139622356395904 learning.py:507] global step 1704: loss = 4.7655 (1.233 sec/step)\n","I0830 16:35:34.825200 139622356395904 learning.py:507] global step 1705: loss = 4.8519 (1.253 sec/step)\n","I0830 16:35:36.070288 139622356395904 learning.py:507] global step 1706: loss = 5.7532 (1.243 sec/step)\n","I0830 16:35:37.316551 139622356395904 learning.py:507] global step 1707: loss = 5.7814 (1.244 sec/step)\n","I0830 16:35:38.539431 139622356395904 learning.py:507] global step 1708: loss = 5.1872 (1.221 sec/step)\n","I0830 16:35:39.738503 139622356395904 learning.py:507] global step 1709: loss = 5.1934 (1.197 sec/step)\n","I0830 16:35:40.976530 139622356395904 learning.py:507] global step 1710: loss = 5.4728 (1.236 sec/step)\n","I0830 16:35:42.242679 139622356395904 learning.py:507] global step 1711: loss = 5.4236 (1.264 sec/step)\n","I0830 16:35:43.502810 139622356395904 learning.py:507] global step 1712: loss = 4.6186 (1.258 sec/step)\n","I0830 16:35:44.740149 139622356395904 learning.py:507] global step 1713: loss = 5.4312 (1.235 sec/step)\n","I0830 16:35:45.953720 139622356395904 learning.py:507] global step 1714: loss = 5.0974 (1.212 sec/step)\n","I0830 16:35:47.222104 139622356395904 learning.py:507] global step 1715: loss = 4.9033 (1.267 sec/step)\n","I0830 16:35:48.484158 139622356395904 learning.py:507] global step 1716: loss = 4.6798 (1.260 sec/step)\n","I0830 16:35:49.677647 139622356395904 learning.py:507] global step 1717: loss = 5.1897 (1.192 sec/step)\n","I0830 16:35:50.895117 139622356395904 learning.py:507] global step 1718: loss = 4.8797 (1.215 sec/step)\n","I0830 16:35:52.129993 139622356395904 learning.py:507] global step 1719: loss = 4.9125 (1.233 sec/step)\n","I0830 16:35:53.346038 139622356395904 learning.py:507] global step 1720: loss = 5.8307 (1.214 sec/step)\n","I0830 16:35:54.581308 139622356395904 learning.py:507] global step 1721: loss = 5.0231 (1.233 sec/step)\n","I0830 16:35:55.867895 139622356395904 learning.py:507] global step 1722: loss = 5.5218 (1.285 sec/step)\n","I0830 16:35:57.111339 139622356395904 learning.py:507] global step 1723: loss = 4.8997 (1.242 sec/step)\n","I0830 16:35:58.300648 139622356395904 learning.py:507] global step 1724: loss = 5.8607 (1.187 sec/step)\n","I0830 16:35:59.531313 139622356395904 learning.py:507] global step 1725: loss = 5.2507 (1.228 sec/step)\n","I0830 16:36:01.730636 139622356395904 learning.py:507] global step 1726: loss = 5.4642 (2.135 sec/step)\n","I0830 16:36:01.999042 139619299985152 supervisor.py:1050] Recording summary at step 1726.\n","I0830 16:36:03.009900 139622356395904 learning.py:507] global step 1727: loss = 5.4735 (1.277 sec/step)\n","I0830 16:36:04.207250 139622356395904 learning.py:507] global step 1728: loss = 5.5605 (1.195 sec/step)\n","I0830 16:36:05.420177 139622356395904 learning.py:507] global step 1729: loss = 5.6733 (1.211 sec/step)\n","I0830 16:36:06.646928 139622356395904 learning.py:507] global step 1730: loss = 6.3196 (1.225 sec/step)\n","I0830 16:36:07.891702 139622356395904 learning.py:507] global step 1731: loss = 4.9317 (1.243 sec/step)\n","I0830 16:36:09.109386 139622356395904 learning.py:507] global step 1732: loss = 5.1379 (1.216 sec/step)\n","I0830 16:36:10.341663 139622356395904 learning.py:507] global step 1733: loss = 4.8131 (1.230 sec/step)\n","I0830 16:36:11.588009 139622356395904 learning.py:507] global step 1734: loss = 5.2172 (1.244 sec/step)\n","I0830 16:36:12.785783 139622356395904 learning.py:507] global step 1735: loss = 5.1791 (1.196 sec/step)\n","I0830 16:36:14.007378 139622356395904 learning.py:507] global step 1736: loss = 4.9850 (1.220 sec/step)\n","I0830 16:36:15.262115 139622356395904 learning.py:507] global step 1737: loss = 4.9443 (1.253 sec/step)\n","I0830 16:36:16.495228 139622356395904 learning.py:507] global step 1738: loss = 5.3754 (1.231 sec/step)\n","I0830 16:36:17.716888 139622356395904 learning.py:507] global step 1739: loss = 5.6893 (1.220 sec/step)\n","I0830 16:36:18.952598 139622356395904 learning.py:507] global step 1740: loss = 5.1960 (1.234 sec/step)\n","I0830 16:36:20.198969 139622356395904 learning.py:507] global step 1741: loss = 6.2253 (1.244 sec/step)\n","I0830 16:36:21.415082 139622356395904 learning.py:507] global step 1742: loss = 5.1585 (1.214 sec/step)\n","I0830 16:36:22.673809 139622356395904 learning.py:507] global step 1743: loss = 4.9780 (1.256 sec/step)\n","I0830 16:36:23.889111 139622356395904 learning.py:507] global step 1744: loss = 5.1352 (1.213 sec/step)\n","I0830 16:36:25.095573 139622356395904 learning.py:507] global step 1745: loss = 4.8464 (1.204 sec/step)\n","I0830 16:36:26.306146 139622356395904 learning.py:507] global step 1746: loss = 5.5214 (1.209 sec/step)\n","I0830 16:36:27.545190 139622356395904 learning.py:507] global step 1747: loss = 5.2332 (1.237 sec/step)\n","I0830 16:36:28.773732 139622356395904 learning.py:507] global step 1748: loss = 4.9777 (1.227 sec/step)\n","I0830 16:36:30.016714 139622356395904 learning.py:507] global step 1749: loss = 4.7459 (1.241 sec/step)\n","I0830 16:36:31.223654 139622356395904 learning.py:507] global step 1750: loss = 5.2358 (1.205 sec/step)\n","I0830 16:36:32.462797 139622356395904 learning.py:507] global step 1751: loss = 4.8751 (1.237 sec/step)\n","I0830 16:36:33.688350 139622356395904 learning.py:507] global step 1752: loss = 5.1675 (1.224 sec/step)\n","I0830 16:36:34.887007 139622356395904 learning.py:507] global step 1753: loss = 6.4859 (1.197 sec/step)\n","I0830 16:36:36.116657 139622356395904 learning.py:507] global step 1754: loss = 4.9285 (1.228 sec/step)\n","I0830 16:36:37.357125 139622356395904 learning.py:507] global step 1755: loss = 4.9238 (1.239 sec/step)\n","I0830 16:36:38.591264 139622356395904 learning.py:507] global step 1756: loss = 5.5181 (1.232 sec/step)\n","I0830 16:36:39.792197 139622356395904 learning.py:507] global step 1757: loss = 6.1461 (1.199 sec/step)\n","I0830 16:36:41.039124 139622356395904 learning.py:507] global step 1758: loss = 5.8158 (1.245 sec/step)\n","I0830 16:36:42.263524 139622356395904 learning.py:507] global step 1759: loss = 4.6775 (1.222 sec/step)\n","I0830 16:36:43.511629 139622356395904 learning.py:507] global step 1760: loss = 5.7090 (1.246 sec/step)\n","I0830 16:36:44.731142 139622356395904 learning.py:507] global step 1761: loss = 5.7701 (1.218 sec/step)\n","I0830 16:36:45.950151 139622356395904 learning.py:507] global step 1762: loss = 4.6749 (1.217 sec/step)\n","I0830 16:36:47.149446 139622356395904 learning.py:507] global step 1763: loss = 5.0271 (1.197 sec/step)\n","I0830 16:36:48.357226 139622356395904 learning.py:507] global step 1764: loss = 4.6906 (1.206 sec/step)\n","I0830 16:36:49.571250 139622356395904 learning.py:507] global step 1765: loss = 5.1815 (1.207 sec/step)\n","I0830 16:36:50.758348 139622356395904 learning.py:507] global step 1766: loss = 5.3636 (1.185 sec/step)\n","I0830 16:36:51.979512 139622356395904 learning.py:507] global step 1767: loss = 6.0505 (1.219 sec/step)\n","I0830 16:36:53.245742 139622356395904 learning.py:507] global step 1768: loss = 5.1311 (1.264 sec/step)\n","I0830 16:36:54.463581 139622356395904 learning.py:507] global step 1769: loss = 6.5154 (1.216 sec/step)\n","I0830 16:36:55.717754 139622356395904 learning.py:507] global step 1770: loss = 4.8042 (1.252 sec/step)\n","I0830 16:36:56.943695 139622356395904 learning.py:507] global step 1771: loss = 4.7846 (1.224 sec/step)\n","I0830 16:36:58.159832 139622356395904 learning.py:507] global step 1772: loss = 4.4329 (1.214 sec/step)\n","I0830 16:36:59.396368 139622356395904 learning.py:507] global step 1773: loss = 5.1786 (1.235 sec/step)\n","I0830 16:37:00.606045 139622356395904 learning.py:507] global step 1774: loss = 4.7781 (1.208 sec/step)\n","I0830 16:37:01.799995 139622356395904 learning.py:507] global step 1775: loss = 5.1259 (1.192 sec/step)\n","I0830 16:37:02.990419 139622356395904 learning.py:507] global step 1776: loss = 6.2016 (1.188 sec/step)\n","I0830 16:37:04.235764 139622356395904 learning.py:507] global step 1777: loss = 5.4076 (1.244 sec/step)\n","I0830 16:37:05.452566 139622356395904 learning.py:507] global step 1778: loss = 4.7473 (1.215 sec/step)\n","I0830 16:37:06.662378 139622356395904 learning.py:507] global step 1779: loss = 5.3177 (1.208 sec/step)\n","I0830 16:37:07.868634 139622356395904 learning.py:507] global step 1780: loss = 5.5242 (1.204 sec/step)\n","I0830 16:37:09.099520 139622356395904 learning.py:507] global step 1781: loss = 5.3419 (1.229 sec/step)\n","I0830 16:37:10.334951 139622356395904 learning.py:507] global step 1782: loss = 4.5358 (1.233 sec/step)\n","I0830 16:37:11.555310 139622356395904 learning.py:507] global step 1783: loss = 4.5311 (1.218 sec/step)\n","I0830 16:37:12.794381 139622356395904 learning.py:507] global step 1784: loss = 4.2778 (1.237 sec/step)\n","I0830 16:37:14.000544 139622356395904 learning.py:507] global step 1785: loss = 5.1568 (1.204 sec/step)\n","I0830 16:37:15.211509 139622356395904 learning.py:507] global step 1786: loss = 6.0458 (1.209 sec/step)\n","I0830 16:37:16.420698 139622356395904 learning.py:507] global step 1787: loss = 5.3291 (1.207 sec/step)\n","I0830 16:37:17.675561 139622356395904 learning.py:507] global step 1788: loss = 5.2147 (1.253 sec/step)\n","I0830 16:37:18.874805 139622356395904 learning.py:507] global step 1789: loss = 5.0450 (1.197 sec/step)\n","I0830 16:37:20.139175 139622356395904 learning.py:507] global step 1790: loss = 5.3117 (1.262 sec/step)\n","I0830 16:37:21.351715 139622356395904 learning.py:507] global step 1791: loss = 4.5344 (1.210 sec/step)\n","I0830 16:37:22.616286 139622356395904 learning.py:507] global step 1792: loss = 5.5201 (1.263 sec/step)\n","I0830 16:37:23.828404 139622356395904 learning.py:507] global step 1793: loss = 4.8749 (1.210 sec/step)\n","I0830 16:37:25.068788 139622356395904 learning.py:507] global step 1794: loss = 6.6206 (1.238 sec/step)\n","I0830 16:37:26.266270 139622356395904 learning.py:507] global step 1795: loss = 5.0094 (1.196 sec/step)\n","I0830 16:37:27.472313 139622356395904 learning.py:507] global step 1796: loss = 5.0782 (1.203 sec/step)\n","I0830 16:37:28.712381 139622356395904 learning.py:507] global step 1797: loss = 5.2598 (1.237 sec/step)\n","I0830 16:37:29.914287 139622356395904 learning.py:507] global step 1798: loss = 4.9567 (1.200 sec/step)\n","I0830 16:37:31.133028 139622356395904 learning.py:507] global step 1799: loss = 4.8768 (1.217 sec/step)\n","I0830 16:37:32.329925 139622356395904 learning.py:507] global step 1800: loss = 5.6413 (1.192 sec/step)\n","I0830 16:37:33.563812 139622356395904 learning.py:507] global step 1801: loss = 5.0745 (1.231 sec/step)\n","I0830 16:37:34.785550 139622356395904 learning.py:507] global step 1802: loss = 4.5207 (1.220 sec/step)\n","I0830 16:37:35.992757 139622356395904 learning.py:507] global step 1803: loss = 5.8022 (1.205 sec/step)\n","I0830 16:37:37.189455 139622356395904 learning.py:507] global step 1804: loss = 4.7294 (1.195 sec/step)\n","I0830 16:37:38.441093 139622356395904 learning.py:507] global step 1805: loss = 4.7734 (1.250 sec/step)\n","I0830 16:37:39.670232 139622356395904 learning.py:507] global step 1806: loss = 4.6471 (1.227 sec/step)\n","I0830 16:37:40.874551 139622356395904 learning.py:507] global step 1807: loss = 5.7121 (1.202 sec/step)\n","I0830 16:37:42.073566 139622356395904 learning.py:507] global step 1808: loss = 4.6118 (1.197 sec/step)\n","I0830 16:37:43.318768 139622356395904 learning.py:507] global step 1809: loss = 4.8014 (1.244 sec/step)\n","I0830 16:37:44.564404 139622356395904 learning.py:507] global step 1810: loss = 5.0067 (1.244 sec/step)\n","I0830 16:37:45.779215 139622356395904 learning.py:507] global step 1811: loss = 5.3745 (1.213 sec/step)\n","I0830 16:37:47.037779 139622356395904 learning.py:507] global step 1812: loss = 5.2836 (1.257 sec/step)\n","I0830 16:37:48.291094 139622356395904 learning.py:507] global step 1813: loss = 5.6797 (1.251 sec/step)\n","I0830 16:37:49.539835 139622356395904 learning.py:507] global step 1814: loss = 4.9481 (1.247 sec/step)\n","I0830 16:37:50.769593 139622356395904 learning.py:507] global step 1815: loss = 5.6158 (1.228 sec/step)\n","I0830 16:37:51.997558 139622356395904 learning.py:507] global step 1816: loss = 5.0633 (1.226 sec/step)\n","I0830 16:37:53.218393 139622356395904 learning.py:507] global step 1817: loss = 5.4282 (1.219 sec/step)\n","I0830 16:37:54.446356 139622356395904 learning.py:507] global step 1818: loss = 5.0739 (1.225 sec/step)\n","I0830 16:37:55.657660 139622356395904 learning.py:507] global step 1819: loss = 4.9648 (1.209 sec/step)\n","I0830 16:37:56.862998 139622356395904 learning.py:507] global step 1820: loss = 5.6101 (1.204 sec/step)\n","I0830 16:37:58.066790 139622356395904 learning.py:507] global step 1821: loss = 5.2679 (1.202 sec/step)\n","I0830 16:37:59.264367 139622356395904 learning.py:507] global step 1822: loss = 5.1891 (1.195 sec/step)\n","I0830 16:38:00.731315 139622356395904 learning.py:507] global step 1823: loss = 5.5578 (1.460 sec/step)\n","I0830 16:38:02.508466 139619299985152 supervisor.py:1050] Recording summary at step 1824.\n","I0830 16:38:02.546739 139622356395904 learning.py:507] global step 1824: loss = 4.6440 (1.813 sec/step)\n","I0830 16:38:03.781753 139622356395904 learning.py:507] global step 1825: loss = 5.3970 (1.233 sec/step)\n","I0830 16:38:05.027292 139622356395904 learning.py:507] global step 1826: loss = 5.2527 (1.244 sec/step)\n","I0830 16:38:06.246789 139622356395904 learning.py:507] global step 1827: loss = 4.4069 (1.218 sec/step)\n","I0830 16:38:07.479324 139622356395904 learning.py:507] global step 1828: loss = 4.7083 (1.231 sec/step)\n","I0830 16:38:08.694477 139622356395904 learning.py:507] global step 1829: loss = 5.4668 (1.213 sec/step)\n","I0830 16:38:09.912266 139622356395904 learning.py:507] global step 1830: loss = 5.1129 (1.216 sec/step)\n","I0830 16:38:11.131335 139622356395904 learning.py:507] global step 1831: loss = 6.7911 (1.217 sec/step)\n","I0830 16:38:12.334015 139622356395904 learning.py:507] global step 1832: loss = 5.6502 (1.201 sec/step)\n","I0830 16:38:13.569752 139622356395904 learning.py:507] global step 1833: loss = 5.4013 (1.234 sec/step)\n","I0830 16:38:14.776643 139622356395904 learning.py:507] global step 1834: loss = 4.7951 (1.204 sec/step)\n","I0830 16:38:16.000960 139622356395904 learning.py:507] global step 1835: loss = 4.6140 (1.222 sec/step)\n","I0830 16:38:17.220701 139622356395904 learning.py:507] global step 1836: loss = 4.8364 (1.218 sec/step)\n","I0830 16:38:18.429313 139622356395904 learning.py:507] global step 1837: loss = 5.1360 (1.207 sec/step)\n","I0830 16:38:19.693458 139622356395904 learning.py:507] global step 1838: loss = 5.1424 (1.262 sec/step)\n","I0830 16:38:20.938090 139622356395904 learning.py:507] global step 1839: loss = 5.2403 (1.243 sec/step)\n","I0830 16:38:22.146189 139622356395904 learning.py:507] global step 1840: loss = 4.7002 (1.206 sec/step)\n","I0830 16:38:23.390093 139622356395904 learning.py:507] global step 1841: loss = 4.4192 (1.242 sec/step)\n","I0830 16:38:24.635819 139622356395904 learning.py:507] global step 1842: loss = 4.8850 (1.244 sec/step)\n","I0830 16:38:25.855321 139622356395904 learning.py:507] global step 1843: loss = 5.3199 (1.218 sec/step)\n","I0830 16:38:27.072489 139622356395904 learning.py:507] global step 1844: loss = 4.9176 (1.215 sec/step)\n","I0830 16:38:28.292614 139622356395904 learning.py:507] global step 1845: loss = 5.1659 (1.218 sec/step)\n","I0830 16:38:29.492891 139622356395904 learning.py:507] global step 1846: loss = 4.5916 (1.199 sec/step)\n","I0830 16:38:30.756412 139622356395904 learning.py:507] global step 1847: loss = 4.8895 (1.262 sec/step)\n","I0830 16:38:31.974321 139622356395904 learning.py:507] global step 1848: loss = 5.0032 (1.216 sec/step)\n","I0830 16:38:33.186455 139622356395904 learning.py:507] global step 1849: loss = 4.4784 (1.210 sec/step)\n","I0830 16:38:34.407575 139622356395904 learning.py:507] global step 1850: loss = 4.6021 (1.219 sec/step)\n","I0830 16:38:35.633542 139622356395904 learning.py:507] global step 1851: loss = 4.8308 (1.224 sec/step)\n","I0830 16:38:36.867331 139622356395904 learning.py:507] global step 1852: loss = 4.6146 (1.232 sec/step)\n","I0830 16:38:38.069451 139622356395904 learning.py:507] global step 1853: loss = 5.2623 (1.200 sec/step)\n","I0830 16:38:39.307786 139622356395904 learning.py:507] global step 1854: loss = 5.1064 (1.236 sec/step)\n","I0830 16:38:40.521890 139622356395904 learning.py:507] global step 1855: loss = 4.8949 (1.212 sec/step)\n","I0830 16:38:41.757729 139622356395904 learning.py:507] global step 1856: loss = 5.3501 (1.234 sec/step)\n","I0830 16:38:42.975133 139622356395904 learning.py:507] global step 1857: loss = 5.0080 (1.216 sec/step)\n","I0830 16:38:44.196890 139622356395904 learning.py:507] global step 1858: loss = 5.4842 (1.220 sec/step)\n","I0830 16:38:45.454922 139622356395904 learning.py:507] global step 1859: loss = 4.8092 (1.256 sec/step)\n","I0830 16:38:46.716900 139622356395904 learning.py:507] global step 1860: loss = 5.8369 (1.260 sec/step)\n","I0830 16:38:47.916389 139622356395904 learning.py:507] global step 1861: loss = 4.7456 (1.198 sec/step)\n","I0830 16:38:49.123333 139622356395904 learning.py:507] global step 1862: loss = 4.6165 (1.205 sec/step)\n","I0830 16:38:50.350707 139622356395904 learning.py:507] global step 1863: loss = 4.8795 (1.225 sec/step)\n","I0830 16:38:51.598712 139622356395904 learning.py:507] global step 1864: loss = 5.4127 (1.246 sec/step)\n","I0830 16:38:52.833419 139622356395904 learning.py:507] global step 1865: loss = 5.9518 (1.228 sec/step)\n","I0830 16:38:54.054969 139622356395904 learning.py:507] global step 1866: loss = 5.5716 (1.219 sec/step)\n","I0830 16:38:55.282527 139622356395904 learning.py:507] global step 1867: loss = 5.1045 (1.226 sec/step)\n","I0830 16:38:56.502537 139622356395904 learning.py:507] global step 1868: loss = 4.7997 (1.218 sec/step)\n","I0830 16:38:57.719093 139622356395904 learning.py:507] global step 1869: loss = 6.8524 (1.215 sec/step)\n","I0830 16:38:58.950521 139622356395904 learning.py:507] global step 1870: loss = 4.8669 (1.229 sec/step)\n","I0830 16:39:00.195563 139622356395904 learning.py:507] global step 1871: loss = 4.3460 (1.243 sec/step)\n","I0830 16:39:01.379602 139622356395904 learning.py:507] global step 1872: loss = 5.4332 (1.182 sec/step)\n","I0830 16:39:02.564665 139622356395904 learning.py:507] global step 1873: loss = 5.3933 (1.183 sec/step)\n","I0830 16:39:03.817313 139622356395904 learning.py:507] global step 1874: loss = 4.7524 (1.251 sec/step)\n","I0830 16:39:05.045165 139622356395904 learning.py:507] global step 1875: loss = 5.3817 (1.226 sec/step)\n","I0830 16:39:06.231691 139622356395904 learning.py:507] global step 1876: loss = 5.3850 (1.185 sec/step)\n","I0830 16:39:07.459671 139622356395904 learning.py:507] global step 1877: loss = 4.3143 (1.226 sec/step)\n","I0830 16:39:08.694396 139622356395904 learning.py:507] global step 1878: loss = 4.9341 (1.233 sec/step)\n","I0830 16:39:09.893821 139622356395904 learning.py:507] global step 1879: loss = 4.6817 (1.197 sec/step)\n","I0830 16:39:11.124478 139622356395904 learning.py:507] global step 1880: loss = 5.5294 (1.229 sec/step)\n","I0830 16:39:12.324983 139622356395904 learning.py:507] global step 1881: loss = 5.0744 (1.199 sec/step)\n","I0830 16:39:13.543865 139622356395904 learning.py:507] global step 1882: loss = 6.4564 (1.217 sec/step)\n","I0830 16:39:14.752614 139622356395904 learning.py:507] global step 1883: loss = 4.8245 (1.207 sec/step)\n","I0830 16:39:15.988751 139622356395904 learning.py:507] global step 1884: loss = 4.7332 (1.234 sec/step)\n","I0830 16:39:17.229517 139622356395904 learning.py:507] global step 1885: loss = 5.3216 (1.239 sec/step)\n","I0830 16:39:18.461112 139622356395904 learning.py:507] global step 1886: loss = 5.2799 (1.230 sec/step)\n","I0830 16:39:19.642116 139622356395904 learning.py:507] global step 1887: loss = 5.1980 (1.179 sec/step)\n","I0830 16:39:20.866646 139622356395904 learning.py:507] global step 1888: loss = 4.9522 (1.223 sec/step)\n","I0830 16:39:22.110270 139622356395904 learning.py:507] global step 1889: loss = 5.4792 (1.242 sec/step)\n","I0830 16:39:23.353742 139622356395904 learning.py:507] global step 1890: loss = 4.6892 (1.242 sec/step)\n","I0830 16:39:24.579764 139622356395904 learning.py:507] global step 1891: loss = 4.6799 (1.224 sec/step)\n","I0830 16:39:25.813637 139622356395904 learning.py:507] global step 1892: loss = 5.7438 (1.232 sec/step)\n","I0830 16:39:27.060397 139622356395904 learning.py:507] global step 1893: loss = 4.7149 (1.245 sec/step)\n","I0830 16:39:28.251581 139622356395904 learning.py:507] global step 1894: loss = 5.4938 (1.189 sec/step)\n","I0830 16:39:29.485102 139622356395904 learning.py:507] global step 1895: loss = 4.8497 (1.232 sec/step)\n","I0830 16:39:30.728139 139622356395904 learning.py:507] global step 1896: loss = 5.2677 (1.241 sec/step)\n","I0830 16:39:31.909471 139622356395904 learning.py:507] global step 1897: loss = 5.5313 (1.180 sec/step)\n","I0830 16:39:33.147311 139622356395904 learning.py:507] global step 1898: loss = 4.7433 (1.236 sec/step)\n","I0830 16:39:34.393161 139622356395904 learning.py:507] global step 1899: loss = 5.4077 (1.244 sec/step)\n","I0830 16:39:35.624100 139622356395904 learning.py:507] global step 1900: loss = 5.7304 (1.229 sec/step)\n","I0830 16:39:36.831455 139622356395904 learning.py:507] global step 1901: loss = 5.8164 (1.205 sec/step)\n","I0830 16:39:38.079198 139622356395904 learning.py:507] global step 1902: loss = 4.8337 (1.246 sec/step)\n","I0830 16:39:39.309189 139622356395904 learning.py:507] global step 1903: loss = 5.2480 (1.228 sec/step)\n","I0830 16:39:40.535420 139622356395904 learning.py:507] global step 1904: loss = 4.6937 (1.224 sec/step)\n","I0830 16:39:41.757358 139622356395904 learning.py:507] global step 1905: loss = 4.9372 (1.220 sec/step)\n","I0830 16:39:43.010455 139622356395904 learning.py:507] global step 1906: loss = 4.9338 (1.251 sec/step)\n","I0830 16:39:44.237486 139622356395904 learning.py:507] global step 1907: loss = 4.5400 (1.225 sec/step)\n","I0830 16:39:45.496174 139622356395904 learning.py:507] global step 1908: loss = 6.6326 (1.257 sec/step)\n","I0830 16:39:46.704732 139622356395904 learning.py:507] global step 1909: loss = 4.5650 (1.207 sec/step)\n","I0830 16:39:47.934379 139622356395904 learning.py:507] global step 1910: loss = 5.4657 (1.228 sec/step)\n","I0830 16:39:49.165100 139622356395904 learning.py:507] global step 1911: loss = 4.4779 (1.229 sec/step)\n","I0830 16:39:50.383393 139622356395904 learning.py:507] global step 1912: loss = 6.1525 (1.216 sec/step)\n","I0830 16:39:51.621129 139622356395904 learning.py:507] global step 1913: loss = 5.6303 (1.236 sec/step)\n","I0830 16:39:52.860159 139622356395904 learning.py:507] global step 1914: loss = 5.6113 (1.237 sec/step)\n","I0830 16:39:54.071760 139622356395904 learning.py:507] global step 1915: loss = 4.6763 (1.210 sec/step)\n","I0830 16:39:55.303694 139622356395904 learning.py:507] global step 1916: loss = 5.7092 (1.230 sec/step)\n","I0830 16:39:56.549350 139622356395904 learning.py:507] global step 1917: loss = 5.9741 (1.244 sec/step)\n","I0830 16:39:57.763440 139622356395904 learning.py:507] global step 1918: loss = 4.5936 (1.212 sec/step)\n","I0830 16:39:59.007742 139622356395904 learning.py:507] global step 1919: loss = 4.5934 (1.242 sec/step)\n","I0830 16:39:59.646014 139619316770560 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n","I0830 16:40:01.257238 139622356395904 learning.py:507] global step 1920: loss = 4.5373 (2.184 sec/step)\n","I0830 16:40:02.832603 139619299985152 supervisor.py:1050] Recording summary at step 1920.\n","I0830 16:40:03.845917 139622356395904 learning.py:507] global step 1921: loss = 4.9300 (2.563 sec/step)\n","I0830 16:40:05.267459 139622356395904 learning.py:507] global step 1922: loss = 5.1041 (1.419 sec/step)\n","I0830 16:40:06.478430 139622356395904 learning.py:507] global step 1923: loss = 5.4475 (1.209 sec/step)\n","I0830 16:40:07.755836 139622356395904 learning.py:507] global step 1924: loss = 4.4364 (1.276 sec/step)\n","I0830 16:40:08.998153 139622356395904 learning.py:507] global step 1925: loss = 5.0620 (1.240 sec/step)\n","I0830 16:40:10.239900 139622356395904 learning.py:507] global step 1926: loss = 4.6256 (1.240 sec/step)\n","I0830 16:40:11.502494 139622356395904 learning.py:507] global step 1927: loss = 4.3061 (1.261 sec/step)\n","I0830 16:40:12.695644 139622356395904 learning.py:507] global step 1928: loss = 4.5119 (1.191 sec/step)\n","I0830 16:40:13.939334 139622356395904 learning.py:507] global step 1929: loss = 4.9865 (1.242 sec/step)\n","I0830 16:40:15.178615 139622356395904 learning.py:507] global step 1930: loss = 5.6263 (1.238 sec/step)\n","I0830 16:40:16.407603 139622356395904 learning.py:507] global step 1931: loss = 5.4904 (1.227 sec/step)\n","I0830 16:40:17.628168 139622356395904 learning.py:507] global step 1932: loss = 5.3043 (1.219 sec/step)\n","I0830 16:40:18.880877 139622356395904 learning.py:507] global step 1933: loss = 4.7980 (1.251 sec/step)\n","I0830 16:40:20.094533 139622356395904 learning.py:507] global step 1934: loss = 5.2489 (1.212 sec/step)\n","I0830 16:40:21.303007 139622356395904 learning.py:507] global step 1935: loss = 5.2169 (1.207 sec/step)\n","I0830 16:40:22.535130 139622356395904 learning.py:507] global step 1936: loss = 4.9354 (1.230 sec/step)\n","I0830 16:40:23.744198 139622356395904 learning.py:507] global step 1937: loss = 5.4615 (1.207 sec/step)\n","I0830 16:40:24.968147 139622356395904 learning.py:507] global step 1938: loss = 5.2480 (1.222 sec/step)\n","I0830 16:40:26.203994 139622356395904 learning.py:507] global step 1939: loss = 5.0603 (1.234 sec/step)\n","I0830 16:40:27.435845 139622356395904 learning.py:507] global step 1940: loss = 5.2880 (1.230 sec/step)\n","I0830 16:40:28.655555 139622356395904 learning.py:507] global step 1941: loss = 5.2934 (1.217 sec/step)\n","I0830 16:40:29.902137 139622356395904 learning.py:507] global step 1942: loss = 4.7168 (1.245 sec/step)\n","I0830 16:40:31.129474 139622356395904 learning.py:507] global step 1943: loss = 5.1228 (1.226 sec/step)\n","I0830 16:40:32.335312 139622356395904 learning.py:507] global step 1944: loss = 4.5474 (1.204 sec/step)\n","I0830 16:40:33.565806 139622356395904 learning.py:507] global step 1945: loss = 5.3727 (1.228 sec/step)\n","I0830 16:40:34.783690 139622356395904 learning.py:507] global step 1946: loss = 5.4817 (1.216 sec/step)\n","I0830 16:40:36.006893 139622356395904 learning.py:507] global step 1947: loss = 4.3747 (1.221 sec/step)\n","I0830 16:40:37.231660 139622356395904 learning.py:507] global step 1948: loss = 4.4816 (1.222 sec/step)\n","I0830 16:40:38.461456 139622356395904 learning.py:507] global step 1949: loss = 5.6158 (1.228 sec/step)\n","I0830 16:40:39.686571 139622356395904 learning.py:507] global step 1950: loss = 5.1220 (1.223 sec/step)\n","I0830 16:40:40.920470 139622356395904 learning.py:507] global step 1951: loss = 5.3775 (1.231 sec/step)\n","I0830 16:40:42.121671 139622356395904 learning.py:507] global step 1952: loss = 4.4869 (1.199 sec/step)\n","I0830 16:40:43.346474 139622356395904 learning.py:507] global step 1953: loss = 4.9763 (1.223 sec/step)\n","I0830 16:40:44.617717 139622356395904 learning.py:507] global step 1954: loss = 5.2871 (1.269 sec/step)\n","I0830 16:40:45.847150 139622356395904 learning.py:507] global step 1955: loss = 4.2330 (1.227 sec/step)\n","I0830 16:40:47.078869 139622356395904 learning.py:507] global step 1956: loss = 4.2431 (1.230 sec/step)\n","I0830 16:40:48.315778 139622356395904 learning.py:507] global step 1957: loss = 5.1352 (1.235 sec/step)\n","I0830 16:40:49.588675 139622356395904 learning.py:507] global step 1958: loss = 5.3744 (1.271 sec/step)\n","I0830 16:40:50.811186 139622356395904 learning.py:507] global step 1959: loss = 5.6432 (1.220 sec/step)\n","I0830 16:40:52.065391 139622356395904 learning.py:507] global step 1960: loss = 4.9449 (1.252 sec/step)\n","I0830 16:40:53.305525 139622356395904 learning.py:507] global step 1961: loss = 5.0897 (1.238 sec/step)\n","I0830 16:40:54.518561 139622356395904 learning.py:507] global step 1962: loss = 4.8889 (1.211 sec/step)\n","I0830 16:40:55.752648 139622356395904 learning.py:507] global step 1963: loss = 5.0382 (1.232 sec/step)\n","I0830 16:40:56.987278 139622356395904 learning.py:507] global step 1964: loss = 5.1462 (1.233 sec/step)\n","I0830 16:40:58.191301 139622356395904 learning.py:507] global step 1965: loss = 5.3302 (1.202 sec/step)\n","I0830 16:40:59.423620 139622356395904 learning.py:507] global step 1966: loss = 4.9347 (1.227 sec/step)\n","I0830 16:41:00.641179 139622356395904 learning.py:507] global step 1967: loss = 5.1019 (1.215 sec/step)\n","I0830 16:41:01.840574 139622356395904 learning.py:507] global step 1968: loss = 5.5685 (1.198 sec/step)\n","I0830 16:41:03.099939 139622356395904 learning.py:507] global step 1969: loss = 5.1711 (1.257 sec/step)\n","I0830 16:41:04.326221 139622356395904 learning.py:507] global step 1970: loss = 5.9839 (1.225 sec/step)\n","I0830 16:41:05.538233 139622356395904 learning.py:507] global step 1971: loss = 4.4013 (1.210 sec/step)\n","I0830 16:41:06.780668 139622356395904 learning.py:507] global step 1972: loss = 4.6587 (1.241 sec/step)\n","I0830 16:41:07.998115 139622356395904 learning.py:507] global step 1973: loss = 4.8489 (1.216 sec/step)\n","I0830 16:41:09.243292 139622356395904 learning.py:507] global step 1974: loss = 5.0680 (1.243 sec/step)\n","I0830 16:41:10.504131 139622356395904 learning.py:507] global step 1975: loss = 5.7655 (1.259 sec/step)\n","I0830 16:41:11.728119 139622356395904 learning.py:507] global step 1976: loss = 5.6108 (1.222 sec/step)\n","I0830 16:41:12.944228 139622356395904 learning.py:507] global step 1977: loss = 4.4354 (1.214 sec/step)\n","I0830 16:41:14.172909 139622356395904 learning.py:507] global step 1978: loss = 4.6264 (1.227 sec/step)\n","I0830 16:41:15.421175 139622356395904 learning.py:507] global step 1979: loss = 6.0741 (1.247 sec/step)\n","I0830 16:41:16.630136 139622356395904 learning.py:507] global step 1980: loss = 4.9705 (1.207 sec/step)\n","I0830 16:41:17.834658 139622356395904 learning.py:507] global step 1981: loss = 5.0206 (1.203 sec/step)\n","I0830 16:41:19.071758 139622356395904 learning.py:507] global step 1982: loss = 5.1890 (1.235 sec/step)\n","I0830 16:41:20.322186 139622356395904 learning.py:507] global step 1983: loss = 4.6557 (1.248 sec/step)\n","I0830 16:41:21.532856 139622356395904 learning.py:507] global step 1984: loss = 4.4526 (1.209 sec/step)\n","I0830 16:41:22.724027 139622356395904 learning.py:507] global step 1985: loss = 4.8417 (1.189 sec/step)\n","I0830 16:41:23.944005 139622356395904 learning.py:507] global step 1986: loss = 4.7344 (1.218 sec/step)\n","I0830 16:41:25.202832 139622356395904 learning.py:507] global step 1987: loss = 4.7882 (1.257 sec/step)\n","I0830 16:41:26.449624 139622356395904 learning.py:507] global step 1988: loss = 4.4275 (1.245 sec/step)\n","I0830 16:41:27.704547 139622356395904 learning.py:507] global step 1989: loss = 4.4913 (1.253 sec/step)\n","I0830 16:41:28.910038 139622356395904 learning.py:507] global step 1990: loss = 5.1746 (1.204 sec/step)\n","I0830 16:41:30.122526 139622356395904 learning.py:507] global step 1991: loss = 5.3513 (1.211 sec/step)\n","I0830 16:41:31.336974 139622356395904 learning.py:507] global step 1992: loss = 6.0600 (1.212 sec/step)\n","I0830 16:41:32.601983 139622356395904 learning.py:507] global step 1993: loss = 5.3378 (1.263 sec/step)\n","I0830 16:41:33.828518 139622356395904 learning.py:507] global step 1994: loss = 5.3907 (1.225 sec/step)\n","I0830 16:41:35.049355 139622356395904 learning.py:507] global step 1995: loss = 5.1918 (1.219 sec/step)\n","I0830 16:41:36.278625 139622356395904 learning.py:507] global step 1996: loss = 4.7580 (1.228 sec/step)\n","I0830 16:41:37.497965 139622356395904 learning.py:507] global step 1997: loss = 4.4067 (1.218 sec/step)\n","I0830 16:41:38.712191 139622356395904 learning.py:507] global step 1998: loss = 4.5764 (1.212 sec/step)\n","I0830 16:41:39.938997 139622356395904 learning.py:507] global step 1999: loss = 5.0414 (1.225 sec/step)\n","I0830 16:41:41.165209 139622356395904 learning.py:507] global step 2000: loss = 5.5974 (1.224 sec/step)\n","I0830 16:41:42.393639 139622356395904 learning.py:507] global step 2001: loss = 5.4986 (1.227 sec/step)\n","I0830 16:41:43.594197 139622356395904 learning.py:507] global step 2002: loss = 6.5015 (1.199 sec/step)\n","I0830 16:41:44.819247 139622356395904 learning.py:507] global step 2003: loss = 5.0251 (1.223 sec/step)\n","I0830 16:41:46.012308 139622356395904 learning.py:507] global step 2004: loss = 5.3420 (1.191 sec/step)\n","I0830 16:41:47.263395 139622356395904 learning.py:507] global step 2005: loss = 4.7932 (1.249 sec/step)\n","I0830 16:41:48.475500 139622356395904 learning.py:507] global step 2006: loss = 5.9363 (1.210 sec/step)\n","I0830 16:41:49.729809 139622356395904 learning.py:507] global step 2007: loss = 5.1981 (1.252 sec/step)\n","I0830 16:41:50.944626 139622356395904 learning.py:507] global step 2008: loss = 5.1517 (1.213 sec/step)\n","I0830 16:41:52.171655 139622356395904 learning.py:507] global step 2009: loss = 4.7349 (1.225 sec/step)\n","I0830 16:41:53.401278 139622356395904 learning.py:507] global step 2010: loss = 4.4312 (1.228 sec/step)\n","I0830 16:41:54.627235 139622356395904 learning.py:507] global step 2011: loss = 5.2153 (1.224 sec/step)\n","I0830 16:41:55.846458 139622356395904 learning.py:507] global step 2012: loss = 5.2189 (1.217 sec/step)\n","I0830 16:41:57.095989 139622356395904 learning.py:507] global step 2013: loss = 4.4669 (1.248 sec/step)\n","I0830 16:41:58.325857 139622356395904 learning.py:507] global step 2014: loss = 4.8447 (1.228 sec/step)\n","I0830 16:41:59.506570 139622356395904 learning.py:507] global step 2015: loss = 4.5805 (1.179 sec/step)\n","I0830 16:42:01.263605 139622356395904 learning.py:507] global step 2016: loss = 5.2515 (1.639 sec/step)\n","I0830 16:42:01.975821 139619299985152 supervisor.py:1050] Recording summary at step 2016.\n","I0830 16:42:02.969575 139622356395904 learning.py:507] global step 2017: loss = 4.5725 (1.692 sec/step)\n","I0830 16:42:04.205202 139622356395904 learning.py:507] global step 2018: loss = 4.5621 (1.233 sec/step)\n","I0830 16:42:05.425299 139622356395904 learning.py:507] global step 2019: loss = 4.6280 (1.218 sec/step)\n","I0830 16:42:06.651235 139622356395904 learning.py:507] global step 2020: loss = 4.9401 (1.224 sec/step)\n","I0830 16:42:07.885345 139622356395904 learning.py:507] global step 2021: loss = 6.3365 (1.232 sec/step)\n","I0830 16:42:09.090520 139622356395904 learning.py:507] global step 2022: loss = 4.4925 (1.203 sec/step)\n","I0830 16:42:10.310982 139622356395904 learning.py:507] global step 2023: loss = 5.6227 (1.219 sec/step)\n","I0830 16:42:11.529638 139622356395904 learning.py:507] global step 2024: loss = 5.3240 (1.217 sec/step)\n","I0830 16:42:12.814826 139622356395904 learning.py:507] global step 2025: loss = 4.7088 (1.283 sec/step)\n","I0830 16:42:14.049005 139622356395904 learning.py:507] global step 2026: loss = 4.9658 (1.232 sec/step)\n","I0830 16:42:15.248314 139622356395904 learning.py:507] global step 2027: loss = 4.7738 (1.197 sec/step)\n","I0830 16:42:16.486726 139622356395904 learning.py:507] global step 2028: loss = 4.3343 (1.237 sec/step)\n","I0830 16:42:17.714494 139622356395904 learning.py:507] global step 2029: loss = 5.9248 (1.225 sec/step)\n","I0830 16:42:18.930567 139622356395904 learning.py:507] global step 2030: loss = 4.9760 (1.214 sec/step)\n","I0830 16:42:20.160948 139622356395904 learning.py:507] global step 2031: loss = 4.4457 (1.229 sec/step)\n","I0830 16:42:21.394634 139622356395904 learning.py:507] global step 2032: loss = 5.2502 (1.232 sec/step)\n","I0830 16:42:22.605917 139622356395904 learning.py:507] global step 2033: loss = 4.9801 (1.209 sec/step)\n","I0830 16:42:23.858807 139622356395904 learning.py:507] global step 2034: loss = 5.1726 (1.251 sec/step)\n","I0830 16:42:25.080475 139622356395904 learning.py:507] global step 2035: loss = 4.6968 (1.220 sec/step)\n","I0830 16:42:26.260391 139622356395904 learning.py:507] global step 2036: loss = 5.3769 (1.177 sec/step)\n","I0830 16:42:27.503172 139622356395904 learning.py:507] global step 2037: loss = 4.9720 (1.241 sec/step)\n","I0830 16:42:28.745436 139622356395904 learning.py:507] global step 2038: loss = 5.3408 (1.241 sec/step)\n","I0830 16:42:30.004625 139622356395904 learning.py:507] global step 2039: loss = 4.6133 (1.257 sec/step)\n","I0830 16:42:31.243610 139622356395904 learning.py:507] global step 2040: loss = 5.0093 (1.237 sec/step)\n","I0830 16:42:32.483211 139622356395904 learning.py:507] global step 2041: loss = 4.3673 (1.238 sec/step)\n","I0830 16:42:33.731037 139622356395904 learning.py:507] global step 2042: loss = 5.7637 (1.246 sec/step)\n","I0830 16:42:34.948073 139622356395904 learning.py:507] global step 2043: loss = 5.2014 (1.215 sec/step)\n","I0830 16:42:36.168738 139622356395904 learning.py:507] global step 2044: loss = 5.3410 (1.218 sec/step)\n","I0830 16:42:37.429557 139622356395904 learning.py:507] global step 2045: loss = 4.6808 (1.259 sec/step)\n","I0830 16:42:38.641155 139622356395904 learning.py:507] global step 2046: loss = 4.5073 (1.210 sec/step)\n","I0830 16:42:39.873861 139622356395904 learning.py:507] global step 2047: loss = 4.6322 (1.231 sec/step)\n","I0830 16:42:41.100023 139622356395904 learning.py:507] global step 2048: loss = 5.2810 (1.224 sec/step)\n","I0830 16:42:42.353997 139622356395904 learning.py:507] global step 2049: loss = 5.0518 (1.252 sec/step)\n","I0830 16:42:43.634008 139622356395904 learning.py:507] global step 2050: loss = 4.5425 (1.278 sec/step)\n","I0830 16:42:44.877618 139622356395904 learning.py:507] global step 2051: loss = 5.4168 (1.241 sec/step)\n","I0830 16:42:46.096821 139622356395904 learning.py:507] global step 2052: loss = 5.2739 (1.217 sec/step)\n","I0830 16:42:47.322731 139622356395904 learning.py:507] global step 2053: loss = 5.3797 (1.224 sec/step)\n","I0830 16:42:48.549659 139622356395904 learning.py:507] global step 2054: loss = 5.5428 (1.225 sec/step)\n","I0830 16:42:49.766221 139622356395904 learning.py:507] global step 2055: loss = 5.1151 (1.215 sec/step)\n","I0830 16:42:50.969269 139622356395904 learning.py:507] global step 2056: loss = 4.5697 (1.201 sec/step)\n","I0830 16:42:52.193346 139622356395904 learning.py:507] global step 2057: loss = 4.6506 (1.222 sec/step)\n","I0830 16:42:53.448385 139622356395904 learning.py:507] global step 2058: loss = 4.9085 (1.253 sec/step)\n","I0830 16:42:54.661901 139622356395904 learning.py:507] global step 2059: loss = 5.0191 (1.211 sec/step)\n","I0830 16:42:55.888343 139622356395904 learning.py:507] global step 2060: loss = 5.1555 (1.225 sec/step)\n","I0830 16:42:57.103991 139622356395904 learning.py:507] global step 2061: loss = 5.6416 (1.214 sec/step)\n","I0830 16:42:58.344286 139622356395904 learning.py:507] global step 2062: loss = 6.0402 (1.238 sec/step)\n","I0830 16:42:59.595279 139622356395904 learning.py:507] global step 2063: loss = 5.3478 (1.249 sec/step)\n","I0830 16:43:00.846629 139622356395904 learning.py:507] global step 2064: loss = 5.0237 (1.249 sec/step)\n","I0830 16:43:02.043350 139622356395904 learning.py:507] global step 2065: loss = 4.8084 (1.195 sec/step)\n","I0830 16:43:03.268722 139622356395904 learning.py:507] global step 2066: loss = 5.1698 (1.224 sec/step)\n","I0830 16:43:04.492095 139622356395904 learning.py:507] global step 2067: loss = 4.3496 (1.221 sec/step)\n","I0830 16:43:05.711460 139622356395904 learning.py:507] global step 2068: loss = 4.8264 (1.217 sec/step)\n","I0830 16:43:06.918281 139622356395904 learning.py:507] global step 2069: loss = 4.4452 (1.205 sec/step)\n","I0830 16:43:08.147862 139622356395904 learning.py:507] global step 2070: loss = 4.8131 (1.227 sec/step)\n","I0830 16:43:09.384653 139622356395904 learning.py:507] global step 2071: loss = 4.6124 (1.234 sec/step)\n","I0830 16:43:10.610951 139622356395904 learning.py:507] global step 2072: loss = 4.4698 (1.225 sec/step)\n","I0830 16:43:11.874877 139622356395904 learning.py:507] global step 2073: loss = 4.8229 (1.261 sec/step)\n","I0830 16:43:13.130882 139622356395904 learning.py:507] global step 2074: loss = 5.0704 (1.254 sec/step)\n","I0830 16:43:14.373437 139622356395904 learning.py:507] global step 2075: loss = 5.5931 (1.241 sec/step)\n","I0830 16:43:15.584034 139622356395904 learning.py:507] global step 2076: loss = 5.7358 (1.209 sec/step)\n","I0830 16:43:16.828668 139622356395904 learning.py:507] global step 2077: loss = 4.7036 (1.243 sec/step)\n","I0830 16:43:18.063210 139622356395904 learning.py:507] global step 2078: loss = 5.7375 (1.233 sec/step)\n","I0830 16:43:19.278162 139622356395904 learning.py:507] global step 2079: loss = 5.0062 (1.213 sec/step)\n","I0830 16:43:20.476365 139622356395904 learning.py:507] global step 2080: loss = 5.5346 (1.196 sec/step)\n","I0830 16:43:21.672197 139622356395904 learning.py:507] global step 2081: loss = 5.6471 (1.194 sec/step)\n","I0830 16:43:22.894413 139622356395904 learning.py:507] global step 2082: loss = 4.9131 (1.220 sec/step)\n","I0830 16:43:24.148610 139622356395904 learning.py:507] global step 2083: loss = 5.5047 (1.252 sec/step)\n","I0830 16:43:25.371314 139622356395904 learning.py:507] global step 2084: loss = 4.3962 (1.221 sec/step)\n","I0830 16:43:26.571153 139622356395904 learning.py:507] global step 2085: loss = 5.3169 (1.198 sec/step)\n","I0830 16:43:27.758098 139622356395904 learning.py:507] global step 2086: loss = 5.3488 (1.185 sec/step)\n","I0830 16:43:29.009379 139622356395904 learning.py:507] global step 2087: loss = 5.0308 (1.249 sec/step)\n","I0830 16:43:30.234956 139622356395904 learning.py:507] global step 2088: loss = 4.5899 (1.224 sec/step)\n","I0830 16:43:31.481080 139622356395904 learning.py:507] global step 2089: loss = 4.4229 (1.244 sec/step)\n","I0830 16:43:32.695169 139622356395904 learning.py:507] global step 2090: loss = 4.5974 (1.212 sec/step)\n","I0830 16:43:33.940828 139622356395904 learning.py:507] global step 2091: loss = 4.3726 (1.244 sec/step)\n","I0830 16:43:35.177422 139622356395904 learning.py:507] global step 2092: loss = 5.3665 (1.235 sec/step)\n","I0830 16:43:36.421976 139622356395904 learning.py:507] global step 2093: loss = 5.9271 (1.243 sec/step)\n","I0830 16:43:37.706960 139622356395904 learning.py:507] global step 2094: loss = 4.6644 (1.283 sec/step)\n","I0830 16:43:38.937758 139622356395904 learning.py:507] global step 2095: loss = 4.5491 (1.229 sec/step)\n","I0830 16:43:40.147573 139622356395904 learning.py:507] global step 2096: loss = 4.4938 (1.208 sec/step)\n","I0830 16:43:41.381335 139622356395904 learning.py:507] global step 2097: loss = 4.7333 (1.232 sec/step)\n","I0830 16:43:42.639352 139622356395904 learning.py:507] global step 2098: loss = 4.2800 (1.256 sec/step)\n","I0830 16:43:43.845923 139622356395904 learning.py:507] global step 2099: loss = 5.5930 (1.205 sec/step)\n","I0830 16:43:45.080367 139622356395904 learning.py:507] global step 2100: loss = 4.2689 (1.233 sec/step)\n","I0830 16:43:46.314903 139622356395904 learning.py:507] global step 2101: loss = 5.5651 (1.233 sec/step)\n","I0830 16:43:47.555236 139622356395904 learning.py:507] global step 2102: loss = 4.8486 (1.238 sec/step)\n","I0830 16:43:48.762293 139622356395904 learning.py:507] global step 2103: loss = 5.5237 (1.205 sec/step)\n","I0830 16:43:49.986126 139622356395904 learning.py:507] global step 2104: loss = 4.4216 (1.222 sec/step)\n","I0830 16:43:51.207553 139622356395904 learning.py:507] global step 2105: loss = 4.7702 (1.219 sec/step)\n","I0830 16:43:52.462994 139622356395904 learning.py:507] global step 2106: loss = 4.5723 (1.254 sec/step)\n","I0830 16:43:53.692702 139622356395904 learning.py:507] global step 2107: loss = 4.5586 (1.228 sec/step)\n","I0830 16:43:54.922566 139622356395904 learning.py:507] global step 2108: loss = 4.6350 (1.228 sec/step)\n","I0830 16:43:56.193613 139622356395904 learning.py:507] global step 2109: loss = 5.0875 (1.269 sec/step)\n","I0830 16:43:57.424598 139622356395904 learning.py:507] global step 2110: loss = 4.5368 (1.229 sec/step)\n","I0830 16:43:58.631294 139622356395904 learning.py:507] global step 2111: loss = 5.7451 (1.205 sec/step)\n","I0830 16:43:59.847712 139622356395904 learning.py:507] global step 2112: loss = 5.2872 (1.209 sec/step)\n","I0830 16:44:01.939042 139619299985152 supervisor.py:1050] Recording summary at step 2113.\n","I0830 16:44:01.978456 139622356395904 learning.py:507] global step 2113: loss = 5.2382 (2.059 sec/step)\n","I0830 16:44:03.217687 139622356395904 learning.py:507] global step 2114: loss = 5.4209 (1.237 sec/step)\n","I0830 16:44:04.449777 139622356395904 learning.py:507] global step 2115: loss = 4.8135 (1.230 sec/step)\n","I0830 16:44:05.706230 139622356395904 learning.py:507] global step 2116: loss = 4.5255 (1.255 sec/step)\n","I0830 16:44:06.942265 139622356395904 learning.py:507] global step 2117: loss = 5.0339 (1.234 sec/step)\n","I0830 16:44:08.164328 139622356395904 learning.py:507] global step 2118: loss = 5.1661 (1.220 sec/step)\n","I0830 16:44:09.417415 139622356395904 learning.py:507] global step 2119: loss = 4.8133 (1.251 sec/step)\n","I0830 16:44:10.626465 139622356395904 learning.py:507] global step 2120: loss = 4.7409 (1.207 sec/step)\n","I0830 16:44:11.812760 139622356395904 learning.py:507] global step 2121: loss = 4.1794 (1.184 sec/step)\n","I0830 16:44:13.023621 139622356395904 learning.py:507] global step 2122: loss = 5.7798 (1.209 sec/step)\n","I0830 16:44:14.213891 139622356395904 learning.py:507] global step 2123: loss = 5.1840 (1.188 sec/step)\n","I0830 16:44:15.416543 139622356395904 learning.py:507] global step 2124: loss = 5.8401 (1.201 sec/step)\n","I0830 16:44:16.650402 139622356395904 learning.py:507] global step 2125: loss = 5.1492 (1.232 sec/step)\n","I0830 16:44:17.889670 139622356395904 learning.py:507] global step 2126: loss = 5.2505 (1.237 sec/step)\n","I0830 16:44:19.117336 139622356395904 learning.py:507] global step 2127: loss = 5.1659 (1.226 sec/step)\n","I0830 16:44:20.346657 139622356395904 learning.py:507] global step 2128: loss = 4.6195 (1.227 sec/step)\n","I0830 16:44:21.549356 139622356395904 learning.py:507] global step 2129: loss = 5.8569 (1.201 sec/step)\n","I0830 16:44:22.749703 139622356395904 learning.py:507] global step 2130: loss = 5.6425 (1.198 sec/step)\n","I0830 16:44:24.001132 139622356395904 learning.py:507] global step 2131: loss = 5.2450 (1.249 sec/step)\n","I0830 16:44:25.218634 139622356395904 learning.py:507] global step 2132: loss = 6.4905 (1.216 sec/step)\n","I0830 16:44:26.441122 139622356395904 learning.py:507] global step 2133: loss = 4.6788 (1.221 sec/step)\n","I0830 16:44:27.644441 139622356395904 learning.py:507] global step 2134: loss = 4.3515 (1.201 sec/step)\n","I0830 16:44:28.845810 139622356395904 learning.py:507] global step 2135: loss = 5.2822 (1.199 sec/step)\n","I0830 16:44:30.059660 139622356395904 learning.py:507] global step 2136: loss = 4.5248 (1.212 sec/step)\n","I0830 16:44:31.251511 139622356395904 learning.py:507] global step 2137: loss = 4.2769 (1.190 sec/step)\n","I0830 16:44:32.481850 139622356395904 learning.py:507] global step 2138: loss = 6.6532 (1.229 sec/step)\n","I0830 16:44:33.736758 139622356395904 learning.py:507] global step 2139: loss = 5.4936 (1.253 sec/step)\n","I0830 16:44:34.962622 139622356395904 learning.py:507] global step 2140: loss = 4.5176 (1.223 sec/step)\n","I0830 16:44:36.176743 139622356395904 learning.py:507] global step 2141: loss = 5.3760 (1.212 sec/step)\n","I0830 16:44:37.385863 139622356395904 learning.py:507] global step 2142: loss = 5.8770 (1.207 sec/step)\n","I0830 16:44:38.632752 139622356395904 learning.py:507] global step 2143: loss = 4.9970 (1.245 sec/step)\n","I0830 16:44:39.845541 139622356395904 learning.py:507] global step 2144: loss = 4.2687 (1.211 sec/step)\n","I0830 16:44:41.087000 139622356395904 learning.py:507] global step 2145: loss = 4.5848 (1.240 sec/step)\n","I0830 16:44:42.304476 139622356395904 learning.py:507] global step 2146: loss = 4.1590 (1.215 sec/step)\n","I0830 16:44:43.529792 139622356395904 learning.py:507] global step 2147: loss = 4.3183 (1.223 sec/step)\n","I0830 16:44:44.755823 139622356395904 learning.py:507] global step 2148: loss = 4.7255 (1.224 sec/step)\n","I0830 16:44:45.988294 139622356395904 learning.py:507] global step 2149: loss = 4.4984 (1.230 sec/step)\n","I0830 16:44:47.212075 139622356395904 learning.py:507] global step 2150: loss = 4.5909 (1.222 sec/step)\n","I0830 16:44:48.426850 139622356395904 learning.py:507] global step 2151: loss = 5.5443 (1.213 sec/step)\n","I0830 16:44:49.643175 139622356395904 learning.py:507] global step 2152: loss = 4.5929 (1.214 sec/step)\n","I0830 16:44:50.911178 139622356395904 learning.py:507] global step 2153: loss = 4.6867 (1.266 sec/step)\n","I0830 16:44:52.134035 139622356395904 learning.py:507] global step 2154: loss = 4.8605 (1.221 sec/step)\n","I0830 16:44:53.367670 139622356395904 learning.py:507] global step 2155: loss = 5.1095 (1.232 sec/step)\n","I0830 16:44:54.563447 139622356395904 learning.py:507] global step 2156: loss = 5.1574 (1.194 sec/step)\n","I0830 16:44:55.813624 139622356395904 learning.py:507] global step 2157: loss = 4.9594 (1.248 sec/step)\n","I0830 16:44:57.063435 139622356395904 learning.py:507] global step 2158: loss = 4.4843 (1.248 sec/step)\n","I0830 16:44:58.270478 139622356395904 learning.py:507] global step 2159: loss = 4.6011 (1.203 sec/step)\n","I0830 16:44:59.533487 139622356395904 learning.py:507] global step 2160: loss = 4.6164 (1.261 sec/step)\n","I0830 16:45:00.763812 139622356395904 learning.py:507] global step 2161: loss = 4.7969 (1.228 sec/step)\n","I0830 16:45:01.985162 139622356395904 learning.py:507] global step 2162: loss = 4.8896 (1.219 sec/step)\n","I0830 16:45:03.240034 139622356395904 learning.py:507] global step 2163: loss = 5.8112 (1.253 sec/step)\n","I0830 16:45:04.486349 139622356395904 learning.py:507] global step 2164: loss = 4.8748 (1.244 sec/step)\n","I0830 16:45:05.696460 139622356395904 learning.py:507] global step 2165: loss = 5.4390 (1.208 sec/step)\n","I0830 16:45:06.925519 139622356395904 learning.py:507] global step 2166: loss = 4.5662 (1.227 sec/step)\n","I0830 16:45:08.147107 139622356395904 learning.py:507] global step 2167: loss = 4.9408 (1.219 sec/step)\n","I0830 16:45:09.369257 139622356395904 learning.py:507] global step 2168: loss = 4.9274 (1.220 sec/step)\n","I0830 16:45:10.589732 139622356395904 learning.py:507] global step 2169: loss = 4.9816 (1.219 sec/step)\n","I0830 16:45:11.818462 139622356395904 learning.py:507] global step 2170: loss = 4.6031 (1.227 sec/step)\n","I0830 16:45:13.048108 139622356395904 learning.py:507] global step 2171: loss = 5.2633 (1.228 sec/step)\n","I0830 16:45:14.251917 139622356395904 learning.py:507] global step 2172: loss = 5.1219 (1.202 sec/step)\n","I0830 16:45:15.497259 139622356395904 learning.py:507] global step 2173: loss = 4.9980 (1.244 sec/step)\n","I0830 16:45:16.701764 139622356395904 learning.py:507] global step 2174: loss = 4.4685 (1.203 sec/step)\n","I0830 16:45:17.907252 139622356395904 learning.py:507] global step 2175: loss = 5.0594 (1.204 sec/step)\n","I0830 16:45:19.141371 139622356395904 learning.py:507] global step 2176: loss = 5.4204 (1.232 sec/step)\n","I0830 16:45:20.357929 139622356395904 learning.py:507] global step 2177: loss = 4.9147 (1.215 sec/step)\n","I0830 16:45:21.603110 139622356395904 learning.py:507] global step 2178: loss = 5.3734 (1.243 sec/step)\n","I0830 16:45:22.839910 139622356395904 learning.py:507] global step 2179: loss = 4.9526 (1.235 sec/step)\n","I0830 16:45:24.083270 139622356395904 learning.py:507] global step 2180: loss = 4.7206 (1.241 sec/step)\n","I0830 16:45:25.296074 139622356395904 learning.py:507] global step 2181: loss = 5.2735 (1.211 sec/step)\n","I0830 16:45:26.538036 139622356395904 learning.py:507] global step 2182: loss = 4.4192 (1.240 sec/step)\n","I0830 16:45:27.783136 139622356395904 learning.py:507] global step 2183: loss = 5.1509 (1.243 sec/step)\n","I0830 16:45:28.986725 139622356395904 learning.py:507] global step 2184: loss = 4.2693 (1.201 sec/step)\n","I0830 16:45:30.220849 139622356395904 learning.py:507] global step 2185: loss = 4.7186 (1.232 sec/step)\n","I0830 16:45:31.460873 139622356395904 learning.py:507] global step 2186: loss = 4.9855 (1.238 sec/step)\n","I0830 16:45:32.661185 139622356395904 learning.py:507] global step 2187: loss = 5.2688 (1.198 sec/step)\n","I0830 16:45:33.874759 139622356395904 learning.py:507] global step 2188: loss = 4.4993 (1.211 sec/step)\n","I0830 16:45:35.107663 139622356395904 learning.py:507] global step 2189: loss = 4.5064 (1.231 sec/step)\n","I0830 16:45:36.367301 139622356395904 learning.py:507] global step 2190: loss = 4.9745 (1.258 sec/step)\n","I0830 16:45:37.582802 139622356395904 learning.py:507] global step 2191: loss = 4.9621 (1.214 sec/step)\n","I0830 16:45:38.830819 139622356395904 learning.py:507] global step 2192: loss = 4.5883 (1.246 sec/step)\n","I0830 16:45:40.024269 139622356395904 learning.py:507] global step 2193: loss = 5.1483 (1.192 sec/step)\n","I0830 16:45:41.240221 139622356395904 learning.py:507] global step 2194: loss = 4.3270 (1.214 sec/step)\n","I0830 16:45:42.471418 139622356395904 learning.py:507] global step 2195: loss = 5.5585 (1.229 sec/step)\n","I0830 16:45:43.711628 139622356395904 learning.py:507] global step 2196: loss = 4.2647 (1.238 sec/step)\n","I0830 16:45:44.889189 139622356395904 learning.py:507] global step 2197: loss = 4.4152 (1.176 sec/step)\n","I0830 16:45:46.094874 139622356395904 learning.py:507] global step 2198: loss = 4.6593 (1.204 sec/step)\n","I0830 16:45:47.338085 139622356395904 learning.py:507] global step 2199: loss = 4.4416 (1.241 sec/step)\n","I0830 16:45:48.565700 139622356395904 learning.py:507] global step 2200: loss = 4.2816 (1.226 sec/step)\n","I0830 16:45:49.757317 139622356395904 learning.py:507] global step 2201: loss = 4.5777 (1.190 sec/step)\n","I0830 16:45:50.981778 139622356395904 learning.py:507] global step 2202: loss = 6.3632 (1.223 sec/step)\n","I0830 16:45:52.247552 139622356395904 learning.py:507] global step 2203: loss = 4.3334 (1.264 sec/step)\n","I0830 16:45:53.473516 139622356395904 learning.py:507] global step 2204: loss = 4.9553 (1.224 sec/step)\n","I0830 16:45:54.720125 139622356395904 learning.py:507] global step 2205: loss = 4.5659 (1.245 sec/step)\n","I0830 16:45:55.956116 139622356395904 learning.py:507] global step 2206: loss = 5.7177 (1.234 sec/step)\n","I0830 16:45:57.170139 139622356395904 learning.py:507] global step 2207: loss = 4.8138 (1.212 sec/step)\n","I0830 16:45:58.404682 139622356395904 learning.py:507] global step 2208: loss = 5.5125 (1.232 sec/step)\n","I0830 16:45:59.736497 139622356395904 learning.py:507] global step 2209: loss = 5.2588 (1.258 sec/step)\n","I0830 16:46:01.940068 139619299985152 supervisor.py:1050] Recording summary at step 2210.\n","I0830 16:46:01.976522 139622356395904 learning.py:507] global step 2210: loss = 6.5496 (2.238 sec/step)\n","I0830 16:46:03.218044 139622356395904 learning.py:507] global step 2211: loss = 4.3837 (1.240 sec/step)\n","I0830 16:46:04.428915 139622356395904 learning.py:507] global step 2212: loss = 4.4363 (1.209 sec/step)\n","I0830 16:46:05.643848 139622356395904 learning.py:507] global step 2213: loss = 5.9175 (1.213 sec/step)\n","I0830 16:46:06.885684 139622356395904 learning.py:507] global step 2214: loss = 4.9790 (1.240 sec/step)\n","I0830 16:46:08.102695 139622356395904 learning.py:507] global step 2215: loss = 4.3559 (1.215 sec/step)\n","I0830 16:46:09.300706 139622356395904 learning.py:507] global step 2216: loss = 4.9428 (1.196 sec/step)\n","I0830 16:46:10.524503 139622356395904 learning.py:507] global step 2217: loss = 5.0641 (1.222 sec/step)\n","I0830 16:46:11.737714 139622356395904 learning.py:507] global step 2218: loss = 4.4633 (1.211 sec/step)\n","I0830 16:46:12.993151 139622356395904 learning.py:507] global step 2219: loss = 4.1261 (1.254 sec/step)\n","I0830 16:46:14.226193 139622356395904 learning.py:507] global step 2220: loss = 4.2782 (1.231 sec/step)\n","I0830 16:46:15.422004 139622356395904 learning.py:507] global step 2221: loss = 4.7784 (1.194 sec/step)\n","I0830 16:46:16.663963 139622356395904 learning.py:507] global step 2222: loss = 4.6077 (1.238 sec/step)\n","I0830 16:46:17.916285 139622356395904 learning.py:507] global step 2223: loss = 5.0714 (1.250 sec/step)\n","I0830 16:46:19.155538 139622356395904 learning.py:507] global step 2224: loss = 4.8253 (1.238 sec/step)\n","I0830 16:46:20.391787 139622356395904 learning.py:507] global step 2225: loss = 4.9449 (1.234 sec/step)\n","I0830 16:46:21.598192 139622356395904 learning.py:507] global step 2226: loss = 4.2648 (1.205 sec/step)\n","I0830 16:46:22.822308 139622356395904 learning.py:507] global step 2227: loss = 4.3740 (1.222 sec/step)\n","I0830 16:46:24.039970 139622356395904 learning.py:507] global step 2228: loss = 4.4453 (1.216 sec/step)\n","I0830 16:46:25.242837 139622356395904 learning.py:507] global step 2229: loss = 4.5622 (1.201 sec/step)\n","I0830 16:46:26.467855 139622356395904 learning.py:507] global step 2230: loss = 5.2086 (1.223 sec/step)\n","I0830 16:46:27.668613 139622356395904 learning.py:507] global step 2231: loss = 4.3779 (1.199 sec/step)\n","I0830 16:46:28.897444 139622356395904 learning.py:507] global step 2232: loss = 4.4613 (1.227 sec/step)\n","I0830 16:46:30.105160 139622356395904 learning.py:507] global step 2233: loss = 4.9571 (1.206 sec/step)\n","I0830 16:46:31.314889 139622356395904 learning.py:507] global step 2234: loss = 4.9211 (1.208 sec/step)\n","I0830 16:46:32.521638 139622356395904 learning.py:507] global step 2235: loss = 4.6585 (1.205 sec/step)\n","I0830 16:46:33.759102 139622356395904 learning.py:507] global step 2236: loss = 4.1441 (1.235 sec/step)\n","I0830 16:46:35.002592 139622356395904 learning.py:507] global step 2237: loss = 5.0469 (1.239 sec/step)\n","I0830 16:46:36.239202 139622356395904 learning.py:507] global step 2238: loss = 4.6050 (1.233 sec/step)\n","I0830 16:46:37.465005 139622356395904 learning.py:507] global step 2239: loss = 4.7636 (1.224 sec/step)\n","I0830 16:46:38.685792 139622356395904 learning.py:507] global step 2240: loss = 4.2244 (1.219 sec/step)\n","I0830 16:46:39.892878 139622356395904 learning.py:507] global step 2241: loss = 5.2271 (1.205 sec/step)\n","I0830 16:46:41.133006 139622356395904 learning.py:507] global step 2242: loss = 4.0331 (1.238 sec/step)\n","I0830 16:46:42.380334 139622356395904 learning.py:507] global step 2243: loss = 4.0510 (1.245 sec/step)\n","I0830 16:46:43.609376 139622356395904 learning.py:507] global step 2244: loss = 4.5905 (1.227 sec/step)\n","I0830 16:46:44.838012 139622356395904 learning.py:507] global step 2245: loss = 5.1743 (1.227 sec/step)\n","I0830 16:46:46.079495 139622356395904 learning.py:507] global step 2246: loss = 4.3137 (1.240 sec/step)\n","I0830 16:46:47.267416 139622356395904 learning.py:507] global step 2247: loss = 4.9545 (1.186 sec/step)\n","I0830 16:46:48.525645 139622356395904 learning.py:507] global step 2248: loss = 4.3147 (1.256 sec/step)\n","I0830 16:46:49.739810 139622356395904 learning.py:507] global step 2249: loss = 4.6963 (1.213 sec/step)\n","I0830 16:46:50.971433 139622356395904 learning.py:507] global step 2250: loss = 4.8110 (1.230 sec/step)\n","I0830 16:46:52.202224 139622356395904 learning.py:507] global step 2251: loss = 4.0864 (1.229 sec/step)\n","I0830 16:46:53.428700 139622356395904 learning.py:507] global step 2252: loss = 4.1294 (1.225 sec/step)\n","I0830 16:46:54.680357 139622356395904 learning.py:507] global step 2253: loss = 4.8336 (1.250 sec/step)\n","I0830 16:46:55.912423 139622356395904 learning.py:507] global step 2254: loss = 4.6240 (1.230 sec/step)\n","I0830 16:46:57.115545 139622356395904 learning.py:507] global step 2255: loss = 5.2955 (1.201 sec/step)\n","I0830 16:46:58.334897 139622356395904 learning.py:507] global step 2256: loss = 4.6782 (1.217 sec/step)\n","I0830 16:46:59.554744 139622356395904 learning.py:507] global step 2257: loss = 5.4959 (1.218 sec/step)\n","I0830 16:47:00.738416 139622356395904 learning.py:507] global step 2258: loss = 5.1452 (1.182 sec/step)\n","I0830 16:47:01.970206 139622356395904 learning.py:507] global step 2259: loss = 4.8399 (1.230 sec/step)\n","I0830 16:47:03.209317 139622356395904 learning.py:507] global step 2260: loss = 5.3797 (1.237 sec/step)\n","I0830 16:47:04.401420 139622356395904 learning.py:507] global step 2261: loss = 4.4087 (1.190 sec/step)\n","I0830 16:47:05.651393 139622356395904 learning.py:507] global step 2262: loss = 5.0146 (1.248 sec/step)\n","I0830 16:47:06.877473 139622356395904 learning.py:507] global step 2263: loss = 4.2277 (1.224 sec/step)\n","I0830 16:47:08.103208 139622356395904 learning.py:507] global step 2264: loss = 3.9526 (1.223 sec/step)\n","I0830 16:47:09.318172 139622356395904 learning.py:507] global step 2265: loss = 4.2565 (1.212 sec/step)\n","I0830 16:47:10.536277 139622356395904 learning.py:507] global step 2266: loss = 4.8620 (1.216 sec/step)\n","I0830 16:47:11.743741 139622356395904 learning.py:507] global step 2267: loss = 5.2172 (1.206 sec/step)\n","I0830 16:47:12.960732 139622356395904 learning.py:507] global step 2268: loss = 4.2133 (1.215 sec/step)\n","I0830 16:47:14.154771 139622356395904 learning.py:507] global step 2269: loss = 4.6058 (1.192 sec/step)\n","I0830 16:47:15.332690 139622356395904 learning.py:507] global step 2270: loss = 4.6862 (1.176 sec/step)\n","I0830 16:47:16.562795 139622356395904 learning.py:507] global step 2271: loss = 4.6966 (1.228 sec/step)\n","I0830 16:47:17.828748 139622356395904 learning.py:507] global step 2272: loss = 4.1407 (1.264 sec/step)\n","I0830 16:47:19.057242 139622356395904 learning.py:507] global step 2273: loss = 4.8141 (1.226 sec/step)\n","I0830 16:47:20.291649 139622356395904 learning.py:507] global step 2274: loss = 4.2936 (1.231 sec/step)\n","I0830 16:47:21.512205 139622356395904 learning.py:507] global step 2275: loss = 5.2478 (1.218 sec/step)\n","I0830 16:47:22.746664 139622356395904 learning.py:507] global step 2276: loss = 4.7067 (1.233 sec/step)\n","I0830 16:47:23.992167 139622356395904 learning.py:507] global step 2277: loss = 4.8244 (1.244 sec/step)\n","I0830 16:47:25.228065 139622356395904 learning.py:507] global step 2278: loss = 5.3162 (1.234 sec/step)\n","I0830 16:47:26.437303 139622356395904 learning.py:507] global step 2279: loss = 4.7893 (1.207 sec/step)\n","I0830 16:47:27.627489 139622356395904 learning.py:507] global step 2280: loss = 5.5954 (1.187 sec/step)\n","I0830 16:47:28.835773 139622356395904 learning.py:507] global step 2281: loss = 4.6860 (1.206 sec/step)\n","I0830 16:47:30.070855 139622356395904 learning.py:507] global step 2282: loss = 4.8351 (1.233 sec/step)\n","I0830 16:47:31.286350 139622356395904 learning.py:507] global step 2283: loss = 4.2662 (1.214 sec/step)\n","I0830 16:47:32.521018 139622356395904 learning.py:507] global step 2284: loss = 5.6792 (1.233 sec/step)\n","I0830 16:47:33.710195 139622356395904 learning.py:507] global step 2285: loss = 5.0797 (1.187 sec/step)\n","I0830 16:47:34.941298 139622356395904 learning.py:507] global step 2286: loss = 4.8468 (1.229 sec/step)\n","I0830 16:47:36.175104 139622356395904 learning.py:507] global step 2287: loss = 3.7998 (1.232 sec/step)\n","I0830 16:47:37.415642 139622356395904 learning.py:507] global step 2288: loss = 4.4665 (1.239 sec/step)\n","I0830 16:47:38.686454 139622356395904 learning.py:507] global step 2289: loss = 4.1315 (1.269 sec/step)\n","I0830 16:47:39.887304 139622356395904 learning.py:507] global step 2290: loss = 4.5207 (1.199 sec/step)\n","I0830 16:47:41.112932 139622356395904 learning.py:507] global step 2291: loss = 4.1472 (1.224 sec/step)\n","I0830 16:47:42.332928 139622356395904 learning.py:507] global step 2292: loss = 4.4342 (1.218 sec/step)\n","I0830 16:47:43.558943 139622356395904 learning.py:507] global step 2293: loss = 3.9490 (1.224 sec/step)\n","I0830 16:47:44.775094 139622356395904 learning.py:507] global step 2294: loss = 5.2263 (1.214 sec/step)\n","I0830 16:47:46.014958 139622356395904 learning.py:507] global step 2295: loss = 4.2057 (1.237 sec/step)\n","I0830 16:47:47.211807 139622356395904 learning.py:507] global step 2296: loss = 4.2257 (1.193 sec/step)\n","I0830 16:47:48.444409 139622356395904 learning.py:507] global step 2297: loss = 5.5959 (1.231 sec/step)\n","I0830 16:47:49.639185 139622356395904 learning.py:507] global step 2298: loss = 4.9078 (1.193 sec/step)\n","I0830 16:47:50.881270 139622356395904 learning.py:507] global step 2299: loss = 4.9792 (1.240 sec/step)\n","I0830 16:47:52.114737 139622356395904 learning.py:507] global step 2300: loss = 4.4618 (1.231 sec/step)\n","I0830 16:47:53.360035 139622356395904 learning.py:507] global step 2301: loss = 4.4448 (1.243 sec/step)\n","I0830 16:47:54.611720 139622356395904 learning.py:507] global step 2302: loss = 5.4105 (1.250 sec/step)\n","I0830 16:47:55.816114 139622356395904 learning.py:507] global step 2303: loss = 5.6229 (1.203 sec/step)\n","I0830 16:47:57.068869 139622356395904 learning.py:507] global step 2304: loss = 3.9301 (1.251 sec/step)\n","I0830 16:47:58.258035 139622356395904 learning.py:507] global step 2305: loss = 4.2973 (1.187 sec/step)\n","I0830 16:47:59.506365 139622356395904 learning.py:507] global step 2306: loss = 4.5298 (1.246 sec/step)\n","I0830 16:48:01.281670 139622356395904 learning.py:507] global step 2307: loss = 4.3217 (1.673 sec/step)\n","I0830 16:48:02.863986 139619299985152 supervisor.py:1050] Recording summary at step 2307.\n","I0830 16:48:02.907462 139622356395904 learning.py:507] global step 2308: loss = 5.0736 (1.373 sec/step)\n","I0830 16:48:04.124102 139622356395904 learning.py:507] global step 2309: loss = 4.4220 (1.215 sec/step)\n","I0830 16:48:05.383950 139622356395904 learning.py:507] global step 2310: loss = 5.7443 (1.258 sec/step)\n","I0830 16:48:06.598630 139622356395904 learning.py:507] global step 2311: loss = 4.9237 (1.213 sec/step)\n","I0830 16:48:07.804282 139622356395904 learning.py:507] global step 2312: loss = 5.1898 (1.204 sec/step)\n","I0830 16:48:09.028309 139622356395904 learning.py:507] global step 2313: loss = 4.8082 (1.222 sec/step)\n","I0830 16:48:10.226788 139622356395904 learning.py:507] global step 2314: loss = 4.9734 (1.196 sec/step)\n","I0830 16:48:11.438471 139622356395904 learning.py:507] global step 2315: loss = 3.9747 (1.210 sec/step)\n","I0830 16:48:12.647024 139622356395904 learning.py:507] global step 2316: loss = 4.4798 (1.207 sec/step)\n","I0830 16:48:13.872568 139622356395904 learning.py:507] global step 2317: loss = 4.1500 (1.223 sec/step)\n","I0830 16:48:15.080803 139622356395904 learning.py:507] global step 2318: loss = 4.8660 (1.206 sec/step)\n","I0830 16:48:16.325819 139622356395904 learning.py:507] global step 2319: loss = 4.2011 (1.243 sec/step)\n","I0830 16:48:17.551974 139622356395904 learning.py:507] global step 2320: loss = 5.0402 (1.224 sec/step)\n","I0830 16:48:18.791528 139622356395904 learning.py:507] global step 2321: loss = 4.6108 (1.238 sec/step)\n","I0830 16:48:20.018707 139622356395904 learning.py:507] global step 2322: loss = 4.7842 (1.225 sec/step)\n","I0830 16:48:21.216988 139622356395904 learning.py:507] global step 2323: loss = 4.8216 (1.196 sec/step)\n","I0830 16:48:22.418154 139622356395904 learning.py:507] global step 2324: loss = 4.8283 (1.199 sec/step)\n","I0830 16:48:23.637313 139622356395904 learning.py:507] global step 2325: loss = 4.6829 (1.217 sec/step)\n","I0830 16:48:24.840037 139622356395904 learning.py:507] global step 2326: loss = 5.0786 (1.201 sec/step)\n","I0830 16:48:26.092804 139622356395904 learning.py:507] global step 2327: loss = 5.8204 (1.251 sec/step)\n","I0830 16:48:27.341281 139622356395904 learning.py:507] global step 2328: loss = 4.6071 (1.246 sec/step)\n","I0830 16:48:28.546646 139622356395904 learning.py:507] global step 2329: loss = 4.6331 (1.203 sec/step)\n","I0830 16:48:29.807917 139622356395904 learning.py:507] global step 2330: loss = 5.2350 (1.259 sec/step)\n","I0830 16:48:31.067088 139622356395904 learning.py:507] global step 2331: loss = 4.7909 (1.257 sec/step)\n","I0830 16:48:32.307568 139622356395904 learning.py:507] global step 2332: loss = 5.4310 (1.239 sec/step)\n","I0830 16:48:33.552342 139622356395904 learning.py:507] global step 2333: loss = 5.2418 (1.243 sec/step)\n","I0830 16:48:34.784949 139622356395904 learning.py:507] global step 2334: loss = 4.2123 (1.231 sec/step)\n","I0830 16:48:35.997667 139622356395904 learning.py:507] global step 2335: loss = 4.6026 (1.211 sec/step)\n","I0830 16:48:37.231028 139622356395904 learning.py:507] global step 2336: loss = 4.4500 (1.231 sec/step)\n","I0830 16:48:38.470644 139622356395904 learning.py:507] global step 2337: loss = 4.8853 (1.238 sec/step)\n","I0830 16:48:39.677310 139622356395904 learning.py:507] global step 2338: loss = 4.0386 (1.205 sec/step)\n","I0830 16:48:40.928807 139622356395904 learning.py:507] global step 2339: loss = 5.0518 (1.250 sec/step)\n","I0830 16:48:42.154906 139622356395904 learning.py:507] global step 2340: loss = 4.1549 (1.224 sec/step)\n","I0830 16:48:43.344196 139622356395904 learning.py:507] global step 2341: loss = 4.1787 (1.188 sec/step)\n","I0830 16:48:44.600347 139622356395904 learning.py:507] global step 2342: loss = 4.5105 (1.254 sec/step)\n","I0830 16:48:45.855494 139622356395904 learning.py:507] global step 2343: loss = 4.5320 (1.253 sec/step)\n","I0830 16:48:47.089492 139622356395904 learning.py:507] global step 2344: loss = 4.5495 (1.232 sec/step)\n","I0830 16:48:48.333478 139622356395904 learning.py:507] global step 2345: loss = 4.0760 (1.242 sec/step)\n","I0830 16:48:49.556125 139622356395904 learning.py:507] global step 2346: loss = 4.1862 (1.221 sec/step)\n","I0830 16:48:50.793400 139622356395904 learning.py:507] global step 2347: loss = 5.6430 (1.235 sec/step)\n","I0830 16:48:51.988605 139622356395904 learning.py:507] global step 2348: loss = 4.7528 (1.193 sec/step)\n","I0830 16:48:53.192130 139622356395904 learning.py:507] global step 2349: loss = 4.8787 (1.202 sec/step)\n","I0830 16:48:54.440765 139622356395904 learning.py:507] global step 2350: loss = 5.7002 (1.247 sec/step)\n","I0830 16:48:55.687345 139622356395904 learning.py:507] global step 2351: loss = 5.0242 (1.245 sec/step)\n","I0830 16:48:56.952043 139622356395904 learning.py:507] global step 2352: loss = 4.3554 (1.263 sec/step)\n","I0830 16:48:58.196425 139622356395904 learning.py:507] global step 2353: loss = 5.1708 (1.242 sec/step)\n","I0830 16:48:59.405805 139622356395904 learning.py:507] global step 2354: loss = 4.9389 (1.207 sec/step)\n","I0830 16:49:00.640908 139622356395904 learning.py:507] global step 2355: loss = 4.1758 (1.233 sec/step)\n","I0830 16:49:01.878592 139622356395904 learning.py:507] global step 2356: loss = 4.6883 (1.236 sec/step)\n","I0830 16:49:03.085235 139622356395904 learning.py:507] global step 2357: loss = 4.2989 (1.205 sec/step)\n","I0830 16:49:04.325928 139622356395904 learning.py:507] global step 2358: loss = 4.1469 (1.239 sec/step)\n","I0830 16:49:05.526645 139622356395904 learning.py:507] global step 2359: loss = 4.5281 (1.199 sec/step)\n","I0830 16:49:06.706934 139622356395904 learning.py:507] global step 2360: loss = 5.4905 (1.178 sec/step)\n","I0830 16:49:07.935781 139622356395904 learning.py:507] global step 2361: loss = 4.1269 (1.227 sec/step)\n","I0830 16:49:09.144947 139622356395904 learning.py:507] global step 2362: loss = 5.1912 (1.207 sec/step)\n","I0830 16:49:10.358135 139622356395904 learning.py:507] global step 2363: loss = 4.1460 (1.211 sec/step)\n","I0830 16:49:11.587896 139622356395904 learning.py:507] global step 2364: loss = 4.5297 (1.228 sec/step)\n","I0830 16:49:12.838620 139622356395904 learning.py:507] global step 2365: loss = 4.9935 (1.249 sec/step)\n","I0830 16:49:14.047882 139622356395904 learning.py:507] global step 2366: loss = 4.3631 (1.207 sec/step)\n","I0830 16:49:15.256139 139622356395904 learning.py:507] global step 2367: loss = 4.0839 (1.206 sec/step)\n","I0830 16:49:16.481284 139622356395904 learning.py:507] global step 2368: loss = 5.0768 (1.223 sec/step)\n","I0830 16:49:17.687544 139622356395904 learning.py:507] global step 2369: loss = 4.4096 (1.204 sec/step)\n","I0830 16:49:18.911274 139622356395904 learning.py:507] global step 2370: loss = 4.1182 (1.222 sec/step)\n","I0830 16:49:20.167587 139622356395904 learning.py:507] global step 2371: loss = 3.6651 (1.254 sec/step)\n","I0830 16:49:21.377588 139622356395904 learning.py:507] global step 2372: loss = 5.9302 (1.208 sec/step)\n","I0830 16:49:22.634979 139622356395904 learning.py:507] global step 2373: loss = 5.4160 (1.256 sec/step)\n","I0830 16:49:23.852858 139622356395904 learning.py:507] global step 2374: loss = 4.2191 (1.216 sec/step)\n","I0830 16:49:25.083824 139622356395904 learning.py:507] global step 2375: loss = 4.4954 (1.229 sec/step)\n","I0830 16:49:26.295175 139622356395904 learning.py:507] global step 2376: loss = 5.0770 (1.209 sec/step)\n","I0830 16:49:27.513720 139622356395904 learning.py:507] global step 2377: loss = 4.1234 (1.217 sec/step)\n","I0830 16:49:28.748276 139622356395904 learning.py:507] global step 2378: loss = 4.0324 (1.233 sec/step)\n","I0830 16:49:29.954995 139622356395904 learning.py:507] global step 2379: loss = 4.4146 (1.205 sec/step)\n","I0830 16:49:31.175019 139622356395904 learning.py:507] global step 2380: loss = 4.3854 (1.218 sec/step)\n","I0830 16:49:32.404626 139622356395904 learning.py:507] global step 2381: loss = 4.5158 (1.227 sec/step)\n","I0830 16:49:33.628306 139622356395904 learning.py:507] global step 2382: loss = 4.2745 (1.222 sec/step)\n","I0830 16:49:34.841216 139622356395904 learning.py:507] global step 2383: loss = 4.3847 (1.211 sec/step)\n","I0830 16:49:36.065894 139622356395904 learning.py:507] global step 2384: loss = 4.5999 (1.223 sec/step)\n","I0830 16:49:37.309514 139622356395904 learning.py:507] global step 2385: loss = 4.5039 (1.242 sec/step)\n","I0830 16:49:38.547815 139622356395904 learning.py:507] global step 2386: loss = 4.3845 (1.236 sec/step)\n","I0830 16:49:39.760906 139622356395904 learning.py:507] global step 2387: loss = 5.6404 (1.211 sec/step)\n","I0830 16:49:40.972912 139622356395904 learning.py:507] global step 2388: loss = 4.6849 (1.210 sec/step)\n","I0830 16:49:42.206842 139622356395904 learning.py:507] global step 2389: loss = 4.9235 (1.232 sec/step)\n","I0830 16:49:43.438215 139622356395904 learning.py:507] global step 2390: loss = 4.3034 (1.229 sec/step)\n","I0830 16:49:44.664536 139622356395904 learning.py:507] global step 2391: loss = 4.2162 (1.224 sec/step)\n","I0830 16:49:45.905579 139622356395904 learning.py:507] global step 2392: loss = 4.1368 (1.239 sec/step)\n","I0830 16:49:47.119364 139622356395904 learning.py:507] global step 2393: loss = 4.3314 (1.212 sec/step)\n","I0830 16:49:48.371193 139622356395904 learning.py:507] global step 2394: loss = 4.4857 (1.250 sec/step)\n","I0830 16:49:49.603796 139622356395904 learning.py:507] global step 2395: loss = 5.0007 (1.231 sec/step)\n","I0830 16:49:50.822091 139622356395904 learning.py:507] global step 2396: loss = 3.9086 (1.217 sec/step)\n","I0830 16:49:52.008838 139622356395904 learning.py:507] global step 2397: loss = 6.1339 (1.185 sec/step)\n","I0830 16:49:53.231095 139622356395904 learning.py:507] global step 2398: loss = 3.8659 (1.220 sec/step)\n","I0830 16:49:54.481986 139622356395904 learning.py:507] global step 2399: loss = 4.2728 (1.249 sec/step)\n","I0830 16:49:55.728113 139622356395904 learning.py:507] global step 2400: loss = 5.0326 (1.244 sec/step)\n","I0830 16:49:56.962309 139622356395904 learning.py:507] global step 2401: loss = 4.3491 (1.232 sec/step)\n","I0830 16:49:58.186714 139622356395904 learning.py:507] global step 2402: loss = 4.7750 (1.223 sec/step)\n","I0830 16:49:59.441218 139622356395904 learning.py:507] global step 2403: loss = 4.5187 (1.252 sec/step)\n","I0830 16:49:59.646023 139619316770560 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n","W0830 16:50:00.226701 139619316770560 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","I0830 16:50:02.710317 139619299985152 supervisor.py:1050] Recording summary at step 2404.\n","I0830 16:50:02.721369 139622356395904 learning.py:507] global step 2404: loss = 4.3925 (3.268 sec/step)\n","I0830 16:50:04.468430 139622356395904 learning.py:507] global step 2405: loss = 4.6399 (1.726 sec/step)\n","I0830 16:50:05.865247 139622356395904 learning.py:507] global step 2406: loss = 5.5560 (1.395 sec/step)\n","I0830 16:50:07.088598 139622356395904 learning.py:507] global step 2407: loss = 4.0977 (1.221 sec/step)\n","I0830 16:50:08.340711 139622356395904 learning.py:507] global step 2408: loss = 4.2989 (1.250 sec/step)\n","I0830 16:50:09.529421 139622356395904 learning.py:507] global step 2409: loss = 4.4145 (1.187 sec/step)\n","I0830 16:50:10.756925 139622356395904 learning.py:507] global step 2410: loss = 4.7238 (1.226 sec/step)\n","I0830 16:50:11.969630 139622356395904 learning.py:507] global step 2411: loss = 4.5555 (1.211 sec/step)\n","I0830 16:50:13.187411 139622356395904 learning.py:507] global step 2412: loss = 4.7789 (1.216 sec/step)\n","I0830 16:50:14.370307 139622356395904 learning.py:507] global step 2413: loss = 4.4094 (1.181 sec/step)\n","I0830 16:50:15.628679 139622356395904 learning.py:507] global step 2414: loss = 4.4364 (1.256 sec/step)\n","I0830 16:50:16.831242 139622356395904 learning.py:507] global step 2415: loss = 5.2897 (1.201 sec/step)\n","I0830 16:50:18.047138 139622356395904 learning.py:507] global step 2416: loss = 4.7230 (1.214 sec/step)\n","I0830 16:50:19.269456 139622356395904 learning.py:507] global step 2417: loss = 5.1195 (1.220 sec/step)\n","I0830 16:50:20.485090 139622356395904 learning.py:507] global step 2418: loss = 4.7634 (1.213 sec/step)\n","I0830 16:50:21.703664 139622356395904 learning.py:507] global step 2419: loss = 4.5873 (1.217 sec/step)\n","I0830 16:50:22.910495 139622356395904 learning.py:507] global step 2420: loss = 5.2089 (1.205 sec/step)\n","I0830 16:50:24.186149 139622356395904 learning.py:507] global step 2421: loss = 4.4127 (1.274 sec/step)\n","I0830 16:50:25.417163 139622356395904 learning.py:507] global step 2422: loss = 3.9134 (1.229 sec/step)\n","I0830 16:50:26.655200 139622356395904 learning.py:507] global step 2423: loss = 5.2831 (1.235 sec/step)\n","I0830 16:50:27.874140 139622356395904 learning.py:507] global step 2424: loss = 4.2578 (1.215 sec/step)\n","I0830 16:50:29.090971 139622356395904 learning.py:507] global step 2425: loss = 4.2462 (1.215 sec/step)\n","I0830 16:50:30.306101 139622356395904 learning.py:507] global step 2426: loss = 4.2318 (1.213 sec/step)\n","I0830 16:50:31.520412 139622356395904 learning.py:507] global step 2427: loss = 5.5310 (1.212 sec/step)\n","I0830 16:50:32.737598 139622356395904 learning.py:507] global step 2428: loss = 3.8723 (1.215 sec/step)\n","I0830 16:50:33.955313 139622356395904 learning.py:507] global step 2429: loss = 4.6966 (1.216 sec/step)\n","I0830 16:50:35.164000 139622356395904 learning.py:507] global step 2430: loss = 4.7839 (1.207 sec/step)\n","I0830 16:50:36.364508 139622356395904 learning.py:507] global step 2431: loss = 4.9438 (1.199 sec/step)\n","I0830 16:50:37.563196 139622356395904 learning.py:507] global step 2432: loss = 4.4445 (1.197 sec/step)\n","I0830 16:50:38.784420 139622356395904 learning.py:507] global step 2433: loss = 4.9780 (1.219 sec/step)\n","I0830 16:50:40.002599 139622356395904 learning.py:507] global step 2434: loss = 5.0034 (1.216 sec/step)\n","I0830 16:50:41.202160 139622356395904 learning.py:507] global step 2435: loss = 4.7318 (1.198 sec/step)\n","I0830 16:50:42.408659 139622356395904 learning.py:507] global step 2436: loss = 4.6480 (1.205 sec/step)\n","I0830 16:50:43.637232 139622356395904 learning.py:507] global step 2437: loss = 4.7911 (1.227 sec/step)\n","I0830 16:50:44.890806 139622356395904 learning.py:507] global step 2438: loss = 4.1365 (1.252 sec/step)\n","I0830 16:50:46.121999 139622356395904 learning.py:507] global step 2439: loss = 4.5676 (1.229 sec/step)\n","I0830 16:50:47.344311 139622356395904 learning.py:507] global step 2440: loss = 5.4433 (1.221 sec/step)\n","I0830 16:50:48.553421 139622356395904 learning.py:507] global step 2441: loss = 4.1177 (1.207 sec/step)\n","I0830 16:50:49.812532 139622356395904 learning.py:507] global step 2442: loss = 4.2897 (1.257 sec/step)\n","I0830 16:50:51.050902 139622356395904 learning.py:507] global step 2443: loss = 4.1076 (1.236 sec/step)\n","I0830 16:50:52.275454 139622356395904 learning.py:507] global step 2444: loss = 4.7435 (1.222 sec/step)\n","I0830 16:50:53.474016 139622356395904 learning.py:507] global step 2445: loss = 4.3576 (1.197 sec/step)\n","I0830 16:50:54.689720 139622356395904 learning.py:507] global step 2446: loss = 4.0819 (1.214 sec/step)\n","I0830 16:50:55.904043 139622356395904 learning.py:507] global step 2447: loss = 4.1262 (1.212 sec/step)\n","I0830 16:50:57.172580 139622356395904 learning.py:507] global step 2448: loss = 4.3419 (1.267 sec/step)\n","I0830 16:50:58.383764 139622356395904 learning.py:507] global step 2449: loss = 4.1910 (1.209 sec/step)\n","I0830 16:50:59.613040 139622356395904 learning.py:507] global step 2450: loss = 4.4569 (1.227 sec/step)\n","I0830 16:51:00.827859 139622356395904 learning.py:507] global step 2451: loss = 4.6104 (1.213 sec/step)\n","I0830 16:51:02.072384 139622356395904 learning.py:507] global step 2452: loss = 4.6661 (1.243 sec/step)\n","I0830 16:51:03.292361 139622356395904 learning.py:507] global step 2453: loss = 5.3558 (1.218 sec/step)\n","I0830 16:51:04.507131 139622356395904 learning.py:507] global step 2454: loss = 3.9820 (1.213 sec/step)\n","I0830 16:51:05.767648 139622356395904 learning.py:507] global step 2455: loss = 4.7660 (1.258 sec/step)\n","I0830 16:51:06.988006 139622356395904 learning.py:507] global step 2456: loss = 4.6415 (1.218 sec/step)\n","I0830 16:51:08.228677 139622356395904 learning.py:507] global step 2457: loss = 3.8528 (1.239 sec/step)\n","I0830 16:51:09.443580 139622356395904 learning.py:507] global step 2458: loss = 3.9619 (1.213 sec/step)\n","I0830 16:51:10.705566 139622356395904 learning.py:507] global step 2459: loss = 4.0015 (1.260 sec/step)\n","I0830 16:51:11.917718 139622356395904 learning.py:507] global step 2460: loss = 3.9852 (1.210 sec/step)\n","I0830 16:51:13.162164 139622356395904 learning.py:507] global step 2461: loss = 4.0584 (1.242 sec/step)\n","I0830 16:51:14.414112 139622356395904 learning.py:507] global step 2462: loss = 5.3493 (1.250 sec/step)\n","I0830 16:51:15.625963 139622356395904 learning.py:507] global step 2463: loss = 4.6420 (1.210 sec/step)\n","I0830 16:51:16.841226 139622356395904 learning.py:507] global step 2464: loss = 4.0021 (1.213 sec/step)\n","I0830 16:51:18.095356 139622356395904 learning.py:507] global step 2465: loss = 4.0057 (1.252 sec/step)\n","I0830 16:51:19.342576 139622356395904 learning.py:507] global step 2466: loss = 5.6525 (1.245 sec/step)\n","I0830 16:51:20.586096 139622356395904 learning.py:507] global step 2467: loss = 3.9068 (1.242 sec/step)\n","I0830 16:51:21.813720 139622356395904 learning.py:507] global step 2468: loss = 3.8995 (1.226 sec/step)\n","I0830 16:51:23.046575 139622356395904 learning.py:507] global step 2469: loss = 4.5301 (1.231 sec/step)\n","I0830 16:51:24.285457 139622356395904 learning.py:507] global step 2470: loss = 4.8321 (1.237 sec/step)\n","I0830 16:51:25.558515 139622356395904 learning.py:507] global step 2471: loss = 4.0514 (1.271 sec/step)\n","I0830 16:51:26.806138 139622356395904 learning.py:507] global step 2472: loss = 3.9570 (1.246 sec/step)\n","I0830 16:51:28.011384 139622356395904 learning.py:507] global step 2473: loss = 4.9481 (1.203 sec/step)\n","I0830 16:51:29.249401 139622356395904 learning.py:507] global step 2474: loss = 6.7924 (1.236 sec/step)\n","I0830 16:51:30.499614 139622356395904 learning.py:507] global step 2475: loss = 4.8238 (1.248 sec/step)\n","I0830 16:51:31.721949 139622356395904 learning.py:507] global step 2476: loss = 4.0069 (1.220 sec/step)\n","I0830 16:51:32.953158 139622356395904 learning.py:507] global step 2477: loss = 4.6617 (1.229 sec/step)\n","I0830 16:51:34.185507 139622356395904 learning.py:507] global step 2478: loss = 5.5443 (1.231 sec/step)\n","I0830 16:51:35.388080 139622356395904 learning.py:507] global step 2479: loss = 4.6063 (1.201 sec/step)\n","I0830 16:51:36.617284 139622356395904 learning.py:507] global step 2480: loss = 4.6186 (1.227 sec/step)\n","I0830 16:51:37.831756 139622356395904 learning.py:507] global step 2481: loss = 4.2121 (1.213 sec/step)\n","I0830 16:51:39.102629 139622356395904 learning.py:507] global step 2482: loss = 5.0611 (1.269 sec/step)\n","I0830 16:51:40.361474 139622356395904 learning.py:507] global step 2483: loss = 4.3654 (1.254 sec/step)\n","I0830 16:51:41.586798 139622356395904 learning.py:507] global step 2484: loss = 4.3704 (1.224 sec/step)\n","I0830 16:51:42.802912 139622356395904 learning.py:507] global step 2485: loss = 4.5927 (1.214 sec/step)\n","I0830 16:51:44.023877 139622356395904 learning.py:507] global step 2486: loss = 4.3555 (1.219 sec/step)\n","I0830 16:51:45.228989 139622356395904 learning.py:507] global step 2487: loss = 4.3570 (1.203 sec/step)\n","I0830 16:51:46.469300 139622356395904 learning.py:507] global step 2488: loss = 4.6793 (1.239 sec/step)\n","I0830 16:51:47.664533 139622356395904 learning.py:507] global step 2489: loss = 4.5554 (1.193 sec/step)\n","I0830 16:51:48.889169 139622356395904 learning.py:507] global step 2490: loss = 4.9115 (1.223 sec/step)\n","I0830 16:51:50.120984 139622356395904 learning.py:507] global step 2491: loss = 3.9693 (1.230 sec/step)\n","I0830 16:51:51.360716 139622356395904 learning.py:507] global step 2492: loss = 3.9472 (1.237 sec/step)\n","I0830 16:51:52.576888 139622356395904 learning.py:507] global step 2493: loss = 4.8228 (1.214 sec/step)\n","I0830 16:51:53.821417 139622356395904 learning.py:507] global step 2494: loss = 3.8897 (1.243 sec/step)\n","I0830 16:51:55.058809 139622356395904 learning.py:507] global step 2495: loss = 4.7308 (1.235 sec/step)\n","I0830 16:51:56.291881 139622356395904 learning.py:507] global step 2496: loss = 4.5767 (1.229 sec/step)\n","I0830 16:51:57.512084 139622356395904 learning.py:507] global step 2497: loss = 4.2143 (1.217 sec/step)\n","I0830 16:51:58.774448 139622356395904 learning.py:507] global step 2498: loss = 4.1121 (1.261 sec/step)\n","I0830 16:52:00.035558 139622356395904 learning.py:507] global step 2499: loss = 4.1930 (1.240 sec/step)\n","I0830 16:52:02.055101 139619299985152 supervisor.py:1050] Recording summary at step 2500.\n","I0830 16:52:02.089266 139622356395904 learning.py:507] global step 2500: loss = 4.6504 (2.052 sec/step)\n","I0830 16:52:03.313871 139622356395904 learning.py:507] global step 2501: loss = 4.3337 (1.223 sec/step)\n","I0830 16:52:04.548941 139622356395904 learning.py:507] global step 2502: loss = 4.2082 (1.233 sec/step)\n","I0830 16:52:05.748691 139622356395904 learning.py:507] global step 2503: loss = 4.6199 (1.197 sec/step)\n","I0830 16:52:06.960933 139622356395904 learning.py:507] global step 2504: loss = 4.0275 (1.210 sec/step)\n","I0830 16:52:08.168572 139622356395904 learning.py:507] global step 2505: loss = 4.4428 (1.206 sec/step)\n","I0830 16:52:09.424291 139622356395904 learning.py:507] global step 2506: loss = 4.2443 (1.253 sec/step)\n","I0830 16:52:10.667254 139622356395904 learning.py:507] global step 2507: loss = 3.9151 (1.241 sec/step)\n","I0830 16:52:11.888027 139622356395904 learning.py:507] global step 2508: loss = 3.8483 (1.219 sec/step)\n","I0830 16:52:13.128697 139622356395904 learning.py:507] global step 2509: loss = 4.7599 (1.239 sec/step)\n","I0830 16:52:14.377671 139622356395904 learning.py:507] global step 2510: loss = 4.5247 (1.247 sec/step)\n","I0830 16:52:15.606355 139622356395904 learning.py:507] global step 2511: loss = 5.3731 (1.227 sec/step)\n","I0830 16:52:16.836975 139622356395904 learning.py:507] global step 2512: loss = 4.4341 (1.229 sec/step)\n","I0830 16:52:18.119144 139622356395904 learning.py:507] global step 2513: loss = 4.1570 (1.280 sec/step)\n","I0830 16:52:19.335420 139622356395904 learning.py:507] global step 2514: loss = 4.3269 (1.214 sec/step)\n","I0830 16:52:20.589362 139622356395904 learning.py:507] global step 2515: loss = 4.5055 (1.252 sec/step)\n","I0830 16:52:21.816267 139622356395904 learning.py:507] global step 2516: loss = 5.1657 (1.225 sec/step)\n","I0830 16:52:23.057786 139622356395904 learning.py:507] global step 2517: loss = 4.7050 (1.239 sec/step)\n","I0830 16:52:24.251100 139622356395904 learning.py:507] global step 2518: loss = 4.3466 (1.191 sec/step)\n","I0830 16:52:25.501563 139622356395904 learning.py:507] global step 2519: loss = 4.5432 (1.249 sec/step)\n","I0830 16:52:26.749690 139622356395904 learning.py:507] global step 2520: loss = 5.0346 (1.246 sec/step)\n","I0830 16:52:27.995632 139622356395904 learning.py:507] global step 2521: loss = 4.9383 (1.244 sec/step)\n","I0830 16:52:29.198644 139622356395904 learning.py:507] global step 2522: loss = 4.3134 (1.201 sec/step)\n","I0830 16:52:30.468485 139622356395904 learning.py:507] global step 2523: loss = 5.6180 (1.268 sec/step)\n","I0830 16:52:31.715689 139622356395904 learning.py:507] global step 2524: loss = 5.8458 (1.245 sec/step)\n","I0830 16:52:32.960500 139622356395904 learning.py:507] global step 2525: loss = 5.0479 (1.243 sec/step)\n","I0830 16:52:34.223378 139622356395904 learning.py:507] global step 2526: loss = 4.4922 (1.261 sec/step)\n","I0830 16:52:35.432187 139622356395904 learning.py:507] global step 2527: loss = 4.5742 (1.207 sec/step)\n","I0830 16:52:36.625943 139622356395904 learning.py:507] global step 2528: loss = 4.0120 (1.192 sec/step)\n","I0830 16:52:37.797577 139622356395904 learning.py:507] global step 2529: loss = 4.8570 (1.170 sec/step)\n","I0830 16:52:39.042369 139622356395904 learning.py:507] global step 2530: loss = 4.2579 (1.243 sec/step)\n","I0830 16:52:40.277773 139622356395904 learning.py:507] global step 2531: loss = 4.8087 (1.233 sec/step)\n","I0830 16:52:41.484377 139622356395904 learning.py:507] global step 2532: loss = 4.3458 (1.205 sec/step)\n","I0830 16:52:42.717377 139622356395904 learning.py:507] global step 2533: loss = 4.7942 (1.231 sec/step)\n","I0830 16:52:43.933785 139622356395904 learning.py:507] global step 2534: loss = 4.6517 (1.214 sec/step)\n","I0830 16:52:45.130239 139622356395904 learning.py:507] global step 2535: loss = 5.2842 (1.194 sec/step)\n","I0830 16:52:46.347406 139622356395904 learning.py:507] global step 2536: loss = 4.1788 (1.215 sec/step)\n","I0830 16:52:47.591873 139622356395904 learning.py:507] global step 2537: loss = 4.0541 (1.243 sec/step)\n","I0830 16:52:48.825198 139622356395904 learning.py:507] global step 2538: loss = 4.9265 (1.231 sec/step)\n","I0830 16:52:50.034300 139622356395904 learning.py:507] global step 2539: loss = 3.8210 (1.207 sec/step)\n","I0830 16:52:51.273491 139622356395904 learning.py:507] global step 2540: loss = 3.7415 (1.238 sec/step)\n","I0830 16:52:52.517266 139622356395904 learning.py:507] global step 2541: loss = 4.2409 (1.242 sec/step)\n","I0830 16:52:53.773455 139622356395904 learning.py:507] global step 2542: loss = 4.5343 (1.254 sec/step)\n","I0830 16:52:55.022112 139622356395904 learning.py:507] global step 2543: loss = 4.1441 (1.247 sec/step)\n","I0830 16:52:56.235318 139622356395904 learning.py:507] global step 2544: loss = 3.5958 (1.211 sec/step)\n","I0830 16:52:57.464320 139622356395904 learning.py:507] global step 2545: loss = 4.8282 (1.227 sec/step)\n","I0830 16:52:58.691702 139622356395904 learning.py:507] global step 2546: loss = 3.8021 (1.225 sec/step)\n","I0830 16:52:59.912448 139622356395904 learning.py:507] global step 2547: loss = 4.5343 (1.219 sec/step)\n","I0830 16:53:01.149259 139622356395904 learning.py:507] global step 2548: loss = 5.8404 (1.233 sec/step)\n","I0830 16:53:02.400113 139622356395904 learning.py:507] global step 2549: loss = 4.5022 (1.248 sec/step)\n","I0830 16:53:03.633953 139622356395904 learning.py:507] global step 2550: loss = 4.4702 (1.231 sec/step)\n","I0830 16:53:04.840485 139622356395904 learning.py:507] global step 2551: loss = 4.3519 (1.204 sec/step)\n","I0830 16:53:06.058118 139622356395904 learning.py:507] global step 2552: loss = 5.1472 (1.216 sec/step)\n","I0830 16:53:07.243323 139622356395904 learning.py:507] global step 2553: loss = 4.4162 (1.183 sec/step)\n","I0830 16:53:08.460814 139622356395904 learning.py:507] global step 2554: loss = 4.5709 (1.216 sec/step)\n","I0830 16:53:09.675842 139622356395904 learning.py:507] global step 2555: loss = 5.1470 (1.213 sec/step)\n","I0830 16:53:10.921480 139622356395904 learning.py:507] global step 2556: loss = 3.6416 (1.244 sec/step)\n","I0830 16:53:12.137842 139622356395904 learning.py:507] global step 2557: loss = 4.1724 (1.214 sec/step)\n","I0830 16:53:13.402728 139622356395904 learning.py:507] global step 2558: loss = 4.2325 (1.263 sec/step)\n","I0830 16:53:14.653839 139622356395904 learning.py:507] global step 2559: loss = 3.9864 (1.249 sec/step)\n","I0830 16:53:15.902666 139622356395904 learning.py:507] global step 2560: loss = 4.2414 (1.247 sec/step)\n","I0830 16:53:17.118939 139622356395904 learning.py:507] global step 2561: loss = 4.3530 (1.214 sec/step)\n","I0830 16:53:18.356412 139622356395904 learning.py:507] global step 2562: loss = 3.8844 (1.235 sec/step)\n","I0830 16:53:19.584594 139622356395904 learning.py:507] global step 2563: loss = 3.7740 (1.226 sec/step)\n","I0830 16:53:20.829030 139622356395904 learning.py:507] global step 2564: loss = 5.2477 (1.242 sec/step)\n","I0830 16:53:22.072088 139622356395904 learning.py:507] global step 2565: loss = 4.3774 (1.241 sec/step)\n","I0830 16:53:23.323614 139622356395904 learning.py:507] global step 2566: loss = 4.3460 (1.250 sec/step)\n","I0830 16:53:24.583563 139622356395904 learning.py:507] global step 2567: loss = 4.6995 (1.258 sec/step)\n","I0830 16:53:25.804824 139622356395904 learning.py:507] global step 2568: loss = 4.8114 (1.219 sec/step)\n","I0830 16:53:27.007027 139622356395904 learning.py:507] global step 2569: loss = 4.0171 (1.201 sec/step)\n","I0830 16:53:28.253533 139622356395904 learning.py:507] global step 2570: loss = 3.8830 (1.245 sec/step)\n","I0830 16:53:29.522032 139622356395904 learning.py:507] global step 2571: loss = 4.1393 (1.267 sec/step)\n","I0830 16:53:30.780463 139622356395904 learning.py:507] global step 2572: loss = 3.7964 (1.257 sec/step)\n","I0830 16:53:32.019424 139622356395904 learning.py:507] global step 2573: loss = 4.6689 (1.236 sec/step)\n","I0830 16:53:33.281753 139622356395904 learning.py:507] global step 2574: loss = 4.3438 (1.260 sec/step)\n","I0830 16:53:34.489814 139622356395904 learning.py:507] global step 2575: loss = 4.3571 (1.206 sec/step)\n","I0830 16:53:35.747433 139622356395904 learning.py:507] global step 2576: loss = 4.8407 (1.255 sec/step)\n","I0830 16:53:36.991734 139622356395904 learning.py:507] global step 2577: loss = 4.2374 (1.242 sec/step)\n","I0830 16:53:38.218434 139622356395904 learning.py:507] global step 2578: loss = 3.6946 (1.225 sec/step)\n","I0830 16:53:39.429732 139622356395904 learning.py:507] global step 2579: loss = 3.9847 (1.209 sec/step)\n","I0830 16:53:40.688591 139622356395904 learning.py:507] global step 2580: loss = 5.7228 (1.257 sec/step)\n","I0830 16:53:41.911310 139622356395904 learning.py:507] global step 2581: loss = 4.9241 (1.221 sec/step)\n","I0830 16:53:43.147867 139622356395904 learning.py:507] global step 2582: loss = 4.2164 (1.235 sec/step)\n","I0830 16:53:44.394718 139622356395904 learning.py:507] global step 2583: loss = 3.7554 (1.245 sec/step)\n","I0830 16:53:45.630574 139622356395904 learning.py:507] global step 2584: loss = 4.7492 (1.233 sec/step)\n","I0830 16:53:46.884292 139622356395904 learning.py:507] global step 2585: loss = 5.4468 (1.252 sec/step)\n","I0830 16:53:48.117581 139622356395904 learning.py:507] global step 2586: loss = 4.2680 (1.231 sec/step)\n","I0830 16:53:49.336189 139622356395904 learning.py:507] global step 2587: loss = 4.0034 (1.217 sec/step)\n","I0830 16:53:50.579530 139622356395904 learning.py:507] global step 2588: loss = 5.5398 (1.241 sec/step)\n","I0830 16:53:51.810186 139622356395904 learning.py:507] global step 2589: loss = 5.7037 (1.229 sec/step)\n","I0830 16:53:53.058030 139622356395904 learning.py:507] global step 2590: loss = 4.6628 (1.246 sec/step)\n","I0830 16:53:54.283463 139622356395904 learning.py:507] global step 2591: loss = 4.0086 (1.223 sec/step)\n","I0830 16:53:55.509321 139622356395904 learning.py:507] global step 2592: loss = 3.8015 (1.223 sec/step)\n","I0830 16:53:56.737312 139622356395904 learning.py:507] global step 2593: loss = 4.0827 (1.226 sec/step)\n","I0830 16:53:57.983565 139622356395904 learning.py:507] global step 2594: loss = 3.9162 (1.244 sec/step)\n","I0830 16:53:59.219209 139622356395904 learning.py:507] global step 2595: loss = 3.8768 (1.234 sec/step)\n","I0830 16:54:00.655009 139622356395904 learning.py:507] global step 2596: loss = 3.7561 (1.386 sec/step)\n","I0830 16:54:02.473234 139619299985152 supervisor.py:1050] Recording summary at step 2597.\n","I0830 16:54:02.509210 139622356395904 learning.py:507] global step 2597: loss = 4.9833 (1.800 sec/step)\n","I0830 16:54:03.745575 139622356395904 learning.py:507] global step 2598: loss = 3.9013 (1.234 sec/step)\n","I0830 16:54:04.981244 139622356395904 learning.py:507] global step 2599: loss = 4.1476 (1.234 sec/step)\n","I0830 16:54:06.202419 139622356395904 learning.py:507] global step 2600: loss = 3.9237 (1.219 sec/step)\n","I0830 16:54:07.406635 139622356395904 learning.py:507] global step 2601: loss = 4.5906 (1.202 sec/step)\n","I0830 16:54:08.646803 139622356395904 learning.py:507] global step 2602: loss = 6.0431 (1.238 sec/step)\n","I0830 16:54:09.873868 139622356395904 learning.py:507] global step 2603: loss = 4.7129 (1.225 sec/step)\n","I0830 16:54:11.090232 139622356395904 learning.py:507] global step 2604: loss = 3.9006 (1.215 sec/step)\n","I0830 16:54:12.349080 139622356395904 learning.py:507] global step 2605: loss = 3.8760 (1.257 sec/step)\n","I0830 16:54:13.560676 139622356395904 learning.py:507] global step 2606: loss = 4.7692 (1.210 sec/step)\n","I0830 16:54:14.792304 139622356395904 learning.py:507] global step 2607: loss = 4.0771 (1.230 sec/step)\n","I0830 16:54:16.031601 139622356395904 learning.py:507] global step 2608: loss = 5.0988 (1.237 sec/step)\n","I0830 16:54:17.236852 139622356395904 learning.py:507] global step 2609: loss = 3.6250 (1.203 sec/step)\n","I0830 16:54:18.465574 139622356395904 learning.py:507] global step 2610: loss = 4.2597 (1.227 sec/step)\n","I0830 16:54:19.711167 139622356395904 learning.py:507] global step 2611: loss = 5.4700 (1.244 sec/step)\n","I0830 16:54:20.954788 139622356395904 learning.py:507] global step 2612: loss = 5.0441 (1.242 sec/step)\n","I0830 16:54:22.167865 139622356395904 learning.py:507] global step 2613: loss = 4.2202 (1.211 sec/step)\n","I0830 16:54:23.372793 139622356395904 learning.py:507] global step 2614: loss = 4.0143 (1.203 sec/step)\n","I0830 16:54:24.598721 139622356395904 learning.py:507] global step 2615: loss = 3.9541 (1.224 sec/step)\n","I0830 16:54:25.795972 139622356395904 learning.py:507] global step 2616: loss = 4.6391 (1.195 sec/step)\n","I0830 16:54:27.009879 139622356395904 learning.py:507] global step 2617: loss = 4.0756 (1.212 sec/step)\n","I0830 16:54:28.226201 139622356395904 learning.py:507] global step 2618: loss = 4.4103 (1.214 sec/step)\n","I0830 16:54:29.453672 139622356395904 learning.py:507] global step 2619: loss = 5.0183 (1.225 sec/step)\n","I0830 16:54:30.708812 139622356395904 learning.py:507] global step 2620: loss = 3.9162 (1.253 sec/step)\n","I0830 16:54:31.957893 139622356395904 learning.py:507] global step 2621: loss = 4.4111 (1.247 sec/step)\n","I0830 16:54:33.210581 139622356395904 learning.py:507] global step 2622: loss = 3.7827 (1.251 sec/step)\n","I0830 16:54:34.472155 139622356395904 learning.py:507] global step 2623: loss = 5.4084 (1.260 sec/step)\n","I0830 16:54:35.712141 139622356395904 learning.py:507] global step 2624: loss = 5.5812 (1.238 sec/step)\n","I0830 16:54:36.913213 139622356395904 learning.py:507] global step 2625: loss = 4.7750 (1.199 sec/step)\n","I0830 16:54:38.161076 139622356395904 learning.py:507] global step 2626: loss = 3.8808 (1.246 sec/step)\n","I0830 16:54:39.402439 139622356395904 learning.py:507] global step 2627: loss = 4.6540 (1.240 sec/step)\n","I0830 16:54:40.622161 139622356395904 learning.py:507] global step 2628: loss = 4.2650 (1.218 sec/step)\n","I0830 16:54:41.849980 139622356395904 learning.py:507] global step 2629: loss = 4.2855 (1.226 sec/step)\n","I0830 16:54:43.096577 139622356395904 learning.py:507] global step 2630: loss = 5.1248 (1.245 sec/step)\n","I0830 16:54:44.325311 139622356395904 learning.py:507] global step 2631: loss = 4.4224 (1.227 sec/step)\n","I0830 16:54:45.537096 139622356395904 learning.py:507] global step 2632: loss = 4.3192 (1.210 sec/step)\n","I0830 16:54:46.745590 139622356395904 learning.py:507] global step 2633: loss = 4.1682 (1.207 sec/step)\n","I0830 16:54:47.955840 139622356395904 learning.py:507] global step 2634: loss = 3.7637 (1.209 sec/step)\n","I0830 16:54:49.193436 139622356395904 learning.py:507] global step 2635: loss = 4.4083 (1.232 sec/step)\n","I0830 16:54:50.422176 139622356395904 learning.py:507] global step 2636: loss = 4.8498 (1.227 sec/step)\n","I0830 16:54:51.612436 139622356395904 learning.py:507] global step 2637: loss = 4.0412 (1.188 sec/step)\n","I0830 16:54:52.816401 139622356395904 learning.py:507] global step 2638: loss = 4.5355 (1.202 sec/step)\n","I0830 16:54:54.033371 139622356395904 learning.py:507] global step 2639: loss = 4.7888 (1.215 sec/step)\n","I0830 16:54:55.231772 139622356395904 learning.py:507] global step 2640: loss = 4.6862 (1.196 sec/step)\n","I0830 16:54:56.465153 139622356395904 learning.py:507] global step 2641: loss = 3.7299 (1.231 sec/step)\n","I0830 16:54:57.693852 139622356395904 learning.py:507] global step 2642: loss = 4.2466 (1.227 sec/step)\n","I0830 16:54:58.937878 139622356395904 learning.py:507] global step 2643: loss = 4.1513 (1.242 sec/step)\n","I0830 16:55:00.185429 139622356395904 learning.py:507] global step 2644: loss = 4.2626 (1.245 sec/step)\n","I0830 16:55:01.416035 139622356395904 learning.py:507] global step 2645: loss = 4.1023 (1.229 sec/step)\n","I0830 16:55:02.651538 139622356395904 learning.py:507] global step 2646: loss = 4.3557 (1.233 sec/step)\n","I0830 16:55:03.869540 139622356395904 learning.py:507] global step 2647: loss = 4.8870 (1.216 sec/step)\n","I0830 16:55:05.117037 139622356395904 learning.py:507] global step 2648: loss = 3.5687 (1.246 sec/step)\n","I0830 16:55:06.314129 139622356395904 learning.py:507] global step 2649: loss = 4.3655 (1.195 sec/step)\n","I0830 16:55:07.524581 139622356395904 learning.py:507] global step 2650: loss = 3.6900 (1.209 sec/step)\n","I0830 16:55:08.771005 139622356395904 learning.py:507] global step 2651: loss = 4.9058 (1.244 sec/step)\n","I0830 16:55:10.023112 139622356395904 learning.py:507] global step 2652: loss = 4.3503 (1.250 sec/step)\n","I0830 16:55:11.259978 139622356395904 learning.py:507] global step 2653: loss = 4.1783 (1.235 sec/step)\n","I0830 16:55:12.493923 139622356395904 learning.py:507] global step 2654: loss = 4.0977 (1.232 sec/step)\n","I0830 16:55:13.734003 139622356395904 learning.py:507] global step 2655: loss = 4.1753 (1.238 sec/step)\n","I0830 16:55:14.962041 139622356395904 learning.py:507] global step 2656: loss = 3.6460 (1.226 sec/step)\n","I0830 16:55:16.178348 139622356395904 learning.py:507] global step 2657: loss = 5.4549 (1.214 sec/step)\n","I0830 16:55:17.434221 139622356395904 learning.py:507] global step 2658: loss = 5.1965 (1.254 sec/step)\n","I0830 16:55:18.676450 139622356395904 learning.py:507] global step 2659: loss = 3.6805 (1.240 sec/step)\n","I0830 16:55:19.905907 139622356395904 learning.py:507] global step 2660: loss = 4.6264 (1.227 sec/step)\n","I0830 16:55:21.173454 139622356395904 learning.py:507] global step 2661: loss = 4.0236 (1.266 sec/step)\n","I0830 16:55:22.383219 139622356395904 learning.py:507] global step 2662: loss = 4.0736 (1.208 sec/step)\n","I0830 16:55:23.614212 139622356395904 learning.py:507] global step 2663: loss = 4.0655 (1.229 sec/step)\n","I0830 16:55:24.865182 139622356395904 learning.py:507] global step 2664: loss = 4.4378 (1.249 sec/step)\n","I0830 16:55:26.117745 139622356395904 learning.py:507] global step 2665: loss = 4.3550 (1.250 sec/step)\n","I0830 16:55:27.308661 139622356395904 learning.py:507] global step 2666: loss = 5.4631 (1.189 sec/step)\n","I0830 16:55:28.539585 139622356395904 learning.py:507] global step 2667: loss = 4.6137 (1.226 sec/step)\n","I0830 16:55:29.734135 139622356395904 learning.py:507] global step 2668: loss = 4.2825 (1.193 sec/step)\n","I0830 16:55:30.982431 139622356395904 learning.py:507] global step 2669: loss = 4.0210 (1.246 sec/step)\n","I0830 16:55:32.225273 139622356395904 learning.py:507] global step 2670: loss = 4.1927 (1.241 sec/step)\n","I0830 16:55:33.445520 139622356395904 learning.py:507] global step 2671: loss = 3.8388 (1.217 sec/step)\n","I0830 16:55:34.656728 139622356395904 learning.py:507] global step 2672: loss = 4.3908 (1.208 sec/step)\n","I0830 16:55:35.864823 139622356395904 learning.py:507] global step 2673: loss = 4.5300 (1.206 sec/step)\n","I0830 16:55:37.065236 139622356395904 learning.py:507] global step 2674: loss = 3.7891 (1.198 sec/step)\n","I0830 16:55:38.286495 139622356395904 learning.py:507] global step 2675: loss = 5.2453 (1.219 sec/step)\n","I0830 16:55:39.489073 139622356395904 learning.py:507] global step 2676: loss = 4.3857 (1.201 sec/step)\n","I0830 16:55:40.761513 139622356395904 learning.py:507] global step 2677: loss = 4.6602 (1.270 sec/step)\n","I0830 16:55:42.006700 139622356395904 learning.py:507] global step 2678: loss = 3.8986 (1.243 sec/step)\n","I0830 16:55:43.241254 139622356395904 learning.py:507] global step 2679: loss = 4.0652 (1.233 sec/step)\n","I0830 16:55:44.451842 139622356395904 learning.py:507] global step 2680: loss = 3.9066 (1.209 sec/step)\n","I0830 16:55:45.699676 139622356395904 learning.py:507] global step 2681: loss = 4.0467 (1.246 sec/step)\n","I0830 16:55:46.933676 139622356395904 learning.py:507] global step 2682: loss = 4.6468 (1.232 sec/step)\n","I0830 16:55:48.133970 139622356395904 learning.py:507] global step 2683: loss = 4.2635 (1.199 sec/step)\n","I0830 16:55:49.370157 139622356395904 learning.py:507] global step 2684: loss = 4.4109 (1.234 sec/step)\n","I0830 16:55:50.619129 139622356395904 learning.py:507] global step 2685: loss = 4.1745 (1.247 sec/step)\n","I0830 16:55:51.879085 139622356395904 learning.py:507] global step 2686: loss = 4.3195 (1.258 sec/step)\n","I0830 16:55:53.125316 139622356395904 learning.py:507] global step 2687: loss = 4.4422 (1.244 sec/step)\n","I0830 16:55:54.344015 139622356395904 learning.py:507] global step 2688: loss = 4.4965 (1.217 sec/step)\n","I0830 16:55:55.574442 139622356395904 learning.py:507] global step 2689: loss = 4.4978 (1.229 sec/step)\n","I0830 16:55:56.830198 139622356395904 learning.py:507] global step 2690: loss = 5.0553 (1.254 sec/step)\n","I0830 16:55:58.053439 139622356395904 learning.py:507] global step 2691: loss = 5.8837 (1.221 sec/step)\n","I0830 16:55:59.297547 139622356395904 learning.py:507] global step 2692: loss = 3.8781 (1.242 sec/step)\n","I0830 16:56:00.925517 139622356395904 learning.py:507] global step 2693: loss = 4.8525 (1.624 sec/step)\n","I0830 16:56:02.838777 139619299985152 supervisor.py:1050] Recording summary at step 2694.\n","I0830 16:56:02.863397 139622356395904 learning.py:507] global step 2694: loss = 3.7623 (1.925 sec/step)\n","I0830 16:56:04.072483 139622356395904 learning.py:507] global step 2695: loss = 3.9504 (1.207 sec/step)\n","I0830 16:56:05.286305 139622356395904 learning.py:507] global step 2696: loss = 5.5977 (1.212 sec/step)\n","I0830 16:56:06.494445 139622356395904 learning.py:507] global step 2697: loss = 4.6015 (1.206 sec/step)\n","I0830 16:56:07.717607 139622356395904 learning.py:507] global step 2698: loss = 3.9307 (1.221 sec/step)\n","I0830 16:56:08.972168 139622356395904 learning.py:507] global step 2699: loss = 3.8313 (1.253 sec/step)\n","I0830 16:56:10.213067 139622356395904 learning.py:507] global step 2700: loss = 3.9277 (1.239 sec/step)\n","I0830 16:56:11.462157 139622356395904 learning.py:507] global step 2701: loss = 4.0302 (1.247 sec/step)\n","I0830 16:56:12.706323 139622356395904 learning.py:507] global step 2702: loss = 3.6089 (1.242 sec/step)\n","I0830 16:56:13.938461 139622356395904 learning.py:507] global step 2703: loss = 4.0783 (1.230 sec/step)\n","I0830 16:56:15.212997 139622356395904 learning.py:507] global step 2704: loss = 3.8766 (1.272 sec/step)\n","I0830 16:56:16.469432 139622356395904 learning.py:507] global step 2705: loss = 4.2123 (1.255 sec/step)\n","I0830 16:56:17.723578 139622356395904 learning.py:507] global step 2706: loss = 4.9966 (1.252 sec/step)\n","I0830 16:56:18.975079 139622356395904 learning.py:507] global step 2707: loss = 4.1423 (1.248 sec/step)\n","I0830 16:56:20.200132 139622356395904 learning.py:507] global step 2708: loss = 4.5490 (1.223 sec/step)\n","I0830 16:56:21.438194 139622356395904 learning.py:507] global step 2709: loss = 5.0331 (1.236 sec/step)\n","I0830 16:56:22.690132 139622356395904 learning.py:507] global step 2710: loss = 4.6927 (1.250 sec/step)\n","I0830 16:56:23.892469 139622356395904 learning.py:507] global step 2711: loss = 4.4010 (1.200 sec/step)\n","I0830 16:56:25.136438 139622356395904 learning.py:507] global step 2712: loss = 5.1936 (1.242 sec/step)\n","I0830 16:56:26.382987 139622356395904 learning.py:507] global step 2713: loss = 4.3447 (1.245 sec/step)\n","I0830 16:56:27.606723 139622356395904 learning.py:507] global step 2714: loss = 4.1683 (1.221 sec/step)\n","I0830 16:56:28.844222 139622356395904 learning.py:507] global step 2715: loss = 4.6254 (1.235 sec/step)\n","I0830 16:56:30.040546 139622356395904 learning.py:507] global step 2716: loss = 4.4895 (1.194 sec/step)\n","I0830 16:56:31.286739 139622356395904 learning.py:507] global step 2717: loss = 3.8745 (1.244 sec/step)\n","I0830 16:56:32.491713 139622356395904 learning.py:507] global step 2718: loss = 5.7211 (1.203 sec/step)\n","I0830 16:56:33.730221 139622356395904 learning.py:507] global step 2719: loss = 3.7663 (1.237 sec/step)\n","I0830 16:56:34.946160 139622356395904 learning.py:507] global step 2720: loss = 5.1207 (1.214 sec/step)\n","I0830 16:56:36.163214 139622356395904 learning.py:507] global step 2721: loss = 4.2986 (1.215 sec/step)\n","I0830 16:56:37.384535 139622356395904 learning.py:507] global step 2722: loss = 4.2612 (1.219 sec/step)\n","I0830 16:56:38.606314 139622356395904 learning.py:507] global step 2723: loss = 3.9575 (1.220 sec/step)\n","I0830 16:56:39.825478 139622356395904 learning.py:507] global step 2724: loss = 4.2225 (1.217 sec/step)\n","I0830 16:56:41.041332 139622356395904 learning.py:507] global step 2725: loss = 3.6284 (1.212 sec/step)\n","I0830 16:56:42.285680 139622356395904 learning.py:507] global step 2726: loss = 4.4209 (1.242 sec/step)\n","I0830 16:56:43.471749 139622356395904 learning.py:507] global step 2727: loss = 4.6764 (1.182 sec/step)\n","I0830 16:56:44.671440 139622356395904 learning.py:507] global step 2728: loss = 3.6581 (1.197 sec/step)\n","I0830 16:56:45.892025 139622356395904 learning.py:507] global step 2729: loss = 4.3102 (1.219 sec/step)\n","I0830 16:56:47.139434 139622356395904 learning.py:507] global step 2730: loss = 4.2910 (1.246 sec/step)\n","I0830 16:56:48.389763 139622356395904 learning.py:507] global step 2731: loss = 4.6702 (1.248 sec/step)\n","I0830 16:56:49.630552 139622356395904 learning.py:507] global step 2732: loss = 4.4909 (1.238 sec/step)\n","I0830 16:56:50.877670 139622356395904 learning.py:507] global step 2733: loss = 4.0107 (1.245 sec/step)\n","I0830 16:56:52.111816 139622356395904 learning.py:507] global step 2734: loss = 4.2498 (1.232 sec/step)\n","I0830 16:56:53.354079 139622356395904 learning.py:507] global step 2735: loss = 4.4977 (1.241 sec/step)\n","I0830 16:56:54.571652 139622356395904 learning.py:507] global step 2736: loss = 4.7126 (1.216 sec/step)\n","I0830 16:56:55.794676 139622356395904 learning.py:507] global step 2737: loss = 3.8632 (1.221 sec/step)\n","I0830 16:56:57.046953 139622356395904 learning.py:507] global step 2738: loss = 3.9929 (1.250 sec/step)\n","I0830 16:56:58.269142 139622356395904 learning.py:507] global step 2739: loss = 3.7387 (1.221 sec/step)\n","I0830 16:56:59.481205 139622356395904 learning.py:507] global step 2740: loss = 4.4985 (1.210 sec/step)\n","I0830 16:57:00.695146 139622356395904 learning.py:507] global step 2741: loss = 4.7443 (1.212 sec/step)\n","I0830 16:57:01.922021 139622356395904 learning.py:507] global step 2742: loss = 4.2498 (1.225 sec/step)\n","I0830 16:57:03.160501 139622356395904 learning.py:507] global step 2743: loss = 3.9102 (1.237 sec/step)\n","I0830 16:57:04.413243 139622356395904 learning.py:507] global step 2744: loss = 4.7895 (1.251 sec/step)\n","I0830 16:57:05.618076 139622356395904 learning.py:507] global step 2745: loss = 5.2420 (1.203 sec/step)\n","I0830 16:57:06.866210 139622356395904 learning.py:507] global step 2746: loss = 4.3835 (1.246 sec/step)\n","I0830 16:57:08.143773 139622356395904 learning.py:507] global step 2747: loss = 4.2492 (1.276 sec/step)\n","I0830 16:57:09.396079 139622356395904 learning.py:507] global step 2748: loss = 4.3001 (1.251 sec/step)\n","I0830 16:57:10.609586 139622356395904 learning.py:507] global step 2749: loss = 4.1799 (1.211 sec/step)\n","I0830 16:57:11.868371 139622356395904 learning.py:507] global step 2750: loss = 4.2459 (1.257 sec/step)\n","I0830 16:57:13.119406 139622356395904 learning.py:507] global step 2751: loss = 4.2279 (1.249 sec/step)\n","I0830 16:57:14.356967 139622356395904 learning.py:507] global step 2752: loss = 4.5092 (1.235 sec/step)\n","I0830 16:57:15.578127 139622356395904 learning.py:507] global step 2753: loss = 5.6479 (1.212 sec/step)\n","I0830 16:57:16.793776 139622356395904 learning.py:507] global step 2754: loss = 3.6035 (1.214 sec/step)\n","I0830 16:57:18.020666 139622356395904 learning.py:507] global step 2755: loss = 4.0911 (1.225 sec/step)\n","I0830 16:57:19.249616 139622356395904 learning.py:507] global step 2756: loss = 4.5209 (1.227 sec/step)\n","I0830 16:57:20.440997 139622356395904 learning.py:507] global step 2757: loss = 3.6525 (1.189 sec/step)\n","I0830 16:57:21.679243 139622356395904 learning.py:507] global step 2758: loss = 3.9984 (1.236 sec/step)\n","I0830 16:57:22.893082 139622356395904 learning.py:507] global step 2759: loss = 5.0866 (1.212 sec/step)\n","I0830 16:57:24.100227 139622356395904 learning.py:507] global step 2760: loss = 4.2793 (1.205 sec/step)\n","I0830 16:57:25.357982 139622356395904 learning.py:507] global step 2761: loss = 5.5336 (1.256 sec/step)\n","I0830 16:57:26.578411 139622356395904 learning.py:507] global step 2762: loss = 5.2739 (1.218 sec/step)\n","I0830 16:57:27.826210 139622356395904 learning.py:507] global step 2763: loss = 4.2101 (1.246 sec/step)\n","I0830 16:57:29.031776 139622356395904 learning.py:507] global step 2764: loss = 4.1288 (1.204 sec/step)\n","I0830 16:57:30.264925 139622356395904 learning.py:507] global step 2765: loss = 4.2015 (1.231 sec/step)\n","I0830 16:57:31.487035 139622356395904 learning.py:507] global step 2766: loss = 4.3089 (1.220 sec/step)\n","I0830 16:57:32.711987 139622356395904 learning.py:507] global step 2767: loss = 4.1046 (1.223 sec/step)\n","I0830 16:57:33.906739 139622356395904 learning.py:507] global step 2768: loss = 4.1188 (1.193 sec/step)\n","I0830 16:57:35.169524 139622356395904 learning.py:507] global step 2769: loss = 4.8101 (1.256 sec/step)\n","I0830 16:57:36.401300 139622356395904 learning.py:507] global step 2770: loss = 5.2582 (1.230 sec/step)\n","I0830 16:57:37.625943 139622356395904 learning.py:507] global step 2771: loss = 4.1523 (1.223 sec/step)\n","I0830 16:57:38.846844 139622356395904 learning.py:507] global step 2772: loss = 5.0270 (1.219 sec/step)\n","I0830 16:57:40.057829 139622356395904 learning.py:507] global step 2773: loss = 4.4790 (1.208 sec/step)\n","I0830 16:57:41.293652 139622356395904 learning.py:507] global step 2774: loss = 3.7890 (1.234 sec/step)\n","I0830 16:57:42.530309 139622356395904 learning.py:507] global step 2775: loss = 4.7862 (1.235 sec/step)\n","I0830 16:57:43.726623 139622356395904 learning.py:507] global step 2776: loss = 5.6651 (1.194 sec/step)\n","I0830 16:57:44.962167 139622356395904 learning.py:507] global step 2777: loss = 5.9026 (1.234 sec/step)\n","I0830 16:57:46.186715 139622356395904 learning.py:507] global step 2778: loss = 4.1387 (1.222 sec/step)\n","I0830 16:57:47.418038 139622356395904 learning.py:507] global step 2779: loss = 3.5752 (1.230 sec/step)\n","I0830 16:57:48.621532 139622356395904 learning.py:507] global step 2780: loss = 3.9459 (1.202 sec/step)\n","I0830 16:57:49.822765 139622356395904 learning.py:507] global step 2781: loss = 4.9918 (1.199 sec/step)\n","I0830 16:57:51.086911 139622356395904 learning.py:507] global step 2782: loss = 3.9982 (1.262 sec/step)\n","I0830 16:57:52.311842 139622356395904 learning.py:507] global step 2783: loss = 3.8588 (1.223 sec/step)\n","I0830 16:57:53.525482 139622356395904 learning.py:507] global step 2784: loss = 4.7224 (1.212 sec/step)\n","I0830 16:57:54.762804 139622356395904 learning.py:507] global step 2785: loss = 3.8200 (1.235 sec/step)\n","I0830 16:57:55.983453 139622356395904 learning.py:507] global step 2786: loss = 4.5494 (1.219 sec/step)\n","I0830 16:57:57.199314 139622356395904 learning.py:507] global step 2787: loss = 3.8553 (1.214 sec/step)\n","I0830 16:57:58.453033 139622356395904 learning.py:507] global step 2788: loss = 4.3678 (1.252 sec/step)\n","I0830 16:57:59.765595 139622356395904 learning.py:507] global step 2789: loss = 3.8097 (1.273 sec/step)\n","I0830 16:58:01.519445 139619299985152 supervisor.py:1050] Recording summary at step 2789.\n","I0830 16:58:01.994379 139622356395904 learning.py:507] global step 2790: loss = 4.2450 (2.227 sec/step)\n","I0830 16:58:03.234442 139622356395904 learning.py:507] global step 2791: loss = 4.5451 (1.238 sec/step)\n","I0830 16:58:04.448571 139622356395904 learning.py:507] global step 2792: loss = 4.4804 (1.212 sec/step)\n","I0830 16:58:05.678915 139622356395904 learning.py:507] global step 2793: loss = 4.2148 (1.228 sec/step)\n","I0830 16:58:06.904700 139622356395904 learning.py:507] global step 2794: loss = 4.0444 (1.224 sec/step)\n","I0830 16:58:08.136340 139622356395904 learning.py:507] global step 2795: loss = 3.9650 (1.230 sec/step)\n","I0830 16:58:09.401500 139622356395904 learning.py:507] global step 2796: loss = 3.6353 (1.263 sec/step)\n","I0830 16:58:10.642254 139622356395904 learning.py:507] global step 2797: loss = 4.3892 (1.239 sec/step)\n","I0830 16:58:11.840801 139622356395904 learning.py:507] global step 2798: loss = 4.6718 (1.197 sec/step)\n","I0830 16:58:13.042317 139622356395904 learning.py:507] global step 2799: loss = 4.4602 (1.199 sec/step)\n","I0830 16:58:14.275623 139622356395904 learning.py:507] global step 2800: loss = 4.5688 (1.231 sec/step)\n","I0830 16:58:15.491275 139622356395904 learning.py:507] global step 2801: loss = 3.6280 (1.214 sec/step)\n","I0830 16:58:16.695433 139622356395904 learning.py:507] global step 2802: loss = 4.3198 (1.202 sec/step)\n","I0830 16:58:17.940941 139622356395904 learning.py:507] global step 2803: loss = 3.2763 (1.244 sec/step)\n","I0830 16:58:19.176994 139622356395904 learning.py:507] global step 2804: loss = 4.1899 (1.234 sec/step)\n","I0830 16:58:20.427273 139622356395904 learning.py:507] global step 2805: loss = 4.3541 (1.248 sec/step)\n","I0830 16:58:21.610088 139622356395904 learning.py:507] global step 2806: loss = 4.4771 (1.181 sec/step)\n","I0830 16:58:22.872117 139622356395904 learning.py:507] global step 2807: loss = 4.2766 (1.260 sec/step)\n","I0830 16:58:24.106098 139622356395904 learning.py:507] global step 2808: loss = 4.0700 (1.232 sec/step)\n","I0830 16:58:25.341616 139622356395904 learning.py:507] global step 2809: loss = 4.0527 (1.234 sec/step)\n","I0830 16:58:26.568461 139622356395904 learning.py:507] global step 2810: loss = 3.4821 (1.225 sec/step)\n","I0830 16:58:27.794145 139622356395904 learning.py:507] global step 2811: loss = 4.1068 (1.224 sec/step)\n","I0830 16:58:29.035174 139622356395904 learning.py:507] global step 2812: loss = 4.6361 (1.239 sec/step)\n","I0830 16:58:30.267555 139622356395904 learning.py:507] global step 2813: loss = 4.4318 (1.230 sec/step)\n","I0830 16:58:31.534192 139622356395904 learning.py:507] global step 2814: loss = 3.9903 (1.265 sec/step)\n","I0830 16:58:32.742690 139622356395904 learning.py:507] global step 2815: loss = 4.2124 (1.207 sec/step)\n","I0830 16:58:33.974905 139622356395904 learning.py:507] global step 2816: loss = 4.1345 (1.230 sec/step)\n","I0830 16:58:35.204141 139622356395904 learning.py:507] global step 2817: loss = 4.4143 (1.227 sec/step)\n","I0830 16:58:36.422295 139622356395904 learning.py:507] global step 2818: loss = 4.3360 (1.216 sec/step)\n","I0830 16:58:37.649362 139622356395904 learning.py:507] global step 2819: loss = 3.6290 (1.225 sec/step)\n","I0830 16:58:38.841139 139622356395904 learning.py:507] global step 2820: loss = 4.2632 (1.190 sec/step)\n","I0830 16:58:40.076208 139622356395904 learning.py:507] global step 2821: loss = 3.8418 (1.233 sec/step)\n","I0830 16:58:41.329034 139622356395904 learning.py:507] global step 2822: loss = 3.9391 (1.250 sec/step)\n","I0830 16:58:42.579485 139622356395904 learning.py:507] global step 2823: loss = 4.2860 (1.248 sec/step)\n","I0830 16:58:43.812555 139622356395904 learning.py:507] global step 2824: loss = 4.5168 (1.228 sec/step)\n","I0830 16:58:45.053575 139622356395904 learning.py:507] global step 2825: loss = 4.6385 (1.239 sec/step)\n","I0830 16:58:46.281138 139622356395904 learning.py:507] global step 2826: loss = 3.9124 (1.226 sec/step)\n","I0830 16:58:47.494860 139622356395904 learning.py:507] global step 2827: loss = 3.9403 (1.212 sec/step)\n","I0830 16:58:48.692297 139622356395904 learning.py:507] global step 2828: loss = 4.9960 (1.196 sec/step)\n","I0830 16:58:49.893992 139622356395904 learning.py:507] global step 2829: loss = 4.0952 (1.200 sec/step)\n","I0830 16:58:51.148949 139622356395904 learning.py:507] global step 2830: loss = 4.3654 (1.253 sec/step)\n","I0830 16:58:52.408035 139622356395904 learning.py:507] global step 2831: loss = 4.1931 (1.257 sec/step)\n","I0830 16:58:53.673837 139622356395904 learning.py:507] global step 2832: loss = 4.1904 (1.264 sec/step)\n","I0830 16:58:54.931825 139622356395904 learning.py:507] global step 2833: loss = 4.1970 (1.256 sec/step)\n","I0830 16:58:56.191278 139622356395904 learning.py:507] global step 2834: loss = 4.1565 (1.257 sec/step)\n","I0830 16:58:57.411766 139622356395904 learning.py:507] global step 2835: loss = 4.0584 (1.219 sec/step)\n","I0830 16:58:58.629027 139622356395904 learning.py:507] global step 2836: loss = 3.6769 (1.216 sec/step)\n","I0830 16:58:59.873764 139622356395904 learning.py:507] global step 2837: loss = 4.0416 (1.243 sec/step)\n","I0830 16:59:01.113559 139622356395904 learning.py:507] global step 2838: loss = 4.5789 (1.238 sec/step)\n","I0830 16:59:02.343488 139622356395904 learning.py:507] global step 2839: loss = 4.8329 (1.228 sec/step)\n","I0830 16:59:03.558880 139622356395904 learning.py:507] global step 2840: loss = 3.5547 (1.213 sec/step)\n","I0830 16:59:04.801154 139622356395904 learning.py:507] global step 2841: loss = 4.3209 (1.240 sec/step)\n","I0830 16:59:05.999892 139622356395904 learning.py:507] global step 2842: loss = 5.1995 (1.197 sec/step)\n","I0830 16:59:07.218149 139622356395904 learning.py:507] global step 2843: loss = 3.7211 (1.216 sec/step)\n","I0830 16:59:08.421661 139622356395904 learning.py:507] global step 2844: loss = 3.5966 (1.202 sec/step)\n","I0830 16:59:09.622707 139622356395904 learning.py:507] global step 2845: loss = 4.4504 (1.195 sec/step)\n","I0830 16:59:10.864529 139622356395904 learning.py:507] global step 2846: loss = 4.4374 (1.239 sec/step)\n","I0830 16:59:12.079433 139622356395904 learning.py:507] global step 2847: loss = 3.8834 (1.213 sec/step)\n","I0830 16:59:13.316131 139622356395904 learning.py:507] global step 2848: loss = 4.0693 (1.235 sec/step)\n","I0830 16:59:14.555275 139622356395904 learning.py:507] global step 2849: loss = 4.5341 (1.237 sec/step)\n","I0830 16:59:15.756124 139622356395904 learning.py:507] global step 2850: loss = 3.4799 (1.199 sec/step)\n","I0830 16:59:16.995157 139622356395904 learning.py:507] global step 2851: loss = 3.6326 (1.237 sec/step)\n","I0830 16:59:18.225388 139622356395904 learning.py:507] global step 2852: loss = 4.0612 (1.228 sec/step)\n","I0830 16:59:19.439605 139622356395904 learning.py:507] global step 2853: loss = 4.2057 (1.212 sec/step)\n","I0830 16:59:20.685807 139622356395904 learning.py:507] global step 2854: loss = 4.4001 (1.244 sec/step)\n","I0830 16:59:21.880357 139622356395904 learning.py:507] global step 2855: loss = 4.5837 (1.193 sec/step)\n","I0830 16:59:23.098367 139622356395904 learning.py:507] global step 2856: loss = 4.2686 (1.216 sec/step)\n","I0830 16:59:24.282149 139622356395904 learning.py:507] global step 2857: loss = 5.0171 (1.182 sec/step)\n","I0830 16:59:25.493799 139622356395904 learning.py:507] global step 2858: loss = 4.9129 (1.210 sec/step)\n","I0830 16:59:26.723180 139622356395904 learning.py:507] global step 2859: loss = 4.3181 (1.228 sec/step)\n","I0830 16:59:27.949689 139622356395904 learning.py:507] global step 2860: loss = 4.1796 (1.225 sec/step)\n","I0830 16:59:29.183894 139622356395904 learning.py:507] global step 2861: loss = 4.6828 (1.232 sec/step)\n","I0830 16:59:30.407572 139622356395904 learning.py:507] global step 2862: loss = 3.8600 (1.222 sec/step)\n","I0830 16:59:31.615201 139622356395904 learning.py:507] global step 2863: loss = 3.9182 (1.206 sec/step)\n","I0830 16:59:32.842832 139622356395904 learning.py:507] global step 2864: loss = 4.3905 (1.226 sec/step)\n","I0830 16:59:34.074956 139622356395904 learning.py:507] global step 2865: loss = 4.8193 (1.230 sec/step)\n","I0830 16:59:35.296700 139622356395904 learning.py:507] global step 2866: loss = 5.1459 (1.220 sec/step)\n","I0830 16:59:36.519854 139622356395904 learning.py:507] global step 2867: loss = 3.6790 (1.221 sec/step)\n","I0830 16:59:37.731149 139622356395904 learning.py:507] global step 2868: loss = 3.4501 (1.209 sec/step)\n","I0830 16:59:38.962552 139622356395904 learning.py:507] global step 2869: loss = 4.6241 (1.230 sec/step)\n","I0830 16:59:40.188276 139622356395904 learning.py:507] global step 2870: loss = 5.5118 (1.224 sec/step)\n","I0830 16:59:41.418510 139622356395904 learning.py:507] global step 2871: loss = 4.5416 (1.229 sec/step)\n","I0830 16:59:42.629091 139622356395904 learning.py:507] global step 2872: loss = 5.0627 (1.208 sec/step)\n","I0830 16:59:43.889611 139622356395904 learning.py:507] global step 2873: loss = 4.7601 (1.258 sec/step)\n","I0830 16:59:45.118468 139622356395904 learning.py:507] global step 2874: loss = 5.4223 (1.227 sec/step)\n","I0830 16:59:46.331986 139622356395904 learning.py:507] global step 2875: loss = 4.0012 (1.212 sec/step)\n","I0830 16:59:47.598348 139622356395904 learning.py:507] global step 2876: loss = 4.2594 (1.265 sec/step)\n","I0830 16:59:48.797879 139622356395904 learning.py:507] global step 2877: loss = 3.8931 (1.198 sec/step)\n","I0830 16:59:50.035293 139622356395904 learning.py:507] global step 2878: loss = 4.7501 (1.235 sec/step)\n","I0830 16:59:51.279578 139622356395904 learning.py:507] global step 2879: loss = 3.9202 (1.242 sec/step)\n","I0830 16:59:52.513974 139622356395904 learning.py:507] global step 2880: loss = 4.3694 (1.233 sec/step)\n","I0830 16:59:53.778117 139622356395904 learning.py:507] global step 2881: loss = 3.4989 (1.262 sec/step)\n","I0830 16:59:54.990633 139622356395904 learning.py:507] global step 2882: loss = 3.8013 (1.211 sec/step)\n","I0830 16:59:56.220986 139622356395904 learning.py:507] global step 2883: loss = 4.5894 (1.228 sec/step)\n","I0830 16:59:57.423666 139622356395904 learning.py:507] global step 2884: loss = 5.1830 (1.201 sec/step)\n","I0830 16:59:58.666325 139622356395904 learning.py:507] global step 2885: loss = 3.7297 (1.241 sec/step)\n","I0830 16:59:59.646142 139619316770560 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n","I0830 17:00:00.294160 139622356395904 learning.py:507] global step 2886: loss = 4.9193 (1.584 sec/step)\n","I0830 17:00:02.776850 139622356395904 learning.py:507] global step 2887: loss = 4.3316 (2.442 sec/step)\n","I0830 17:00:03.959694 139619299985152 supervisor.py:1050] Recording summary at step 2887.\n","I0830 17:00:04.623599 139622356395904 learning.py:507] global step 2888: loss = 4.2165 (1.713 sec/step)\n","I0830 17:00:05.830393 139622356395904 learning.py:507] global step 2889: loss = 4.8072 (1.205 sec/step)\n","I0830 17:00:07.097631 139622356395904 learning.py:507] global step 2890: loss = 3.9977 (1.266 sec/step)\n","I0830 17:00:08.353399 139622356395904 learning.py:507] global step 2891: loss = 3.8280 (1.254 sec/step)\n","I0830 17:00:09.602192 139622356395904 learning.py:507] global step 2892: loss = 3.6202 (1.247 sec/step)\n","I0830 17:00:10.801612 139622356395904 learning.py:507] global step 2893: loss = 5.0153 (1.198 sec/step)\n","I0830 17:00:12.064151 139622356395904 learning.py:507] global step 2894: loss = 4.7311 (1.261 sec/step)\n","I0830 17:00:13.272019 139622356395904 learning.py:507] global step 2895: loss = 4.1135 (1.206 sec/step)\n","I0830 17:00:14.456649 139622356395904 learning.py:507] global step 2896: loss = 4.2419 (1.183 sec/step)\n","I0830 17:00:15.692530 139622356395904 learning.py:507] global step 2897: loss = 4.9353 (1.234 sec/step)\n","I0830 17:00:16.925846 139622356395904 learning.py:507] global step 2898: loss = 3.9807 (1.232 sec/step)\n","I0830 17:00:18.113556 139622356395904 learning.py:507] global step 2899: loss = 4.7259 (1.186 sec/step)\n","I0830 17:00:19.337471 139622356395904 learning.py:507] global step 2900: loss = 4.0061 (1.222 sec/step)\n","I0830 17:00:20.549477 139622356395904 learning.py:507] global step 2901: loss = 4.0669 (1.210 sec/step)\n","I0830 17:00:21.783807 139622356395904 learning.py:507] global step 2902: loss = 3.4241 (1.232 sec/step)\n","I0830 17:00:23.013571 139622356395904 learning.py:507] global step 2903: loss = 3.8143 (1.228 sec/step)\n","I0830 17:00:24.236862 139622356395904 learning.py:507] global step 2904: loss = 4.4264 (1.221 sec/step)\n","I0830 17:00:25.433880 139622356395904 learning.py:507] global step 2905: loss = 4.2043 (1.194 sec/step)\n","I0830 17:00:26.643175 139622356395904 learning.py:507] global step 2906: loss = 3.6153 (1.208 sec/step)\n","I0830 17:00:27.835802 139622356395904 learning.py:507] global step 2907: loss = 4.8428 (1.191 sec/step)\n","I0830 17:00:29.039453 139622356395904 learning.py:507] global step 2908: loss = 4.1846 (1.202 sec/step)\n","I0830 17:00:30.256157 139622356395904 learning.py:507] global step 2909: loss = 3.7489 (1.214 sec/step)\n","I0830 17:00:31.462866 139622356395904 learning.py:507] global step 2910: loss = 3.8648 (1.205 sec/step)\n","I0830 17:00:32.718535 139622356395904 learning.py:507] global step 2911: loss = 4.2489 (1.254 sec/step)\n","I0830 17:00:33.918733 139622356395904 learning.py:507] global step 2912: loss = 4.4915 (1.198 sec/step)\n","I0830 17:00:35.140911 139622356395904 learning.py:507] global step 2913: loss = 4.0257 (1.220 sec/step)\n","I0830 17:00:36.419109 139622356395904 learning.py:507] global step 2914: loss = 4.3516 (1.276 sec/step)\n","I0830 17:00:37.634202 139622356395904 learning.py:507] global step 2915: loss = 4.0393 (1.213 sec/step)\n","I0830 17:00:38.864688 139622356395904 learning.py:507] global step 2916: loss = 3.3039 (1.229 sec/step)\n","I0830 17:00:40.082256 139622356395904 learning.py:507] global step 2917: loss = 4.5240 (1.216 sec/step)\n","I0830 17:00:41.285645 139622356395904 learning.py:507] global step 2918: loss = 4.2577 (1.198 sec/step)\n","I0830 17:00:42.492086 139622356395904 learning.py:507] global step 2919: loss = 3.7193 (1.205 sec/step)\n","I0830 17:00:43.702675 139622356395904 learning.py:507] global step 2920: loss = 3.3240 (1.209 sec/step)\n","I0830 17:00:44.964037 139622356395904 learning.py:507] global step 2921: loss = 5.1547 (1.260 sec/step)\n","I0830 17:00:46.167983 139622356395904 learning.py:507] global step 2922: loss = 4.1651 (1.202 sec/step)\n","I0830 17:00:47.395101 139622356395904 learning.py:507] global step 2923: loss = 4.5594 (1.225 sec/step)\n","I0830 17:00:48.601699 139622356395904 learning.py:507] global step 2924: loss = 3.8960 (1.205 sec/step)\n","I0830 17:00:49.850659 139622356395904 learning.py:507] global step 2925: loss = 3.4695 (1.247 sec/step)\n","I0830 17:00:51.059759 139622356395904 learning.py:507] global step 2926: loss = 4.3494 (1.207 sec/step)\n","I0830 17:00:52.284437 139622356395904 learning.py:507] global step 2927: loss = 4.1941 (1.223 sec/step)\n","I0830 17:00:53.527987 139622356395904 learning.py:507] global step 2928: loss = 3.8631 (1.242 sec/step)\n","I0830 17:00:54.725034 139622356395904 learning.py:507] global step 2929: loss = 4.0596 (1.195 sec/step)\n","I0830 17:00:55.934148 139622356395904 learning.py:507] global step 2930: loss = 5.0047 (1.207 sec/step)\n","I0830 17:00:57.150765 139622356395904 learning.py:507] global step 2931: loss = 3.5510 (1.214 sec/step)\n","I0830 17:00:58.403273 139622356395904 learning.py:507] global step 2932: loss = 3.4730 (1.251 sec/step)\n","I0830 17:00:59.649217 139622356395904 learning.py:507] global step 2933: loss = 3.7281 (1.244 sec/step)\n","I0830 17:01:00.853213 139622356395904 learning.py:507] global step 2934: loss = 3.4539 (1.202 sec/step)\n","I0830 17:01:02.121272 139622356395904 learning.py:507] global step 2935: loss = 4.0845 (1.266 sec/step)\n","I0830 17:01:03.325959 139622356395904 learning.py:507] global step 2936: loss = 3.3307 (1.203 sec/step)\n","I0830 17:01:04.553793 139622356395904 learning.py:507] global step 2937: loss = 3.7438 (1.226 sec/step)\n","I0830 17:01:05.814774 139622356395904 learning.py:507] global step 2938: loss = 4.8289 (1.259 sec/step)\n","I0830 17:01:07.023538 139622356395904 learning.py:507] global step 2939: loss = 3.5162 (1.206 sec/step)\n","I0830 17:01:08.273126 139622356395904 learning.py:507] global step 2940: loss = 4.5554 (1.248 sec/step)\n","I0830 17:01:09.473882 139622356395904 learning.py:507] global step 2941: loss = 3.6830 (1.199 sec/step)\n","I0830 17:01:10.692790 139622356395904 learning.py:507] global step 2942: loss = 4.0771 (1.217 sec/step)\n","I0830 17:01:11.918012 139622356395904 learning.py:507] global step 2943: loss = 3.7688 (1.223 sec/step)\n","I0830 17:01:13.147034 139622356395904 learning.py:507] global step 2944: loss = 3.5636 (1.227 sec/step)\n","I0830 17:01:14.346672 139622356395904 learning.py:507] global step 2945: loss = 5.0227 (1.198 sec/step)\n","I0830 17:01:15.578667 139622356395904 learning.py:507] global step 2946: loss = 3.6182 (1.230 sec/step)\n","I0830 17:01:16.813966 139622356395904 learning.py:507] global step 2947: loss = 3.5081 (1.233 sec/step)\n","I0830 17:01:18.021559 139622356395904 learning.py:507] global step 2948: loss = 3.6826 (1.206 sec/step)\n","I0830 17:01:19.270824 139622356395904 learning.py:507] global step 2949: loss = 3.5377 (1.247 sec/step)\n","I0830 17:01:20.486095 139622356395904 learning.py:507] global step 2950: loss = 4.0941 (1.213 sec/step)\n","I0830 17:01:21.721275 139622356395904 learning.py:507] global step 2951: loss = 5.3654 (1.233 sec/step)\n","I0830 17:01:22.953728 139622356395904 learning.py:507] global step 2952: loss = 4.1835 (1.230 sec/step)\n","I0830 17:01:24.204112 139622356395904 learning.py:507] global step 2953: loss = 4.0932 (1.248 sec/step)\n","I0830 17:01:25.450416 139622356395904 learning.py:507] global step 2954: loss = 4.5502 (1.244 sec/step)\n","I0830 17:01:26.685513 139622356395904 learning.py:507] global step 2955: loss = 5.4865 (1.233 sec/step)\n","I0830 17:01:27.924086 139622356395904 learning.py:507] global step 2956: loss = 3.7669 (1.236 sec/step)\n","I0830 17:01:29.176163 139622356395904 learning.py:507] global step 2957: loss = 4.0020 (1.250 sec/step)\n","I0830 17:01:30.399940 139622356395904 learning.py:507] global step 2958: loss = 4.6757 (1.221 sec/step)\n","I0830 17:01:31.655948 139622356395904 learning.py:507] global step 2959: loss = 3.9408 (1.254 sec/step)\n","I0830 17:01:32.901351 139622356395904 learning.py:507] global step 2960: loss = 3.4679 (1.243 sec/step)\n","I0830 17:01:34.134308 139622356395904 learning.py:507] global step 2961: loss = 4.6946 (1.231 sec/step)\n","I0830 17:01:35.377159 139622356395904 learning.py:507] global step 2962: loss = 3.7721 (1.241 sec/step)\n","I0830 17:01:36.631327 139622356395904 learning.py:507] global step 2963: loss = 3.6498 (1.252 sec/step)\n","I0830 17:01:37.865180 139622356395904 learning.py:507] global step 2964: loss = 5.2289 (1.232 sec/step)\n","I0830 17:01:39.099071 139622356395904 learning.py:507] global step 2965: loss = 3.8875 (1.232 sec/step)\n","I0830 17:01:40.341732 139622356395904 learning.py:507] global step 2966: loss = 5.7346 (1.241 sec/step)\n","I0830 17:01:41.581606 139622356395904 learning.py:507] global step 2967: loss = 4.4892 (1.238 sec/step)\n","I0830 17:01:42.835367 139622356395904 learning.py:507] global step 2968: loss = 3.7669 (1.252 sec/step)\n","I0830 17:01:44.036043 139622356395904 learning.py:507] global step 2969: loss = 3.9545 (1.199 sec/step)\n","I0830 17:01:45.283920 139622356395904 learning.py:507] global step 2970: loss = 3.4335 (1.246 sec/step)\n","I0830 17:01:46.516432 139622356395904 learning.py:507] global step 2971: loss = 4.9097 (1.231 sec/step)\n","I0830 17:01:47.743250 139622356395904 learning.py:507] global step 2972: loss = 5.1638 (1.225 sec/step)\n","I0830 17:01:48.980426 139622356395904 learning.py:507] global step 2973: loss = 3.9428 (1.235 sec/step)\n","I0830 17:01:50.205444 139622356395904 learning.py:507] global step 2974: loss = 4.6112 (1.223 sec/step)\n","I0830 17:01:51.449561 139622356395904 learning.py:507] global step 2975: loss = 4.1840 (1.242 sec/step)\n","I0830 17:01:52.650979 139622356395904 learning.py:507] global step 2976: loss = 5.5379 (1.200 sec/step)\n","I0830 17:01:53.850893 139622356395904 learning.py:507] global step 2977: loss = 3.7566 (1.198 sec/step)\n","I0830 17:01:55.074184 139622356395904 learning.py:507] global step 2978: loss = 4.2503 (1.222 sec/step)\n","I0830 17:01:56.315605 139622356395904 learning.py:507] global step 2979: loss = 3.8619 (1.239 sec/step)\n","I0830 17:01:57.549616 139622356395904 learning.py:507] global step 2980: loss = 3.3372 (1.232 sec/step)\n","I0830 17:01:58.737518 139622356395904 learning.py:507] global step 2981: loss = 4.1544 (1.186 sec/step)\n","I0830 17:02:00.044703 139622356395904 learning.py:507] global step 2982: loss = 4.0727 (1.260 sec/step)\n","I0830 17:02:01.859355 139619299985152 supervisor.py:1050] Recording summary at step 2982.\n","I0830 17:02:01.903265 139622356395904 learning.py:507] global step 2983: loss = 4.4005 (1.851 sec/step)\n","I0830 17:02:03.147172 139622356395904 learning.py:507] global step 2984: loss = 4.0416 (1.242 sec/step)\n","I0830 17:02:04.382274 139622356395904 learning.py:507] global step 2985: loss = 4.4710 (1.233 sec/step)\n","I0830 17:02:05.629178 139622356395904 learning.py:507] global step 2986: loss = 3.7999 (1.245 sec/step)\n","I0830 17:02:06.837638 139622356395904 learning.py:507] global step 2987: loss = 4.7263 (1.207 sec/step)\n","I0830 17:02:08.044690 139622356395904 learning.py:507] global step 2988: loss = 4.9433 (1.205 sec/step)\n","I0830 17:02:09.295747 139622356395904 learning.py:507] global step 2989: loss = 3.6249 (1.249 sec/step)\n","I0830 17:02:10.512196 139622356395904 learning.py:507] global step 2990: loss = 3.9464 (1.215 sec/step)\n","I0830 17:02:11.752966 139622356395904 learning.py:507] global step 2991: loss = 4.3450 (1.239 sec/step)\n","I0830 17:02:12.994481 139622356395904 learning.py:507] global step 2992: loss = 4.1440 (1.239 sec/step)\n","I0830 17:02:14.206138 139622356395904 learning.py:507] global step 2993: loss = 4.3675 (1.210 sec/step)\n","I0830 17:02:15.460918 139622356395904 learning.py:507] global step 2994: loss = 6.0039 (1.253 sec/step)\n","I0830 17:02:16.697162 139622356395904 learning.py:507] global step 2995: loss = 3.9946 (1.234 sec/step)\n","I0830 17:02:17.943580 139622356395904 learning.py:507] global step 2996: loss = 4.5615 (1.244 sec/step)\n","I0830 17:02:19.136149 139622356395904 learning.py:507] global step 2997: loss = 5.3056 (1.190 sec/step)\n","I0830 17:02:20.363511 139622356395904 learning.py:507] global step 2998: loss = 4.2495 (1.225 sec/step)\n","I0830 17:02:21.564552 139622356395904 learning.py:507] global step 2999: loss = 5.0931 (1.199 sec/step)\n","I0830 17:02:22.761871 139622356395904 learning.py:507] global step 3000: loss = 4.3502 (1.195 sec/step)\n","I0830 17:02:24.005443 139622356395904 learning.py:507] global step 3001: loss = 3.2921 (1.242 sec/step)\n","I0830 17:02:25.221765 139622356395904 learning.py:507] global step 3002: loss = 4.1445 (1.214 sec/step)\n","I0830 17:02:26.430438 139622356395904 learning.py:507] global step 3003: loss = 3.6964 (1.207 sec/step)\n","I0830 17:02:27.678002 139622356395904 learning.py:507] global step 3004: loss = 4.0811 (1.246 sec/step)\n","I0830 17:02:28.899466 139622356395904 learning.py:507] global step 3005: loss = 3.4852 (1.220 sec/step)\n","I0830 17:02:30.088388 139622356395904 learning.py:507] global step 3006: loss = 4.6771 (1.187 sec/step)\n","I0830 17:02:31.318197 139622356395904 learning.py:507] global step 3007: loss = 4.2521 (1.228 sec/step)\n","I0830 17:02:32.520040 139622356395904 learning.py:507] global step 3008: loss = 3.5177 (1.199 sec/step)\n","I0830 17:02:33.766927 139622356395904 learning.py:507] global step 3009: loss = 3.7853 (1.245 sec/step)\n","I0830 17:02:34.962420 139622356395904 learning.py:507] global step 3010: loss = 4.9994 (1.193 sec/step)\n","I0830 17:02:36.223021 139622356395904 learning.py:507] global step 3011: loss = 4.3729 (1.259 sec/step)\n","I0830 17:02:37.462584 139622356395904 learning.py:507] global step 3012: loss = 3.4067 (1.238 sec/step)\n","I0830 17:02:38.686643 139622356395904 learning.py:507] global step 3013: loss = 4.6750 (1.222 sec/step)\n","I0830 17:02:39.933298 139622356395904 learning.py:507] global step 3014: loss = 4.3826 (1.244 sec/step)\n","I0830 17:02:41.181101 139622356395904 learning.py:507] global step 3015: loss = 4.0291 (1.246 sec/step)\n","I0830 17:02:42.398464 139622356395904 learning.py:507] global step 3016: loss = 3.3746 (1.216 sec/step)\n","I0830 17:02:43.604640 139622356395904 learning.py:507] global step 3017: loss = 3.9696 (1.204 sec/step)\n","I0830 17:02:44.895418 139622356395904 learning.py:507] global step 3018: loss = 4.0742 (1.289 sec/step)\n","I0830 17:02:46.114467 139622356395904 learning.py:507] global step 3019: loss = 3.5031 (1.217 sec/step)\n","I0830 17:02:47.364642 139622356395904 learning.py:507] global step 3020: loss = 4.0759 (1.248 sec/step)\n","I0830 17:02:48.586741 139622356395904 learning.py:507] global step 3021: loss = 4.5519 (1.220 sec/step)\n","I0830 17:02:49.845691 139622356395904 learning.py:507] global step 3022: loss = 3.8099 (1.257 sec/step)\n","I0830 17:02:51.052035 139622356395904 learning.py:507] global step 3023: loss = 4.5533 (1.204 sec/step)\n","I0830 17:02:52.298597 139622356395904 learning.py:507] global step 3024: loss = 3.7878 (1.244 sec/step)\n","I0830 17:02:53.525690 139622356395904 learning.py:507] global step 3025: loss = 4.7491 (1.225 sec/step)\n","I0830 17:02:54.733768 139622356395904 learning.py:507] global step 3026: loss = 6.4145 (1.206 sec/step)\n","I0830 17:02:55.961091 139622356395904 learning.py:507] global step 3027: loss = 3.9807 (1.225 sec/step)\n","I0830 17:02:57.174839 139622356395904 learning.py:507] global step 3028: loss = 5.2493 (1.212 sec/step)\n","I0830 17:02:58.403285 139622356395904 learning.py:507] global step 3029: loss = 4.0553 (1.227 sec/step)\n","I0830 17:02:59.618143 139622356395904 learning.py:507] global step 3030: loss = 3.2046 (1.213 sec/step)\n","I0830 17:03:00.843137 139622356395904 learning.py:507] global step 3031: loss = 4.6028 (1.223 sec/step)\n","I0830 17:03:02.041296 139622356395904 learning.py:507] global step 3032: loss = 5.3484 (1.196 sec/step)\n","I0830 17:03:03.256661 139622356395904 learning.py:507] global step 3033: loss = 3.7815 (1.213 sec/step)\n","I0830 17:03:04.470163 139622356395904 learning.py:507] global step 3034: loss = 3.8429 (1.211 sec/step)\n","I0830 17:03:05.665851 139622356395904 learning.py:507] global step 3035: loss = 4.3650 (1.194 sec/step)\n","I0830 17:03:06.894399 139622356395904 learning.py:507] global step 3036: loss = 4.1458 (1.226 sec/step)\n","I0830 17:03:08.140515 139622356395904 learning.py:507] global step 3037: loss = 3.9523 (1.244 sec/step)\n","I0830 17:03:09.379268 139622356395904 learning.py:507] global step 3038: loss = 4.2720 (1.237 sec/step)\n","I0830 17:03:10.613356 139622356395904 learning.py:507] global step 3039: loss = 3.8590 (1.232 sec/step)\n","I0830 17:03:11.832230 139622356395904 learning.py:507] global step 3040: loss = 5.0086 (1.217 sec/step)\n","I0830 17:03:13.066043 139622356395904 learning.py:507] global step 3041: loss = 4.5651 (1.232 sec/step)\n","I0830 17:03:14.314155 139622356395904 learning.py:507] global step 3042: loss = 4.3148 (1.246 sec/step)\n","I0830 17:03:15.556562 139622356395904 learning.py:507] global step 3043: loss = 4.0798 (1.240 sec/step)\n","I0830 17:03:16.772331 139622356395904 learning.py:507] global step 3044: loss = 4.1818 (1.214 sec/step)\n","I0830 17:03:17.965010 139622356395904 learning.py:507] global step 3045: loss = 4.2761 (1.188 sec/step)\n","I0830 17:03:19.164197 139622356395904 learning.py:507] global step 3046: loss = 5.4839 (1.197 sec/step)\n","I0830 17:03:20.408408 139622356395904 learning.py:507] global step 3047: loss = 4.5466 (1.242 sec/step)\n","I0830 17:03:21.640932 139622356395904 learning.py:507] global step 3048: loss = 3.3878 (1.231 sec/step)\n","I0830 17:03:22.875170 139622356395904 learning.py:507] global step 3049: loss = 3.6717 (1.232 sec/step)\n","I0830 17:03:24.113168 139622356395904 learning.py:507] global step 3050: loss = 3.9047 (1.236 sec/step)\n","I0830 17:03:25.352073 139622356395904 learning.py:507] global step 3051: loss = 4.0281 (1.237 sec/step)\n","I0830 17:03:26.567793 139622356395904 learning.py:507] global step 3052: loss = 4.2399 (1.214 sec/step)\n","I0830 17:03:27.800475 139622356395904 learning.py:507] global step 3053: loss = 4.1487 (1.231 sec/step)\n","I0830 17:03:29.019453 139622356395904 learning.py:507] global step 3054: loss = 4.8112 (1.217 sec/step)\n","I0830 17:03:30.198470 139622356395904 learning.py:507] global step 3055: loss = 4.3330 (1.177 sec/step)\n","I0830 17:03:31.429744 139622356395904 learning.py:507] global step 3056: loss = 3.4149 (1.229 sec/step)\n","I0830 17:03:32.629415 139622356395904 learning.py:507] global step 3057: loss = 3.8357 (1.198 sec/step)\n","I0830 17:03:33.838480 139622356395904 learning.py:507] global step 3058: loss = 5.0959 (1.207 sec/step)\n","I0830 17:03:35.064684 139622356395904 learning.py:507] global step 3059: loss = 4.8249 (1.224 sec/step)\n","I0830 17:03:36.278542 139622356395904 learning.py:507] global step 3060: loss = 5.0753 (1.212 sec/step)\n","I0830 17:03:37.511765 139622356395904 learning.py:507] global step 3061: loss = 4.0887 (1.231 sec/step)\n","I0830 17:03:38.754942 139622356395904 learning.py:507] global step 3062: loss = 5.4569 (1.241 sec/step)\n","I0830 17:03:39.989486 139622356395904 learning.py:507] global step 3063: loss = 3.2716 (1.233 sec/step)\n","I0830 17:03:41.213743 139622356395904 learning.py:507] global step 3064: loss = 4.2276 (1.223 sec/step)\n","I0830 17:03:42.434115 139622356395904 learning.py:507] global step 3065: loss = 5.1979 (1.216 sec/step)\n","I0830 17:03:43.674873 139622356395904 learning.py:507] global step 3066: loss = 4.6115 (1.238 sec/step)\n","I0830 17:03:44.902829 139622356395904 learning.py:507] global step 3067: loss = 3.7887 (1.226 sec/step)\n","I0830 17:03:46.143380 139622356395904 learning.py:507] global step 3068: loss = 3.8283 (1.238 sec/step)\n","I0830 17:03:47.395433 139622356395904 learning.py:507] global step 3069: loss = 4.2689 (1.250 sec/step)\n","I0830 17:03:48.619967 139622356395904 learning.py:507] global step 3070: loss = 3.5586 (1.221 sec/step)\n","I0830 17:03:49.846567 139622356395904 learning.py:507] global step 3071: loss = 4.3731 (1.224 sec/step)\n","I0830 17:03:51.070119 139622356395904 learning.py:507] global step 3072: loss = 4.9574 (1.221 sec/step)\n","I0830 17:03:52.271649 139622356395904 learning.py:507] global step 3073: loss = 3.4852 (1.200 sec/step)\n","I0830 17:03:53.477173 139622356395904 learning.py:507] global step 3074: loss = 5.0196 (1.203 sec/step)\n","I0830 17:03:54.708609 139622356395904 learning.py:507] global step 3075: loss = 4.7325 (1.230 sec/step)\n","I0830 17:03:55.978678 139622356395904 learning.py:507] global step 3076: loss = 4.1146 (1.268 sec/step)\n","I0830 17:03:57.181793 139622356395904 learning.py:507] global step 3077: loss = 4.1871 (1.201 sec/step)\n","I0830 17:03:58.409754 139622356395904 learning.py:507] global step 3078: loss = 4.0429 (1.226 sec/step)\n","I0830 17:03:59.614935 139622356395904 learning.py:507] global step 3079: loss = 3.6423 (1.203 sec/step)\n","I0830 17:04:01.681025 139622356395904 learning.py:507] global step 3080: loss = 4.4603 (1.944 sec/step)\n","I0830 17:04:02.006153 139619299985152 supervisor.py:1050] Recording summary at step 3080.\n","I0830 17:04:02.975114 139622356395904 learning.py:507] global step 3081: loss = 4.3477 (1.291 sec/step)\n","I0830 17:04:04.195844 139622356395904 learning.py:507] global step 3082: loss = 4.1620 (1.219 sec/step)\n","I0830 17:04:05.419789 139622356395904 learning.py:507] global step 3083: loss = 3.9567 (1.222 sec/step)\n","I0830 17:04:06.637006 139622356395904 learning.py:507] global step 3084: loss = 3.9800 (1.215 sec/step)\n","I0830 17:04:07.889800 139622356395904 learning.py:507] global step 3085: loss = 4.8703 (1.250 sec/step)\n","I0830 17:04:09.129468 139622356395904 learning.py:507] global step 3086: loss = 4.2203 (1.237 sec/step)\n","I0830 17:04:10.345994 139622356395904 learning.py:507] global step 3087: loss = 4.6866 (1.214 sec/step)\n","I0830 17:04:11.573083 139622356395904 learning.py:507] global step 3088: loss = 3.5887 (1.225 sec/step)\n","I0830 17:04:12.812197 139622356395904 learning.py:507] global step 3089: loss = 4.5034 (1.237 sec/step)\n","I0830 17:04:14.023717 139622356395904 learning.py:507] global step 3090: loss = 4.3422 (1.210 sec/step)\n","I0830 17:04:15.261580 139622356395904 learning.py:507] global step 3091: loss = 3.4646 (1.236 sec/step)\n","I0830 17:04:16.486691 139622356395904 learning.py:507] global step 3092: loss = 3.9631 (1.223 sec/step)\n","I0830 17:04:17.697859 139622356395904 learning.py:507] global step 3093: loss = 4.1217 (1.209 sec/step)\n","I0830 17:04:18.952255 139622356395904 learning.py:507] global step 3094: loss = 4.6180 (1.252 sec/step)\n","I0830 17:04:20.195396 139622356395904 learning.py:507] global step 3095: loss = 4.0399 (1.241 sec/step)\n","I0830 17:04:21.447660 139622356395904 learning.py:507] global step 3096: loss = 4.3528 (1.250 sec/step)\n","I0830 17:04:22.691837 139622356395904 learning.py:507] global step 3097: loss = 3.2272 (1.242 sec/step)\n","I0830 17:04:23.899688 139622356395904 learning.py:507] global step 3098: loss = 4.1093 (1.206 sec/step)\n","I0830 17:04:25.123072 139622356395904 learning.py:507] global step 3099: loss = 3.2550 (1.222 sec/step)\n","I0830 17:04:26.352784 139622356395904 learning.py:507] global step 3100: loss = 3.8259 (1.228 sec/step)\n","I0830 17:04:27.554892 139622356395904 learning.py:507] global step 3101: loss = 4.3011 (1.200 sec/step)\n","I0830 17:04:28.793973 139622356395904 learning.py:507] global step 3102: loss = 3.5661 (1.237 sec/step)\n","I0830 17:04:30.009569 139622356395904 learning.py:507] global step 3103: loss = 3.6438 (1.214 sec/step)\n","I0830 17:04:31.239219 139622356395904 learning.py:507] global step 3104: loss = 3.7608 (1.228 sec/step)\n","I0830 17:04:32.448019 139622356395904 learning.py:507] global step 3105: loss = 4.0226 (1.207 sec/step)\n","I0830 17:04:33.648848 139622356395904 learning.py:507] global step 3106: loss = 4.0281 (1.199 sec/step)\n","I0830 17:04:34.882258 139622356395904 learning.py:507] global step 3107: loss = 3.4195 (1.232 sec/step)\n","I0830 17:04:36.114330 139622356395904 learning.py:507] global step 3108: loss = 3.3949 (1.230 sec/step)\n","I0830 17:04:37.322721 139622356395904 learning.py:507] global step 3109: loss = 4.0804 (1.206 sec/step)\n","I0830 17:04:38.565301 139622356395904 learning.py:507] global step 3110: loss = 3.9712 (1.241 sec/step)\n","I0830 17:04:39.801689 139622356395904 learning.py:507] global step 3111: loss = 4.9284 (1.235 sec/step)\n","I0830 17:04:41.061394 139622356395904 learning.py:507] global step 3112: loss = 5.1434 (1.258 sec/step)\n","I0830 17:04:42.294018 139622356395904 learning.py:507] global step 3113: loss = 4.2597 (1.231 sec/step)\n","I0830 17:04:43.503159 139622356395904 learning.py:507] global step 3114: loss = 3.7532 (1.207 sec/step)\n","I0830 17:04:44.751136 139622356395904 learning.py:507] global step 3115: loss = 3.7148 (1.246 sec/step)\n","I0830 17:04:45.968020 139622356395904 learning.py:507] global step 3116: loss = 3.7663 (1.215 sec/step)\n","I0830 17:04:47.189741 139622356395904 learning.py:507] global step 3117: loss = 3.4960 (1.217 sec/step)\n","I0830 17:04:48.419960 139622356395904 learning.py:507] global step 3118: loss = 4.1225 (1.228 sec/step)\n","I0830 17:04:49.623293 139622356395904 learning.py:507] global step 3119: loss = 3.9824 (1.201 sec/step)\n","I0830 17:04:50.833376 139622356395904 learning.py:507] global step 3120: loss = 3.6643 (1.208 sec/step)\n","I0830 17:04:52.047461 139622356395904 learning.py:507] global step 3121: loss = 3.8770 (1.212 sec/step)\n","I0830 17:04:53.242484 139622356395904 learning.py:507] global step 3122: loss = 3.7758 (1.193 sec/step)\n","I0830 17:04:54.456659 139622356395904 learning.py:507] global step 3123: loss = 4.2402 (1.212 sec/step)\n","I0830 17:04:55.706437 139622356395904 learning.py:507] global step 3124: loss = 3.9248 (1.248 sec/step)\n","I0830 17:04:56.937484 139622356395904 learning.py:507] global step 3125: loss = 4.4145 (1.229 sec/step)\n","I0830 17:04:58.188298 139622356395904 learning.py:507] global step 3126: loss = 3.8631 (1.249 sec/step)\n","I0830 17:04:59.444534 139622356395904 learning.py:507] global step 3127: loss = 3.9362 (1.254 sec/step)\n","I0830 17:05:00.644840 139622356395904 learning.py:507] global step 3128: loss = 3.4144 (1.198 sec/step)\n","I0830 17:05:01.878502 139622356395904 learning.py:507] global step 3129: loss = 4.0521 (1.231 sec/step)\n","I0830 17:05:03.061241 139622356395904 learning.py:507] global step 3130: loss = 4.2451 (1.181 sec/step)\n","I0830 17:05:04.313407 139622356395904 learning.py:507] global step 3131: loss = 3.4761 (1.250 sec/step)\n","I0830 17:05:05.544629 139622356395904 learning.py:507] global step 3132: loss = 4.1684 (1.229 sec/step)\n","I0830 17:05:06.775476 139622356395904 learning.py:507] global step 3133: loss = 3.3178 (1.229 sec/step)\n","I0830 17:05:07.980329 139622356395904 learning.py:507] global step 3134: loss = 4.5819 (1.203 sec/step)\n","I0830 17:05:09.194511 139622356395904 learning.py:507] global step 3135: loss = 3.3900 (1.212 sec/step)\n","I0830 17:05:10.438916 139622356395904 learning.py:507] global step 3136: loss = 3.4757 (1.242 sec/step)\n","I0830 17:05:11.693567 139622356395904 learning.py:507] global step 3137: loss = 4.1935 (1.253 sec/step)\n","I0830 17:05:12.940263 139622356395904 learning.py:507] global step 3138: loss = 3.8957 (1.245 sec/step)\n","I0830 17:05:14.160348 139622356395904 learning.py:507] global step 3139: loss = 4.3240 (1.218 sec/step)\n","I0830 17:05:15.353940 139622356395904 learning.py:507] global step 3140: loss = 3.6221 (1.192 sec/step)\n","I0830 17:05:16.579914 139622356395904 learning.py:507] global step 3141: loss = 4.8278 (1.224 sec/step)\n","I0830 17:05:17.853254 139622356395904 learning.py:507] global step 3142: loss = 3.8892 (1.271 sec/step)\n","I0830 17:05:19.057425 139622356395904 learning.py:507] global step 3143: loss = 4.6898 (1.202 sec/step)\n","I0830 17:05:20.284630 139622356395904 learning.py:507] global step 3144: loss = 4.0089 (1.225 sec/step)\n","I0830 17:05:21.509967 139622356395904 learning.py:507] global step 3145: loss = 4.0968 (1.223 sec/step)\n","I0830 17:05:22.701364 139622356395904 learning.py:507] global step 3146: loss = 3.6740 (1.189 sec/step)\n","I0830 17:05:23.947021 139622356395904 learning.py:507] global step 3147: loss = 3.2072 (1.244 sec/step)\n","I0830 17:05:25.196130 139622356395904 learning.py:507] global step 3148: loss = 3.9154 (1.247 sec/step)\n","I0830 17:05:26.429218 139622356395904 learning.py:507] global step 3149: loss = 3.7486 (1.231 sec/step)\n","I0830 17:05:27.651832 139622356395904 learning.py:507] global step 3150: loss = 3.2111 (1.217 sec/step)\n","I0830 17:05:28.851583 139622356395904 learning.py:507] global step 3151: loss = 4.1882 (1.198 sec/step)\n","I0830 17:05:30.070944 139622356395904 learning.py:507] global step 3152: loss = 4.6204 (1.217 sec/step)\n","I0830 17:05:31.278958 139622356395904 learning.py:507] global step 3153: loss = 3.8424 (1.206 sec/step)\n","I0830 17:05:32.479959 139622356395904 learning.py:507] global step 3154: loss = 3.9582 (1.199 sec/step)\n","I0830 17:05:33.756595 139622356395904 learning.py:507] global step 3155: loss = 4.2611 (1.275 sec/step)\n","I0830 17:05:34.956120 139622356395904 learning.py:507] global step 3156: loss = 3.4003 (1.198 sec/step)\n","I0830 17:05:36.180743 139622356395904 learning.py:507] global step 3157: loss = 4.5714 (1.223 sec/step)\n","I0830 17:05:37.387100 139622356395904 learning.py:507] global step 3158: loss = 5.2700 (1.205 sec/step)\n","I0830 17:05:38.605963 139622356395904 learning.py:507] global step 3159: loss = 3.7197 (1.217 sec/step)\n","I0830 17:05:39.880269 139622356395904 learning.py:507] global step 3160: loss = 3.4836 (1.272 sec/step)\n","I0830 17:05:41.141727 139622356395904 learning.py:507] global step 3161: loss = 3.1949 (1.259 sec/step)\n","I0830 17:05:42.402413 139622356395904 learning.py:507] global step 3162: loss = 5.2780 (1.259 sec/step)\n","I0830 17:05:43.656686 139622356395904 learning.py:507] global step 3163: loss = 3.5577 (1.252 sec/step)\n","I0830 17:05:44.865665 139622356395904 learning.py:507] global step 3164: loss = 3.9032 (1.207 sec/step)\n","I0830 17:05:46.126830 139622356395904 learning.py:507] global step 3165: loss = 5.2826 (1.259 sec/step)\n","I0830 17:05:47.385169 139622356395904 learning.py:507] global step 3166: loss = 3.1336 (1.256 sec/step)\n","I0830 17:05:48.595729 139622356395904 learning.py:507] global step 3167: loss = 3.6738 (1.208 sec/step)\n","I0830 17:05:49.846451 139622356395904 learning.py:507] global step 3168: loss = 4.9275 (1.249 sec/step)\n","I0830 17:05:51.092598 139622356395904 learning.py:507] global step 3169: loss = 3.7952 (1.244 sec/step)\n","I0830 17:05:52.306706 139622356395904 learning.py:507] global step 3170: loss = 4.4191 (1.212 sec/step)\n","I0830 17:05:53.516625 139622356395904 learning.py:507] global step 3171: loss = 3.2180 (1.208 sec/step)\n","I0830 17:05:54.735915 139622356395904 learning.py:507] global step 3172: loss = 4.4881 (1.218 sec/step)\n","I0830 17:05:55.940991 139622356395904 learning.py:507] global step 3173: loss = 4.2932 (1.203 sec/step)\n","I0830 17:05:57.162097 139622356395904 learning.py:507] global step 3174: loss = 4.0357 (1.219 sec/step)\n","I0830 17:05:58.379307 139622356395904 learning.py:507] global step 3175: loss = 3.6802 (1.216 sec/step)\n","I0830 17:05:59.598579 139622356395904 learning.py:507] global step 3176: loss = 4.7866 (1.217 sec/step)\n","I0830 17:06:01.936435 139622356395904 learning.py:507] global step 3177: loss = 3.9760 (1.850 sec/step)\n","I0830 17:06:01.944257 139619299985152 supervisor.py:1050] Recording summary at step 3177.\n","I0830 17:06:03.189033 139622356395904 learning.py:507] global step 3178: loss = 5.1135 (1.250 sec/step)\n","I0830 17:06:04.392307 139622356395904 learning.py:507] global step 3179: loss = 4.0093 (1.201 sec/step)\n","I0830 17:06:05.627872 139622356395904 learning.py:507] global step 3180: loss = 3.3715 (1.234 sec/step)\n","I0830 17:06:06.853609 139622356395904 learning.py:507] global step 3181: loss = 3.5474 (1.224 sec/step)\n","I0830 17:06:08.061295 139622356395904 learning.py:507] global step 3182: loss = 3.5261 (1.206 sec/step)\n","I0830 17:06:09.277041 139622356395904 learning.py:507] global step 3183: loss = 4.2718 (1.214 sec/step)\n","I0830 17:06:10.505448 139622356395904 learning.py:507] global step 3184: loss = 3.4732 (1.226 sec/step)\n","I0830 17:06:11.735989 139622356395904 learning.py:507] global step 3185: loss = 4.0861 (1.229 sec/step)\n","I0830 17:06:12.985466 139622356395904 learning.py:507] global step 3186: loss = 4.3071 (1.248 sec/step)\n","I0830 17:06:14.205116 139622356395904 learning.py:507] global step 3187: loss = 3.5757 (1.218 sec/step)\n","I0830 17:06:15.433102 139622356395904 learning.py:507] global step 3188: loss = 4.4841 (1.226 sec/step)\n","I0830 17:06:16.671142 139622356395904 learning.py:507] global step 3189: loss = 4.3000 (1.236 sec/step)\n","I0830 17:06:17.916076 139622356395904 learning.py:507] global step 3190: loss = 3.7835 (1.243 sec/step)\n","I0830 17:06:19.130901 139622356395904 learning.py:507] global step 3191: loss = 3.8880 (1.213 sec/step)\n","I0830 17:06:20.341908 139622356395904 learning.py:507] global step 3192: loss = 4.2733 (1.209 sec/step)\n","I0830 17:06:21.569140 139622356395904 learning.py:507] global step 3193: loss = 3.8247 (1.225 sec/step)\n","I0830 17:06:22.812350 139622356395904 learning.py:507] global step 3194: loss = 3.5944 (1.241 sec/step)\n","I0830 17:06:24.052458 139622356395904 learning.py:507] global step 3195: loss = 4.5611 (1.238 sec/step)\n","I0830 17:06:25.284634 139622356395904 learning.py:507] global step 3196: loss = 3.1654 (1.230 sec/step)\n","I0830 17:06:26.477479 139622356395904 learning.py:507] global step 3197: loss = 3.2605 (1.191 sec/step)\n","I0830 17:06:27.704627 139622356395904 learning.py:507] global step 3198: loss = 4.2654 (1.225 sec/step)\n","I0830 17:06:28.946500 139622356395904 learning.py:507] global step 3199: loss = 3.8666 (1.240 sec/step)\n","I0830 17:06:30.140743 139622356395904 learning.py:507] global step 3200: loss = 4.6260 (1.192 sec/step)\n","I0830 17:06:31.376443 139622356395904 learning.py:507] global step 3201: loss = 4.1715 (1.233 sec/step)\n","I0830 17:06:32.606315 139622356395904 learning.py:507] global step 3202: loss = 3.4639 (1.228 sec/step)\n","I0830 17:06:33.835537 139622356395904 learning.py:507] global step 3203: loss = 3.4053 (1.227 sec/step)\n","I0830 17:06:35.066033 139622356395904 learning.py:507] global step 3204: loss = 4.2314 (1.228 sec/step)\n","I0830 17:06:36.315350 139622356395904 learning.py:507] global step 3205: loss = 4.9370 (1.247 sec/step)\n","I0830 17:06:37.572111 139622356395904 learning.py:507] global step 3206: loss = 4.4685 (1.255 sec/step)\n","I0830 17:06:38.825539 139622356395904 learning.py:507] global step 3207: loss = 3.7438 (1.252 sec/step)\n","I0830 17:06:40.102768 139622356395904 learning.py:507] global step 3208: loss = 4.3322 (1.275 sec/step)\n","I0830 17:06:41.316385 139622356395904 learning.py:507] global step 3209: loss = 5.0213 (1.212 sec/step)\n","I0830 17:06:42.542800 139622356395904 learning.py:507] global step 3210: loss = 3.1215 (1.222 sec/step)\n","I0830 17:06:43.758573 139622356395904 learning.py:507] global step 3211: loss = 4.2284 (1.214 sec/step)\n","I0830 17:06:44.993485 139622356395904 learning.py:507] global step 3212: loss = 3.5315 (1.233 sec/step)\n","I0830 17:06:46.222322 139622356395904 learning.py:507] global step 3213: loss = 4.5385 (1.227 sec/step)\n","I0830 17:06:47.419348 139622356395904 learning.py:507] global step 3214: loss = 5.2624 (1.195 sec/step)\n","I0830 17:06:48.682738 139622356395904 learning.py:507] global step 3215: loss = 3.3562 (1.261 sec/step)\n","I0830 17:06:49.925986 139622356395904 learning.py:507] global step 3216: loss = 3.6834 (1.241 sec/step)\n","I0830 17:06:51.139248 139622356395904 learning.py:507] global step 3217: loss = 4.0745 (1.211 sec/step)\n","I0830 17:06:52.409389 139622356395904 learning.py:507] global step 3218: loss = 3.6276 (1.268 sec/step)\n","I0830 17:06:53.645400 139622356395904 learning.py:507] global step 3219: loss = 4.4214 (1.234 sec/step)\n","I0830 17:06:54.848752 139622356395904 learning.py:507] global step 3220: loss = 3.8213 (1.201 sec/step)\n","I0830 17:06:56.098452 139622356395904 learning.py:507] global step 3221: loss = 4.2715 (1.248 sec/step)\n","I0830 17:06:57.318641 139622356395904 learning.py:507] global step 3222: loss = 3.4284 (1.218 sec/step)\n","I0830 17:06:58.550022 139622356395904 learning.py:507] global step 3223: loss = 3.5764 (1.229 sec/step)\n","I0830 17:06:59.805001 139622356395904 learning.py:507] global step 3224: loss = 3.7602 (1.253 sec/step)\n","I0830 17:07:01.020587 139622356395904 learning.py:507] global step 3225: loss = 3.7844 (1.213 sec/step)\n","I0830 17:07:02.256618 139622356395904 learning.py:507] global step 3226: loss = 4.8498 (1.234 sec/step)\n","I0830 17:07:03.486589 139622356395904 learning.py:507] global step 3227: loss = 3.8616 (1.228 sec/step)\n","I0830 17:07:04.748526 139622356395904 learning.py:507] global step 3228: loss = 3.8921 (1.260 sec/step)\n","I0830 17:07:06.011325 139622356395904 learning.py:507] global step 3229: loss = 3.9058 (1.261 sec/step)\n","I0830 17:07:07.233728 139622356395904 learning.py:507] global step 3230: loss = 4.9728 (1.221 sec/step)\n","I0830 17:07:08.474917 139622356395904 learning.py:507] global step 3231: loss = 3.6270 (1.239 sec/step)\n","I0830 17:07:09.701438 139622356395904 learning.py:507] global step 3232: loss = 3.3431 (1.225 sec/step)\n","I0830 17:07:10.900223 139622356395904 learning.py:507] global step 3233: loss = 4.2492 (1.197 sec/step)\n","I0830 17:07:12.119328 139622356395904 learning.py:507] global step 3234: loss = 3.9895 (1.217 sec/step)\n","I0830 17:07:13.361037 139622356395904 learning.py:507] global step 3235: loss = 3.5706 (1.240 sec/step)\n","I0830 17:07:14.595934 139622356395904 learning.py:507] global step 3236: loss = 4.4013 (1.233 sec/step)\n","I0830 17:07:15.814980 139622356395904 learning.py:507] global step 3237: loss = 5.0399 (1.217 sec/step)\n","I0830 17:07:17.026448 139622356395904 learning.py:507] global step 3238: loss = 4.2344 (1.210 sec/step)\n","I0830 17:07:18.228141 139622356395904 learning.py:507] global step 3239: loss = 3.5373 (1.200 sec/step)\n","I0830 17:07:19.473731 139622356395904 learning.py:507] global step 3240: loss = 3.4445 (1.244 sec/step)\n","I0830 17:07:20.696398 139622356395904 learning.py:507] global step 3241: loss = 4.5285 (1.220 sec/step)\n","I0830 17:07:21.937374 139622356395904 learning.py:507] global step 3242: loss = 4.3007 (1.239 sec/step)\n","I0830 17:07:23.183212 139622356395904 learning.py:507] global step 3243: loss = 4.9952 (1.244 sec/step)\n","I0830 17:07:24.439220 139622356395904 learning.py:507] global step 3244: loss = 3.3872 (1.254 sec/step)\n","I0830 17:07:25.626273 139622356395904 learning.py:507] global step 3245: loss = 3.7607 (1.185 sec/step)\n","I0830 17:07:26.820593 139622356395904 learning.py:507] global step 3246: loss = 4.1310 (1.192 sec/step)\n","I0830 17:07:28.058820 139622356395904 learning.py:507] global step 3247: loss = 4.3777 (1.236 sec/step)\n","I0830 17:07:29.262912 139622356395904 learning.py:507] global step 3248: loss = 3.2952 (1.202 sec/step)\n","I0830 17:07:30.475175 139622356395904 learning.py:507] global step 3249: loss = 4.7420 (1.210 sec/step)\n","I0830 17:07:31.701368 139622356395904 learning.py:507] global step 3250: loss = 3.7382 (1.224 sec/step)\n","I0830 17:07:32.901279 139622356395904 learning.py:507] global step 3251: loss = 3.6354 (1.198 sec/step)\n","I0830 17:07:34.124192 139622356395904 learning.py:507] global step 3252: loss = 3.5643 (1.221 sec/step)\n","I0830 17:07:35.359205 139622356395904 learning.py:507] global step 3253: loss = 4.1234 (1.233 sec/step)\n","I0830 17:07:36.592228 139622356395904 learning.py:507] global step 3254: loss = 3.2157 (1.231 sec/step)\n","I0830 17:07:37.814138 139622356395904 learning.py:507] global step 3255: loss = 3.6692 (1.220 sec/step)\n","I0830 17:07:39.045412 139622356395904 learning.py:507] global step 3256: loss = 3.7056 (1.229 sec/step)\n","I0830 17:07:40.284972 139622356395904 learning.py:507] global step 3257: loss = 3.5397 (1.238 sec/step)\n","I0830 17:07:41.500044 139622356395904 learning.py:507] global step 3258: loss = 4.4167 (1.213 sec/step)\n","I0830 17:07:42.703536 139622356395904 learning.py:507] global step 3259: loss = 4.4345 (1.202 sec/step)\n","I0830 17:07:43.932699 139622356395904 learning.py:507] global step 3260: loss = 4.1147 (1.227 sec/step)\n","I0830 17:07:45.145616 139622356395904 learning.py:507] global step 3261: loss = 3.2198 (1.211 sec/step)\n","I0830 17:07:46.363574 139622356395904 learning.py:507] global step 3262: loss = 4.2903 (1.216 sec/step)\n","I0830 17:07:47.621623 139622356395904 learning.py:507] global step 3263: loss = 3.6344 (1.256 sec/step)\n","I0830 17:07:48.840809 139622356395904 learning.py:507] global step 3264: loss = 3.5853 (1.217 sec/step)\n","I0830 17:07:50.082606 139622356395904 learning.py:507] global step 3265: loss = 4.5572 (1.240 sec/step)\n","I0830 17:07:51.289911 139622356395904 learning.py:507] global step 3266: loss = 4.2428 (1.205 sec/step)\n","I0830 17:07:52.551044 139622356395904 learning.py:507] global step 3267: loss = 3.5433 (1.259 sec/step)\n","I0830 17:07:53.789358 139622356395904 learning.py:507] global step 3268: loss = 3.6276 (1.236 sec/step)\n","I0830 17:07:54.979031 139622356395904 learning.py:507] global step 3269: loss = 3.6352 (1.188 sec/step)\n","I0830 17:07:56.222082 139622356395904 learning.py:507] global step 3270: loss = 4.0550 (1.241 sec/step)\n","I0830 17:07:57.471558 139622356395904 learning.py:507] global step 3271: loss = 3.5808 (1.248 sec/step)\n","I0830 17:07:58.729612 139622356395904 learning.py:507] global step 3272: loss = 4.2740 (1.256 sec/step)\n","I0830 17:07:59.990891 139622356395904 learning.py:507] global step 3273: loss = 4.3545 (1.253 sec/step)\n","I0830 17:08:01.948718 139619299985152 supervisor.py:1050] Recording summary at step 3274.\n","I0830 17:08:01.985808 139622356395904 learning.py:507] global step 3274: loss = 3.6937 (1.993 sec/step)\n","I0830 17:08:03.203074 139622356395904 learning.py:507] global step 3275: loss = 4.0661 (1.215 sec/step)\n","I0830 17:08:04.450681 139622356395904 learning.py:507] global step 3276: loss = 4.1666 (1.246 sec/step)\n","I0830 17:08:05.672392 139622356395904 learning.py:507] global step 3277: loss = 4.5899 (1.220 sec/step)\n","I0830 17:08:06.911139 139622356395904 learning.py:507] global step 3278: loss = 4.0245 (1.237 sec/step)\n","I0830 17:08:08.098765 139622356395904 learning.py:507] global step 3279: loss = 3.3517 (1.186 sec/step)\n","I0830 17:08:09.303423 139622356395904 learning.py:507] global step 3280: loss = 4.1110 (1.203 sec/step)\n","I0830 17:08:10.530970 139622356395904 learning.py:507] global step 3281: loss = 3.1849 (1.226 sec/step)\n","I0830 17:08:11.769102 139622356395904 learning.py:507] global step 3282: loss = 4.2826 (1.236 sec/step)\n","I0830 17:08:12.984328 139622356395904 learning.py:507] global step 3283: loss = 4.4459 (1.213 sec/step)\n","I0830 17:08:14.182911 139622356395904 learning.py:507] global step 3284: loss = 3.5823 (1.197 sec/step)\n","I0830 17:08:15.413083 139622356395904 learning.py:507] global step 3285: loss = 4.7000 (1.228 sec/step)\n","I0830 17:08:16.640766 139622356395904 learning.py:507] global step 3286: loss = 3.9440 (1.226 sec/step)\n","I0830 17:08:17.866941 139622356395904 learning.py:507] global step 3287: loss = 3.5345 (1.224 sec/step)\n","I0830 17:08:19.120686 139622356395904 learning.py:507] global step 3288: loss = 3.6283 (1.252 sec/step)\n","I0830 17:08:20.353909 139622356395904 learning.py:507] global step 3289: loss = 3.2591 (1.232 sec/step)\n","I0830 17:08:21.545012 139622356395904 learning.py:507] global step 3290: loss = 4.3897 (1.189 sec/step)\n","I0830 17:08:22.801671 139622356395904 learning.py:507] global step 3291: loss = 3.7180 (1.254 sec/step)\n","I0830 17:08:24.037723 139622356395904 learning.py:507] global step 3292: loss = 4.2739 (1.234 sec/step)\n","I0830 17:08:25.290839 139622356395904 learning.py:507] global step 3293: loss = 3.7865 (1.248 sec/step)\n","I0830 17:08:26.495936 139622356395904 learning.py:507] global step 3294: loss = 3.2000 (1.203 sec/step)\n","I0830 17:08:27.708437 139622356395904 learning.py:507] global step 3295: loss = 3.8379 (1.210 sec/step)\n","I0830 17:08:28.925176 139622356395904 learning.py:507] global step 3296: loss = 4.8359 (1.215 sec/step)\n","I0830 17:08:30.185138 139622356395904 learning.py:507] global step 3297: loss = 3.7018 (1.258 sec/step)\n","I0830 17:08:31.402656 139622356395904 learning.py:507] global step 3298: loss = 4.5015 (1.216 sec/step)\n","I0830 17:08:32.611471 139622356395904 learning.py:507] global step 3299: loss = 4.3418 (1.207 sec/step)\n","I0830 17:08:33.851204 139622356395904 learning.py:507] global step 3300: loss = 3.6486 (1.238 sec/step)\n","I0830 17:08:35.066282 139622356395904 learning.py:507] global step 3301: loss = 3.2312 (1.213 sec/step)\n","I0830 17:08:36.288256 139622356395904 learning.py:507] global step 3302: loss = 3.6903 (1.220 sec/step)\n","I0830 17:08:37.518075 139622356395904 learning.py:507] global step 3303: loss = 3.6488 (1.228 sec/step)\n","I0830 17:08:38.723241 139622356395904 learning.py:507] global step 3304: loss = 3.1430 (1.203 sec/step)\n","I0830 17:08:39.974503 139622356395904 learning.py:507] global step 3305: loss = 3.2106 (1.250 sec/step)\n","I0830 17:08:41.202656 139622356395904 learning.py:507] global step 3306: loss = 3.6521 (1.226 sec/step)\n","I0830 17:08:42.392168 139622356395904 learning.py:507] global step 3307: loss = 3.0145 (1.188 sec/step)\n","I0830 17:08:43.642038 139622356395904 learning.py:507] global step 3308: loss = 4.0317 (1.248 sec/step)\n","I0830 17:08:44.886981 139622356395904 learning.py:507] global step 3309: loss = 3.1206 (1.243 sec/step)\n","I0830 17:08:46.144492 139622356395904 learning.py:507] global step 3310: loss = 3.1823 (1.256 sec/step)\n","I0830 17:08:47.415277 139622356395904 learning.py:507] global step 3311: loss = 3.6079 (1.269 sec/step)\n","I0830 17:08:48.620341 139622356395904 learning.py:507] global step 3312: loss = 3.5518 (1.203 sec/step)\n","I0830 17:08:49.847856 139622356395904 learning.py:507] global step 3313: loss = 3.4335 (1.225 sec/step)\n","I0830 17:08:51.074518 139622356395904 learning.py:507] global step 3314: loss = 4.5733 (1.225 sec/step)\n","I0830 17:08:52.321388 139622356395904 learning.py:507] global step 3315: loss = 3.6366 (1.245 sec/step)\n","I0830 17:08:53.525003 139622356395904 learning.py:507] global step 3316: loss = 3.5707 (1.202 sec/step)\n","I0830 17:08:54.756198 139622356395904 learning.py:507] global step 3317: loss = 3.6470 (1.229 sec/step)\n","I0830 17:08:56.013212 139622356395904 learning.py:507] global step 3318: loss = 3.6044 (1.255 sec/step)\n","I0830 17:08:57.227818 139622356395904 learning.py:507] global step 3319: loss = 3.7705 (1.213 sec/step)\n","I0830 17:08:58.466819 139622356395904 learning.py:507] global step 3320: loss = 3.7459 (1.237 sec/step)\n","I0830 17:08:59.723739 139622356395904 learning.py:507] global step 3321: loss = 4.2903 (1.255 sec/step)\n","I0830 17:09:00.942130 139622356395904 learning.py:507] global step 3322: loss = 3.5427 (1.217 sec/step)\n","I0830 17:09:02.146014 139622356395904 learning.py:507] global step 3323: loss = 4.3613 (1.202 sec/step)\n","I0830 17:09:03.383072 139622356395904 learning.py:507] global step 3324: loss = 4.1882 (1.235 sec/step)\n","I0830 17:09:04.624021 139622356395904 learning.py:507] global step 3325: loss = 4.2100 (1.239 sec/step)\n","I0830 17:09:05.865184 139622356395904 learning.py:507] global step 3326: loss = 3.2718 (1.239 sec/step)\n","I0830 17:09:07.077562 139622356395904 learning.py:507] global step 3327: loss = 3.3661 (1.211 sec/step)\n","I0830 17:09:08.286560 139622356395904 learning.py:507] global step 3328: loss = 3.8780 (1.207 sec/step)\n","I0830 17:09:09.483954 139622356395904 learning.py:507] global step 3329: loss = 3.8671 (1.195 sec/step)\n","I0830 17:09:10.759538 139622356395904 learning.py:507] global step 3330: loss = 4.0113 (1.274 sec/step)\n","I0830 17:09:11.988042 139622356395904 learning.py:507] global step 3331: loss = 4.3939 (1.226 sec/step)\n","I0830 17:09:13.216943 139622356395904 learning.py:507] global step 3332: loss = 3.4864 (1.227 sec/step)\n","I0830 17:09:14.426939 139622356395904 learning.py:507] global step 3333: loss = 3.9763 (1.208 sec/step)\n","I0830 17:09:15.663186 139622356395904 learning.py:507] global step 3334: loss = 4.1730 (1.234 sec/step)\n","I0830 17:09:16.899212 139622356395904 learning.py:507] global step 3335: loss = 4.1724 (1.234 sec/step)\n","I0830 17:09:18.099701 139622356395904 learning.py:507] global step 3336: loss = 5.0974 (1.198 sec/step)\n","I0830 17:09:19.316276 139622356395904 learning.py:507] global step 3337: loss = 4.1182 (1.215 sec/step)\n","I0830 17:09:20.553155 139622356395904 learning.py:507] global step 3338: loss = 5.2196 (1.235 sec/step)\n","I0830 17:09:21.817970 139622356395904 learning.py:507] global step 3339: loss = 3.5988 (1.263 sec/step)\n","I0830 17:09:23.045486 139622356395904 learning.py:507] global step 3340: loss = 4.7047 (1.225 sec/step)\n","I0830 17:09:24.258147 139622356395904 learning.py:507] global step 3341: loss = 3.3841 (1.211 sec/step)\n","I0830 17:09:25.503271 139622356395904 learning.py:507] global step 3342: loss = 3.4399 (1.244 sec/step)\n","I0830 17:09:26.717261 139622356395904 learning.py:507] global step 3343: loss = 3.8194 (1.212 sec/step)\n","I0830 17:09:27.955331 139622356395904 learning.py:507] global step 3344: loss = 4.0764 (1.236 sec/step)\n","I0830 17:09:29.187571 139622356395904 learning.py:507] global step 3345: loss = 4.0747 (1.231 sec/step)\n","I0830 17:09:30.399263 139622356395904 learning.py:507] global step 3346: loss = 3.8019 (1.210 sec/step)\n","I0830 17:09:31.625115 139622356395904 learning.py:507] global step 3347: loss = 3.4765 (1.223 sec/step)\n","I0830 17:09:32.832831 139622356395904 learning.py:507] global step 3348: loss = 3.9978 (1.205 sec/step)\n","I0830 17:09:34.101397 139622356395904 learning.py:507] global step 3349: loss = 4.4977 (1.267 sec/step)\n","I0830 17:09:35.322574 139622356395904 learning.py:507] global step 3350: loss = 4.3274 (1.219 sec/step)\n","I0830 17:09:36.542701 139622356395904 learning.py:507] global step 3351: loss = 3.2860 (1.218 sec/step)\n","I0830 17:09:37.761720 139622356395904 learning.py:507] global step 3352: loss = 3.2456 (1.217 sec/step)\n","I0830 17:09:39.003421 139622356395904 learning.py:507] global step 3353: loss = 4.7511 (1.240 sec/step)\n","I0830 17:09:40.230701 139622356395904 learning.py:507] global step 3354: loss = 4.0124 (1.226 sec/step)\n","I0830 17:09:41.461819 139622356395904 learning.py:507] global step 3355: loss = 4.7913 (1.229 sec/step)\n","I0830 17:09:42.671512 139622356395904 learning.py:507] global step 3356: loss = 3.9015 (1.208 sec/step)\n","I0830 17:09:43.880531 139622356395904 learning.py:507] global step 3357: loss = 3.8823 (1.207 sec/step)\n","I0830 17:09:45.077208 139622356395904 learning.py:507] global step 3358: loss = 3.2293 (1.195 sec/step)\n","I0830 17:09:46.294990 139622356395904 learning.py:507] global step 3359: loss = 3.5792 (1.216 sec/step)\n","I0830 17:09:47.587863 139622356395904 learning.py:507] global step 3360: loss = 3.8662 (1.291 sec/step)\n","I0830 17:09:48.837468 139622356395904 learning.py:507] global step 3361: loss = 3.8399 (1.248 sec/step)\n","I0830 17:09:50.055934 139622356395904 learning.py:507] global step 3362: loss = 3.5233 (1.215 sec/step)\n","I0830 17:09:51.299083 139622356395904 learning.py:507] global step 3363: loss = 4.2012 (1.241 sec/step)\n","I0830 17:09:52.545366 139622356395904 learning.py:507] global step 3364: loss = 3.7437 (1.245 sec/step)\n","I0830 17:09:53.742507 139622356395904 learning.py:507] global step 3365: loss = 3.6340 (1.195 sec/step)\n","I0830 17:09:54.987602 139622356395904 learning.py:507] global step 3366: loss = 4.3643 (1.243 sec/step)\n","I0830 17:09:56.199306 139622356395904 learning.py:507] global step 3367: loss = 3.3062 (1.209 sec/step)\n","I0830 17:09:57.445809 139622356395904 learning.py:507] global step 3368: loss = 3.7752 (1.245 sec/step)\n","I0830 17:09:58.667720 139622356395904 learning.py:507] global step 3369: loss = 3.7813 (1.220 sec/step)\n","I0830 17:09:59.647105 139619316770560 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n","I0830 17:10:00.368783 139622356395904 learning.py:507] global step 3370: loss = 3.5284 (1.675 sec/step)\n","I0830 17:10:02.782367 139619299985152 supervisor.py:1050] Recording summary at step 3370.\n","I0830 17:10:03.511014 139622356395904 learning.py:507] global step 3371: loss = 3.3598 (3.136 sec/step)\n","I0830 17:10:04.908017 139622356395904 learning.py:507] global step 3372: loss = 3.4243 (1.395 sec/step)\n","I0830 17:10:06.155940 139622356395904 learning.py:507] global step 3373: loss = 3.5261 (1.246 sec/step)\n","I0830 17:10:07.377994 139622356395904 learning.py:507] global step 3374: loss = 3.8527 (1.220 sec/step)\n","I0830 17:10:08.576152 139622356395904 learning.py:507] global step 3375: loss = 4.8810 (1.196 sec/step)\n","I0830 17:10:09.778333 139622356395904 learning.py:507] global step 3376: loss = 4.1425 (1.200 sec/step)\n","I0830 17:10:11.019987 139622356395904 learning.py:507] global step 3377: loss = 4.0276 (1.240 sec/step)\n","I0830 17:10:12.252964 139622356395904 learning.py:507] global step 3378: loss = 4.6146 (1.231 sec/step)\n","I0830 17:10:13.463323 139622356395904 learning.py:507] global step 3379: loss = 3.8538 (1.209 sec/step)\n","I0830 17:10:14.693180 139622356395904 learning.py:507] global step 3380: loss = 3.8494 (1.228 sec/step)\n","I0830 17:10:15.929886 139622356395904 learning.py:507] global step 3381: loss = 3.2270 (1.235 sec/step)\n","I0830 17:10:17.144593 139622356395904 learning.py:507] global step 3382: loss = 3.2790 (1.213 sec/step)\n","I0830 17:10:18.376817 139622356395904 learning.py:507] global step 3383: loss = 5.1147 (1.230 sec/step)\n","I0830 17:10:19.621170 139622356395904 learning.py:507] global step 3384: loss = 4.1075 (1.242 sec/step)\n","I0830 17:10:20.871974 139622356395904 learning.py:507] global step 3385: loss = 4.2285 (1.249 sec/step)\n","I0830 17:10:22.110803 139622356395904 learning.py:507] global step 3386: loss = 3.7471 (1.236 sec/step)\n","I0830 17:10:23.341734 139622356395904 learning.py:507] global step 3387: loss = 3.2195 (1.229 sec/step)\n","I0830 17:10:24.598638 139622356395904 learning.py:507] global step 3388: loss = 4.0876 (1.255 sec/step)\n","I0830 17:10:25.819383 139622356395904 learning.py:507] global step 3389: loss = 3.1314 (1.219 sec/step)\n","I0830 17:10:27.051505 139622356395904 learning.py:507] global step 3390: loss = 3.8756 (1.230 sec/step)\n","I0830 17:10:28.258682 139622356395904 learning.py:507] global step 3391: loss = 4.6991 (1.205 sec/step)\n","I0830 17:10:29.507026 139622356395904 learning.py:507] global step 3392: loss = 3.7953 (1.247 sec/step)\n","I0830 17:10:30.718543 139622356395904 learning.py:507] global step 3393: loss = 4.3608 (1.210 sec/step)\n","I0830 17:10:31.942423 139622356395904 learning.py:507] global step 3394: loss = 3.2382 (1.222 sec/step)\n","I0830 17:10:33.195519 139622356395904 learning.py:507] global step 3395: loss = 3.6182 (1.251 sec/step)\n","I0830 17:10:34.431820 139622356395904 learning.py:507] global step 3396: loss = 3.5350 (1.234 sec/step)\n","I0830 17:10:35.650193 139622356395904 learning.py:507] global step 3397: loss = 3.6745 (1.217 sec/step)\n","I0830 17:10:36.890122 139622356395904 learning.py:507] global step 3398: loss = 4.4141 (1.238 sec/step)\n","I0830 17:10:38.158405 139622356395904 learning.py:507] global step 3399: loss = 4.4213 (1.267 sec/step)\n","I0830 17:10:39.398308 139622356395904 learning.py:507] global step 3400: loss = 6.5484 (1.238 sec/step)\n","I0830 17:10:40.605561 139622356395904 learning.py:507] global step 3401: loss = 4.4614 (1.205 sec/step)\n","I0830 17:10:41.820644 139622356395904 learning.py:507] global step 3402: loss = 3.2905 (1.213 sec/step)\n","I0830 17:10:43.054158 139622356395904 learning.py:507] global step 3403: loss = 4.0146 (1.232 sec/step)\n","I0830 17:10:44.327505 139622356395904 learning.py:507] global step 3404: loss = 4.4090 (1.271 sec/step)\n","I0830 17:10:45.548316 139622356395904 learning.py:507] global step 3405: loss = 4.1982 (1.219 sec/step)\n","I0830 17:10:46.801315 139622356395904 learning.py:507] global step 3406: loss = 3.0250 (1.248 sec/step)\n","I0830 17:10:48.036089 139622356395904 learning.py:507] global step 3407: loss = 3.6479 (1.232 sec/step)\n","I0830 17:10:49.288014 139622356395904 learning.py:507] global step 3408: loss = 4.1544 (1.250 sec/step)\n","I0830 17:10:50.515976 139622356395904 learning.py:507] global step 3409: loss = 3.5202 (1.226 sec/step)\n","I0830 17:10:51.747607 139622356395904 learning.py:507] global step 3410: loss = 3.4083 (1.230 sec/step)\n","I0830 17:10:53.009180 139622356395904 learning.py:507] global step 3411: loss = 3.4525 (1.259 sec/step)\n","I0830 17:10:54.208820 139622356395904 learning.py:507] global step 3412: loss = 5.0295 (1.198 sec/step)\n","I0830 17:10:55.426092 139622356395904 learning.py:507] global step 3413: loss = 4.9828 (1.215 sec/step)\n","I0830 17:10:56.634906 139622356395904 learning.py:507] global step 3414: loss = 3.3640 (1.207 sec/step)\n","I0830 17:10:57.852147 139622356395904 learning.py:507] global step 3415: loss = 4.0455 (1.215 sec/step)\n","I0830 17:10:59.090707 139622356395904 learning.py:507] global step 3416: loss = 3.2617 (1.236 sec/step)\n","I0830 17:11:00.314903 139622356395904 learning.py:507] global step 3417: loss = 3.6472 (1.222 sec/step)\n","I0830 17:11:01.553375 139622356395904 learning.py:507] global step 3418: loss = 2.9360 (1.237 sec/step)\n","I0830 17:11:02.774793 139622356395904 learning.py:507] global step 3419: loss = 4.5823 (1.220 sec/step)\n","I0830 17:11:03.987174 139622356395904 learning.py:507] global step 3420: loss = 3.9062 (1.210 sec/step)\n","I0830 17:11:05.213138 139622356395904 learning.py:507] global step 3421: loss = 4.5181 (1.220 sec/step)\n","I0830 17:11:06.451460 139622356395904 learning.py:507] global step 3422: loss = 4.2325 (1.237 sec/step)\n","I0830 17:11:07.700642 139622356395904 learning.py:507] global step 3423: loss = 3.7802 (1.247 sec/step)\n","I0830 17:11:08.945541 139622356395904 learning.py:507] global step 3424: loss = 3.8509 (1.243 sec/step)\n","I0830 17:11:10.156573 139622356395904 learning.py:507] global step 3425: loss = 3.1185 (1.209 sec/step)\n","I0830 17:11:11.369125 139622356395904 learning.py:507] global step 3426: loss = 3.4649 (1.210 sec/step)\n","I0830 17:11:12.611458 139622356395904 learning.py:507] global step 3427: loss = 4.8197 (1.240 sec/step)\n","I0830 17:11:13.824514 139622356395904 learning.py:507] global step 3428: loss = 5.0372 (1.211 sec/step)\n","I0830 17:11:15.084600 139622356395904 learning.py:507] global step 3429: loss = 3.7241 (1.258 sec/step)\n","I0830 17:11:16.294599 139622356395904 learning.py:507] global step 3430: loss = 5.5308 (1.208 sec/step)\n","I0830 17:11:17.550094 139622356395904 learning.py:507] global step 3431: loss = 4.8782 (1.253 sec/step)\n","I0830 17:11:18.748221 139622356395904 learning.py:507] global step 3432: loss = 3.6667 (1.196 sec/step)\n","I0830 17:11:19.960044 139622356395904 learning.py:507] global step 3433: loss = 4.4121 (1.210 sec/step)\n","I0830 17:11:21.148077 139622356395904 learning.py:507] global step 3434: loss = 4.3764 (1.186 sec/step)\n","I0830 17:11:22.375489 139622356395904 learning.py:507] global step 3435: loss = 3.5784 (1.226 sec/step)\n","I0830 17:11:23.619702 139622356395904 learning.py:507] global step 3436: loss = 3.7911 (1.242 sec/step)\n","I0830 17:11:24.805372 139622356395904 learning.py:507] global step 3437: loss = 3.9561 (1.184 sec/step)\n","I0830 17:11:26.044116 139622356395904 learning.py:507] global step 3438: loss = 3.5568 (1.237 sec/step)\n","I0830 17:11:27.257035 139622356395904 learning.py:507] global step 3439: loss = 3.8749 (1.211 sec/step)\n","I0830 17:11:28.462150 139622356395904 learning.py:507] global step 3440: loss = 4.3912 (1.203 sec/step)\n","I0830 17:11:29.694687 139622356395904 learning.py:507] global step 3441: loss = 3.5783 (1.231 sec/step)\n","I0830 17:11:30.915436 139622356395904 learning.py:507] global step 3442: loss = 3.7647 (1.216 sec/step)\n","I0830 17:11:32.141958 139622356395904 learning.py:507] global step 3443: loss = 3.3966 (1.225 sec/step)\n","I0830 17:11:33.329803 139622356395904 learning.py:507] global step 3444: loss = 4.1795 (1.186 sec/step)\n","I0830 17:11:34.572261 139622356395904 learning.py:507] global step 3445: loss = 3.3113 (1.241 sec/step)\n","I0830 17:11:35.779285 139622356395904 learning.py:507] global step 3446: loss = 3.3574 (1.205 sec/step)\n","I0830 17:11:37.019179 139622356395904 learning.py:507] global step 3447: loss = 4.5033 (1.238 sec/step)\n","I0830 17:11:38.215607 139622356395904 learning.py:507] global step 3448: loss = 4.8902 (1.195 sec/step)\n","I0830 17:11:39.445204 139622356395904 learning.py:507] global step 3449: loss = 3.7863 (1.228 sec/step)\n","I0830 17:11:40.670743 139622356395904 learning.py:507] global step 3450: loss = 4.6320 (1.223 sec/step)\n","I0830 17:11:41.928405 139622356395904 learning.py:507] global step 3451: loss = 3.3973 (1.255 sec/step)\n","I0830 17:11:43.163760 139622356395904 learning.py:507] global step 3452: loss = 4.9316 (1.234 sec/step)\n","I0830 17:11:44.355247 139622356395904 learning.py:507] global step 3453: loss = 3.8874 (1.189 sec/step)\n","I0830 17:11:45.554104 139622356395904 learning.py:507] global step 3454: loss = 3.8236 (1.197 sec/step)\n","I0830 17:11:46.765699 139622356395904 learning.py:507] global step 3455: loss = 3.5788 (1.209 sec/step)\n","I0830 17:11:47.980709 139622356395904 learning.py:507] global step 3456: loss = 5.2051 (1.213 sec/step)\n","I0830 17:11:49.198065 139622356395904 learning.py:507] global step 3457: loss = 3.7729 (1.216 sec/step)\n","I0830 17:11:50.408776 139622356395904 learning.py:507] global step 3458: loss = 5.1318 (1.209 sec/step)\n","I0830 17:11:51.613790 139622356395904 learning.py:507] global step 3459: loss = 3.9933 (1.203 sec/step)\n","I0830 17:11:52.829409 139622356395904 learning.py:507] global step 3460: loss = 3.7381 (1.214 sec/step)\n","I0830 17:11:54.032357 139622356395904 learning.py:507] global step 3461: loss = 3.1215 (1.201 sec/step)\n","I0830 17:11:55.226244 139622356395904 learning.py:507] global step 3462: loss = 4.7212 (1.192 sec/step)\n","I0830 17:11:56.432511 139622356395904 learning.py:507] global step 3463: loss = 4.8935 (1.205 sec/step)\n","I0830 17:11:57.667436 139622356395904 learning.py:507] global step 3464: loss = 4.6883 (1.233 sec/step)\n","I0830 17:11:58.862547 139622356395904 learning.py:507] global step 3465: loss = 4.5192 (1.193 sec/step)\n","I0830 17:12:00.186193 139622356395904 learning.py:507] global step 3466: loss = 3.6465 (1.255 sec/step)\n","I0830 17:12:02.099415 139619299985152 supervisor.py:1050] Recording summary at step 3467.\n","I0830 17:12:02.119637 139622356395904 learning.py:507] global step 3467: loss = 3.3587 (1.927 sec/step)\n","I0830 17:12:03.352908 139622356395904 learning.py:507] global step 3468: loss = 4.0094 (1.231 sec/step)\n","I0830 17:12:04.591802 139622356395904 learning.py:507] global step 3469: loss = 3.2229 (1.237 sec/step)\n","I0830 17:12:05.828248 139622356395904 learning.py:507] global step 3470: loss = 3.6025 (1.234 sec/step)\n","I0830 17:12:07.045466 139622356395904 learning.py:507] global step 3471: loss = 4.0043 (1.215 sec/step)\n","I0830 17:12:08.259034 139622356395904 learning.py:507] global step 3472: loss = 3.3281 (1.212 sec/step)\n","I0830 17:12:09.490914 139622356395904 learning.py:507] global step 3473: loss = 4.2871 (1.230 sec/step)\n","I0830 17:12:10.683356 139622356395904 learning.py:507] global step 3474: loss = 3.6459 (1.191 sec/step)\n","I0830 17:12:11.909137 139622356395904 learning.py:507] global step 3475: loss = 4.5020 (1.224 sec/step)\n","I0830 17:12:13.144119 139622356395904 learning.py:507] global step 3476: loss = 3.4914 (1.233 sec/step)\n","I0830 17:12:14.355066 139622356395904 learning.py:507] global step 3477: loss = 4.2050 (1.209 sec/step)\n","I0830 17:12:15.574136 139622356395904 learning.py:507] global step 3478: loss = 3.6429 (1.217 sec/step)\n","I0830 17:12:16.803821 139622356395904 learning.py:507] global step 3479: loss = 4.7547 (1.228 sec/step)\n","I0830 17:12:18.017642 139622356395904 learning.py:507] global step 3480: loss = 5.4333 (1.212 sec/step)\n","I0830 17:12:19.251842 139622356395904 learning.py:507] global step 3481: loss = 4.0513 (1.232 sec/step)\n","I0830 17:12:20.445214 139622356395904 learning.py:507] global step 3482: loss = 3.3839 (1.191 sec/step)\n","I0830 17:12:21.672781 139622356395904 learning.py:507] global step 3483: loss = 3.1536 (1.226 sec/step)\n","I0830 17:12:22.874588 139622356395904 learning.py:507] global step 3484: loss = 3.3948 (1.200 sec/step)\n","I0830 17:12:24.070688 139622356395904 learning.py:507] global step 3485: loss = 3.1035 (1.194 sec/step)\n","I0830 17:12:25.319294 139622356395904 learning.py:507] global step 3486: loss = 3.7565 (1.247 sec/step)\n","I0830 17:12:26.519204 139622356395904 learning.py:507] global step 3487: loss = 3.9280 (1.198 sec/step)\n","I0830 17:12:27.725017 139622356395904 learning.py:507] global step 3488: loss = 3.7697 (1.204 sec/step)\n","I0830 17:12:28.921990 139622356395904 learning.py:507] global step 3489: loss = 3.6890 (1.195 sec/step)\n","I0830 17:12:30.134220 139622356395904 learning.py:507] global step 3490: loss = 2.9654 (1.210 sec/step)\n","I0830 17:12:31.376704 139622356395904 learning.py:507] global step 3491: loss = 3.8773 (1.241 sec/step)\n","I0830 17:12:32.599127 139622356395904 learning.py:507] global step 3492: loss = 3.3850 (1.221 sec/step)\n","I0830 17:12:33.831513 139622356395904 learning.py:507] global step 3493: loss = 4.0483 (1.230 sec/step)\n","I0830 17:12:35.057580 139622356395904 learning.py:507] global step 3494: loss = 6.2987 (1.224 sec/step)\n","I0830 17:12:36.250720 139622356395904 learning.py:507] global step 3495: loss = 4.8224 (1.191 sec/step)\n","I0830 17:12:37.454415 139622356395904 learning.py:507] global step 3496: loss = 3.9333 (1.201 sec/step)\n","I0830 17:12:38.701816 139622356395904 learning.py:507] global step 3497: loss = 4.0642 (1.246 sec/step)\n","I0830 17:12:39.992131 139622356395904 learning.py:507] global step 3498: loss = 3.8507 (1.289 sec/step)\n","I0830 17:12:41.196511 139622356395904 learning.py:507] global step 3499: loss = 3.5164 (1.202 sec/step)\n","I0830 17:12:42.437040 139622356395904 learning.py:507] global step 3500: loss = 4.2995 (1.239 sec/step)\n","I0830 17:12:43.665543 139622356395904 learning.py:507] global step 3501: loss = 4.1790 (1.227 sec/step)\n","I0830 17:12:44.900263 139622356395904 learning.py:507] global step 3502: loss = 3.2263 (1.233 sec/step)\n","I0830 17:12:46.130075 139622356395904 learning.py:507] global step 3503: loss = 4.2327 (1.228 sec/step)\n","I0830 17:12:47.347288 139622356395904 learning.py:507] global step 3504: loss = 4.0495 (1.215 sec/step)\n","I0830 17:12:48.580041 139622356395904 learning.py:507] global step 3505: loss = 4.9679 (1.231 sec/step)\n","I0830 17:12:49.797306 139622356395904 learning.py:507] global step 3506: loss = 3.7242 (1.215 sec/step)\n","I0830 17:12:51.057037 139622356395904 learning.py:507] global step 3507: loss = 3.4072 (1.258 sec/step)\n","I0830 17:12:52.272905 139622356395904 learning.py:507] global step 3508: loss = 3.8459 (1.214 sec/step)\n","I0830 17:12:53.478622 139622356395904 learning.py:507] global step 3509: loss = 3.0933 (1.204 sec/step)\n","I0830 17:12:54.713620 139622356395904 learning.py:507] global step 3510: loss = 4.9585 (1.233 sec/step)\n","I0830 17:12:55.909100 139622356395904 learning.py:507] global step 3511: loss = 3.7563 (1.194 sec/step)\n","I0830 17:12:57.147932 139622356395904 learning.py:507] global step 3512: loss = 4.7168 (1.237 sec/step)\n","I0830 17:12:58.352596 139622356395904 learning.py:507] global step 3513: loss = 3.8926 (1.203 sec/step)\n","I0830 17:12:59.604092 139622356395904 learning.py:507] global step 3514: loss = 3.6999 (1.250 sec/step)\n","I0830 17:13:00.827493 139622356395904 learning.py:507] global step 3515: loss = 3.9898 (1.221 sec/step)\n","I0830 17:13:02.032517 139622356395904 learning.py:507] global step 3516: loss = 4.2761 (1.203 sec/step)\n","I0830 17:13:03.259490 139622356395904 learning.py:507] global step 3517: loss = 4.2749 (1.225 sec/step)\n","I0830 17:13:04.451479 139622356395904 learning.py:507] global step 3518: loss = 4.1675 (1.190 sec/step)\n","I0830 17:13:05.653888 139622356395904 learning.py:507] global step 3519: loss = 3.9857 (1.200 sec/step)\n","I0830 17:13:06.905876 139622356395904 learning.py:507] global step 3520: loss = 4.4984 (1.250 sec/step)\n","I0830 17:13:08.115130 139622356395904 learning.py:507] global step 3521: loss = 3.2808 (1.207 sec/step)\n","I0830 17:13:09.333204 139622356395904 learning.py:507] global step 3522: loss = 3.9650 (1.216 sec/step)\n","I0830 17:13:10.576151 139622356395904 learning.py:507] global step 3523: loss = 4.2403 (1.241 sec/step)\n","I0830 17:13:11.817513 139622356395904 learning.py:507] global step 3524: loss = 4.1510 (1.240 sec/step)\n","I0830 17:13:13.009595 139622356395904 learning.py:507] global step 3525: loss = 4.3611 (1.190 sec/step)\n","I0830 17:13:14.251469 139622356395904 learning.py:507] global step 3526: loss = 5.0762 (1.240 sec/step)\n","I0830 17:13:15.440634 139622356395904 learning.py:507] global step 3527: loss = 4.4307 (1.187 sec/step)\n","I0830 17:13:16.672268 139622356395904 learning.py:507] global step 3528: loss = 3.4904 (1.230 sec/step)\n","I0830 17:13:17.873036 139622356395904 learning.py:507] global step 3529: loss = 3.1363 (1.199 sec/step)\n","I0830 17:13:19.104639 139622356395904 learning.py:507] global step 3530: loss = 3.6203 (1.230 sec/step)\n","I0830 17:13:20.306574 139622356395904 learning.py:507] global step 3531: loss = 3.4087 (1.200 sec/step)\n","I0830 17:13:21.550996 139622356395904 learning.py:507] global step 3532: loss = 4.0768 (1.243 sec/step)\n","I0830 17:13:22.776672 139622356395904 learning.py:507] global step 3533: loss = 3.9764 (1.224 sec/step)\n","I0830 17:13:23.992043 139622356395904 learning.py:507] global step 3534: loss = 3.7354 (1.213 sec/step)\n","I0830 17:13:25.250087 139622356395904 learning.py:507] global step 3535: loss = 3.1106 (1.255 sec/step)\n","I0830 17:13:26.468037 139622356395904 learning.py:507] global step 3536: loss = 3.9039 (1.216 sec/step)\n","I0830 17:13:27.685017 139622356395904 learning.py:507] global step 3537: loss = 3.4033 (1.215 sec/step)\n","I0830 17:13:28.905192 139622356395904 learning.py:507] global step 3538: loss = 3.6248 (1.218 sec/step)\n","I0830 17:13:30.121784 139622356395904 learning.py:507] global step 3539: loss = 3.7849 (1.215 sec/step)\n","I0830 17:13:31.360466 139622356395904 learning.py:507] global step 3540: loss = 3.8432 (1.237 sec/step)\n","I0830 17:13:32.601127 139622356395904 learning.py:507] global step 3541: loss = 3.4145 (1.239 sec/step)\n","I0830 17:13:33.811843 139622356395904 learning.py:507] global step 3542: loss = 3.2886 (1.209 sec/step)\n","I0830 17:13:35.043036 139622356395904 learning.py:507] global step 3543: loss = 3.2759 (1.229 sec/step)\n","I0830 17:13:36.271342 139622356395904 learning.py:507] global step 3544: loss = 4.0396 (1.226 sec/step)\n","I0830 17:13:37.472730 139622356395904 learning.py:507] global step 3545: loss = 4.3727 (1.199 sec/step)\n","I0830 17:13:38.668692 139622356395904 learning.py:507] global step 3546: loss = 3.5231 (1.194 sec/step)\n","I0830 17:13:39.861701 139622356395904 learning.py:507] global step 3547: loss = 4.1430 (1.191 sec/step)\n","I0830 17:13:41.093114 139622356395904 learning.py:507] global step 3548: loss = 4.0174 (1.229 sec/step)\n","I0830 17:13:42.334872 139622356395904 learning.py:507] global step 3549: loss = 3.5448 (1.240 sec/step)\n","I0830 17:13:43.539636 139622356395904 learning.py:507] global step 3550: loss = 3.4077 (1.203 sec/step)\n","I0830 17:13:44.774347 139622356395904 learning.py:507] global step 3551: loss = 3.4841 (1.233 sec/step)\n","I0830 17:13:46.003760 139622356395904 learning.py:507] global step 3552: loss = 3.1403 (1.227 sec/step)\n","I0830 17:13:47.195999 139622356395904 learning.py:507] global step 3553: loss = 3.2752 (1.190 sec/step)\n","I0830 17:13:48.451197 139622356395904 learning.py:507] global step 3554: loss = 3.0727 (1.253 sec/step)\n","I0830 17:13:49.684150 139622356395904 learning.py:507] global step 3555: loss = 4.3559 (1.231 sec/step)\n","I0830 17:13:50.908093 139622356395904 learning.py:507] global step 3556: loss = 4.4757 (1.222 sec/step)\n","I0830 17:13:52.152081 139622356395904 learning.py:507] global step 3557: loss = 4.2579 (1.242 sec/step)\n","I0830 17:13:53.385862 139622356395904 learning.py:507] global step 3558: loss = 3.1565 (1.232 sec/step)\n","I0830 17:13:54.604411 139622356395904 learning.py:507] global step 3559: loss = 3.1341 (1.217 sec/step)\n","I0830 17:13:55.822571 139622356395904 learning.py:507] global step 3560: loss = 3.9657 (1.216 sec/step)\n","I0830 17:13:57.060397 139622356395904 learning.py:507] global step 3561: loss = 2.8134 (1.236 sec/step)\n","I0830 17:13:58.295573 139622356395904 learning.py:507] global step 3562: loss = 3.4082 (1.233 sec/step)\n","I0830 17:13:59.525533 139622356395904 learning.py:507] global step 3563: loss = 4.5786 (1.228 sec/step)\n","I0830 17:14:01.296876 139622356395904 learning.py:507] global step 3564: loss = 3.9262 (1.705 sec/step)\n","I0830 17:14:02.118185 139619299985152 supervisor.py:1050] Recording summary at step 3564.\n","I0830 17:14:02.895950 139622356395904 learning.py:507] global step 3565: loss = 4.8613 (1.590 sec/step)\n","I0830 17:14:04.091396 139622356395904 learning.py:507] global step 3566: loss = 3.6283 (1.193 sec/step)\n","I0830 17:14:05.326424 139622356395904 learning.py:507] global step 3567: loss = 3.6039 (1.233 sec/step)\n","I0830 17:14:06.540935 139622356395904 learning.py:507] global step 3568: loss = 3.8968 (1.210 sec/step)\n","I0830 17:14:07.779311 139622356395904 learning.py:507] global step 3569: loss = 3.0731 (1.236 sec/step)\n","I0830 17:14:09.039861 139622356395904 learning.py:507] global step 3570: loss = 3.8100 (1.259 sec/step)\n","I0830 17:14:10.288794 139622356395904 learning.py:507] global step 3571: loss = 3.7196 (1.247 sec/step)\n","I0830 17:14:11.488497 139622356395904 learning.py:507] global step 3572: loss = 4.2098 (1.198 sec/step)\n","I0830 17:14:12.707311 139622356395904 learning.py:507] global step 3573: loss = 3.2158 (1.214 sec/step)\n","I0830 17:14:13.959163 139622356395904 learning.py:507] global step 3574: loss = 4.6828 (1.250 sec/step)\n","I0830 17:14:15.185811 139622356395904 learning.py:507] global step 3575: loss = 4.5064 (1.225 sec/step)\n","I0830 17:14:16.416104 139622356395904 learning.py:507] global step 3576: loss = 3.4910 (1.229 sec/step)\n","I0830 17:14:17.623660 139622356395904 learning.py:507] global step 3577: loss = 3.2239 (1.206 sec/step)\n","I0830 17:14:18.846987 139622356395904 learning.py:507] global step 3578: loss = 3.5770 (1.221 sec/step)\n","I0830 17:14:20.032196 139622356395904 learning.py:507] global step 3579: loss = 3.9985 (1.183 sec/step)\n","I0830 17:14:21.233028 139622356395904 learning.py:507] global step 3580: loss = 2.9851 (1.199 sec/step)\n","I0830 17:14:22.449540 139622356395904 learning.py:507] global step 3581: loss = 3.4041 (1.215 sec/step)\n","I0830 17:14:23.696961 139622356395904 learning.py:507] global step 3582: loss = 3.8147 (1.246 sec/step)\n","I0830 17:14:24.987838 139622356395904 learning.py:507] global step 3583: loss = 3.7024 (1.289 sec/step)\n","I0830 17:14:26.183574 139622356395904 learning.py:507] global step 3584: loss = 4.4840 (1.194 sec/step)\n","I0830 17:14:27.394145 139622356395904 learning.py:507] global step 3585: loss = 3.4570 (1.209 sec/step)\n","I0830 17:14:28.612809 139622356395904 learning.py:507] global step 3586: loss = 3.2644 (1.217 sec/step)\n","I0830 17:14:29.812045 139622356395904 learning.py:507] global step 3587: loss = 3.5519 (1.198 sec/step)\n","I0830 17:14:31.033818 139622356395904 learning.py:507] global step 3588: loss = 3.3057 (1.220 sec/step)\n","I0830 17:14:32.265651 139622356395904 learning.py:507] global step 3589: loss = 3.8164 (1.230 sec/step)\n","I0830 17:14:33.497902 139622356395904 learning.py:507] global step 3590: loss = 3.7982 (1.230 sec/step)\n","I0830 17:14:34.744614 139622356395904 learning.py:507] global step 3591: loss = 3.4867 (1.245 sec/step)\n","I0830 17:14:35.953542 139622356395904 learning.py:507] global step 3592: loss = 4.3312 (1.207 sec/step)\n","I0830 17:14:37.155222 139622356395904 learning.py:507] global step 3593: loss = 3.7589 (1.200 sec/step)\n","I0830 17:14:38.370797 139622356395904 learning.py:507] global step 3594: loss = 4.3048 (1.214 sec/step)\n","I0830 17:14:39.568501 139622356395904 learning.py:507] global step 3595: loss = 5.1278 (1.196 sec/step)\n","I0830 17:14:40.790825 139622356395904 learning.py:507] global step 3596: loss = 4.1280 (1.220 sec/step)\n","I0830 17:14:42.002166 139622356395904 learning.py:507] global step 3597: loss = 3.5221 (1.209 sec/step)\n","I0830 17:14:43.191826 139622356395904 learning.py:507] global step 3598: loss = 3.6670 (1.188 sec/step)\n","I0830 17:14:44.394258 139622356395904 learning.py:507] global step 3599: loss = 4.6349 (1.200 sec/step)\n","I0830 17:14:45.602906 139622356395904 learning.py:507] global step 3600: loss = 4.2460 (1.207 sec/step)\n","I0830 17:14:46.815207 139622356395904 learning.py:507] global step 3601: loss = 4.5833 (1.210 sec/step)\n","I0830 17:14:48.010612 139622356395904 learning.py:507] global step 3602: loss = 3.3154 (1.194 sec/step)\n","I0830 17:14:49.276692 139622356395904 learning.py:507] global step 3603: loss = 3.0956 (1.264 sec/step)\n","I0830 17:14:50.522203 139622356395904 learning.py:507] global step 3604: loss = 3.2134 (1.244 sec/step)\n","I0830 17:14:51.736032 139622356395904 learning.py:507] global step 3605: loss = 4.1578 (1.212 sec/step)\n","I0830 17:14:52.954838 139622356395904 learning.py:507] global step 3606: loss = 3.7337 (1.217 sec/step)\n","I0830 17:14:54.152415 139622356395904 learning.py:507] global step 3607: loss = 4.2315 (1.192 sec/step)\n","I0830 17:14:55.375373 139622356395904 learning.py:507] global step 3608: loss = 3.5202 (1.221 sec/step)\n","I0830 17:14:56.612824 139622356395904 learning.py:507] global step 3609: loss = 3.3678 (1.236 sec/step)\n","I0830 17:14:57.861341 139622356395904 learning.py:507] global step 3610: loss = 3.6509 (1.247 sec/step)\n","I0830 17:14:59.109843 139622356395904 learning.py:507] global step 3611: loss = 3.8515 (1.246 sec/step)\n","I0830 17:15:00.302696 139622356395904 learning.py:507] global step 3612: loss = 3.2448 (1.191 sec/step)\n","I0830 17:15:01.514563 139622356395904 learning.py:507] global step 3613: loss = 3.0043 (1.210 sec/step)\n","I0830 17:15:02.713810 139622356395904 learning.py:507] global step 3614: loss = 4.1922 (1.197 sec/step)\n","I0830 17:15:03.948789 139622356395904 learning.py:507] global step 3615: loss = 3.8267 (1.233 sec/step)\n","I0830 17:15:05.127482 139622356395904 learning.py:507] global step 3616: loss = 4.0058 (1.177 sec/step)\n","I0830 17:15:06.345820 139622356395904 learning.py:507] global step 3617: loss = 4.0989 (1.216 sec/step)\n","I0830 17:15:07.560551 139622356395904 learning.py:507] global step 3618: loss = 5.0200 (1.213 sec/step)\n","I0830 17:15:08.746678 139622356395904 learning.py:507] global step 3619: loss = 3.8620 (1.184 sec/step)\n","I0830 17:15:09.976539 139622356395904 learning.py:507] global step 3620: loss = 5.6244 (1.228 sec/step)\n","I0830 17:15:11.185448 139622356395904 learning.py:507] global step 3621: loss = 4.7245 (1.207 sec/step)\n","I0830 17:15:12.408251 139622356395904 learning.py:507] global step 3622: loss = 3.2940 (1.221 sec/step)\n","I0830 17:15:13.605994 139622356395904 learning.py:507] global step 3623: loss = 4.0193 (1.196 sec/step)\n","I0830 17:15:14.847780 139622356395904 learning.py:507] global step 3624: loss = 4.8652 (1.240 sec/step)\n","I0830 17:15:16.073255 139622356395904 learning.py:507] global step 3625: loss = 4.5987 (1.224 sec/step)\n","I0830 17:15:17.265126 139622356395904 learning.py:507] global step 3626: loss = 4.1794 (1.190 sec/step)\n","I0830 17:15:18.483148 139622356395904 learning.py:507] global step 3627: loss = 3.6847 (1.216 sec/step)\n","I0830 17:15:19.719669 139622356395904 learning.py:507] global step 3628: loss = 3.3695 (1.233 sec/step)\n","I0830 17:15:20.929978 139622356395904 learning.py:507] global step 3629: loss = 4.9838 (1.208 sec/step)\n","I0830 17:15:22.142782 139622356395904 learning.py:507] global step 3630: loss = 3.9332 (1.211 sec/step)\n","I0830 17:15:23.379568 139622356395904 learning.py:507] global step 3631: loss = 4.6899 (1.235 sec/step)\n","I0830 17:15:24.595464 139622356395904 learning.py:507] global step 3632: loss = 4.4026 (1.214 sec/step)\n","I0830 17:15:25.787981 139622356395904 learning.py:507] global step 3633: loss = 3.9821 (1.191 sec/step)\n","I0830 17:15:26.998031 139622356395904 learning.py:507] global step 3634: loss = 3.3272 (1.208 sec/step)\n","I0830 17:15:28.209907 139622356395904 learning.py:507] global step 3635: loss = 3.9001 (1.210 sec/step)\n","I0830 17:15:29.440375 139622356395904 learning.py:507] global step 3636: loss = 3.7195 (1.228 sec/step)\n","I0830 17:15:30.661216 139622356395904 learning.py:507] global step 3637: loss = 4.8276 (1.219 sec/step)\n","I0830 17:15:31.907959 139622356395904 learning.py:507] global step 3638: loss = 3.5994 (1.245 sec/step)\n","I0830 17:15:33.142159 139622356395904 learning.py:507] global step 3639: loss = 4.0961 (1.232 sec/step)\n","I0830 17:15:34.317171 139622356395904 learning.py:507] global step 3640: loss = 4.2078 (1.173 sec/step)\n","I0830 17:15:35.546612 139622356395904 learning.py:507] global step 3641: loss = 3.4178 (1.227 sec/step)\n","I0830 17:15:36.777184 139622356395904 learning.py:507] global step 3642: loss = 5.5798 (1.229 sec/step)\n","I0830 17:15:38.009441 139622356395904 learning.py:507] global step 3643: loss = 3.4616 (1.230 sec/step)\n","I0830 17:15:39.205873 139622356395904 learning.py:507] global step 3644: loss = 3.6017 (1.194 sec/step)\n","I0830 17:15:40.397602 139622356395904 learning.py:507] global step 3645: loss = 4.2807 (1.189 sec/step)\n","I0830 17:15:41.629785 139622356395904 learning.py:507] global step 3646: loss = 2.8458 (1.230 sec/step)\n","I0830 17:15:42.842896 139622356395904 learning.py:507] global step 3647: loss = 3.3880 (1.211 sec/step)\n","I0830 17:15:44.070638 139622356395904 learning.py:507] global step 3648: loss = 4.4143 (1.226 sec/step)\n","I0830 17:15:45.293973 139622356395904 learning.py:507] global step 3649: loss = 2.9849 (1.221 sec/step)\n","I0830 17:15:46.483990 139622356395904 learning.py:507] global step 3650: loss = 3.4095 (1.188 sec/step)\n","I0830 17:15:47.704527 139622356395904 learning.py:507] global step 3651: loss = 3.8732 (1.219 sec/step)\n","I0830 17:15:48.936823 139622356395904 learning.py:507] global step 3652: loss = 4.5850 (1.230 sec/step)\n","I0830 17:15:50.136909 139622356395904 learning.py:507] global step 3653: loss = 4.4491 (1.198 sec/step)\n","I0830 17:15:51.361303 139622356395904 learning.py:507] global step 3654: loss = 3.6878 (1.222 sec/step)\n","I0830 17:15:52.601034 139622356395904 learning.py:507] global step 3655: loss = 2.9656 (1.238 sec/step)\n","I0830 17:15:53.773749 139622356395904 learning.py:507] global step 3656: loss = 4.2252 (1.171 sec/step)\n","I0830 17:15:55.020630 139622356395904 learning.py:507] global step 3657: loss = 3.4598 (1.245 sec/step)\n","I0830 17:15:56.218008 139622356395904 learning.py:507] global step 3658: loss = 4.3954 (1.196 sec/step)\n","I0830 17:15:57.431342 139622356395904 learning.py:507] global step 3659: loss = 3.1712 (1.212 sec/step)\n","I0830 17:15:58.662751 139622356395904 learning.py:507] global step 3660: loss = 3.3242 (1.230 sec/step)\n","I0830 17:15:59.889777 139622356395904 learning.py:507] global step 3661: loss = 4.3321 (1.225 sec/step)\n","I0830 17:16:01.820235 139619299985152 supervisor.py:1050] Recording summary at step 3662.\n","I0830 17:16:01.843137 139622356395904 learning.py:507] global step 3662: loss = 4.1965 (1.920 sec/step)\n","I0830 17:16:03.051159 139622356395904 learning.py:507] global step 3663: loss = 3.6438 (1.206 sec/step)\n","I0830 17:16:04.252778 139622356395904 learning.py:507] global step 3664: loss = 3.6976 (1.200 sec/step)\n","I0830 17:16:05.474569 139622356395904 learning.py:507] global step 3665: loss = 5.3512 (1.220 sec/step)\n","I0830 17:16:06.695277 139622356395904 learning.py:507] global step 3666: loss = 3.6680 (1.219 sec/step)\n","I0830 17:16:07.875813 139622356395904 learning.py:507] global step 3667: loss = 4.0480 (1.175 sec/step)\n","I0830 17:16:09.123685 139622356395904 learning.py:507] global step 3668: loss = 3.7470 (1.246 sec/step)\n","I0830 17:16:10.319386 139622356395904 learning.py:507] global step 3669: loss = 4.2278 (1.194 sec/step)\n","I0830 17:16:11.530481 139622356395904 learning.py:507] global step 3670: loss = 3.3764 (1.209 sec/step)\n","I0830 17:16:12.775923 139622356395904 learning.py:507] global step 3671: loss = 3.3720 (1.244 sec/step)\n","I0830 17:16:13.963872 139622356395904 learning.py:507] global step 3672: loss = 3.9186 (1.186 sec/step)\n","I0830 17:16:15.159439 139622356395904 learning.py:507] global step 3673: loss = 4.0170 (1.193 sec/step)\n","I0830 17:16:16.395033 139622356395904 learning.py:507] global step 3674: loss = 4.0316 (1.234 sec/step)\n","I0830 17:16:17.665819 139622356395904 learning.py:507] global step 3675: loss = 4.3504 (1.269 sec/step)\n","I0830 17:16:18.873595 139622356395904 learning.py:507] global step 3676: loss = 3.7319 (1.206 sec/step)\n","I0830 17:16:20.088838 139622356395904 learning.py:507] global step 3677: loss = 2.7425 (1.214 sec/step)\n","I0830 17:16:21.293941 139622356395904 learning.py:507] global step 3678: loss = 3.4433 (1.203 sec/step)\n","I0830 17:16:22.522707 139622356395904 learning.py:507] global step 3679: loss = 3.8360 (1.227 sec/step)\n","I0830 17:16:23.725819 139622356395904 learning.py:507] global step 3680: loss = 3.3934 (1.201 sec/step)\n","I0830 17:16:24.961483 139622356395904 learning.py:507] global step 3681: loss = 3.0540 (1.234 sec/step)\n","I0830 17:16:26.160323 139622356395904 learning.py:507] global step 3682: loss = 3.1964 (1.197 sec/step)\n","I0830 17:16:27.389156 139622356395904 learning.py:507] global step 3683: loss = 3.4337 (1.227 sec/step)\n","I0830 17:16:28.615900 139622356395904 learning.py:507] global step 3684: loss = 3.0131 (1.225 sec/step)\n","I0830 17:16:29.827169 139622356395904 learning.py:507] global step 3685: loss = 3.8347 (1.209 sec/step)\n","I0830 17:16:31.043850 139622356395904 learning.py:507] global step 3686: loss = 3.4653 (1.215 sec/step)\n","I0830 17:16:32.285633 139622356395904 learning.py:507] global step 3687: loss = 3.1833 (1.240 sec/step)\n","I0830 17:16:33.508956 139622356395904 learning.py:507] global step 3688: loss = 3.6178 (1.221 sec/step)\n","I0830 17:16:34.708546 139622356395904 learning.py:507] global step 3689: loss = 3.8190 (1.198 sec/step)\n","I0830 17:16:35.939600 139622356395904 learning.py:507] global step 3690: loss = 4.6153 (1.229 sec/step)\n","I0830 17:16:37.132945 139622356395904 learning.py:507] global step 3691: loss = 3.3813 (1.192 sec/step)\n","I0830 17:16:38.375267 139622356395904 learning.py:507] global step 3692: loss = 5.3422 (1.241 sec/step)\n","I0830 17:16:39.601227 139622356395904 learning.py:507] global step 3693: loss = 3.9352 (1.224 sec/step)\n","I0830 17:16:40.845534 139622356395904 learning.py:507] global step 3694: loss = 3.5012 (1.242 sec/step)\n","I0830 17:16:42.040696 139622356395904 learning.py:507] global step 3695: loss = 3.8161 (1.193 sec/step)\n","I0830 17:16:43.220681 139622356395904 learning.py:507] global step 3696: loss = 3.8826 (1.178 sec/step)\n","I0830 17:16:44.442199 139622356395904 learning.py:507] global step 3697: loss = 3.7004 (1.220 sec/step)\n","I0830 17:16:45.674343 139622356395904 learning.py:507] global step 3698: loss = 3.6383 (1.230 sec/step)\n","I0830 17:16:46.870415 139622356395904 learning.py:507] global step 3699: loss = 4.5720 (1.194 sec/step)\n","I0830 17:16:48.088266 139622356395904 learning.py:507] global step 3700: loss = 2.7966 (1.216 sec/step)\n","I0830 17:16:49.293412 139622356395904 learning.py:507] global step 3701: loss = 3.1967 (1.203 sec/step)\n","I0830 17:16:50.515640 139622356395904 learning.py:507] global step 3702: loss = 3.7228 (1.220 sec/step)\n","I0830 17:16:51.768559 139622356395904 learning.py:507] global step 3703: loss = 3.1797 (1.251 sec/step)\n","I0830 17:16:53.028100 139622356395904 learning.py:507] global step 3704: loss = 3.4367 (1.258 sec/step)\n","I0830 17:16:54.257822 139622356395904 learning.py:507] global step 3705: loss = 5.5390 (1.228 sec/step)\n","I0830 17:16:55.493613 139622356395904 learning.py:507] global step 3706: loss = 4.3173 (1.234 sec/step)\n","I0830 17:16:56.701628 139622356395904 learning.py:507] global step 3707: loss = 4.2871 (1.206 sec/step)\n","I0830 17:16:57.935701 139622356395904 learning.py:507] global step 3708: loss = 3.2719 (1.232 sec/step)\n","I0830 17:16:59.181956 139622356395904 learning.py:507] global step 3709: loss = 3.9163 (1.244 sec/step)\n","I0830 17:17:00.409923 139622356395904 learning.py:507] global step 3710: loss = 3.0475 (1.226 sec/step)\n","I0830 17:17:01.708208 139622356395904 learning.py:507] global step 3711: loss = 3.6739 (1.296 sec/step)\n","I0830 17:17:02.962867 139622356395904 learning.py:507] global step 3712: loss = 4.0953 (1.253 sec/step)\n","I0830 17:17:04.205589 139622356395904 learning.py:507] global step 3713: loss = 3.2457 (1.241 sec/step)\n","I0830 17:17:05.455746 139622356395904 learning.py:507] global step 3714: loss = 4.0589 (1.248 sec/step)\n","I0830 17:17:06.686711 139622356395904 learning.py:507] global step 3715: loss = 3.3292 (1.229 sec/step)\n","I0830 17:17:07.932006 139622356395904 learning.py:507] global step 3716: loss = 2.9534 (1.244 sec/step)\n","I0830 17:17:09.160961 139622356395904 learning.py:507] global step 3717: loss = 3.5767 (1.227 sec/step)\n","I0830 17:17:10.390552 139622356395904 learning.py:507] global step 3718: loss = 3.4826 (1.228 sec/step)\n","I0830 17:17:11.643243 139622356395904 learning.py:507] global step 3719: loss = 2.9712 (1.251 sec/step)\n","I0830 17:17:12.875145 139622356395904 learning.py:507] global step 3720: loss = 3.1234 (1.230 sec/step)\n","I0830 17:17:14.098656 139622356395904 learning.py:507] global step 3721: loss = 3.5079 (1.222 sec/step)\n","I0830 17:17:15.353025 139622356395904 learning.py:507] global step 3722: loss = 3.9883 (1.253 sec/step)\n","I0830 17:17:16.540769 139622356395904 learning.py:507] global step 3723: loss = 5.1990 (1.186 sec/step)\n","I0830 17:17:17.774259 139622356395904 learning.py:507] global step 3724: loss = 3.5806 (1.232 sec/step)\n","I0830 17:17:18.994184 139622356395904 learning.py:507] global step 3725: loss = 4.1560 (1.218 sec/step)\n","I0830 17:17:20.220039 139622356395904 learning.py:507] global step 3726: loss = 4.7596 (1.224 sec/step)\n","I0830 17:17:21.416334 139622356395904 learning.py:507] global step 3727: loss = 3.1126 (1.195 sec/step)\n","I0830 17:17:22.643729 139622356395904 learning.py:507] global step 3728: loss = 3.5081 (1.225 sec/step)\n","I0830 17:17:23.854246 139622356395904 learning.py:507] global step 3729: loss = 3.5577 (1.209 sec/step)\n","I0830 17:17:25.050603 139622356395904 learning.py:507] global step 3730: loss = 4.0940 (1.194 sec/step)\n","I0830 17:17:26.246018 139622356395904 learning.py:507] global step 3731: loss = 4.5566 (1.194 sec/step)\n","I0830 17:17:27.455414 139622356395904 learning.py:507] global step 3732: loss = 3.1798 (1.207 sec/step)\n","I0830 17:17:28.659817 139622356395904 learning.py:507] global step 3733: loss = 3.4577 (1.202 sec/step)\n","I0830 17:17:29.864885 139622356395904 learning.py:507] global step 3734: loss = 3.6228 (1.203 sec/step)\n","I0830 17:17:31.070101 139622356395904 learning.py:507] global step 3735: loss = 3.4416 (1.203 sec/step)\n","I0830 17:17:32.306196 139622356395904 learning.py:507] global step 3736: loss = 3.9138 (1.233 sec/step)\n","I0830 17:17:33.539624 139622356395904 learning.py:507] global step 3737: loss = 3.0652 (1.231 sec/step)\n","I0830 17:17:34.754106 139622356395904 learning.py:507] global step 3738: loss = 4.0457 (1.212 sec/step)\n","I0830 17:17:35.995232 139622356395904 learning.py:507] global step 3739: loss = 3.3319 (1.239 sec/step)\n","I0830 17:17:37.213984 139622356395904 learning.py:507] global step 3740: loss = 3.2407 (1.217 sec/step)\n","I0830 17:17:38.460180 139622356395904 learning.py:507] global step 3741: loss = 3.9692 (1.238 sec/step)\n","I0830 17:17:39.674814 139622356395904 learning.py:507] global step 3742: loss = 3.7131 (1.213 sec/step)\n","I0830 17:17:40.867304 139622356395904 learning.py:507] global step 3743: loss = 4.4551 (1.191 sec/step)\n","I0830 17:17:42.094080 139622356395904 learning.py:507] global step 3744: loss = 4.5249 (1.225 sec/step)\n","I0830 17:17:43.323642 139622356395904 learning.py:507] global step 3745: loss = 4.1310 (1.228 sec/step)\n","I0830 17:17:44.557209 139622356395904 learning.py:507] global step 3746: loss = 3.2084 (1.232 sec/step)\n","I0830 17:17:45.814346 139622356395904 learning.py:507] global step 3747: loss = 3.2005 (1.255 sec/step)\n","I0830 17:17:47.013039 139622356395904 learning.py:507] global step 3748: loss = 3.7448 (1.197 sec/step)\n","I0830 17:17:48.247972 139622356395904 learning.py:507] global step 3749: loss = 3.3305 (1.233 sec/step)\n","I0830 17:17:49.478127 139622356395904 learning.py:507] global step 3750: loss = 3.8433 (1.228 sec/step)\n","I0830 17:17:50.678074 139622356395904 learning.py:507] global step 3751: loss = 3.4327 (1.198 sec/step)\n","I0830 17:17:51.917290 139622356395904 learning.py:507] global step 3752: loss = 3.6686 (1.237 sec/step)\n","I0830 17:17:53.109462 139622356395904 learning.py:507] global step 3753: loss = 3.4108 (1.190 sec/step)\n","I0830 17:17:54.326736 139622356395904 learning.py:507] global step 3754: loss = 3.2062 (1.215 sec/step)\n","I0830 17:17:55.527400 139622356395904 learning.py:507] global step 3755: loss = 3.9286 (1.198 sec/step)\n","I0830 17:17:56.791608 139622356395904 learning.py:507] global step 3756: loss = 4.1947 (1.262 sec/step)\n","I0830 17:17:58.052524 139622356395904 learning.py:507] global step 3757: loss = 3.3874 (1.259 sec/step)\n","I0830 17:17:59.265739 139622356395904 learning.py:507] global step 3758: loss = 4.1952 (1.211 sec/step)\n","I0830 17:18:00.531464 139622356395904 learning.py:507] global step 3759: loss = 2.7796 (1.239 sec/step)\n","I0830 17:18:02.314308 139619299985152 supervisor.py:1050] Recording summary at step 3759.\n","I0830 17:18:02.370288 139622356395904 learning.py:507] global step 3760: loss = 3.5600 (1.837 sec/step)\n","I0830 17:18:03.595393 139622356395904 learning.py:507] global step 3761: loss = 3.4645 (1.223 sec/step)\n","I0830 17:18:04.810248 139622356395904 learning.py:507] global step 3762: loss = 3.7326 (1.213 sec/step)\n","I0830 17:18:06.002766 139622356395904 learning.py:507] global step 3763: loss = 4.0608 (1.191 sec/step)\n","I0830 17:18:07.229621 139622356395904 learning.py:507] global step 3764: loss = 3.7917 (1.225 sec/step)\n","I0830 17:18:08.464360 139622356395904 learning.py:507] global step 3765: loss = 3.3832 (1.233 sec/step)\n","I0830 17:18:09.651865 139622356395904 learning.py:507] global step 3766: loss = 4.2308 (1.186 sec/step)\n","I0830 17:18:10.876310 139622356395904 learning.py:507] global step 3767: loss = 2.8630 (1.223 sec/step)\n","I0830 17:18:12.117989 139622356395904 learning.py:507] global step 3768: loss = 3.2606 (1.240 sec/step)\n","I0830 17:18:13.315277 139622356395904 learning.py:507] global step 3769: loss = 3.1981 (1.196 sec/step)\n","I0830 17:18:14.511236 139622356395904 learning.py:507] global step 3770: loss = 3.9651 (1.194 sec/step)\n","I0830 17:18:15.709217 139622356395904 learning.py:507] global step 3771: loss = 3.8249 (1.196 sec/step)\n","I0830 17:18:16.913201 139622356395904 learning.py:507] global step 3772: loss = 3.3869 (1.202 sec/step)\n","I0830 17:18:18.155445 139622356395904 learning.py:507] global step 3773: loss = 4.0886 (1.240 sec/step)\n","I0830 17:18:19.396669 139622356395904 learning.py:507] global step 3774: loss = 4.2542 (1.240 sec/step)\n","I0830 17:18:20.619190 139622356395904 learning.py:507] global step 3775: loss = 3.0610 (1.221 sec/step)\n","I0830 17:18:21.824482 139622356395904 learning.py:507] global step 3776: loss = 4.4795 (1.203 sec/step)\n","I0830 17:18:23.019273 139622356395904 learning.py:507] global step 3777: loss = 4.4433 (1.193 sec/step)\n","I0830 17:18:24.242174 139622356395904 learning.py:507] global step 3778: loss = 5.4171 (1.221 sec/step)\n","I0830 17:18:25.450026 139622356395904 learning.py:507] global step 3779: loss = 3.4116 (1.206 sec/step)\n","I0830 17:18:26.665080 139622356395904 learning.py:507] global step 3780: loss = 3.5227 (1.213 sec/step)\n","I0830 17:18:27.887650 139622356395904 learning.py:507] global step 3781: loss = 3.1172 (1.220 sec/step)\n","I0830 17:18:29.087929 139622356395904 learning.py:507] global step 3782: loss = 3.2272 (1.198 sec/step)\n","I0830 17:18:30.292437 139622356395904 learning.py:507] global step 3783: loss = 3.5473 (1.203 sec/step)\n","I0830 17:18:31.509835 139622356395904 learning.py:507] global step 3784: loss = 3.1801 (1.215 sec/step)\n","I0830 17:18:32.720868 139622356395904 learning.py:507] global step 3785: loss = 3.1525 (1.209 sec/step)\n","I0830 17:18:33.967252 139622356395904 learning.py:507] global step 3786: loss = 4.2505 (1.244 sec/step)\n","I0830 17:18:35.182907 139622356395904 learning.py:507] global step 3787: loss = 3.4896 (1.214 sec/step)\n","I0830 17:18:36.388945 139622356395904 learning.py:507] global step 3788: loss = 2.8408 (1.204 sec/step)\n","I0830 17:18:37.650013 139622356395904 learning.py:507] global step 3789: loss = 3.3470 (1.259 sec/step)\n","I0830 17:18:38.876259 139622356395904 learning.py:507] global step 3790: loss = 4.3515 (1.224 sec/step)\n","I0830 17:18:40.119908 139622356395904 learning.py:507] global step 3791: loss = 3.1296 (1.242 sec/step)\n","I0830 17:18:41.315886 139622356395904 learning.py:507] global step 3792: loss = 3.5633 (1.194 sec/step)\n","I0830 17:18:42.522872 139622356395904 learning.py:507] global step 3793: loss = 3.2698 (1.205 sec/step)\n","I0830 17:18:43.736178 139622356395904 learning.py:507] global step 3794: loss = 4.2763 (1.211 sec/step)\n","I0830 17:18:44.968409 139622356395904 learning.py:507] global step 3795: loss = 3.8893 (1.230 sec/step)\n","I0830 17:18:46.201955 139622356395904 learning.py:507] global step 3796: loss = 4.4484 (1.232 sec/step)\n","I0830 17:18:47.407070 139622356395904 learning.py:507] global step 3797: loss = 3.9648 (1.203 sec/step)\n","I0830 17:18:48.599695 139622356395904 learning.py:507] global step 3798: loss = 2.9802 (1.191 sec/step)\n","I0830 17:18:49.801470 139622356395904 learning.py:507] global step 3799: loss = 3.5583 (1.200 sec/step)\n","I0830 17:18:51.036813 139622356395904 learning.py:507] global step 3800: loss = 3.6366 (1.233 sec/step)\n","I0830 17:18:52.280979 139622356395904 learning.py:507] global step 3801: loss = 3.5424 (1.242 sec/step)\n","I0830 17:18:53.527205 139622356395904 learning.py:507] global step 3802: loss = 3.7734 (1.244 sec/step)\n","I0830 17:18:54.775169 139622356395904 learning.py:507] global step 3803: loss = 3.7611 (1.246 sec/step)\n","I0830 17:18:55.984908 139622356395904 learning.py:507] global step 3804: loss = 3.3753 (1.207 sec/step)\n","I0830 17:18:57.180886 139622356395904 learning.py:507] global step 3805: loss = 4.1477 (1.194 sec/step)\n","I0830 17:18:58.401318 139622356395904 learning.py:507] global step 3806: loss = 3.4056 (1.219 sec/step)\n","I0830 17:18:59.637281 139622356395904 learning.py:507] global step 3807: loss = 3.7407 (1.234 sec/step)\n","I0830 17:19:00.842276 139622356395904 learning.py:507] global step 3808: loss = 3.5409 (1.203 sec/step)\n","I0830 17:19:02.070164 139622356395904 learning.py:507] global step 3809: loss = 4.1659 (1.226 sec/step)\n","I0830 17:19:03.308682 139622356395904 learning.py:507] global step 3810: loss = 3.5752 (1.237 sec/step)\n","I0830 17:19:04.536679 139622356395904 learning.py:507] global step 3811: loss = 4.6938 (1.226 sec/step)\n","I0830 17:19:05.739765 139622356395904 learning.py:507] global step 3812: loss = 3.7621 (1.201 sec/step)\n","I0830 17:19:06.997317 139622356395904 learning.py:507] global step 3813: loss = 3.8144 (1.253 sec/step)\n","I0830 17:19:08.226794 139622356395904 learning.py:507] global step 3814: loss = 3.6355 (1.227 sec/step)\n","I0830 17:19:09.471443 139622356395904 learning.py:507] global step 3815: loss = 4.0188 (1.243 sec/step)\n","I0830 17:19:10.699452 139622356395904 learning.py:507] global step 3816: loss = 4.3628 (1.226 sec/step)\n","I0830 17:19:11.958563 139622356395904 learning.py:507] global step 3817: loss = 3.0421 (1.257 sec/step)\n","I0830 17:19:13.168334 139622356395904 learning.py:507] global step 3818: loss = 4.6587 (1.208 sec/step)\n","I0830 17:19:14.360639 139622356395904 learning.py:507] global step 3819: loss = 3.5503 (1.191 sec/step)\n","I0830 17:19:15.577284 139622356395904 learning.py:507] global step 3820: loss = 2.8281 (1.215 sec/step)\n","I0830 17:19:16.781114 139622356395904 learning.py:507] global step 3821: loss = 3.9641 (1.202 sec/step)\n","I0830 17:19:17.978801 139622356395904 learning.py:507] global step 3822: loss = 3.8744 (1.196 sec/step)\n","I0830 17:19:19.224270 139622356395904 learning.py:507] global step 3823: loss = 4.0949 (1.244 sec/step)\n","I0830 17:19:20.424301 139622356395904 learning.py:507] global step 3824: loss = 4.0609 (1.198 sec/step)\n","I0830 17:19:21.640896 139622356395904 learning.py:507] global step 3825: loss = 3.9537 (1.215 sec/step)\n","I0830 17:19:22.841883 139622356395904 learning.py:507] global step 3826: loss = 3.8180 (1.199 sec/step)\n","I0830 17:19:24.040925 139622356395904 learning.py:507] global step 3827: loss = 3.0495 (1.197 sec/step)\n","I0830 17:19:25.278272 139622356395904 learning.py:507] global step 3828: loss = 3.5609 (1.235 sec/step)\n","I0830 17:19:26.478513 139622356395904 learning.py:507] global step 3829: loss = 4.0571 (1.198 sec/step)\n","I0830 17:19:27.659997 139622356395904 learning.py:507] global step 3830: loss = 4.7585 (1.180 sec/step)\n","I0830 17:19:28.911010 139622356395904 learning.py:507] global step 3831: loss = 4.1772 (1.249 sec/step)\n","I0830 17:19:30.156751 139622356395904 learning.py:507] global step 3832: loss = 3.2119 (1.244 sec/step)\n","I0830 17:19:31.390007 139622356395904 learning.py:507] global step 3833: loss = 3.2253 (1.231 sec/step)\n","I0830 17:19:32.627463 139622356395904 learning.py:507] global step 3834: loss = 3.9806 (1.235 sec/step)\n","I0830 17:19:33.844923 139622356395904 learning.py:507] global step 3835: loss = 2.9821 (1.215 sec/step)\n","I0830 17:19:35.051437 139622356395904 learning.py:507] global step 3836: loss = 3.4872 (1.204 sec/step)\n","I0830 17:19:36.269044 139622356395904 learning.py:507] global step 3837: loss = 4.1718 (1.215 sec/step)\n","I0830 17:19:37.516007 139622356395904 learning.py:507] global step 3838: loss = 3.5384 (1.244 sec/step)\n","I0830 17:19:38.769842 139622356395904 learning.py:507] global step 3839: loss = 3.3212 (1.252 sec/step)\n","I0830 17:19:39.973162 139622356395904 learning.py:507] global step 3840: loss = 2.8677 (1.201 sec/step)\n","I0830 17:19:41.183115 139622356395904 learning.py:507] global step 3841: loss = 4.0101 (1.208 sec/step)\n","I0830 17:19:42.395507 139622356395904 learning.py:507] global step 3842: loss = 3.8900 (1.210 sec/step)\n","I0830 17:19:43.630849 139622356395904 learning.py:507] global step 3843: loss = 3.2699 (1.233 sec/step)\n","I0830 17:19:44.869840 139622356395904 learning.py:507] global step 3844: loss = 3.8910 (1.237 sec/step)\n","I0830 17:19:46.098825 139622356395904 learning.py:507] global step 3845: loss = 4.0781 (1.227 sec/step)\n","I0830 17:19:47.327487 139622356395904 learning.py:507] global step 3846: loss = 3.5881 (1.226 sec/step)\n","I0830 17:19:48.544296 139622356395904 learning.py:507] global step 3847: loss = 3.2586 (1.215 sec/step)\n","I0830 17:19:49.778859 139622356395904 learning.py:507] global step 3848: loss = 4.2222 (1.233 sec/step)\n","I0830 17:19:51.001832 139622356395904 learning.py:507] global step 3849: loss = 3.6224 (1.221 sec/step)\n","I0830 17:19:52.236697 139622356395904 learning.py:507] global step 3850: loss = 4.8985 (1.233 sec/step)\n","I0830 17:19:53.476749 139622356395904 learning.py:507] global step 3851: loss = 3.3386 (1.238 sec/step)\n","I0830 17:19:54.684142 139622356395904 learning.py:507] global step 3852: loss = 4.0949 (1.205 sec/step)\n","I0830 17:19:55.907251 139622356395904 learning.py:507] global step 3853: loss = 3.3732 (1.221 sec/step)\n","I0830 17:19:57.101762 139622356395904 learning.py:507] global step 3854: loss = 3.5490 (1.193 sec/step)\n","I0830 17:19:58.308192 139622356395904 learning.py:507] global step 3855: loss = 3.9260 (1.204 sec/step)\n","I0830 17:19:59.527390 139622356395904 learning.py:507] global step 3856: loss = 3.4403 (1.218 sec/step)\n","I0830 17:19:59.645663 139619316770560 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n","I0830 17:20:01.879215 139622356395904 learning.py:507] global step 3857: loss = 3.9340 (2.328 sec/step)\n","I0830 17:20:03.178566 139619299985152 supervisor.py:1050] Recording summary at step 3857.\n","I0830 17:20:04.173030 139622356395904 learning.py:507] global step 3858: loss = 3.6543 (2.281 sec/step)\n","I0830 17:20:05.597931 139622356395904 learning.py:507] global step 3859: loss = 3.3157 (1.423 sec/step)\n","I0830 17:20:06.855846 139622356395904 learning.py:507] global step 3860: loss = 3.6759 (1.256 sec/step)\n","I0830 17:20:08.028108 139622356395904 learning.py:507] global step 3861: loss = 3.6742 (1.170 sec/step)\n","I0830 17:20:09.258041 139622356395904 learning.py:507] global step 3862: loss = 3.5798 (1.227 sec/step)\n","I0830 17:20:10.500974 139622356395904 learning.py:507] global step 3863: loss = 3.2813 (1.241 sec/step)\n","I0830 17:20:11.713897 139622356395904 learning.py:507] global step 3864: loss = 3.9326 (1.211 sec/step)\n","I0830 17:20:12.919637 139622356395904 learning.py:507] global step 3865: loss = 3.9231 (1.204 sec/step)\n","I0830 17:20:14.126457 139622356395904 learning.py:507] global step 3866: loss = 3.6964 (1.205 sec/step)\n","I0830 17:20:15.338985 139622356395904 learning.py:507] global step 3867: loss = 3.3090 (1.210 sec/step)\n","I0830 17:20:16.578777 139622356395904 learning.py:507] global step 3868: loss = 4.6205 (1.238 sec/step)\n","I0830 17:20:17.831177 139622356395904 learning.py:507] global step 3869: loss = 5.0830 (1.251 sec/step)\n","I0830 17:20:19.040452 139622356395904 learning.py:507] global step 3870: loss = 3.7489 (1.207 sec/step)\n","I0830 17:20:20.247216 139622356395904 learning.py:507] global step 3871: loss = 2.9983 (1.205 sec/step)\n","I0830 17:20:21.456546 139622356395904 learning.py:507] global step 3872: loss = 3.4886 (1.207 sec/step)\n","I0830 17:20:22.679626 139622356395904 learning.py:507] global step 3873: loss = 4.1099 (1.221 sec/step)\n","I0830 17:20:23.899854 139622356395904 learning.py:507] global step 3874: loss = 3.5279 (1.219 sec/step)\n","I0830 17:20:25.128442 139622356395904 learning.py:507] global step 3875: loss = 2.8059 (1.227 sec/step)\n","I0830 17:20:26.321751 139622356395904 learning.py:507] global step 3876: loss = 3.3966 (1.191 sec/step)\n","I0830 17:20:27.487397 139622356395904 learning.py:507] global step 3877: loss = 4.1782 (1.164 sec/step)\n","I0830 17:20:28.687421 139622356395904 learning.py:507] global step 3878: loss = 4.3587 (1.198 sec/step)\n","I0830 17:20:29.870340 139622356395904 learning.py:507] global step 3879: loss = 4.2578 (1.181 sec/step)\n","I0830 17:20:31.093346 139622356395904 learning.py:507] global step 3880: loss = 4.6892 (1.221 sec/step)\n","I0830 17:20:32.309100 139622356395904 learning.py:507] global step 3881: loss = 3.1055 (1.213 sec/step)\n","I0830 17:20:33.502228 139622356395904 learning.py:507] global step 3882: loss = 2.9379 (1.191 sec/step)\n","I0830 17:20:34.746719 139622356395904 learning.py:507] global step 3883: loss = 3.6966 (1.243 sec/step)\n","I0830 17:20:35.967838 139622356395904 learning.py:507] global step 3884: loss = 3.8331 (1.219 sec/step)\n","I0830 17:20:37.187009 139622356395904 learning.py:507] global step 3885: loss = 3.3145 (1.217 sec/step)\n","I0830 17:20:38.378221 139622356395904 learning.py:507] global step 3886: loss = 3.7909 (1.189 sec/step)\n","I0830 17:20:39.626372 139622356395904 learning.py:507] global step 3887: loss = 4.0083 (1.246 sec/step)\n","I0830 17:20:40.818727 139622356395904 learning.py:507] global step 3888: loss = 2.8988 (1.190 sec/step)\n","I0830 17:20:42.010000 139622356395904 learning.py:507] global step 3889: loss = 4.1640 (1.189 sec/step)\n","I0830 17:20:43.238517 139622356395904 learning.py:507] global step 3890: loss = 2.9762 (1.227 sec/step)\n","I0830 17:20:44.442125 139622356395904 learning.py:507] global step 3891: loss = 3.0917 (1.202 sec/step)\n","I0830 17:20:45.651234 139622356395904 learning.py:507] global step 3892: loss = 3.8299 (1.207 sec/step)\n","I0830 17:20:46.885684 139622356395904 learning.py:507] global step 3893: loss = 3.8406 (1.233 sec/step)\n","I0830 17:20:48.054413 139622356395904 learning.py:507] global step 3894: loss = 3.2113 (1.167 sec/step)\n","I0830 17:20:49.272680 139622356395904 learning.py:507] global step 3895: loss = 3.7977 (1.216 sec/step)\n","I0830 17:20:50.502902 139622356395904 learning.py:507] global step 3896: loss = 3.2553 (1.228 sec/step)\n","I0830 17:20:51.694693 139622356395904 learning.py:507] global step 3897: loss = 3.4871 (1.189 sec/step)\n","I0830 17:20:52.925641 139622356395904 learning.py:507] global step 3898: loss = 3.1202 (1.229 sec/step)\n","I0830 17:20:54.153222 139622356395904 learning.py:507] global step 3899: loss = 3.4462 (1.225 sec/step)\n","I0830 17:20:55.409675 139622356395904 learning.py:507] global step 3900: loss = 3.6986 (1.254 sec/step)\n","I0830 17:20:56.629222 139622356395904 learning.py:507] global step 3901: loss = 4.2213 (1.218 sec/step)\n","I0830 17:20:57.840091 139622356395904 learning.py:507] global step 3902: loss = 3.6720 (1.209 sec/step)\n","I0830 17:20:59.040453 139622356395904 learning.py:507] global step 3903: loss = 3.8818 (1.199 sec/step)\n","I0830 17:21:00.232898 139622356395904 learning.py:507] global step 3904: loss = 3.1536 (1.190 sec/step)\n","I0830 17:21:01.403394 139622356395904 learning.py:507] global step 3905: loss = 3.5411 (1.168 sec/step)\n","I0830 17:21:02.644090 139622356395904 learning.py:507] global step 3906: loss = 3.4272 (1.239 sec/step)\n","I0830 17:21:03.853539 139622356395904 learning.py:507] global step 3907: loss = 3.8587 (1.208 sec/step)\n","I0830 17:21:05.061978 139622356395904 learning.py:507] global step 3908: loss = 3.2357 (1.206 sec/step)\n","I0830 17:21:06.244014 139622356395904 learning.py:507] global step 3909: loss = 3.3903 (1.180 sec/step)\n","I0830 17:21:07.417733 139622356395904 learning.py:507] global step 3910: loss = 3.0703 (1.172 sec/step)\n","I0830 17:21:08.605787 139622356395904 learning.py:507] global step 3911: loss = 3.5623 (1.186 sec/step)\n","I0830 17:21:09.784611 139622356395904 learning.py:507] global step 3912: loss = 3.2889 (1.177 sec/step)\n","I0830 17:21:11.018343 139622356395904 learning.py:507] global step 3913: loss = 3.5431 (1.232 sec/step)\n","I0830 17:21:12.252719 139622356395904 learning.py:507] global step 3914: loss = 3.1683 (1.232 sec/step)\n","I0830 17:21:13.488703 139622356395904 learning.py:507] global step 3915: loss = 4.0393 (1.234 sec/step)\n","I0830 17:21:14.737077 139622356395904 learning.py:507] global step 3916: loss = 4.1216 (1.246 sec/step)\n","I0830 17:21:15.978963 139622356395904 learning.py:507] global step 3917: loss = 3.7476 (1.240 sec/step)\n","I0830 17:21:17.156502 139622356395904 learning.py:507] global step 3918: loss = 2.7120 (1.175 sec/step)\n","I0830 17:21:18.421619 139622356395904 learning.py:507] global step 3919: loss = 2.8810 (1.263 sec/step)\n","I0830 17:21:19.695304 139622356395904 learning.py:507] global step 3920: loss = 3.5322 (1.270 sec/step)\n","I0830 17:21:20.891573 139622356395904 learning.py:507] global step 3921: loss = 4.3193 (1.194 sec/step)\n","I0830 17:21:22.115163 139622356395904 learning.py:507] global step 3922: loss = 4.0586 (1.219 sec/step)\n","I0830 17:21:23.322779 139622356395904 learning.py:507] global step 3923: loss = 3.7592 (1.206 sec/step)\n","I0830 17:21:24.545456 139622356395904 learning.py:507] global step 3924: loss = 4.7544 (1.221 sec/step)\n","I0830 17:21:25.753445 139622356395904 learning.py:507] global step 3925: loss = 4.1830 (1.205 sec/step)\n","I0830 17:21:26.964303 139622356395904 learning.py:507] global step 3926: loss = 3.3209 (1.208 sec/step)\n","I0830 17:21:28.188174 139622356395904 learning.py:507] global step 3927: loss = 3.1558 (1.222 sec/step)\n","I0830 17:21:29.436494 139622356395904 learning.py:507] global step 3928: loss = 3.3022 (1.246 sec/step)\n","I0830 17:21:30.662225 139622356395904 learning.py:507] global step 3929: loss = 4.1936 (1.224 sec/step)\n","I0830 17:21:31.876026 139622356395904 learning.py:507] global step 3930: loss = 3.5823 (1.212 sec/step)\n","I0830 17:21:33.088947 139622356395904 learning.py:507] global step 3931: loss = 3.8389 (1.211 sec/step)\n","I0830 17:21:34.329484 139622356395904 learning.py:507] global step 3932: loss = 3.5330 (1.239 sec/step)\n","I0830 17:21:35.561730 139622356395904 learning.py:507] global step 3933: loss = 3.3942 (1.230 sec/step)\n","I0830 17:21:36.790012 139622356395904 learning.py:507] global step 3934: loss = 4.5379 (1.227 sec/step)\n","I0830 17:21:37.977398 139622356395904 learning.py:507] global step 3935: loss = 3.8768 (1.186 sec/step)\n","I0830 17:21:39.249009 139622356395904 learning.py:507] global step 3936: loss = 4.4585 (1.270 sec/step)\n","I0830 17:21:40.459278 139622356395904 learning.py:507] global step 3937: loss = 3.4088 (1.208 sec/step)\n","I0830 17:21:41.679158 139622356395904 learning.py:507] global step 3938: loss = 3.9989 (1.218 sec/step)\n","I0830 17:21:42.885825 139622356395904 learning.py:507] global step 3939: loss = 4.4490 (1.205 sec/step)\n","I0830 17:21:44.095023 139622356395904 learning.py:507] global step 3940: loss = 3.4425 (1.207 sec/step)\n","I0830 17:21:45.319807 139622356395904 learning.py:507] global step 3941: loss = 3.1614 (1.223 sec/step)\n","I0830 17:21:46.533371 139622356395904 learning.py:507] global step 3942: loss = 2.6856 (1.211 sec/step)\n","I0830 17:21:47.780404 139622356395904 learning.py:507] global step 3943: loss = 4.8234 (1.245 sec/step)\n","I0830 17:21:49.014428 139622356395904 learning.py:507] global step 3944: loss = 3.4009 (1.232 sec/step)\n","I0830 17:21:50.256522 139622356395904 learning.py:507] global step 3945: loss = 3.2751 (1.240 sec/step)\n","I0830 17:21:51.473779 139622356395904 learning.py:507] global step 3946: loss = 3.6929 (1.215 sec/step)\n","I0830 17:21:52.681512 139622356395904 learning.py:507] global step 3947: loss = 3.2848 (1.206 sec/step)\n","I0830 17:21:53.883672 139622356395904 learning.py:507] global step 3948: loss = 3.2789 (1.200 sec/step)\n","I0830 17:21:55.118564 139622356395904 learning.py:507] global step 3949: loss = 4.0401 (1.233 sec/step)\n","I0830 17:21:56.313918 139622356395904 learning.py:507] global step 3950: loss = 3.6905 (1.194 sec/step)\n","I0830 17:21:57.535108 139622356395904 learning.py:507] global step 3951: loss = 4.0260 (1.219 sec/step)\n","I0830 17:21:58.749635 139622356395904 learning.py:507] global step 3952: loss = 2.8491 (1.213 sec/step)\n","I0830 17:22:00.056297 139622356395904 learning.py:507] global step 3953: loss = 3.5219 (1.216 sec/step)\n","I0830 17:22:02.000796 139622356395904 learning.py:507] global step 3954: loss = 3.6402 (1.887 sec/step)\n","I0830 17:22:02.062353 139619299985152 supervisor.py:1050] Recording summary at step 3954.\n","I0830 17:22:03.246879 139622356395904 learning.py:507] global step 3955: loss = 3.9992 (1.244 sec/step)\n","I0830 17:22:04.444843 139622356395904 learning.py:507] global step 3956: loss = 3.9217 (1.196 sec/step)\n","I0830 17:22:05.652303 139622356395904 learning.py:507] global step 3957: loss = 3.2795 (1.206 sec/step)\n","I0830 17:22:06.901910 139622356395904 learning.py:507] global step 3958: loss = 4.9539 (1.248 sec/step)\n","I0830 17:22:08.130065 139622356395904 learning.py:507] global step 3959: loss = 4.0090 (1.226 sec/step)\n","I0830 17:22:09.358984 139622356395904 learning.py:507] global step 3960: loss = 3.4178 (1.227 sec/step)\n","I0830 17:22:10.593113 139622356395904 learning.py:507] global step 3961: loss = 4.0226 (1.232 sec/step)\n","I0830 17:22:11.799822 139622356395904 learning.py:507] global step 3962: loss = 3.9257 (1.205 sec/step)\n","I0830 17:22:13.005592 139622356395904 learning.py:507] global step 3963: loss = 3.5322 (1.203 sec/step)\n","I0830 17:22:14.229628 139622356395904 learning.py:507] global step 3964: loss = 4.6038 (1.222 sec/step)\n","I0830 17:22:15.438209 139622356395904 learning.py:507] global step 3965: loss = 3.0521 (1.206 sec/step)\n","I0830 17:22:16.673542 139622356395904 learning.py:507] global step 3966: loss = 3.5564 (1.233 sec/step)\n","I0830 17:22:17.916590 139622356395904 learning.py:507] global step 3967: loss = 3.0251 (1.241 sec/step)\n","I0830 17:22:19.135203 139622356395904 learning.py:507] global step 3968: loss = 3.0870 (1.217 sec/step)\n","I0830 17:22:20.374134 139622356395904 learning.py:507] global step 3969: loss = 4.7488 (1.237 sec/step)\n","I0830 17:22:21.599580 139622356395904 learning.py:507] global step 3970: loss = 3.7599 (1.223 sec/step)\n","I0830 17:22:22.794420 139622356395904 learning.py:507] global step 3971: loss = 3.3062 (1.193 sec/step)\n","I0830 17:22:24.013108 139622356395904 learning.py:507] global step 3972: loss = 3.4617 (1.217 sec/step)\n","I0830 17:22:25.255702 139622356395904 learning.py:507] global step 3973: loss = 3.3214 (1.241 sec/step)\n","I0830 17:22:26.445758 139622356395904 learning.py:507] global step 3974: loss = 4.2866 (1.188 sec/step)\n","I0830 17:22:27.677930 139622356395904 learning.py:507] global step 3975: loss = 2.9094 (1.230 sec/step)\n","I0830 17:22:28.885335 139622356395904 learning.py:507] global step 3976: loss = 3.8863 (1.206 sec/step)\n","I0830 17:22:30.085983 139622356395904 learning.py:507] global step 3977: loss = 3.7267 (1.199 sec/step)\n","I0830 17:22:31.308482 139622356395904 learning.py:507] global step 3978: loss = 3.5273 (1.220 sec/step)\n","I0830 17:22:32.531582 139622356395904 learning.py:507] global step 3979: loss = 4.1727 (1.221 sec/step)\n","I0830 17:22:33.734927 139622356395904 learning.py:507] global step 3980: loss = 3.8262 (1.202 sec/step)\n","I0830 17:22:34.963295 139622356395904 learning.py:507] global step 3981: loss = 3.5660 (1.227 sec/step)\n","I0830 17:22:36.181449 139622356395904 learning.py:507] global step 3982: loss = 3.7504 (1.216 sec/step)\n","I0830 17:22:37.416270 139622356395904 learning.py:507] global step 3983: loss = 2.8377 (1.233 sec/step)\n","I0830 17:22:38.655710 139622356395904 learning.py:507] global step 3984: loss = 3.8573 (1.237 sec/step)\n","I0830 17:22:39.894426 139622356395904 learning.py:507] global step 3985: loss = 3.9027 (1.237 sec/step)\n","I0830 17:22:41.114136 139622356395904 learning.py:507] global step 3986: loss = 3.7629 (1.218 sec/step)\n","I0830 17:22:42.361706 139622356395904 learning.py:507] global step 3987: loss = 3.8642 (1.246 sec/step)\n","I0830 17:22:43.608014 139622356395904 learning.py:507] global step 3988: loss = 4.0947 (1.244 sec/step)\n","I0830 17:22:44.860544 139622356395904 learning.py:507] global step 3989: loss = 2.6835 (1.251 sec/step)\n","I0830 17:22:46.060125 139622356395904 learning.py:507] global step 3990: loss = 2.9830 (1.197 sec/step)\n","I0830 17:22:47.302848 139622356395904 learning.py:507] global step 3991: loss = 4.3716 (1.241 sec/step)\n","I0830 17:22:48.538566 139622356395904 learning.py:507] global step 3992: loss = 3.7227 (1.234 sec/step)\n","I0830 17:22:49.779899 139622356395904 learning.py:507] global step 3993: loss = 4.7611 (1.240 sec/step)\n","I0830 17:22:51.004173 139622356395904 learning.py:507] global step 3994: loss = 3.2236 (1.222 sec/step)\n","I0830 17:22:52.236728 139622356395904 learning.py:507] global step 3995: loss = 3.4728 (1.231 sec/step)\n","I0830 17:22:53.441208 139622356395904 learning.py:507] global step 3996: loss = 3.1336 (1.203 sec/step)\n","I0830 17:22:54.628514 139622356395904 learning.py:507] global step 3997: loss = 3.3362 (1.183 sec/step)\n","I0830 17:22:55.846584 139622356395904 learning.py:507] global step 3998: loss = 4.0387 (1.216 sec/step)\n","I0830 17:22:57.014690 139622356395904 learning.py:507] global step 3999: loss = 4.6538 (1.166 sec/step)\n","I0830 17:22:58.250798 139622356395904 learning.py:507] global step 4000: loss = 4.2095 (1.234 sec/step)\n","I0830 17:22:59.448412 139622356395904 learning.py:507] global step 4001: loss = 3.1515 (1.196 sec/step)\n","I0830 17:23:00.695897 139622356395904 learning.py:507] global step 4002: loss = 3.5603 (1.246 sec/step)\n","I0830 17:23:01.898384 139622356395904 learning.py:507] global step 4003: loss = 3.7676 (1.201 sec/step)\n","I0830 17:23:03.121977 139622356395904 learning.py:507] global step 4004: loss = 2.9545 (1.222 sec/step)\n","I0830 17:23:04.361269 139622356395904 learning.py:507] global step 4005: loss = 3.1003 (1.238 sec/step)\n","I0830 17:23:05.604120 139622356395904 learning.py:507] global step 4006: loss = 3.8717 (1.241 sec/step)\n","I0830 17:23:06.877214 139622356395904 learning.py:507] global step 4007: loss = 3.3983 (1.271 sec/step)\n","I0830 17:23:08.096397 139622356395904 learning.py:507] global step 4008: loss = 3.5504 (1.218 sec/step)\n","I0830 17:23:09.293643 139622356395904 learning.py:507] global step 4009: loss = 4.0044 (1.195 sec/step)\n","I0830 17:23:10.516393 139622356395904 learning.py:507] global step 4010: loss = 3.8889 (1.221 sec/step)\n","I0830 17:23:11.752509 139622356395904 learning.py:507] global step 4011: loss = 3.7779 (1.234 sec/step)\n","I0830 17:23:12.956275 139622356395904 learning.py:507] global step 4012: loss = 3.0636 (1.202 sec/step)\n","I0830 17:23:14.157849 139622356395904 learning.py:507] global step 4013: loss = 3.4527 (1.198 sec/step)\n","I0830 17:23:15.402696 139622356395904 learning.py:507] global step 4014: loss = 4.3538 (1.243 sec/step)\n","I0830 17:23:16.638470 139622356395904 learning.py:507] global step 4015: loss = 3.0892 (1.234 sec/step)\n","I0830 17:23:17.873139 139622356395904 learning.py:507] global step 4016: loss = 4.0742 (1.233 sec/step)\n","I0830 17:23:19.088337 139622356395904 learning.py:507] global step 4017: loss = 3.7001 (1.213 sec/step)\n","I0830 17:23:20.337139 139622356395904 learning.py:507] global step 4018: loss = 3.6038 (1.247 sec/step)\n","I0830 17:23:21.573350 139622356395904 learning.py:507] global step 4019: loss = 3.3618 (1.234 sec/step)\n","I0830 17:23:22.815631 139622356395904 learning.py:507] global step 4020: loss = 4.2831 (1.240 sec/step)\n","I0830 17:23:24.049993 139622356395904 learning.py:507] global step 4021: loss = 3.4264 (1.232 sec/step)\n","I0830 17:23:25.273789 139622356395904 learning.py:507] global step 4022: loss = 4.1983 (1.222 sec/step)\n","I0830 17:23:26.494936 139622356395904 learning.py:507] global step 4023: loss = 3.5006 (1.219 sec/step)\n","I0830 17:23:27.723627 139622356395904 learning.py:507] global step 4024: loss = 3.7494 (1.227 sec/step)\n","I0830 17:23:28.939713 139622356395904 learning.py:507] global step 4025: loss = 3.5312 (1.214 sec/step)\n","I0830 17:23:30.180819 139622356395904 learning.py:507] global step 4026: loss = 3.6664 (1.239 sec/step)\n","I0830 17:23:31.437599 139622356395904 learning.py:507] global step 4027: loss = 4.0669 (1.255 sec/step)\n","I0830 17:23:32.670870 139622356395904 learning.py:507] global step 4028: loss = 3.5555 (1.231 sec/step)\n","I0830 17:23:33.891705 139622356395904 learning.py:507] global step 4029: loss = 3.5856 (1.219 sec/step)\n","I0830 17:23:35.086469 139622356395904 learning.py:507] global step 4030: loss = 3.7180 (1.193 sec/step)\n","I0830 17:23:36.311784 139622356395904 learning.py:507] global step 4031: loss = 3.1381 (1.223 sec/step)\n","I0830 17:23:37.562435 139622356395904 learning.py:507] global step 4032: loss = 3.6719 (1.249 sec/step)\n","I0830 17:23:38.760676 139622356395904 learning.py:507] global step 4033: loss = 3.2432 (1.196 sec/step)\n","I0830 17:23:39.986276 139622356395904 learning.py:507] global step 4034: loss = 4.1538 (1.224 sec/step)\n","I0830 17:23:41.185616 139622356395904 learning.py:507] global step 4035: loss = 5.6799 (1.197 sec/step)\n","I0830 17:23:42.382366 139622356395904 learning.py:507] global step 4036: loss = 3.7203 (1.195 sec/step)\n","I0830 17:23:43.616353 139622356395904 learning.py:507] global step 4037: loss = 4.1107 (1.232 sec/step)\n","I0830 17:23:44.850566 139622356395904 learning.py:507] global step 4038: loss = 2.9929 (1.232 sec/step)\n","I0830 17:23:46.050348 139622356395904 learning.py:507] global step 4039: loss = 3.4828 (1.198 sec/step)\n","I0830 17:23:47.277678 139622356395904 learning.py:507] global step 4040: loss = 3.8101 (1.225 sec/step)\n","I0830 17:23:48.528585 139622356395904 learning.py:507] global step 4041: loss = 5.3232 (1.249 sec/step)\n","I0830 17:23:49.728326 139622356395904 learning.py:507] global step 4042: loss = 3.4286 (1.198 sec/step)\n","I0830 17:23:50.918894 139622356395904 learning.py:507] global step 4043: loss = 3.6721 (1.189 sec/step)\n","I0830 17:23:52.163309 139622356395904 learning.py:507] global step 4044: loss = 4.7299 (1.243 sec/step)\n","I0830 17:23:53.404800 139622356395904 learning.py:507] global step 4045: loss = 3.8836 (1.240 sec/step)\n","I0830 17:23:54.672864 139622356395904 learning.py:507] global step 4046: loss = 3.1145 (1.266 sec/step)\n","I0830 17:23:55.935970 139622356395904 learning.py:507] global step 4047: loss = 3.5808 (1.261 sec/step)\n","I0830 17:23:57.164414 139622356395904 learning.py:507] global step 4048: loss = 4.0281 (1.227 sec/step)\n","I0830 17:23:58.383414 139622356395904 learning.py:507] global step 4049: loss = 3.3081 (1.217 sec/step)\n","I0830 17:23:59.632528 139622356395904 learning.py:507] global step 4050: loss = 3.1272 (1.247 sec/step)\n","I0830 17:24:01.735566 139622356395904 learning.py:507] global step 4051: loss = 3.0362 (2.009 sec/step)\n","I0830 17:24:01.762490 139619299985152 supervisor.py:1050] Recording summary at step 4051.\n","I0830 17:24:02.961486 139622356395904 learning.py:507] global step 4052: loss = 4.2630 (1.224 sec/step)\n","I0830 17:24:04.191843 139622356395904 learning.py:507] global step 4053: loss = 3.2746 (1.229 sec/step)\n","I0830 17:24:05.435509 139622356395904 learning.py:507] global step 4054: loss = 4.3754 (1.242 sec/step)\n","I0830 17:24:06.682129 139622356395904 learning.py:507] global step 4055: loss = 3.3076 (1.245 sec/step)\n","I0830 17:24:07.898989 139622356395904 learning.py:507] global step 4056: loss = 2.9170 (1.215 sec/step)\n","I0830 17:24:09.078768 139622356395904 learning.py:507] global step 4057: loss = 3.8702 (1.177 sec/step)\n","I0830 17:24:10.315039 139622356395904 learning.py:507] global step 4058: loss = 2.9797 (1.234 sec/step)\n","I0830 17:24:11.545896 139622356395904 learning.py:507] global step 4059: loss = 3.2096 (1.229 sec/step)\n","I0830 17:24:12.779371 139622356395904 learning.py:507] global step 4060: loss = 4.2797 (1.231 sec/step)\n","I0830 17:24:13.993951 139622356395904 learning.py:507] global step 4061: loss = 4.6702 (1.213 sec/step)\n","I0830 17:24:15.192136 139622356395904 learning.py:507] global step 4062: loss = 3.5783 (1.196 sec/step)\n","I0830 17:24:16.381927 139622356395904 learning.py:507] global step 4063: loss = 4.8980 (1.188 sec/step)\n","I0830 17:24:17.638613 139622356395904 learning.py:507] global step 4064: loss = 4.1831 (1.255 sec/step)\n","I0830 17:24:18.901425 139622356395904 learning.py:507] global step 4065: loss = 2.9484 (1.261 sec/step)\n","I0830 17:24:20.121895 139622356395904 learning.py:507] global step 4066: loss = 3.7069 (1.218 sec/step)\n","I0830 17:24:21.307511 139622356395904 learning.py:507] global step 4067: loss = 5.9021 (1.184 sec/step)\n","I0830 17:24:22.491238 139622356395904 learning.py:507] global step 4068: loss = 3.3760 (1.182 sec/step)\n","I0830 17:24:23.696491 139622356395904 learning.py:507] global step 4069: loss = 3.8748 (1.203 sec/step)\n","I0830 17:24:24.948560 139622356395904 learning.py:507] global step 4070: loss = 3.2545 (1.250 sec/step)\n","I0830 17:24:26.196122 139622356395904 learning.py:507] global step 4071: loss = 4.4441 (1.246 sec/step)\n","I0830 17:24:27.406973 139622356395904 learning.py:507] global step 4072: loss = 3.0967 (1.209 sec/step)\n","I0830 17:24:28.622635 139622356395904 learning.py:507] global step 4073: loss = 4.1439 (1.214 sec/step)\n","I0830 17:24:29.859724 139622356395904 learning.py:507] global step 4074: loss = 3.5619 (1.235 sec/step)\n","I0830 17:24:31.095981 139622356395904 learning.py:507] global step 4075: loss = 2.6011 (1.235 sec/step)\n","I0830 17:24:32.348687 139622356395904 learning.py:507] global step 4076: loss = 3.0911 (1.250 sec/step)\n","I0830 17:24:33.557777 139622356395904 learning.py:507] global step 4077: loss = 3.2720 (1.207 sec/step)\n","I0830 17:24:34.782292 139622356395904 learning.py:507] global step 4078: loss = 3.2412 (1.222 sec/step)\n","I0830 17:24:35.989817 139622356395904 learning.py:507] global step 4079: loss = 3.3710 (1.206 sec/step)\n","I0830 17:24:37.174224 139622356395904 learning.py:507] global step 4080: loss = 4.4929 (1.182 sec/step)\n","I0830 17:24:38.414528 139622356395904 learning.py:507] global step 4081: loss = 4.8206 (1.239 sec/step)\n","I0830 17:24:39.624193 139622356395904 learning.py:507] global step 4082: loss = 3.3516 (1.208 sec/step)\n","I0830 17:24:40.861133 139622356395904 learning.py:507] global step 4083: loss = 4.2836 (1.235 sec/step)\n","I0830 17:24:42.074698 139622356395904 learning.py:507] global step 4084: loss = 4.0654 (1.211 sec/step)\n","I0830 17:24:43.315809 139622356395904 learning.py:507] global step 4085: loss = 4.5963 (1.239 sec/step)\n","I0830 17:24:44.561270 139622356395904 learning.py:507] global step 4086: loss = 3.8379 (1.244 sec/step)\n","I0830 17:24:45.815214 139622356395904 learning.py:507] global step 4087: loss = 3.4950 (1.252 sec/step)\n","I0830 17:24:47.064813 139622356395904 learning.py:507] global step 4088: loss = 5.9783 (1.248 sec/step)\n","I0830 17:24:48.288705 139622356395904 learning.py:507] global step 4089: loss = 3.4971 (1.222 sec/step)\n","I0830 17:24:49.496174 139622356395904 learning.py:507] global step 4090: loss = 4.5976 (1.206 sec/step)\n","I0830 17:24:50.732339 139622356395904 learning.py:507] global step 4091: loss = 3.8273 (1.234 sec/step)\n","I0830 17:24:51.972131 139622356395904 learning.py:507] global step 4092: loss = 4.9562 (1.238 sec/step)\n","I0830 17:24:53.199286 139622356395904 learning.py:507] global step 4093: loss = 2.9674 (1.225 sec/step)\n","I0830 17:24:54.435799 139622356395904 learning.py:507] global step 4094: loss = 3.2942 (1.234 sec/step)\n","I0830 17:24:55.678339 139622356395904 learning.py:507] global step 4095: loss = 3.7121 (1.241 sec/step)\n","I0830 17:24:56.908964 139622356395904 learning.py:507] global step 4096: loss = 3.5223 (1.229 sec/step)\n","I0830 17:24:58.165111 139622356395904 learning.py:507] global step 4097: loss = 3.4568 (1.254 sec/step)\n","I0830 17:24:59.372243 139622356395904 learning.py:507] global step 4098: loss = 3.9391 (1.205 sec/step)\n","I0830 17:25:00.572698 139622356395904 learning.py:507] global step 4099: loss = 3.3194 (1.198 sec/step)\n","I0830 17:25:01.808295 139622356395904 learning.py:507] global step 4100: loss = 3.1233 (1.234 sec/step)\n","I0830 17:25:03.039551 139622356395904 learning.py:507] global step 4101: loss = 3.6873 (1.230 sec/step)\n","I0830 17:25:04.243708 139622356395904 learning.py:507] global step 4102: loss = 2.9039 (1.202 sec/step)\n","I0830 17:25:05.462605 139622356395904 learning.py:507] global step 4103: loss = 3.2270 (1.217 sec/step)\n","I0830 17:25:06.657182 139622356395904 learning.py:507] global step 4104: loss = 2.8632 (1.193 sec/step)\n","I0830 17:25:07.899643 139622356395904 learning.py:507] global step 4105: loss = 5.1721 (1.240 sec/step)\n","I0830 17:25:09.137963 139622356395904 learning.py:507] global step 4106: loss = 3.7668 (1.236 sec/step)\n","I0830 17:25:10.349998 139622356395904 learning.py:507] global step 4107: loss = 4.1681 (1.210 sec/step)\n","I0830 17:25:11.556784 139622356395904 learning.py:507] global step 4108: loss = 3.6540 (1.205 sec/step)\n","I0830 17:25:12.794008 139622356395904 learning.py:507] global step 4109: loss = 4.1830 (1.235 sec/step)\n","I0830 17:25:13.988166 139622356395904 learning.py:507] global step 4110: loss = 3.5248 (1.192 sec/step)\n","I0830 17:25:15.198492 139622356395904 learning.py:507] global step 4111: loss = 3.2082 (1.208 sec/step)\n","I0830 17:25:16.413755 139622356395904 learning.py:507] global step 4112: loss = 3.9591 (1.213 sec/step)\n","I0830 17:25:17.640571 139622356395904 learning.py:507] global step 4113: loss = 3.3236 (1.225 sec/step)\n","I0830 17:25:18.866906 139622356395904 learning.py:507] global step 4114: loss = 3.2481 (1.224 sec/step)\n","I0830 17:25:20.089673 139622356395904 learning.py:507] global step 4115: loss = 3.1073 (1.221 sec/step)\n","I0830 17:25:21.341929 139622356395904 learning.py:507] global step 4116: loss = 2.9787 (1.250 sec/step)\n","I0830 17:25:22.555935 139622356395904 learning.py:507] global step 4117: loss = 3.7818 (1.212 sec/step)\n","I0830 17:25:23.770872 139622356395904 learning.py:507] global step 4118: loss = 3.0682 (1.213 sec/step)\n","I0830 17:25:24.969611 139622356395904 learning.py:507] global step 4119: loss = 3.0158 (1.197 sec/step)\n","I0830 17:25:26.179899 139622356395904 learning.py:507] global step 4120: loss = 4.8286 (1.208 sec/step)\n","I0830 17:25:27.411269 139622356395904 learning.py:507] global step 4121: loss = 2.8584 (1.230 sec/step)\n","I0830 17:25:28.624963 139622356395904 learning.py:507] global step 4122: loss = 3.6750 (1.212 sec/step)\n","I0830 17:25:29.879905 139622356395904 learning.py:507] global step 4123: loss = 3.7300 (1.253 sec/step)\n","I0830 17:25:31.071930 139622356395904 learning.py:507] global step 4124: loss = 3.6981 (1.190 sec/step)\n","I0830 17:25:32.263491 139622356395904 learning.py:507] global step 4125: loss = 3.7594 (1.190 sec/step)\n","I0830 17:25:33.509164 139622356395904 learning.py:507] global step 4126: loss = 2.7538 (1.244 sec/step)\n","I0830 17:25:34.721385 139622356395904 learning.py:507] global step 4127: loss = 3.3641 (1.210 sec/step)\n","I0830 17:25:35.918712 139622356395904 learning.py:507] global step 4128: loss = 3.8151 (1.195 sec/step)\n","I0830 17:25:37.139091 139622356395904 learning.py:507] global step 4129: loss = 3.2468 (1.218 sec/step)\n","I0830 17:25:38.377193 139622356395904 learning.py:507] global step 4130: loss = 2.7926 (1.236 sec/step)\n","I0830 17:25:39.602013 139622356395904 learning.py:507] global step 4131: loss = 3.9352 (1.223 sec/step)\n","I0830 17:25:40.820768 139622356395904 learning.py:507] global step 4132: loss = 3.3795 (1.217 sec/step)\n","I0830 17:25:42.033589 139622356395904 learning.py:507] global step 4133: loss = 3.4157 (1.211 sec/step)\n","I0830 17:25:43.277838 139622356395904 learning.py:507] global step 4134: loss = 3.5444 (1.243 sec/step)\n","I0830 17:25:44.523562 139622356395904 learning.py:507] global step 4135: loss = 3.2897 (1.243 sec/step)\n","I0830 17:25:45.759551 139622356395904 learning.py:507] global step 4136: loss = 3.9742 (1.233 sec/step)\n","I0830 17:25:46.988675 139622356395904 learning.py:507] global step 4137: loss = 3.9432 (1.227 sec/step)\n","I0830 17:25:48.190668 139622356395904 learning.py:507] global step 4138: loss = 3.0198 (1.200 sec/step)\n","I0830 17:25:49.379786 139622356395904 learning.py:507] global step 4139: loss = 4.2501 (1.187 sec/step)\n","I0830 17:25:50.593021 139622356395904 learning.py:507] global step 4140: loss = 3.4825 (1.211 sec/step)\n","I0830 17:25:51.808397 139622356395904 learning.py:507] global step 4141: loss = 4.0652 (1.214 sec/step)\n","I0830 17:25:53.020504 139622356395904 learning.py:507] global step 4142: loss = 3.5252 (1.210 sec/step)\n","I0830 17:25:54.214204 139622356395904 learning.py:507] global step 4143: loss = 4.1335 (1.192 sec/step)\n","I0830 17:25:55.454657 139622356395904 learning.py:507] global step 4144: loss = 3.2651 (1.238 sec/step)\n","I0830 17:25:56.675117 139622356395904 learning.py:507] global step 4145: loss = 3.1319 (1.219 sec/step)\n","I0830 17:25:57.907583 139622356395904 learning.py:507] global step 4146: loss = 3.4220 (1.231 sec/step)\n","I0830 17:25:59.124968 139622356395904 learning.py:507] global step 4147: loss = 4.5209 (1.216 sec/step)\n","I0830 17:26:00.540014 139622356395904 learning.py:507] global step 4148: loss = 2.9318 (1.407 sec/step)\n","I0830 17:26:02.467884 139619299985152 supervisor.py:1050] Recording summary at step 4149.\n","I0830 17:26:02.497648 139622356395904 learning.py:507] global step 4149: loss = 4.3147 (1.955 sec/step)\n","I0830 17:26:03.733209 139622356395904 learning.py:507] global step 4150: loss = 4.5138 (1.234 sec/step)\n","I0830 17:26:04.924512 139622356395904 learning.py:507] global step 4151: loss = 3.0456 (1.190 sec/step)\n","I0830 17:26:06.137470 139622356395904 learning.py:507] global step 4152: loss = 2.9653 (1.211 sec/step)\n","I0830 17:26:07.373864 139622356395904 learning.py:507] global step 4153: loss = 3.9263 (1.235 sec/step)\n","I0830 17:26:08.594013 139622356395904 learning.py:507] global step 4154: loss = 3.1865 (1.218 sec/step)\n","I0830 17:26:09.802635 139622356395904 learning.py:507] global step 4155: loss = 4.0001 (1.207 sec/step)\n","I0830 17:26:11.005944 139622356395904 learning.py:507] global step 4156: loss = 3.2844 (1.201 sec/step)\n","I0830 17:26:12.259020 139622356395904 learning.py:507] global step 4157: loss = 4.2738 (1.251 sec/step)\n","I0830 17:26:13.454906 139622356395904 learning.py:507] global step 4158: loss = 3.4778 (1.194 sec/step)\n","I0830 17:26:14.685710 139622356395904 learning.py:507] global step 4159: loss = 3.1540 (1.229 sec/step)\n","I0830 17:26:15.889618 139622356395904 learning.py:507] global step 4160: loss = 2.9516 (1.202 sec/step)\n","I0830 17:26:17.117262 139622356395904 learning.py:507] global step 4161: loss = 3.0600 (1.226 sec/step)\n","I0830 17:26:18.374398 139622356395904 learning.py:507] global step 4162: loss = 3.3751 (1.255 sec/step)\n","I0830 17:26:19.557198 139622356395904 learning.py:507] global step 4163: loss = 3.2441 (1.181 sec/step)\n","I0830 17:26:20.793338 139622356395904 learning.py:507] global step 4164: loss = 3.4938 (1.234 sec/step)\n","I0830 17:26:21.994026 139622356395904 learning.py:507] global step 4165: loss = 3.2550 (1.199 sec/step)\n","I0830 17:26:23.193505 139622356395904 learning.py:507] global step 4166: loss = 3.4580 (1.198 sec/step)\n","I0830 17:26:24.430241 139622356395904 learning.py:507] global step 4167: loss = 3.5879 (1.235 sec/step)\n","I0830 17:26:25.638678 139622356395904 learning.py:507] global step 4168: loss = 3.5420 (1.207 sec/step)\n","I0830 17:26:26.866141 139622356395904 learning.py:507] global step 4169: loss = 3.5908 (1.225 sec/step)\n","I0830 17:26:28.071446 139622356395904 learning.py:507] global step 4170: loss = 3.2139 (1.203 sec/step)\n","I0830 17:26:29.312219 139622356395904 learning.py:507] global step 4171: loss = 3.9605 (1.239 sec/step)\n","I0830 17:26:30.552189 139622356395904 learning.py:507] global step 4172: loss = 3.6419 (1.238 sec/step)\n","I0830 17:26:31.802988 139622356395904 learning.py:507] global step 4173: loss = 3.3148 (1.249 sec/step)\n","I0830 17:26:32.994976 139622356395904 learning.py:507] global step 4174: loss = 2.9992 (1.189 sec/step)\n","I0830 17:26:34.202728 139622356395904 learning.py:507] global step 4175: loss = 3.3377 (1.206 sec/step)\n","I0830 17:26:35.412716 139622356395904 learning.py:507] global step 4176: loss = 2.6822 (1.208 sec/step)\n","I0830 17:26:36.649561 139622356395904 learning.py:507] global step 4177: loss = 3.1001 (1.235 sec/step)\n","I0830 17:26:37.881629 139622356395904 learning.py:507] global step 4178: loss = 4.8962 (1.230 sec/step)\n","I0830 17:26:39.114065 139622356395904 learning.py:507] global step 4179: loss = 3.4572 (1.230 sec/step)\n","I0830 17:26:40.357841 139622356395904 learning.py:507] global step 4180: loss = 3.1984 (1.242 sec/step)\n","I0830 17:26:41.601993 139622356395904 learning.py:507] global step 4181: loss = 3.3766 (1.242 sec/step)\n","I0830 17:26:42.801756 139622356395904 learning.py:507] global step 4182: loss = 4.7653 (1.198 sec/step)\n","I0830 17:26:44.045821 139622356395904 learning.py:507] global step 4183: loss = 3.2430 (1.242 sec/step)\n","I0830 17:26:45.235466 139622356395904 learning.py:507] global step 4184: loss = 3.3205 (1.187 sec/step)\n","I0830 17:26:46.476664 139622356395904 learning.py:507] global step 4185: loss = 3.4092 (1.239 sec/step)\n","I0830 17:26:47.703462 139622356395904 learning.py:507] global step 4186: loss = 3.4345 (1.225 sec/step)\n","I0830 17:26:48.924223 139622356395904 learning.py:507] global step 4187: loss = 3.5145 (1.219 sec/step)\n","I0830 17:26:50.174581 139622356395904 learning.py:507] global step 4188: loss = 3.4678 (1.249 sec/step)\n","I0830 17:26:51.377584 139622356395904 learning.py:507] global step 4189: loss = 3.3101 (1.201 sec/step)\n","I0830 17:26:52.627191 139622356395904 learning.py:507] global step 4190: loss = 3.5313 (1.248 sec/step)\n","I0830 17:26:53.852578 139622356395904 learning.py:507] global step 4191: loss = 5.4365 (1.223 sec/step)\n","I0830 17:26:55.089853 139622356395904 learning.py:507] global step 4192: loss = 3.2992 (1.235 sec/step)\n","I0830 17:26:56.358628 139622356395904 learning.py:507] global step 4193: loss = 3.6603 (1.267 sec/step)\n","I0830 17:26:57.550330 139622356395904 learning.py:507] global step 4194: loss = 4.3934 (1.190 sec/step)\n","I0830 17:26:58.765146 139622356395904 learning.py:507] global step 4195: loss = 4.2373 (1.213 sec/step)\n","I0830 17:26:59.991353 139622356395904 learning.py:507] global step 4196: loss = 4.2094 (1.224 sec/step)\n","I0830 17:27:01.204995 139622356395904 learning.py:507] global step 4197: loss = 3.2255 (1.212 sec/step)\n","I0830 17:27:02.454465 139622356395904 learning.py:507] global step 4198: loss = 3.0623 (1.248 sec/step)\n","I0830 17:27:03.673611 139622356395904 learning.py:507] global step 4199: loss = 4.7346 (1.217 sec/step)\n","I0830 17:27:04.877938 139622356395904 learning.py:507] global step 4200: loss = 3.7193 (1.203 sec/step)\n","I0830 17:27:06.072427 139622356395904 learning.py:507] global step 4201: loss = 3.6827 (1.193 sec/step)\n","I0830 17:27:07.314078 139622356395904 learning.py:507] global step 4202: loss = 3.1979 (1.240 sec/step)\n","I0830 17:27:08.560415 139622356395904 learning.py:507] global step 4203: loss = 3.3929 (1.244 sec/step)\n","I0830 17:27:09.760469 139622356395904 learning.py:507] global step 4204: loss = 4.0159 (1.198 sec/step)\n","I0830 17:27:11.014675 139622356395904 learning.py:507] global step 4205: loss = 3.6010 (1.252 sec/step)\n","I0830 17:27:12.235860 139622356395904 learning.py:507] global step 4206: loss = 3.3623 (1.219 sec/step)\n","I0830 17:27:13.466299 139622356395904 learning.py:507] global step 4207: loss = 3.3605 (1.228 sec/step)\n","I0830 17:27:14.712075 139622356395904 learning.py:507] global step 4208: loss = 3.8920 (1.244 sec/step)\n","I0830 17:27:15.918604 139622356395904 learning.py:507] global step 4209: loss = 2.7561 (1.205 sec/step)\n","I0830 17:27:17.143699 139622356395904 learning.py:507] global step 4210: loss = 2.8763 (1.223 sec/step)\n","I0830 17:27:18.342468 139622356395904 learning.py:507] global step 4211: loss = 3.2372 (1.197 sec/step)\n","I0830 17:27:19.560866 139622356395904 learning.py:507] global step 4212: loss = 2.8726 (1.216 sec/step)\n","I0830 17:27:20.762511 139622356395904 learning.py:507] global step 4213: loss = 3.5828 (1.200 sec/step)\n","I0830 17:27:22.022347 139622356395904 learning.py:507] global step 4214: loss = 4.0316 (1.258 sec/step)\n","I0830 17:27:23.274068 139622356395904 learning.py:507] global step 4215: loss = 3.8235 (1.250 sec/step)\n","I0830 17:27:24.457484 139622356395904 learning.py:507] global step 4216: loss = 3.1179 (1.181 sec/step)\n","I0830 17:27:25.658084 139622356395904 learning.py:507] global step 4217: loss = 3.0602 (1.199 sec/step)\n","I0830 17:27:26.853616 139622356395904 learning.py:507] global step 4218: loss = 3.6545 (1.194 sec/step)\n","I0830 17:27:28.099765 139622356395904 learning.py:507] global step 4219: loss = 3.6576 (1.244 sec/step)\n","I0830 17:27:29.346230 139622356395904 learning.py:507] global step 4220: loss = 2.9906 (1.244 sec/step)\n","I0830 17:27:30.591487 139622356395904 learning.py:507] global step 4221: loss = 3.5858 (1.243 sec/step)\n","I0830 17:27:31.783649 139622356395904 learning.py:507] global step 4222: loss = 3.1474 (1.190 sec/step)\n","I0830 17:27:33.015403 139622356395904 learning.py:507] global step 4223: loss = 2.8984 (1.230 sec/step)\n","I0830 17:27:34.289039 139622356395904 learning.py:507] global step 4224: loss = 3.8628 (1.272 sec/step)\n","I0830 17:27:35.501926 139622356395904 learning.py:507] global step 4225: loss = 3.2341 (1.211 sec/step)\n","I0830 17:27:36.774256 139622356395904 learning.py:507] global step 4226: loss = 2.7731 (1.270 sec/step)\n","I0830 17:27:37.982612 139622356395904 learning.py:507] global step 4227: loss = 3.1546 (1.206 sec/step)\n","I0830 17:27:39.169449 139622356395904 learning.py:507] global step 4228: loss = 3.0774 (1.185 sec/step)\n","I0830 17:27:40.396188 139622356395904 learning.py:507] global step 4229: loss = 3.8639 (1.225 sec/step)\n","I0830 17:27:41.605206 139622356395904 learning.py:507] global step 4230: loss = 3.4316 (1.207 sec/step)\n","I0830 17:27:42.834827 139622356395904 learning.py:507] global step 4231: loss = 3.5861 (1.228 sec/step)\n","I0830 17:27:44.038964 139622356395904 learning.py:507] global step 4232: loss = 3.9805 (1.202 sec/step)\n","I0830 17:27:45.250413 139622356395904 learning.py:507] global step 4233: loss = 3.3274 (1.209 sec/step)\n","I0830 17:27:46.459118 139622356395904 learning.py:507] global step 4234: loss = 3.6817 (1.206 sec/step)\n","I0830 17:27:47.668619 139622356395904 learning.py:507] global step 4235: loss = 3.7966 (1.208 sec/step)\n","I0830 17:27:48.902691 139622356395904 learning.py:507] global step 4236: loss = 3.1747 (1.232 sec/step)\n","I0830 17:27:50.143834 139622356395904 learning.py:507] global step 4237: loss = 3.1347 (1.239 sec/step)\n","I0830 17:27:51.359875 139622356395904 learning.py:507] global step 4238: loss = 4.2415 (1.214 sec/step)\n","I0830 17:27:52.569194 139622356395904 learning.py:507] global step 4239: loss = 2.9264 (1.207 sec/step)\n","I0830 17:27:53.827262 139622356395904 learning.py:507] global step 4240: loss = 4.4561 (1.256 sec/step)\n","I0830 17:27:55.035807 139622356395904 learning.py:507] global step 4241: loss = 3.4395 (1.207 sec/step)\n","I0830 17:27:56.250088 139622356395904 learning.py:507] global step 4242: loss = 4.1197 (1.212 sec/step)\n","I0830 17:27:57.529761 139622356395904 learning.py:507] global step 4243: loss = 3.2937 (1.278 sec/step)\n","I0830 17:27:58.756706 139622356395904 learning.py:507] global step 4244: loss = 2.8753 (1.225 sec/step)\n","I0830 17:28:00.147696 139622356395904 learning.py:507] global step 4245: loss = 2.9119 (1.282 sec/step)\n","I0830 17:28:02.076081 139619299985152 supervisor.py:1050] Recording summary at step 4246.\n","I0830 17:28:02.103790 139622356395904 learning.py:507] global step 4246: loss = 4.1368 (1.954 sec/step)\n","I0830 17:28:03.349617 139622356395904 learning.py:507] global step 4247: loss = 4.8588 (1.244 sec/step)\n","I0830 17:28:04.563121 139622356395904 learning.py:507] global step 4248: loss = 4.0387 (1.211 sec/step)\n","I0830 17:28:05.739765 139622356395904 learning.py:507] global step 4249: loss = 2.7458 (1.175 sec/step)\n","I0830 17:28:06.972783 139622356395904 learning.py:507] global step 4250: loss = 3.2683 (1.231 sec/step)\n","I0830 17:28:08.188804 139622356395904 learning.py:507] global step 4251: loss = 2.7957 (1.214 sec/step)\n","I0830 17:28:09.420870 139622356395904 learning.py:507] global step 4252: loss = 3.0216 (1.230 sec/step)\n","I0830 17:28:10.672538 139622356395904 learning.py:507] global step 4253: loss = 3.0833 (1.250 sec/step)\n","I0830 17:28:11.884111 139622356395904 learning.py:507] global step 4254: loss = 3.6266 (1.210 sec/step)\n","I0830 17:28:13.085449 139622356395904 learning.py:507] global step 4255: loss = 3.1775 (1.199 sec/step)\n","I0830 17:28:14.272127 139622356395904 learning.py:507] global step 4256: loss = 3.4130 (1.185 sec/step)\n","I0830 17:28:15.469890 139622356395904 learning.py:507] global step 4257: loss = 3.2921 (1.196 sec/step)\n","I0830 17:28:16.668309 139622356395904 learning.py:507] global step 4258: loss = 3.6331 (1.197 sec/step)\n","I0830 17:28:17.880275 139622356395904 learning.py:507] global step 4259: loss = 5.0051 (1.209 sec/step)\n","I0830 17:28:19.102980 139622356395904 learning.py:507] global step 4260: loss = 3.6621 (1.220 sec/step)\n","I0830 17:28:20.301921 139622356395904 learning.py:507] global step 4261: loss = 3.2787 (1.197 sec/step)\n","I0830 17:28:21.518952 139622356395904 learning.py:507] global step 4262: loss = 3.6654 (1.215 sec/step)\n","I0830 17:28:22.752738 139622356395904 learning.py:507] global step 4263: loss = 3.6856 (1.231 sec/step)\n","I0830 17:28:23.957276 139622356395904 learning.py:507] global step 4264: loss = 3.7017 (1.203 sec/step)\n","I0830 17:28:25.189227 139622356395904 learning.py:507] global step 4265: loss = 5.1665 (1.230 sec/step)\n","I0830 17:28:26.393975 139622356395904 learning.py:507] global step 4266: loss = 3.2088 (1.203 sec/step)\n","I0830 17:28:27.621469 139622356395904 learning.py:507] global step 4267: loss = 2.8547 (1.226 sec/step)\n","I0830 17:28:28.847069 139622356395904 learning.py:507] global step 4268: loss = 2.8958 (1.224 sec/step)\n","I0830 17:28:30.042823 139622356395904 learning.py:507] global step 4269: loss = 3.2631 (1.194 sec/step)\n","I0830 17:28:31.249993 139622356395904 learning.py:507] global step 4270: loss = 4.2986 (1.204 sec/step)\n","I0830 17:28:32.466254 139622356395904 learning.py:507] global step 4271: loss = 2.4978 (1.214 sec/step)\n","I0830 17:28:33.668168 139622356395904 learning.py:507] global step 4272: loss = 2.9676 (1.200 sec/step)\n","I0830 17:28:34.908329 139622356395904 learning.py:507] global step 4273: loss = 4.0776 (1.237 sec/step)\n","I0830 17:28:36.107798 139622356395904 learning.py:507] global step 4274: loss = 4.2822 (1.197 sec/step)\n","I0830 17:28:37.329758 139622356395904 learning.py:507] global step 4275: loss = 3.6428 (1.220 sec/step)\n","I0830 17:28:38.542170 139622356395904 learning.py:507] global step 4276: loss = 2.7768 (1.211 sec/step)\n","I0830 17:28:39.751634 139622356395904 learning.py:507] global step 4277: loss = 3.3577 (1.208 sec/step)\n","I0830 17:28:40.967895 139622356395904 learning.py:507] global step 4278: loss = 3.7250 (1.215 sec/step)\n","I0830 17:28:42.189696 139622356395904 learning.py:507] global step 4279: loss = 3.1755 (1.220 sec/step)\n","I0830 17:28:43.401784 139622356395904 learning.py:507] global step 4280: loss = 3.0940 (1.210 sec/step)\n","I0830 17:28:44.604030 139622356395904 learning.py:507] global step 4281: loss = 2.9394 (1.200 sec/step)\n","I0830 17:28:45.822374 139622356395904 learning.py:507] global step 4282: loss = 3.7561 (1.216 sec/step)\n","I0830 17:28:47.027974 139622356395904 learning.py:507] global step 4283: loss = 2.7961 (1.204 sec/step)\n","I0830 17:28:48.266475 139622356395904 learning.py:507] global step 4284: loss = 3.5808 (1.237 sec/step)\n","I0830 17:28:49.494462 139622356395904 learning.py:507] global step 4285: loss = 4.0696 (1.226 sec/step)\n","I0830 17:28:50.711533 139622356395904 learning.py:507] global step 4286: loss = 3.4615 (1.215 sec/step)\n","I0830 17:28:51.962962 139622356395904 learning.py:507] global step 4287: loss = 3.3926 (1.250 sec/step)\n","I0830 17:28:53.166234 139622356395904 learning.py:507] global step 4288: loss = 4.2772 (1.201 sec/step)\n","I0830 17:28:54.412856 139622356395904 learning.py:507] global step 4289: loss = 3.4534 (1.245 sec/step)\n","I0830 17:28:55.640016 139622356395904 learning.py:507] global step 4290: loss = 3.3874 (1.225 sec/step)\n","I0830 17:28:56.863525 139622356395904 learning.py:507] global step 4291: loss = 3.9595 (1.222 sec/step)\n","I0830 17:28:58.082772 139622356395904 learning.py:507] global step 4292: loss = 2.8402 (1.217 sec/step)\n","I0830 17:28:59.332466 139622356395904 learning.py:507] global step 4293: loss = 2.8678 (1.248 sec/step)\n","I0830 17:29:00.560977 139622356395904 learning.py:507] global step 4294: loss = 3.1477 (1.227 sec/step)\n","I0830 17:29:01.786844 139622356395904 learning.py:507] global step 4295: loss = 3.1035 (1.224 sec/step)\n","I0830 17:29:03.019655 139622356395904 learning.py:507] global step 4296: loss = 3.5483 (1.231 sec/step)\n","I0830 17:29:04.232209 139622356395904 learning.py:507] global step 4297: loss = 3.2268 (1.211 sec/step)\n","I0830 17:29:05.457551 139622356395904 learning.py:507] global step 4298: loss = 3.7898 (1.224 sec/step)\n","I0830 17:29:06.687316 139622356395904 learning.py:507] global step 4299: loss = 3.0490 (1.228 sec/step)\n","I0830 17:29:07.895540 139622356395904 learning.py:507] global step 4300: loss = 2.6128 (1.206 sec/step)\n","I0830 17:29:09.147524 139622356395904 learning.py:507] global step 4301: loss = 3.5245 (1.250 sec/step)\n","I0830 17:29:10.335439 139622356395904 learning.py:507] global step 4302: loss = 5.4368 (1.186 sec/step)\n","I0830 17:29:11.532824 139622356395904 learning.py:507] global step 4303: loss = 3.3564 (1.195 sec/step)\n","I0830 17:29:12.734974 139622356395904 learning.py:507] global step 4304: loss = 3.4855 (1.200 sec/step)\n","I0830 17:29:13.988010 139622356395904 learning.py:507] global step 4305: loss = 3.4322 (1.248 sec/step)\n","I0830 17:29:15.230713 139622356395904 learning.py:507] global step 4306: loss = 3.2501 (1.241 sec/step)\n","I0830 17:29:16.477149 139622356395904 learning.py:507] global step 4307: loss = 2.8963 (1.244 sec/step)\n","I0830 17:29:17.690477 139622356395904 learning.py:507] global step 4308: loss = 3.8241 (1.212 sec/step)\n","I0830 17:29:18.893003 139622356395904 learning.py:507] global step 4309: loss = 3.4427 (1.201 sec/step)\n","I0830 17:29:20.138746 139622356395904 learning.py:507] global step 4310: loss = 3.4669 (1.244 sec/step)\n","I0830 17:29:21.353873 139622356395904 learning.py:507] global step 4311: loss = 3.9949 (1.214 sec/step)\n","I0830 17:29:22.594914 139622356395904 learning.py:507] global step 4312: loss = 3.0593 (1.239 sec/step)\n","I0830 17:29:23.813600 139622356395904 learning.py:507] global step 4313: loss = 3.6872 (1.217 sec/step)\n","I0830 17:29:25.012109 139622356395904 learning.py:507] global step 4314: loss = 3.7897 (1.196 sec/step)\n","I0830 17:29:26.202166 139622356395904 learning.py:507] global step 4315: loss = 3.1306 (1.188 sec/step)\n","I0830 17:29:27.434011 139622356395904 learning.py:507] global step 4316: loss = 3.7055 (1.230 sec/step)\n","I0830 17:29:28.676239 139622356395904 learning.py:507] global step 4317: loss = 3.3396 (1.240 sec/step)\n","I0830 17:29:29.865553 139622356395904 learning.py:507] global step 4318: loss = 3.1342 (1.187 sec/step)\n","I0830 17:29:31.093029 139622356395904 learning.py:507] global step 4319: loss = 3.5442 (1.226 sec/step)\n","I0830 17:29:32.330557 139622356395904 learning.py:507] global step 4320: loss = 3.8767 (1.236 sec/step)\n","I0830 17:29:33.525745 139622356395904 learning.py:507] global step 4321: loss = 3.2369 (1.193 sec/step)\n","I0830 17:29:34.720144 139622356395904 learning.py:507] global step 4322: loss = 4.4437 (1.193 sec/step)\n","I0830 17:29:35.963027 139622356395904 learning.py:507] global step 4323: loss = 3.7943 (1.241 sec/step)\n","I0830 17:29:37.152014 139622356395904 learning.py:507] global step 4324: loss = 4.0942 (1.187 sec/step)\n","I0830 17:29:38.372795 139622356395904 learning.py:507] global step 4325: loss = 4.4740 (1.219 sec/step)\n","I0830 17:29:39.555728 139622356395904 learning.py:507] global step 4326: loss = 3.5112 (1.181 sec/step)\n","I0830 17:29:40.805263 139622356395904 learning.py:507] global step 4327: loss = 3.0188 (1.247 sec/step)\n","I0830 17:29:42.047347 139622356395904 learning.py:507] global step 4328: loss = 3.1941 (1.240 sec/step)\n","I0830 17:29:43.247389 139622356395904 learning.py:507] global step 4329: loss = 3.0761 (1.198 sec/step)\n","I0830 17:29:44.473200 139622356395904 learning.py:507] global step 4330: loss = 4.3205 (1.224 sec/step)\n","I0830 17:29:45.651750 139622356395904 learning.py:507] global step 4331: loss = 3.6306 (1.177 sec/step)\n","I0830 17:29:46.881879 139622356395904 learning.py:507] global step 4332: loss = 3.1892 (1.228 sec/step)\n","I0830 17:29:48.128198 139622356395904 learning.py:507] global step 4333: loss = 3.4175 (1.245 sec/step)\n","I0830 17:29:49.367703 139622356395904 learning.py:507] global step 4334: loss = 3.4195 (1.238 sec/step)\n","I0830 17:29:50.563770 139622356395904 learning.py:507] global step 4335: loss = 2.9525 (1.194 sec/step)\n","I0830 17:29:51.787610 139622356395904 learning.py:507] global step 4336: loss = 2.8080 (1.222 sec/step)\n","I0830 17:29:53.021489 139622356395904 learning.py:507] global step 4337: loss = 3.4966 (1.232 sec/step)\n","I0830 17:29:54.261716 139622356395904 learning.py:507] global step 4338: loss = 3.8732 (1.238 sec/step)\n","I0830 17:29:55.491966 139622356395904 learning.py:507] global step 4339: loss = 2.8192 (1.229 sec/step)\n","I0830 17:29:56.730463 139622356395904 learning.py:507] global step 4340: loss = 3.3057 (1.237 sec/step)\n","I0830 17:29:57.975643 139622356395904 learning.py:507] global step 4341: loss = 3.1119 (1.243 sec/step)\n","I0830 17:29:59.185434 139622356395904 learning.py:507] global step 4342: loss = 3.1813 (1.208 sec/step)\n","I0830 17:29:59.645657 139619316770560 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n","I0830 17:30:01.105674 139622356395904 learning.py:507] global step 4343: loss = 3.4581 (1.405 sec/step)\n","I0830 17:30:02.580804 139619299985152 supervisor.py:1050] Recording summary at step 4343.\n","I0830 17:30:03.567551 139622356395904 learning.py:507] global step 4344: loss = 3.3403 (2.451 sec/step)\n","I0830 17:30:05.152269 139622356395904 learning.py:507] global step 4345: loss = 4.1726 (1.577 sec/step)\n","I0830 17:30:06.378594 139622356395904 learning.py:507] global step 4346: loss = 3.3412 (1.224 sec/step)\n","I0830 17:30:07.584480 139622356395904 learning.py:507] global step 4347: loss = 2.8777 (1.204 sec/step)\n","I0830 17:30:08.781601 139622356395904 learning.py:507] global step 4348: loss = 4.1148 (1.195 sec/step)\n","I0830 17:30:09.959490 139622356395904 learning.py:507] global step 4349: loss = 3.4263 (1.176 sec/step)\n","I0830 17:30:11.153778 139622356395904 learning.py:507] global step 4350: loss = 3.2525 (1.192 sec/step)\n","I0830 17:30:12.349767 139622356395904 learning.py:507] global step 4351: loss = 2.8585 (1.194 sec/step)\n","I0830 17:30:13.591169 139622356395904 learning.py:507] global step 4352: loss = 2.6992 (1.239 sec/step)\n","I0830 17:30:14.801557 139622356395904 learning.py:507] global step 4353: loss = 3.1703 (1.209 sec/step)\n","I0830 17:30:16.040093 139622356395904 learning.py:507] global step 4354: loss = 3.0735 (1.237 sec/step)\n","I0830 17:30:17.256036 139622356395904 learning.py:507] global step 4355: loss = 3.8720 (1.214 sec/step)\n","I0830 17:30:18.486042 139622356395904 learning.py:507] global step 4356: loss = 3.6245 (1.228 sec/step)\n","I0830 17:30:19.717189 139622356395904 learning.py:507] global step 4357: loss = 3.6026 (1.229 sec/step)\n","I0830 17:30:20.963771 139622356395904 learning.py:507] global step 4358: loss = 3.5066 (1.244 sec/step)\n","I0830 17:30:22.216509 139622356395904 learning.py:507] global step 4359: loss = 2.8473 (1.250 sec/step)\n","I0830 17:30:23.420039 139622356395904 learning.py:507] global step 4360: loss = 2.7877 (1.202 sec/step)\n","I0830 17:30:24.621881 139622356395904 learning.py:507] global step 4361: loss = 6.1182 (1.200 sec/step)\n","I0830 17:30:25.831315 139622356395904 learning.py:507] global step 4362: loss = 4.0328 (1.207 sec/step)\n","I0830 17:30:27.084278 139622356395904 learning.py:507] global step 4363: loss = 3.2306 (1.251 sec/step)\n","I0830 17:30:28.289472 139622356395904 learning.py:507] global step 4364: loss = 3.2179 (1.204 sec/step)\n","I0830 17:30:29.513719 139622356395904 learning.py:507] global step 4365: loss = 3.3819 (1.222 sec/step)\n","I0830 17:30:30.764869 139622356395904 learning.py:507] global step 4366: loss = 3.2473 (1.249 sec/step)\n","I0830 17:30:31.992920 139622356395904 learning.py:507] global step 4367: loss = 3.9606 (1.226 sec/step)\n","I0830 17:30:33.219520 139622356395904 learning.py:507] global step 4368: loss = 3.1441 (1.225 sec/step)\n","I0830 17:30:34.417500 139622356395904 learning.py:507] global step 4369: loss = 3.3568 (1.196 sec/step)\n","I0830 17:30:35.635611 139622356395904 learning.py:507] global step 4370: loss = 3.7408 (1.216 sec/step)\n","I0830 17:30:36.866563 139622356395904 learning.py:507] global step 4371: loss = 3.0880 (1.229 sec/step)\n","I0830 17:30:38.082135 139622356395904 learning.py:507] global step 4372: loss = 3.0096 (1.210 sec/step)\n","I0830 17:30:39.285442 139622356395904 learning.py:507] global step 4373: loss = 3.0073 (1.202 sec/step)\n","I0830 17:30:40.530844 139622356395904 learning.py:507] global step 4374: loss = 3.7828 (1.243 sec/step)\n","I0830 17:30:41.788005 139622356395904 learning.py:507] global step 4375: loss = 2.7187 (1.255 sec/step)\n","I0830 17:30:42.992088 139622356395904 learning.py:507] global step 4376: loss = 2.9323 (1.202 sec/step)\n","I0830 17:30:44.184212 139622356395904 learning.py:507] global step 4377: loss = 3.1460 (1.190 sec/step)\n","I0830 17:30:45.397406 139622356395904 learning.py:507] global step 4378: loss = 4.7278 (1.211 sec/step)\n","I0830 17:30:46.664892 139622356395904 learning.py:507] global step 4379: loss = 3.3916 (1.266 sec/step)\n","I0830 17:30:47.889803 139622356395904 learning.py:507] global step 4380: loss = 2.5879 (1.223 sec/step)\n","I0830 17:30:49.142940 139622356395904 learning.py:507] global step 4381: loss = 3.6052 (1.251 sec/step)\n","I0830 17:30:50.377199 139622356395904 learning.py:507] global step 4382: loss = 3.0699 (1.232 sec/step)\n","I0830 17:30:51.587504 139622356395904 learning.py:507] global step 4383: loss = 2.8479 (1.208 sec/step)\n","I0830 17:30:52.777271 139622356395904 learning.py:507] global step 4384: loss = 3.1118 (1.188 sec/step)\n","I0830 17:30:53.991354 139622356395904 learning.py:507] global step 4385: loss = 2.9751 (1.212 sec/step)\n","I0830 17:30:55.246457 139622356395904 learning.py:507] global step 4386: loss = 3.0746 (1.253 sec/step)\n","I0830 17:30:56.467312 139622356395904 learning.py:507] global step 4387: loss = 3.4180 (1.219 sec/step)\n","I0830 17:30:57.712626 139622356395904 learning.py:507] global step 4388: loss = 3.9080 (1.243 sec/step)\n","I0830 17:30:58.914457 139622356395904 learning.py:507] global step 4389: loss = 5.3457 (1.200 sec/step)\n","I0830 17:31:00.164822 139622356395904 learning.py:507] global step 4390: loss = 3.4731 (1.248 sec/step)\n","I0830 17:31:01.391186 139622356395904 learning.py:507] global step 4391: loss = 3.6475 (1.224 sec/step)\n","I0830 17:31:02.619043 139622356395904 learning.py:507] global step 4392: loss = 3.0086 (1.226 sec/step)\n","I0830 17:31:03.831479 139622356395904 learning.py:507] global step 4393: loss = 2.8069 (1.211 sec/step)\n","I0830 17:31:05.041662 139622356395904 learning.py:507] global step 4394: loss = 3.3382 (1.208 sec/step)\n","I0830 17:31:06.251303 139622356395904 learning.py:507] global step 4395: loss = 4.2136 (1.208 sec/step)\n","I0830 17:31:07.496482 139622356395904 learning.py:507] global step 4396: loss = 4.3698 (1.243 sec/step)\n","I0830 17:31:08.735478 139622356395904 learning.py:507] global step 4397: loss = 4.4836 (1.237 sec/step)\n","I0830 17:31:09.945626 139622356395904 learning.py:507] global step 4398: loss = 3.4147 (1.208 sec/step)\n","I0830 17:31:11.178148 139622356395904 learning.py:507] global step 4399: loss = 3.0979 (1.231 sec/step)\n","I0830 17:31:12.413096 139622356395904 learning.py:507] global step 4400: loss = 4.0127 (1.233 sec/step)\n","I0830 17:31:13.667239 139622356395904 learning.py:507] global step 4401: loss = 3.3184 (1.252 sec/step)\n","I0830 17:31:14.883151 139622356395904 learning.py:507] global step 4402: loss = 3.8646 (1.214 sec/step)\n","I0830 17:31:16.108562 139622356395904 learning.py:507] global step 4403: loss = 2.9702 (1.224 sec/step)\n","I0830 17:31:17.358977 139622356395904 learning.py:507] global step 4404: loss = 2.8880 (1.249 sec/step)\n","I0830 17:31:18.608223 139622356395904 learning.py:507] global step 4405: loss = 3.4232 (1.247 sec/step)\n","I0830 17:31:19.836872 139622356395904 learning.py:507] global step 4406: loss = 5.0740 (1.226 sec/step)\n","I0830 17:31:21.078735 139622356395904 learning.py:507] global step 4407: loss = 3.1763 (1.240 sec/step)\n","I0830 17:31:22.285924 139622356395904 learning.py:507] global step 4408: loss = 3.6552 (1.205 sec/step)\n","I0830 17:31:23.494637 139622356395904 learning.py:507] global step 4409: loss = 4.5734 (1.206 sec/step)\n","I0830 17:31:24.723417 139622356395904 learning.py:507] global step 4410: loss = 3.6378 (1.227 sec/step)\n","I0830 17:31:25.974369 139622356395904 learning.py:507] global step 4411: loss = 3.3272 (1.246 sec/step)\n","I0830 17:31:27.189116 139622356395904 learning.py:507] global step 4412: loss = 4.4597 (1.213 sec/step)\n","I0830 17:31:28.429083 139622356395904 learning.py:507] global step 4413: loss = 3.3422 (1.238 sec/step)\n","I0830 17:31:29.647490 139622356395904 learning.py:507] global step 4414: loss = 5.7169 (1.216 sec/step)\n","I0830 17:31:30.832097 139622356395904 learning.py:507] global step 4415: loss = 3.3020 (1.182 sec/step)\n","I0830 17:31:32.043342 139622356395904 learning.py:507] global step 4416: loss = 3.0626 (1.209 sec/step)\n","I0830 17:31:33.261128 139622356395904 learning.py:507] global step 4417: loss = 3.9862 (1.216 sec/step)\n","I0830 17:31:34.458486 139622356395904 learning.py:507] global step 4418: loss = 3.3576 (1.196 sec/step)\n","I0830 17:31:35.730940 139622356395904 learning.py:507] global step 4419: loss = 3.3250 (1.271 sec/step)\n","I0830 17:31:36.955920 139622356395904 learning.py:507] global step 4420: loss = 3.7788 (1.223 sec/step)\n","I0830 17:31:38.206954 139622356395904 learning.py:507] global step 4421: loss = 3.5408 (1.249 sec/step)\n","I0830 17:31:39.405403 139622356395904 learning.py:507] global step 4422: loss = 3.2729 (1.197 sec/step)\n","I0830 17:31:40.624375 139622356395904 learning.py:507] global step 4423: loss = 3.3918 (1.217 sec/step)\n","I0830 17:31:41.821540 139622356395904 learning.py:507] global step 4424: loss = 3.7681 (1.195 sec/step)\n","I0830 17:31:43.021029 139622356395904 learning.py:507] global step 4425: loss = 3.8114 (1.198 sec/step)\n","I0830 17:31:44.226769 139622356395904 learning.py:507] global step 4426: loss = 3.0136 (1.204 sec/step)\n","I0830 17:31:45.450460 139622356395904 learning.py:507] global step 4427: loss = 3.1804 (1.222 sec/step)\n","I0830 17:31:46.653307 139622356395904 learning.py:507] global step 4428: loss = 2.7323 (1.201 sec/step)\n","I0830 17:31:47.850629 139622356395904 learning.py:507] global step 4429: loss = 2.8285 (1.195 sec/step)\n","I0830 17:31:49.078600 139622356395904 learning.py:507] global step 4430: loss = 3.2804 (1.226 sec/step)\n","I0830 17:31:50.307825 139622356395904 learning.py:507] global step 4431: loss = 4.7160 (1.227 sec/step)\n","I0830 17:31:51.523560 139622356395904 learning.py:507] global step 4432: loss = 2.7524 (1.214 sec/step)\n","I0830 17:31:52.727724 139622356395904 learning.py:507] global step 4433: loss = 3.6875 (1.202 sec/step)\n","I0830 17:31:53.988796 139622356395904 learning.py:507] global step 4434: loss = 3.4175 (1.259 sec/step)\n","I0830 17:31:55.203535 139622356395904 learning.py:507] global step 4435: loss = 4.2579 (1.213 sec/step)\n","I0830 17:31:56.447592 139622356395904 learning.py:507] global step 4436: loss = 2.9625 (1.242 sec/step)\n","I0830 17:31:57.683252 139622356395904 learning.py:507] global step 4437: loss = 3.2097 (1.234 sec/step)\n","I0830 17:31:58.925199 139622356395904 learning.py:507] global step 4438: loss = 4.1971 (1.240 sec/step)\n","I0830 17:32:00.230144 139622356395904 learning.py:507] global step 4439: loss = 2.7197 (1.239 sec/step)\n","I0830 17:32:02.346675 139619299985152 supervisor.py:1050] Recording summary at step 4440.\n","I0830 17:32:02.381611 139622356395904 learning.py:507] global step 4440: loss = 2.7472 (2.148 sec/step)\n","I0830 17:32:03.590723 139622356395904 learning.py:507] global step 4441: loss = 3.4949 (1.207 sec/step)\n","I0830 17:32:04.772436 139622356395904 learning.py:507] global step 4442: loss = 2.8045 (1.180 sec/step)\n","I0830 17:32:06.011366 139622356395904 learning.py:507] global step 4443: loss = 2.9188 (1.237 sec/step)\n","I0830 17:32:07.261241 139622356395904 learning.py:507] global step 4444: loss = 3.5688 (1.248 sec/step)\n","I0830 17:32:08.489787 139622356395904 learning.py:507] global step 4445: loss = 3.1212 (1.227 sec/step)\n","I0830 17:32:09.701066 139622356395904 learning.py:507] global step 4446: loss = 3.8200 (1.210 sec/step)\n","I0830 17:32:10.932403 139622356395904 learning.py:507] global step 4447: loss = 3.3777 (1.230 sec/step)\n","I0830 17:32:12.146143 139622356395904 learning.py:507] global step 4448: loss = 3.2399 (1.212 sec/step)\n","I0830 17:32:13.350119 139622356395904 learning.py:507] global step 4449: loss = 3.1235 (1.202 sec/step)\n","I0830 17:32:14.550268 139622356395904 learning.py:507] global step 4450: loss = 4.2221 (1.198 sec/step)\n","I0830 17:32:15.796308 139622356395904 learning.py:507] global step 4451: loss = 2.7269 (1.244 sec/step)\n","I0830 17:32:16.979757 139622356395904 learning.py:507] global step 4452: loss = 3.5069 (1.182 sec/step)\n","I0830 17:32:18.225965 139622356395904 learning.py:507] global step 4453: loss = 3.3304 (1.244 sec/step)\n","I0830 17:32:19.442082 139622356395904 learning.py:507] global step 4454: loss = 2.9058 (1.214 sec/step)\n","I0830 17:32:20.682559 139622356395904 learning.py:507] global step 4455: loss = 3.0529 (1.239 sec/step)\n","I0830 17:32:21.914900 139622356395904 learning.py:507] global step 4456: loss = 2.8279 (1.230 sec/step)\n","I0830 17:32:23.128947 139622356395904 learning.py:507] global step 4457: loss = 4.2268 (1.212 sec/step)\n","I0830 17:32:24.382245 139622356395904 learning.py:507] global step 4458: loss = 3.2208 (1.251 sec/step)\n","I0830 17:32:25.649016 139622356395904 learning.py:507] global step 4459: loss = 3.9849 (1.264 sec/step)\n","I0830 17:32:26.878753 139622356395904 learning.py:507] global step 4460: loss = 2.9868 (1.228 sec/step)\n","I0830 17:32:28.065617 139622356395904 learning.py:507] global step 4461: loss = 3.0717 (1.185 sec/step)\n","I0830 17:32:29.269266 139622356395904 learning.py:507] global step 4462: loss = 3.5655 (1.201 sec/step)\n","I0830 17:32:30.476283 139622356395904 learning.py:507] global step 4463: loss = 3.4522 (1.205 sec/step)\n","I0830 17:32:31.724152 139622356395904 learning.py:507] global step 4464: loss = 3.5154 (1.246 sec/step)\n","I0830 17:32:32.974798 139622356395904 learning.py:507] global step 4465: loss = 3.3216 (1.249 sec/step)\n","I0830 17:32:34.200934 139622356395904 learning.py:507] global step 4466: loss = 4.1417 (1.224 sec/step)\n","I0830 17:32:35.419081 139622356395904 learning.py:507] global step 4467: loss = 2.7945 (1.216 sec/step)\n","I0830 17:32:36.623203 139622356395904 learning.py:507] global step 4468: loss = 3.1886 (1.202 sec/step)\n","I0830 17:32:37.831549 139622356395904 learning.py:507] global step 4469: loss = 3.5459 (1.206 sec/step)\n","I0830 17:32:39.083493 139622356395904 learning.py:507] global step 4470: loss = 2.9285 (1.250 sec/step)\n","I0830 17:32:40.329513 139622356395904 learning.py:507] global step 4471: loss = 3.6105 (1.244 sec/step)\n","I0830 17:32:41.555813 139622356395904 learning.py:507] global step 4472: loss = 3.9608 (1.224 sec/step)\n","I0830 17:32:42.774451 139622356395904 learning.py:507] global step 4473: loss = 2.8262 (1.217 sec/step)\n","I0830 17:32:44.004768 139622356395904 learning.py:507] global step 4474: loss = 3.5961 (1.228 sec/step)\n","I0830 17:32:45.260814 139622356395904 learning.py:507] global step 4475: loss = 3.3184 (1.254 sec/step)\n","I0830 17:32:46.474785 139622356395904 learning.py:507] global step 4476: loss = 2.9631 (1.212 sec/step)\n","I0830 17:32:47.696446 139622356395904 learning.py:507] global step 4477: loss = 3.4985 (1.220 sec/step)\n","I0830 17:32:48.920878 139622356395904 learning.py:507] global step 4478: loss = 3.9551 (1.223 sec/step)\n","I0830 17:32:50.130764 139622356395904 learning.py:507] global step 4479: loss = 3.6835 (1.208 sec/step)\n","I0830 17:32:51.315201 139622356395904 learning.py:507] global step 4480: loss = 3.7372 (1.182 sec/step)\n","I0830 17:32:52.557716 139622356395904 learning.py:507] global step 4481: loss = 2.8823 (1.241 sec/step)\n","I0830 17:32:53.760559 139622356395904 learning.py:507] global step 4482: loss = 2.8259 (1.200 sec/step)\n","I0830 17:32:54.965638 139622356395904 learning.py:507] global step 4483: loss = 4.0160 (1.203 sec/step)\n","I0830 17:32:56.158860 139622356395904 learning.py:507] global step 4484: loss = 3.4718 (1.191 sec/step)\n","I0830 17:32:57.379105 139622356395904 learning.py:507] global step 4485: loss = 3.8123 (1.219 sec/step)\n","I0830 17:32:58.622843 139622356395904 learning.py:507] global step 4486: loss = 3.9929 (1.242 sec/step)\n","I0830 17:32:59.856621 139622356395904 learning.py:507] global step 4487: loss = 4.6117 (1.232 sec/step)\n","I0830 17:33:01.069720 139622356395904 learning.py:507] global step 4488: loss = 3.2035 (1.211 sec/step)\n","I0830 17:33:02.314450 139622356395904 learning.py:507] global step 4489: loss = 3.2172 (1.243 sec/step)\n","I0830 17:33:03.524967 139622356395904 learning.py:507] global step 4490: loss = 3.2818 (1.209 sec/step)\n","I0830 17:33:04.770153 139622356395904 learning.py:507] global step 4491: loss = 3.6211 (1.243 sec/step)\n","I0830 17:33:05.980037 139622356395904 learning.py:507] global step 4492: loss = 2.6696 (1.208 sec/step)\n","I0830 17:33:07.232341 139622356395904 learning.py:507] global step 4493: loss = 3.4633 (1.250 sec/step)\n","I0830 17:33:08.460669 139622356395904 learning.py:507] global step 4494: loss = 3.7645 (1.227 sec/step)\n","I0830 17:33:09.681611 139622356395904 learning.py:507] global step 4495: loss = 3.4042 (1.219 sec/step)\n","I0830 17:33:10.901337 139622356395904 learning.py:507] global step 4496: loss = 2.9025 (1.218 sec/step)\n","I0830 17:33:12.124538 139622356395904 learning.py:507] global step 4497: loss = 2.9073 (1.222 sec/step)\n","I0830 17:33:13.340346 139622356395904 learning.py:507] global step 4498: loss = 3.1453 (1.214 sec/step)\n","I0830 17:33:14.570779 139622356395904 learning.py:507] global step 4499: loss = 4.5889 (1.229 sec/step)\n","I0830 17:33:15.774067 139622356395904 learning.py:507] global step 4500: loss = 3.1390 (1.202 sec/step)\n","I0830 17:33:16.979253 139622356395904 learning.py:507] global step 4501: loss = 2.5870 (1.203 sec/step)\n","I0830 17:33:18.190668 139622356395904 learning.py:507] global step 4502: loss = 3.3760 (1.209 sec/step)\n","I0830 17:33:19.437611 139622356395904 learning.py:507] global step 4503: loss = 3.0581 (1.245 sec/step)\n","I0830 17:33:20.654922 139622356395904 learning.py:507] global step 4504: loss = 2.6505 (1.215 sec/step)\n","I0830 17:33:21.880211 139622356395904 learning.py:507] global step 4505: loss = 3.6544 (1.224 sec/step)\n","I0830 17:33:23.080867 139622356395904 learning.py:507] global step 4506: loss = 3.1472 (1.199 sec/step)\n","I0830 17:33:24.374701 139622356395904 learning.py:507] global step 4507: loss = 3.2485 (1.291 sec/step)\n","I0830 17:33:25.582676 139622356395904 learning.py:507] global step 4508: loss = 3.2922 (1.205 sec/step)\n","I0830 17:33:26.800707 139622356395904 learning.py:507] global step 4509: loss = 3.9427 (1.216 sec/step)\n","I0830 17:33:28.035240 139622356395904 learning.py:507] global step 4510: loss = 4.0597 (1.232 sec/step)\n","I0830 17:33:29.272720 139622356395904 learning.py:507] global step 4511: loss = 3.2968 (1.236 sec/step)\n","I0830 17:33:30.516044 139622356395904 learning.py:507] global step 4512: loss = 4.3324 (1.242 sec/step)\n","I0830 17:33:31.758297 139622356395904 learning.py:507] global step 4513: loss = 3.6123 (1.240 sec/step)\n","I0830 17:33:32.935916 139622356395904 learning.py:507] global step 4514: loss = 3.0075 (1.174 sec/step)\n","I0830 17:33:34.145568 139622356395904 learning.py:507] global step 4515: loss = 3.1212 (1.207 sec/step)\n","I0830 17:33:35.350520 139622356395904 learning.py:507] global step 4516: loss = 3.2900 (1.203 sec/step)\n","I0830 17:33:36.554438 139622356395904 learning.py:507] global step 4517: loss = 3.0445 (1.202 sec/step)\n","I0830 17:33:37.774841 139622356395904 learning.py:507] global step 4518: loss = 3.5111 (1.218 sec/step)\n","I0830 17:33:38.996426 139622356395904 learning.py:507] global step 4519: loss = 3.1968 (1.220 sec/step)\n","I0830 17:33:40.227973 139622356395904 learning.py:507] global step 4520: loss = 3.9981 (1.229 sec/step)\n","I0830 17:33:41.410082 139622356395904 learning.py:507] global step 4521: loss = 3.6462 (1.180 sec/step)\n","I0830 17:33:42.591794 139622356395904 learning.py:507] global step 4522: loss = 3.4950 (1.180 sec/step)\n","I0830 17:33:43.814642 139622356395904 learning.py:507] global step 4523: loss = 3.5092 (1.221 sec/step)\n","I0830 17:33:45.012525 139622356395904 learning.py:507] global step 4524: loss = 3.7115 (1.196 sec/step)\n","I0830 17:33:46.206474 139622356395904 learning.py:507] global step 4525: loss = 3.0928 (1.192 sec/step)\n","I0830 17:33:47.413736 139622356395904 learning.py:507] global step 4526: loss = 3.0620 (1.205 sec/step)\n","I0830 17:33:48.646442 139622356395904 learning.py:507] global step 4527: loss = 3.4269 (1.231 sec/step)\n","I0830 17:33:49.875578 139622356395904 learning.py:507] global step 4528: loss = 3.3506 (1.227 sec/step)\n","I0830 17:33:51.077372 139622356395904 learning.py:507] global step 4529: loss = 3.2495 (1.200 sec/step)\n","I0830 17:33:52.295713 139622356395904 learning.py:507] global step 4530: loss = 2.9206 (1.216 sec/step)\n","I0830 17:33:53.508838 139622356395904 learning.py:507] global step 4531: loss = 3.5281 (1.211 sec/step)\n","I0830 17:33:54.726161 139622356395904 learning.py:507] global step 4532: loss = 2.9965 (1.216 sec/step)\n","I0830 17:33:55.928295 139622356395904 learning.py:507] global step 4533: loss = 3.6818 (1.200 sec/step)\n","I0830 17:33:57.175710 139622356395904 learning.py:507] global step 4534: loss = 4.0651 (1.245 sec/step)\n","I0830 17:33:58.371038 139622356395904 learning.py:507] global step 4535: loss = 3.1341 (1.193 sec/step)\n","I0830 17:33:59.610620 139622356395904 learning.py:507] global step 4536: loss = 3.0245 (1.238 sec/step)\n","I0830 17:34:01.339495 139622356395904 learning.py:507] global step 4537: loss = 2.6464 (1.588 sec/step)\n","I0830 17:34:02.331766 139619299985152 supervisor.py:1050] Recording summary at step 4537.\n","I0830 17:34:02.968952 139622356395904 learning.py:507] global step 4538: loss = 3.7204 (1.618 sec/step)\n","I0830 17:34:04.160993 139622356395904 learning.py:507] global step 4539: loss = 2.9591 (1.190 sec/step)\n","I0830 17:34:05.378529 139622356395904 learning.py:507] global step 4540: loss = 3.0738 (1.216 sec/step)\n","I0830 17:34:06.601817 139622356395904 learning.py:507] global step 4541: loss = 3.5003 (1.222 sec/step)\n","I0830 17:34:07.837081 139622356395904 learning.py:507] global step 4542: loss = 3.3536 (1.233 sec/step)\n","I0830 17:34:09.066305 139622356395904 learning.py:507] global step 4543: loss = 3.4452 (1.227 sec/step)\n","I0830 17:34:10.285696 139622356395904 learning.py:507] global step 4544: loss = 3.1738 (1.217 sec/step)\n","I0830 17:34:11.505748 139622356395904 learning.py:507] global step 4545: loss = 3.9905 (1.218 sec/step)\n","I0830 17:34:12.736183 139622356395904 learning.py:507] global step 4546: loss = 3.7949 (1.229 sec/step)\n","I0830 17:34:13.933615 139622356395904 learning.py:507] global step 4547: loss = 3.3761 (1.196 sec/step)\n","I0830 17:34:15.166613 139622356395904 learning.py:507] global step 4548: loss = 3.0782 (1.231 sec/step)\n","I0830 17:34:16.394850 139622356395904 learning.py:507] global step 4549: loss = 3.0480 (1.226 sec/step)\n","I0830 17:34:17.604189 139622356395904 learning.py:507] global step 4550: loss = 4.0240 (1.207 sec/step)\n","I0830 17:34:18.790983 139622356395904 learning.py:507] global step 4551: loss = 3.3967 (1.185 sec/step)\n","I0830 17:34:20.015608 139622356395904 learning.py:507] global step 4552: loss = 3.3957 (1.223 sec/step)\n","I0830 17:34:21.229443 139622356395904 learning.py:507] global step 4553: loss = 2.7450 (1.212 sec/step)\n","I0830 17:34:22.423771 139622356395904 learning.py:507] global step 4554: loss = 4.9208 (1.193 sec/step)\n","I0830 17:34:23.643105 139622356395904 learning.py:507] global step 4555: loss = 2.6798 (1.217 sec/step)\n","I0830 17:34:24.881325 139622356395904 learning.py:507] global step 4556: loss = 3.7215 (1.236 sec/step)\n","I0830 17:34:26.102007 139622356395904 learning.py:507] global step 4557: loss = 3.0609 (1.219 sec/step)\n","I0830 17:34:27.334142 139622356395904 learning.py:507] global step 4558: loss = 3.0450 (1.230 sec/step)\n","I0830 17:34:28.501785 139622356395904 learning.py:507] global step 4559: loss = 4.1932 (1.166 sec/step)\n","I0830 17:34:29.752824 139622356395904 learning.py:507] global step 4560: loss = 3.5789 (1.249 sec/step)\n","I0830 17:34:30.938995 139622356395904 learning.py:507] global step 4561: loss = 3.3419 (1.185 sec/step)\n","I0830 17:34:32.152564 139622356395904 learning.py:507] global step 4562: loss = 3.2092 (1.212 sec/step)\n","I0830 17:34:33.379453 139622356395904 learning.py:507] global step 4563: loss = 3.2035 (1.225 sec/step)\n","I0830 17:34:34.615487 139622356395904 learning.py:507] global step 4564: loss = 3.5016 (1.234 sec/step)\n","I0830 17:34:35.831791 139622356395904 learning.py:507] global step 4565: loss = 4.1216 (1.214 sec/step)\n","I0830 17:34:37.068779 139622356395904 learning.py:507] global step 4566: loss = 3.3340 (1.234 sec/step)\n","I0830 17:34:38.298873 139622356395904 learning.py:507] global step 4567: loss = 3.5787 (1.228 sec/step)\n","I0830 17:34:39.515809 139622356395904 learning.py:507] global step 4568: loss = 3.7449 (1.215 sec/step)\n","I0830 17:34:40.743003 139622356395904 learning.py:507] global step 4569: loss = 3.7168 (1.225 sec/step)\n","I0830 17:34:41.955183 139622356395904 learning.py:507] global step 4570: loss = 3.0145 (1.210 sec/step)\n","I0830 17:34:43.170262 139622356395904 learning.py:507] global step 4571: loss = 3.2192 (1.213 sec/step)\n","I0830 17:34:44.404711 139622356395904 learning.py:507] global step 4572: loss = 2.9450 (1.233 sec/step)\n","I0830 17:34:45.627382 139622356395904 learning.py:507] global step 4573: loss = 4.9618 (1.221 sec/step)\n","I0830 17:34:46.827792 139622356395904 learning.py:507] global step 4574: loss = 3.4646 (1.199 sec/step)\n","I0830 17:34:48.028745 139622356395904 learning.py:507] global step 4575: loss = 3.4619 (1.199 sec/step)\n","I0830 17:34:49.279032 139622356395904 learning.py:507] global step 4576: loss = 3.6888 (1.248 sec/step)\n","I0830 17:34:50.496017 139622356395904 learning.py:507] global step 4577: loss = 3.0595 (1.215 sec/step)\n","I0830 17:34:51.724542 139622356395904 learning.py:507] global step 4578: loss = 5.0635 (1.226 sec/step)\n","I0830 17:34:52.983856 139622356395904 learning.py:507] global step 4579: loss = 3.5929 (1.257 sec/step)\n","I0830 17:34:54.224986 139622356395904 learning.py:507] global step 4580: loss = 3.4507 (1.239 sec/step)\n","I0830 17:34:55.476817 139622356395904 learning.py:507] global step 4581: loss = 3.2498 (1.250 sec/step)\n","I0830 17:34:56.679233 139622356395904 learning.py:507] global step 4582: loss = 3.1653 (1.201 sec/step)\n","I0830 17:34:57.926371 139622356395904 learning.py:507] global step 4583: loss = 3.7390 (1.245 sec/step)\n","I0830 17:34:59.150516 139622356395904 learning.py:507] global step 4584: loss = 4.8813 (1.222 sec/step)\n","I0830 17:35:00.375800 139622356395904 learning.py:507] global step 4585: loss = 3.1545 (1.223 sec/step)\n","I0830 17:35:01.589172 139622356395904 learning.py:507] global step 4586: loss = 2.8753 (1.212 sec/step)\n","I0830 17:35:02.825724 139622356395904 learning.py:507] global step 4587: loss = 2.8858 (1.235 sec/step)\n","I0830 17:35:04.066248 139622356395904 learning.py:507] global step 4588: loss = 3.0335 (1.239 sec/step)\n","I0830 17:35:05.285109 139622356395904 learning.py:507] global step 4589: loss = 2.9929 (1.217 sec/step)\n","I0830 17:35:06.474208 139622356395904 learning.py:507] global step 4590: loss = 3.1064 (1.187 sec/step)\n","I0830 17:35:07.676543 139622356395904 learning.py:507] global step 4591: loss = 3.8217 (1.200 sec/step)\n","I0830 17:35:08.923158 139622356395904 learning.py:507] global step 4592: loss = 3.6848 (1.245 sec/step)\n","I0830 17:35:10.166212 139622356395904 learning.py:507] global step 4593: loss = 2.9611 (1.241 sec/step)\n","I0830 17:35:11.370193 139622356395904 learning.py:507] global step 4594: loss = 4.9835 (1.202 sec/step)\n","I0830 17:35:12.555276 139622356395904 learning.py:507] global step 4595: loss = 3.1140 (1.183 sec/step)\n","I0830 17:35:13.793208 139622356395904 learning.py:507] global step 4596: loss = 3.0950 (1.236 sec/step)\n","I0830 17:35:15.026274 139622356395904 learning.py:507] global step 4597: loss = 3.5692 (1.231 sec/step)\n","I0830 17:35:16.229950 139622356395904 learning.py:507] global step 4598: loss = 2.4402 (1.202 sec/step)\n","I0830 17:35:17.440760 139622356395904 learning.py:507] global step 4599: loss = 4.1534 (1.209 sec/step)\n","I0830 17:35:18.701127 139622356395904 learning.py:507] global step 4600: loss = 3.0697 (1.259 sec/step)\n","I0830 17:35:19.956958 139622356395904 learning.py:507] global step 4601: loss = 3.6282 (1.254 sec/step)\n","I0830 17:35:21.199900 139622356395904 learning.py:507] global step 4602: loss = 4.0002 (1.241 sec/step)\n","I0830 17:35:22.411280 139622356395904 learning.py:507] global step 4603: loss = 3.7911 (1.209 sec/step)\n","I0830 17:35:23.637747 139622356395904 learning.py:507] global step 4604: loss = 2.7371 (1.225 sec/step)\n","I0830 17:35:24.880188 139622356395904 learning.py:507] global step 4605: loss = 3.5935 (1.241 sec/step)\n","I0830 17:35:26.111388 139622356395904 learning.py:507] global step 4606: loss = 2.8528 (1.230 sec/step)\n","I0830 17:35:27.299170 139622356395904 learning.py:507] global step 4607: loss = 3.7749 (1.186 sec/step)\n","I0830 17:35:28.518286 139622356395904 learning.py:507] global step 4608: loss = 3.3980 (1.217 sec/step)\n","I0830 17:35:29.748593 139622356395904 learning.py:507] global step 4609: loss = 3.2724 (1.229 sec/step)\n","I0830 17:35:30.987261 139622356395904 learning.py:507] global step 4610: loss = 3.0437 (1.237 sec/step)\n","I0830 17:35:32.193548 139622356395904 learning.py:507] global step 4611: loss = 3.2466 (1.204 sec/step)\n","I0830 17:35:33.411340 139622356395904 learning.py:507] global step 4612: loss = 2.9411 (1.216 sec/step)\n","I0830 17:35:34.673550 139622356395904 learning.py:507] global step 4613: loss = 5.6487 (1.260 sec/step)\n","I0830 17:35:35.886511 139622356395904 learning.py:507] global step 4614: loss = 3.4290 (1.211 sec/step)\n","I0830 17:35:37.106657 139622356395904 learning.py:507] global step 4615: loss = 2.6047 (1.218 sec/step)\n","I0830 17:35:38.328632 139622356395904 learning.py:507] global step 4616: loss = 5.2741 (1.220 sec/step)\n","I0830 17:35:39.548418 139622356395904 learning.py:507] global step 4617: loss = 2.9354 (1.218 sec/step)\n","I0830 17:35:40.762121 139622356395904 learning.py:507] global step 4618: loss = 3.6024 (1.212 sec/step)\n","I0830 17:35:42.001937 139622356395904 learning.py:507] global step 4619: loss = 3.5904 (1.238 sec/step)\n","I0830 17:35:43.221173 139622356395904 learning.py:507] global step 4620: loss = 3.1228 (1.217 sec/step)\n","I0830 17:35:44.453392 139622356395904 learning.py:507] global step 4621: loss = 3.3179 (1.230 sec/step)\n","I0830 17:35:45.667805 139622356395904 learning.py:507] global step 4622: loss = 3.3533 (1.213 sec/step)\n","I0830 17:35:46.903421 139622356395904 learning.py:507] global step 4623: loss = 3.0339 (1.234 sec/step)\n","I0830 17:35:48.125674 139622356395904 learning.py:507] global step 4624: loss = 3.2610 (1.220 sec/step)\n","I0830 17:35:49.322964 139622356395904 learning.py:507] global step 4625: loss = 3.2269 (1.195 sec/step)\n","I0830 17:35:50.612336 139622356395904 learning.py:507] global step 4626: loss = 3.4877 (1.287 sec/step)\n","I0830 17:35:51.858127 139622356395904 learning.py:507] global step 4627: loss = 3.5219 (1.243 sec/step)\n","I0830 17:35:53.082345 139622356395904 learning.py:507] global step 4628: loss = 2.6747 (1.223 sec/step)\n","I0830 17:35:54.327005 139622356395904 learning.py:507] global step 4629: loss = 2.9005 (1.243 sec/step)\n","I0830 17:35:55.540682 139622356395904 learning.py:507] global step 4630: loss = 2.8393 (1.212 sec/step)\n","I0830 17:35:56.774373 139622356395904 learning.py:507] global step 4631: loss = 4.5052 (1.232 sec/step)\n","I0830 17:35:58.022495 139622356395904 learning.py:507] global step 4632: loss = 3.0964 (1.246 sec/step)\n","I0830 17:35:59.261236 139622356395904 learning.py:507] global step 4633: loss = 3.0111 (1.237 sec/step)\n","I0830 17:36:00.687317 139622356395904 learning.py:507] global step 4634: loss = 3.1460 (1.414 sec/step)\n","I0830 17:36:02.582308 139619299985152 supervisor.py:1050] Recording summary at step 4635.\n","I0830 17:36:02.599445 139622356395904 learning.py:507] global step 4635: loss = 2.8832 (1.906 sec/step)\n","I0830 17:36:03.836385 139622356395904 learning.py:507] global step 4636: loss = 3.0266 (1.235 sec/step)\n","I0830 17:36:05.077386 139622356395904 learning.py:507] global step 4637: loss = 3.1685 (1.239 sec/step)\n","I0830 17:36:06.325949 139622356395904 learning.py:507] global step 4638: loss = 3.6392 (1.247 sec/step)\n","I0830 17:36:07.576419 139622356395904 learning.py:507] global step 4639: loss = 3.6087 (1.249 sec/step)\n","I0830 17:36:08.782980 139622356395904 learning.py:507] global step 4640: loss = 3.5406 (1.205 sec/step)\n","I0830 17:36:10.029218 139622356395904 learning.py:507] global step 4641: loss = 2.9335 (1.244 sec/step)\n","I0830 17:36:11.290267 139622356395904 learning.py:507] global step 4642: loss = 2.5633 (1.259 sec/step)\n","I0830 17:36:12.515605 139622356395904 learning.py:507] global step 4643: loss = 3.8952 (1.223 sec/step)\n","I0830 17:36:13.710669 139622356395904 learning.py:507] global step 4644: loss = 3.9178 (1.193 sec/step)\n","I0830 17:36:14.952771 139622356395904 learning.py:507] global step 4645: loss = 4.2764 (1.240 sec/step)\n","I0830 17:36:16.168122 139622356395904 learning.py:507] global step 4646: loss = 2.6308 (1.214 sec/step)\n","I0830 17:36:17.391581 139622356395904 learning.py:507] global step 4647: loss = 2.9466 (1.221 sec/step)\n","I0830 17:36:18.646169 139622356395904 learning.py:507] global step 4648: loss = 2.6246 (1.253 sec/step)\n","I0830 17:36:19.889975 139622356395904 learning.py:507] global step 4649: loss = 3.2533 (1.242 sec/step)\n","I0830 17:36:21.105568 139622356395904 learning.py:507] global step 4650: loss = 2.9315 (1.213 sec/step)\n","I0830 17:36:22.279790 139622356395904 learning.py:507] global step 4651: loss = 3.1065 (1.172 sec/step)\n","I0830 17:36:23.481760 139622356395904 learning.py:507] global step 4652: loss = 3.5659 (1.200 sec/step)\n","I0830 17:36:24.713300 139622356395904 learning.py:507] global step 4653: loss = 3.2600 (1.230 sec/step)\n","I0830 17:36:25.957610 139622356395904 learning.py:507] global step 4654: loss = 3.1696 (1.243 sec/step)\n","I0830 17:36:27.186690 139622356395904 learning.py:507] global step 4655: loss = 4.2569 (1.227 sec/step)\n","I0830 17:36:28.418594 139622356395904 learning.py:507] global step 4656: loss = 4.4474 (1.230 sec/step)\n","I0830 17:36:29.609192 139622356395904 learning.py:507] global step 4657: loss = 3.3094 (1.189 sec/step)\n","I0830 17:36:30.851391 139622356395904 learning.py:507] global step 4658: loss = 3.2332 (1.240 sec/step)\n","I0830 17:36:32.088937 139622356395904 learning.py:507] global step 4659: loss = 3.6609 (1.235 sec/step)\n","I0830 17:36:33.309468 139622356395904 learning.py:507] global step 4660: loss = 2.4185 (1.218 sec/step)\n","I0830 17:36:34.488929 139622356395904 learning.py:507] global step 4661: loss = 2.9812 (1.178 sec/step)\n","I0830 17:36:35.695904 139622356395904 learning.py:507] global step 4662: loss = 2.7946 (1.205 sec/step)\n","I0830 17:36:36.933931 139622356395904 learning.py:507] global step 4663: loss = 3.9544 (1.236 sec/step)\n","I0830 17:36:38.199289 139622356395904 learning.py:507] global step 4664: loss = 2.6460 (1.264 sec/step)\n","I0830 17:36:39.407827 139622356395904 learning.py:507] global step 4665: loss = 3.0141 (1.206 sec/step)\n","I0830 17:36:40.638927 139622356395904 learning.py:507] global step 4666: loss = 2.9998 (1.229 sec/step)\n","I0830 17:36:41.851546 139622356395904 learning.py:507] global step 4667: loss = 3.1171 (1.211 sec/step)\n","I0830 17:36:43.071760 139622356395904 learning.py:507] global step 4668: loss = 4.1290 (1.218 sec/step)\n","I0830 17:36:44.294945 139622356395904 learning.py:507] global step 4669: loss = 3.4126 (1.222 sec/step)\n","I0830 17:36:45.499035 139622356395904 learning.py:507] global step 4670: loss = 2.5850 (1.202 sec/step)\n","I0830 17:36:46.714139 139622356395904 learning.py:507] global step 4671: loss = 3.1852 (1.213 sec/step)\n","I0830 17:36:47.925490 139622356395904 learning.py:507] global step 4672: loss = 2.8659 (1.209 sec/step)\n","I0830 17:36:49.151188 139622356395904 learning.py:507] global step 4673: loss = 3.2714 (1.224 sec/step)\n","I0830 17:36:50.400035 139622356395904 learning.py:507] global step 4674: loss = 2.8870 (1.247 sec/step)\n","I0830 17:36:51.628075 139622356395904 learning.py:507] global step 4675: loss = 3.6900 (1.226 sec/step)\n","I0830 17:36:52.813753 139622356395904 learning.py:507] global step 4676: loss = 3.4839 (1.184 sec/step)\n","I0830 17:36:54.026845 139622356395904 learning.py:507] global step 4677: loss = 4.1378 (1.211 sec/step)\n","I0830 17:36:55.270350 139622356395904 learning.py:507] global step 4678: loss = 4.0562 (1.241 sec/step)\n","I0830 17:36:56.466651 139622356395904 learning.py:507] global step 4679: loss = 4.9045 (1.194 sec/step)\n","I0830 17:36:57.693757 139622356395904 learning.py:507] global step 4680: loss = 3.1190 (1.225 sec/step)\n","I0830 17:36:58.924339 139622356395904 learning.py:507] global step 4681: loss = 4.6270 (1.228 sec/step)\n","I0830 17:37:00.185089 139622356395904 learning.py:507] global step 4682: loss = 3.9369 (1.259 sec/step)\n","I0830 17:37:01.394975 139622356395904 learning.py:507] global step 4683: loss = 5.1401 (1.208 sec/step)\n","I0830 17:37:02.662418 139622356395904 learning.py:507] global step 4684: loss = 2.9675 (1.265 sec/step)\n","I0830 17:37:03.914977 139622356395904 learning.py:507] global step 4685: loss = 3.3180 (1.251 sec/step)\n","I0830 17:37:05.125474 139622356395904 learning.py:507] global step 4686: loss = 2.7242 (1.208 sec/step)\n","I0830 17:37:06.328501 139622356395904 learning.py:507] global step 4687: loss = 2.8636 (1.200 sec/step)\n","I0830 17:37:07.521399 139622356395904 learning.py:507] global step 4688: loss = 3.3981 (1.191 sec/step)\n","I0830 17:37:08.756219 139622356395904 learning.py:507] global step 4689: loss = 3.0923 (1.233 sec/step)\n","I0830 17:37:09.973045 139622356395904 learning.py:507] global step 4690: loss = 3.8191 (1.215 sec/step)\n","I0830 17:37:11.213433 139622356395904 learning.py:507] global step 4691: loss = 4.8948 (1.238 sec/step)\n","I0830 17:37:12.403566 139622356395904 learning.py:507] global step 4692: loss = 3.0714 (1.188 sec/step)\n","I0830 17:37:13.621662 139622356395904 learning.py:507] global step 4693: loss = 3.0690 (1.216 sec/step)\n","I0830 17:37:14.853933 139622356395904 learning.py:507] global step 4694: loss = 2.7664 (1.230 sec/step)\n","I0830 17:37:16.080780 139622356395904 learning.py:507] global step 4695: loss = 3.2342 (1.225 sec/step)\n","I0830 17:37:17.308515 139622356395904 learning.py:507] global step 4696: loss = 3.5826 (1.226 sec/step)\n","I0830 17:37:18.501882 139622356395904 learning.py:507] global step 4697: loss = 3.2570 (1.192 sec/step)\n","I0830 17:37:19.745021 139622356395904 learning.py:507] global step 4698: loss = 4.2087 (1.241 sec/step)\n","I0830 17:37:20.960544 139622356395904 learning.py:507] global step 4699: loss = 3.6704 (1.213 sec/step)\n","I0830 17:37:22.184491 139622356395904 learning.py:507] global step 4700: loss = 3.2020 (1.222 sec/step)\n","I0830 17:37:23.413259 139622356395904 learning.py:507] global step 4701: loss = 4.3072 (1.227 sec/step)\n","I0830 17:37:24.631621 139622356395904 learning.py:507] global step 4702: loss = 3.0894 (1.216 sec/step)\n","I0830 17:37:25.846986 139622356395904 learning.py:507] global step 4703: loss = 2.6694 (1.214 sec/step)\n","I0830 17:37:27.040098 139622356395904 learning.py:507] global step 4704: loss = 3.6569 (1.191 sec/step)\n","I0830 17:37:28.271565 139622356395904 learning.py:507] global step 4705: loss = 3.0337 (1.230 sec/step)\n","I0830 17:37:29.518730 139622356395904 learning.py:507] global step 4706: loss = 2.5357 (1.245 sec/step)\n","I0830 17:37:30.781821 139622356395904 learning.py:507] global step 4707: loss = 4.4621 (1.261 sec/step)\n","I0830 17:37:32.036927 139622356395904 learning.py:507] global step 4708: loss = 4.4900 (1.253 sec/step)\n","I0830 17:37:33.265137 139622356395904 learning.py:507] global step 4709: loss = 3.2974 (1.226 sec/step)\n","I0830 17:37:34.465556 139622356395904 learning.py:507] global step 4710: loss = 3.3184 (1.198 sec/step)\n","I0830 17:37:35.737516 139622356395904 learning.py:507] global step 4711: loss = 3.9241 (1.270 sec/step)\n","I0830 17:37:36.953844 139622356395904 learning.py:507] global step 4712: loss = 3.7907 (1.214 sec/step)\n","I0830 17:37:38.137743 139622356395904 learning.py:507] global step 4713: loss = 3.0422 (1.182 sec/step)\n","I0830 17:37:39.361242 139622356395904 learning.py:507] global step 4714: loss = 3.0292 (1.221 sec/step)\n","I0830 17:37:40.548377 139622356395904 learning.py:507] global step 4715: loss = 2.6253 (1.185 sec/step)\n","I0830 17:37:41.765288 139622356395904 learning.py:507] global step 4716: loss = 3.3619 (1.215 sec/step)\n","I0830 17:37:42.983820 139622356395904 learning.py:507] global step 4717: loss = 2.7739 (1.216 sec/step)\n","I0830 17:37:44.220528 139622356395904 learning.py:507] global step 4718: loss = 3.7526 (1.234 sec/step)\n","I0830 17:37:45.472014 139622356395904 learning.py:507] global step 4719: loss = 3.3634 (1.249 sec/step)\n","I0830 17:37:46.694199 139622356395904 learning.py:507] global step 4720: loss = 3.0978 (1.220 sec/step)\n","I0830 17:37:47.917624 139622356395904 learning.py:507] global step 4721: loss = 3.1314 (1.222 sec/step)\n","I0830 17:37:49.156308 139622356395904 learning.py:507] global step 4722: loss = 3.4190 (1.237 sec/step)\n","I0830 17:37:50.380887 139622356395904 learning.py:507] global step 4723: loss = 3.8030 (1.223 sec/step)\n","I0830 17:37:51.615002 139622356395904 learning.py:507] global step 4724: loss = 2.9434 (1.233 sec/step)\n","I0830 17:37:52.837667 139622356395904 learning.py:507] global step 4725: loss = 2.8790 (1.221 sec/step)\n","I0830 17:37:54.071300 139622356395904 learning.py:507] global step 4726: loss = 2.8076 (1.232 sec/step)\n","I0830 17:37:55.323391 139622356395904 learning.py:507] global step 4727: loss = 3.2211 (1.250 sec/step)\n","I0830 17:37:56.557625 139622356395904 learning.py:507] global step 4728: loss = 3.7886 (1.232 sec/step)\n","I0830 17:37:57.783446 139622356395904 learning.py:507] global step 4729: loss = 2.4380 (1.224 sec/step)\n","I0830 17:37:59.028231 139622356395904 learning.py:507] global step 4730: loss = 4.1071 (1.243 sec/step)\n","I0830 17:38:00.400776 139622356395904 learning.py:507] global step 4731: loss = 3.5459 (1.300 sec/step)\n","I0830 17:38:02.500239 139619299985152 supervisor.py:1050] Recording summary at step 4732.\n","I0830 17:38:02.530600 139622356395904 learning.py:507] global step 4732: loss = 3.8235 (2.128 sec/step)\n","I0830 17:38:03.750606 139622356395904 learning.py:507] global step 4733: loss = 3.0376 (1.218 sec/step)\n","I0830 17:38:04.959247 139622356395904 learning.py:507] global step 4734: loss = 2.3846 (1.207 sec/step)\n","I0830 17:38:06.198157 139622356395904 learning.py:507] global step 4735: loss = 2.6699 (1.237 sec/step)\n","I0830 17:38:07.392652 139622356395904 learning.py:507] global step 4736: loss = 3.8912 (1.192 sec/step)\n","I0830 17:38:08.641606 139622356395904 learning.py:507] global step 4737: loss = 2.5632 (1.247 sec/step)\n","I0830 17:38:09.882187 139622356395904 learning.py:507] global step 4738: loss = 3.3052 (1.236 sec/step)\n","I0830 17:38:11.133960 139622356395904 learning.py:507] global step 4739: loss = 3.7374 (1.250 sec/step)\n","I0830 17:38:12.350442 139622356395904 learning.py:507] global step 4740: loss = 4.4199 (1.214 sec/step)\n","I0830 17:38:13.552024 139622356395904 learning.py:507] global step 4741: loss = 3.4008 (1.200 sec/step)\n","I0830 17:38:14.784841 139622356395904 learning.py:507] global step 4742: loss = 3.2110 (1.231 sec/step)\n","I0830 17:38:16.041014 139622356395904 learning.py:507] global step 4743: loss = 3.4841 (1.254 sec/step)\n","I0830 17:38:17.247119 139622356395904 learning.py:507] global step 4744: loss = 3.4235 (1.204 sec/step)\n","I0830 17:38:18.460407 139622356395904 learning.py:507] global step 4745: loss = 3.2965 (1.211 sec/step)\n","I0830 17:38:19.688810 139622356395904 learning.py:507] global step 4746: loss = 3.1819 (1.226 sec/step)\n","I0830 17:38:20.924836 139622356395904 learning.py:507] global step 4747: loss = 4.1497 (1.234 sec/step)\n","I0830 17:38:22.158435 139622356395904 learning.py:507] global step 4748: loss = 3.1793 (1.232 sec/step)\n","I0830 17:38:23.396023 139622356395904 learning.py:507] global step 4749: loss = 3.4443 (1.236 sec/step)\n","I0830 17:38:24.616182 139622356395904 learning.py:507] global step 4750: loss = 2.3263 (1.218 sec/step)\n","I0830 17:38:25.836297 139622356395904 learning.py:507] global step 4751: loss = 3.2631 (1.218 sec/step)\n","I0830 17:38:27.065996 139622356395904 learning.py:507] global step 4752: loss = 4.4106 (1.228 sec/step)\n","I0830 17:38:28.288879 139622356395904 learning.py:507] global step 4753: loss = 4.2607 (1.221 sec/step)\n","I0830 17:38:29.536137 139622356395904 learning.py:507] global step 4754: loss = 3.2366 (1.245 sec/step)\n","I0830 17:38:30.733184 139622356395904 learning.py:507] global step 4755: loss = 2.8520 (1.194 sec/step)\n","I0830 17:38:31.939636 139622356395904 learning.py:507] global step 4756: loss = 4.5590 (1.204 sec/step)\n","I0830 17:38:33.156677 139622356395904 learning.py:507] global step 4757: loss = 3.2771 (1.215 sec/step)\n","I0830 17:38:34.375126 139622356395904 learning.py:507] global step 4758: loss = 2.6995 (1.217 sec/step)\n","I0830 17:38:35.638218 139622356395904 learning.py:507] global step 4759: loss = 3.1348 (1.261 sec/step)\n","I0830 17:38:36.852543 139622356395904 learning.py:507] global step 4760: loss = 4.0112 (1.213 sec/step)\n","I0830 17:38:38.080409 139622356395904 learning.py:507] global step 4761: loss = 4.3601 (1.226 sec/step)\n","I0830 17:38:39.301310 139622356395904 learning.py:507] global step 4762: loss = 3.4875 (1.219 sec/step)\n","I0830 17:38:40.480348 139622356395904 learning.py:507] global step 4763: loss = 2.5046 (1.177 sec/step)\n","I0830 17:38:41.708514 139622356395904 learning.py:507] global step 4764: loss = 2.7545 (1.226 sec/step)\n","I0830 17:38:42.921711 139622356395904 learning.py:507] global step 4765: loss = 2.6644 (1.211 sec/step)\n","I0830 17:38:44.151627 139622356395904 learning.py:507] global step 4766: loss = 3.2277 (1.228 sec/step)\n","I0830 17:38:45.366821 139622356395904 learning.py:507] global step 4767: loss = 2.8105 (1.214 sec/step)\n","I0830 17:38:46.599759 139622356395904 learning.py:507] global step 4768: loss = 2.6571 (1.231 sec/step)\n","I0830 17:38:47.847453 139622356395904 learning.py:507] global step 4769: loss = 3.2162 (1.246 sec/step)\n","I0830 17:38:49.068834 139622356395904 learning.py:507] global step 4770: loss = 2.7153 (1.220 sec/step)\n","I0830 17:38:50.276978 139622356395904 learning.py:507] global step 4771: loss = 3.2360 (1.207 sec/step)\n","I0830 17:38:51.536393 139622356395904 learning.py:507] global step 4772: loss = 3.5771 (1.258 sec/step)\n","I0830 17:38:52.753895 139622356395904 learning.py:507] global step 4773: loss = 2.4120 (1.215 sec/step)\n","I0830 17:38:53.999216 139622356395904 learning.py:507] global step 4774: loss = 3.6119 (1.243 sec/step)\n","I0830 17:38:55.188418 139622356395904 learning.py:507] global step 4775: loss = 3.5949 (1.187 sec/step)\n","I0830 17:38:56.395701 139622356395904 learning.py:507] global step 4776: loss = 2.8942 (1.205 sec/step)\n","I0830 17:38:57.639620 139622356395904 learning.py:507] global step 4777: loss = 4.4023 (1.242 sec/step)\n","I0830 17:38:58.839679 139622356395904 learning.py:507] global step 4778: loss = 3.6498 (1.198 sec/step)\n","I0830 17:39:00.060589 139622356395904 learning.py:507] global step 4779: loss = 2.5970 (1.219 sec/step)\n","I0830 17:39:01.311832 139622356395904 learning.py:507] global step 4780: loss = 2.5978 (1.246 sec/step)\n","I0830 17:39:02.568970 139622356395904 learning.py:507] global step 4781: loss = 3.5624 (1.255 sec/step)\n","I0830 17:39:03.808112 139622356395904 learning.py:507] global step 4782: loss = 3.4396 (1.237 sec/step)\n","I0830 17:39:05.038004 139622356395904 learning.py:507] global step 4783: loss = 3.4469 (1.228 sec/step)\n","I0830 17:39:06.244729 139622356395904 learning.py:507] global step 4784: loss = 2.7024 (1.205 sec/step)\n","I0830 17:39:07.498198 139622356395904 learning.py:507] global step 4785: loss = 3.3983 (1.251 sec/step)\n","I0830 17:39:08.722684 139622356395904 learning.py:507] global step 4786: loss = 4.1433 (1.222 sec/step)\n","I0830 17:39:09.962183 139622356395904 learning.py:507] global step 4787: loss = 2.7647 (1.235 sec/step)\n","I0830 17:39:11.202859 139622356395904 learning.py:507] global step 4788: loss = 3.9218 (1.239 sec/step)\n","I0830 17:39:12.430888 139622356395904 learning.py:507] global step 4789: loss = 3.6261 (1.225 sec/step)\n","I0830 17:39:13.644436 139622356395904 learning.py:507] global step 4790: loss = 3.0092 (1.211 sec/step)\n","I0830 17:39:14.872820 139622356395904 learning.py:507] global step 4791: loss = 3.8911 (1.227 sec/step)\n","I0830 17:39:16.119787 139622356395904 learning.py:507] global step 4792: loss = 2.8283 (1.245 sec/step)\n","I0830 17:39:17.354202 139622356395904 learning.py:507] global step 4793: loss = 4.0066 (1.232 sec/step)\n","I0830 17:39:18.587882 139622356395904 learning.py:507] global step 4794: loss = 3.4581 (1.231 sec/step)\n","I0830 17:39:19.821918 139622356395904 learning.py:507] global step 4795: loss = 3.7597 (1.232 sec/step)\n","I0830 17:39:21.039461 139622356395904 learning.py:507] global step 4796: loss = 3.3581 (1.215 sec/step)\n","I0830 17:39:22.295688 139622356395904 learning.py:507] global step 4797: loss = 2.5835 (1.254 sec/step)\n","I0830 17:39:23.501398 139622356395904 learning.py:507] global step 4798: loss = 4.1885 (1.204 sec/step)\n","I0830 17:39:24.731905 139622356395904 learning.py:507] global step 4799: loss = 2.5444 (1.229 sec/step)\n","I0830 17:39:25.958527 139622356395904 learning.py:507] global step 4800: loss = 3.8482 (1.225 sec/step)\n","I0830 17:39:27.198216 139622356395904 learning.py:507] global step 4801: loss = 4.3965 (1.238 sec/step)\n","I0830 17:39:28.413366 139622356395904 learning.py:507] global step 4802: loss = 2.8884 (1.213 sec/step)\n","I0830 17:39:29.615602 139622356395904 learning.py:507] global step 4803: loss = 5.2254 (1.200 sec/step)\n","I0830 17:39:30.854927 139622356395904 learning.py:507] global step 4804: loss = 3.0807 (1.237 sec/step)\n","I0830 17:39:32.090459 139622356395904 learning.py:507] global step 4805: loss = 3.8189 (1.234 sec/step)\n","I0830 17:39:33.329610 139622356395904 learning.py:507] global step 4806: loss = 2.8371 (1.237 sec/step)\n","I0830 17:39:34.552236 139622356395904 learning.py:507] global step 4807: loss = 2.8831 (1.221 sec/step)\n","I0830 17:39:35.759864 139622356395904 learning.py:507] global step 4808: loss = 4.4279 (1.206 sec/step)\n","I0830 17:39:37.020344 139622356395904 learning.py:507] global step 4809: loss = 3.1861 (1.259 sec/step)\n","I0830 17:39:38.231370 139622356395904 learning.py:507] global step 4810: loss = 3.3244 (1.209 sec/step)\n","I0830 17:39:39.454633 139622356395904 learning.py:507] global step 4811: loss = 3.9946 (1.222 sec/step)\n","I0830 17:39:40.676329 139622356395904 learning.py:507] global step 4812: loss = 3.2385 (1.220 sec/step)\n","I0830 17:39:41.912604 139622356395904 learning.py:507] global step 4813: loss = 3.4559 (1.234 sec/step)\n","I0830 17:39:43.202025 139622356395904 learning.py:507] global step 4814: loss = 2.6589 (1.287 sec/step)\n","I0830 17:39:44.379237 139622356395904 learning.py:507] global step 4815: loss = 3.4341 (1.175 sec/step)\n","I0830 17:39:45.639177 139622356395904 learning.py:507] global step 4816: loss = 3.0276 (1.258 sec/step)\n","I0830 17:39:46.829910 139622356395904 learning.py:507] global step 4817: loss = 3.1453 (1.189 sec/step)\n","I0830 17:39:48.067823 139622356395904 learning.py:507] global step 4818: loss = 2.7090 (1.236 sec/step)\n","I0830 17:39:49.258035 139622356395904 learning.py:507] global step 4819: loss = 2.8018 (1.188 sec/step)\n","I0830 17:39:50.496388 139622356395904 learning.py:507] global step 4820: loss = 3.8044 (1.236 sec/step)\n","I0830 17:39:51.733629 139622356395904 learning.py:507] global step 4821: loss = 3.2951 (1.235 sec/step)\n","I0830 17:39:52.988445 139622356395904 learning.py:507] global step 4822: loss = 4.0992 (1.253 sec/step)\n","I0830 17:39:54.202746 139622356395904 learning.py:507] global step 4823: loss = 3.6818 (1.212 sec/step)\n","I0830 17:39:55.422089 139622356395904 learning.py:507] global step 4824: loss = 3.3967 (1.217 sec/step)\n","I0830 17:39:56.628016 139622356395904 learning.py:507] global step 4825: loss = 3.6758 (1.204 sec/step)\n","I0830 17:39:57.899483 139622356395904 learning.py:507] global step 4826: loss = 3.2319 (1.270 sec/step)\n","I0830 17:39:59.124134 139622356395904 learning.py:507] global step 4827: loss = 3.7187 (1.223 sec/step)\n","I0830 17:39:59.646658 139619316770560 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n","I0830 17:40:01.836729 139622356395904 learning.py:507] global step 4828: loss = 4.3310 (2.248 sec/step)\n","I0830 17:40:02.297394 139619299985152 supervisor.py:1050] Recording summary at step 4828.\n","I0830 17:40:03.523247 139622356395904 learning.py:507] global step 4829: loss = 3.8199 (1.621 sec/step)\n","I0830 17:40:05.134966 139622356395904 learning.py:507] global step 4830: loss = 2.8570 (1.605 sec/step)\n","I0830 17:40:06.407202 139622356395904 learning.py:507] global step 4831: loss = 5.3151 (1.270 sec/step)\n","I0830 17:40:07.635487 139622356395904 learning.py:507] global step 4832: loss = 3.5380 (1.226 sec/step)\n","I0830 17:40:08.838412 139622356395904 learning.py:507] global step 4833: loss = 3.2205 (1.201 sec/step)\n","I0830 17:40:10.029949 139622356395904 learning.py:507] global step 4834: loss = 3.1857 (1.190 sec/step)\n","I0830 17:40:11.259544 139622356395904 learning.py:507] global step 4835: loss = 3.5917 (1.228 sec/step)\n","I0830 17:40:12.524009 139622356395904 learning.py:507] global step 4836: loss = 3.6447 (1.263 sec/step)\n","I0830 17:40:13.719510 139622356395904 learning.py:507] global step 4837: loss = 2.8220 (1.193 sec/step)\n","I0830 17:40:14.939107 139622356395904 learning.py:507] global step 4838: loss = 3.9886 (1.216 sec/step)\n","I0830 17:40:16.176345 139622356395904 learning.py:507] global step 4839: loss = 2.8602 (1.235 sec/step)\n","I0830 17:40:17.467190 139622356395904 learning.py:507] global step 4840: loss = 3.2518 (1.289 sec/step)\n","I0830 17:40:18.665354 139622356395904 learning.py:507] global step 4841: loss = 2.7790 (1.196 sec/step)\n","I0830 17:40:19.883813 139622356395904 learning.py:507] global step 4842: loss = 3.0541 (1.216 sec/step)\n","I0830 17:40:21.095891 139622356395904 learning.py:507] global step 4843: loss = 3.2594 (1.210 sec/step)\n","I0830 17:40:22.313207 139622356395904 learning.py:507] global step 4844: loss = 4.1834 (1.215 sec/step)\n","I0830 17:40:23.535678 139622356395904 learning.py:507] global step 4845: loss = 3.5676 (1.221 sec/step)\n","I0830 17:40:24.748775 139622356395904 learning.py:507] global step 4846: loss = 3.5578 (1.211 sec/step)\n","I0830 17:40:25.955440 139622356395904 learning.py:507] global step 4847: loss = 4.5531 (1.205 sec/step)\n","I0830 17:40:27.144006 139622356395904 learning.py:507] global step 4848: loss = 3.2076 (1.187 sec/step)\n","I0830 17:40:28.342889 139622356395904 learning.py:507] global step 4849: loss = 3.2315 (1.197 sec/step)\n","I0830 17:40:29.546768 139622356395904 learning.py:507] global step 4850: loss = 2.7412 (1.202 sec/step)\n","I0830 17:40:30.792972 139622356395904 learning.py:507] global step 4851: loss = 3.3582 (1.244 sec/step)\n","I0830 17:40:32.026174 139622356395904 learning.py:507] global step 4852: loss = 3.4507 (1.231 sec/step)\n","I0830 17:40:33.247121 139622356395904 learning.py:507] global step 4853: loss = 2.7108 (1.219 sec/step)\n","I0830 17:40:34.473902 139622356395904 learning.py:507] global step 4854: loss = 2.6053 (1.225 sec/step)\n","I0830 17:40:35.712805 139622356395904 learning.py:507] global step 4855: loss = 3.0409 (1.237 sec/step)\n","I0830 17:40:36.931198 139622356395904 learning.py:507] global step 4856: loss = 3.2459 (1.217 sec/step)\n","I0830 17:40:38.151516 139622356395904 learning.py:507] global step 4857: loss = 3.3725 (1.218 sec/step)\n","I0830 17:40:39.377962 139622356395904 learning.py:507] global step 4858: loss = 3.0184 (1.225 sec/step)\n","I0830 17:40:40.585832 139622356395904 learning.py:507] global step 4859: loss = 3.8181 (1.206 sec/step)\n","I0830 17:40:41.824206 139622356395904 learning.py:507] global step 4860: loss = 3.0454 (1.236 sec/step)\n","I0830 17:40:43.037893 139622356395904 learning.py:507] global step 4861: loss = 3.2551 (1.212 sec/step)\n","I0830 17:40:44.261869 139622356395904 learning.py:507] global step 4862: loss = 4.0682 (1.222 sec/step)\n","I0830 17:40:45.505131 139622356395904 learning.py:507] global step 4863: loss = 4.4214 (1.241 sec/step)\n","I0830 17:40:46.710811 139622356395904 learning.py:507] global step 4864: loss = 3.2947 (1.203 sec/step)\n","I0830 17:40:47.904954 139622356395904 learning.py:507] global step 4865: loss = 3.1798 (1.192 sec/step)\n","I0830 17:40:49.128583 139622356395904 learning.py:507] global step 4866: loss = 2.9196 (1.222 sec/step)\n","I0830 17:40:50.346586 139622356395904 learning.py:507] global step 4867: loss = 2.9373 (1.216 sec/step)\n","I0830 17:40:51.576374 139622356395904 learning.py:507] global step 4868: loss = 2.6399 (1.228 sec/step)\n","I0830 17:40:52.844174 139622356395904 learning.py:507] global step 4869: loss = 4.1802 (1.266 sec/step)\n","I0830 17:40:54.058454 139622356395904 learning.py:507] global step 4870: loss = 2.8079 (1.213 sec/step)\n","I0830 17:40:55.259658 139622356395904 learning.py:507] global step 4871: loss = 3.3044 (1.199 sec/step)\n","I0830 17:40:56.536123 139622356395904 learning.py:507] global step 4872: loss = 2.6220 (1.275 sec/step)\n","I0830 17:40:57.751338 139622356395904 learning.py:507] global step 4873: loss = 3.0634 (1.213 sec/step)\n","I0830 17:40:58.970369 139622356395904 learning.py:507] global step 4874: loss = 3.2142 (1.217 sec/step)\n","I0830 17:41:00.172282 139622356395904 learning.py:507] global step 4875: loss = 3.3457 (1.200 sec/step)\n","I0830 17:41:01.394730 139622356395904 learning.py:507] global step 4876: loss = 3.8629 (1.218 sec/step)\n","I0830 17:41:02.635714 139622356395904 learning.py:507] global step 4877: loss = 3.2829 (1.239 sec/step)\n","I0830 17:41:03.890346 139622356395904 learning.py:507] global step 4878: loss = 2.6153 (1.252 sec/step)\n","I0830 17:41:05.105617 139622356395904 learning.py:507] global step 4879: loss = 4.0900 (1.213 sec/step)\n","I0830 17:41:06.384480 139622356395904 learning.py:507] global step 4880: loss = 3.3706 (1.277 sec/step)\n","I0830 17:41:07.625345 139622356395904 learning.py:507] global step 4881: loss = 2.4606 (1.239 sec/step)\n","I0830 17:41:08.857907 139622356395904 learning.py:507] global step 4882: loss = 3.3556 (1.231 sec/step)\n","I0830 17:41:10.074411 139622356395904 learning.py:507] global step 4883: loss = 3.8837 (1.214 sec/step)\n","I0830 17:41:11.291731 139622356395904 learning.py:507] global step 4884: loss = 3.4250 (1.215 sec/step)\n","I0830 17:41:12.534794 139622356395904 learning.py:507] global step 4885: loss = 3.8105 (1.241 sec/step)\n","I0830 17:41:13.743643 139622356395904 learning.py:507] global step 4886: loss = 3.2264 (1.207 sec/step)\n","I0830 17:41:14.990768 139622356395904 learning.py:507] global step 4887: loss = 3.2791 (1.245 sec/step)\n","I0830 17:41:16.228222 139622356395904 learning.py:507] global step 4888: loss = 3.0105 (1.235 sec/step)\n","I0830 17:41:17.472964 139622356395904 learning.py:507] global step 4889: loss = 3.1613 (1.243 sec/step)\n","I0830 17:41:18.684555 139622356395904 learning.py:507] global step 4890: loss = 2.6182 (1.210 sec/step)\n","I0830 17:41:19.924252 139622356395904 learning.py:507] global step 4891: loss = 3.1986 (1.238 sec/step)\n","I0830 17:41:21.142746 139622356395904 learning.py:507] global step 4892: loss = 3.6302 (1.216 sec/step)\n","I0830 17:41:22.335616 139622356395904 learning.py:507] global step 4893: loss = 3.0843 (1.190 sec/step)\n","I0830 17:41:23.559498 139622356395904 learning.py:507] global step 4894: loss = 3.7688 (1.222 sec/step)\n","I0830 17:41:24.807570 139622356395904 learning.py:507] global step 4895: loss = 3.3161 (1.246 sec/step)\n","I0830 17:41:26.041298 139622356395904 learning.py:507] global step 4896: loss = 3.7070 (1.232 sec/step)\n","I0830 17:41:27.271223 139622356395904 learning.py:507] global step 4897: loss = 2.9776 (1.228 sec/step)\n","I0830 17:41:28.495223 139622356395904 learning.py:507] global step 4898: loss = 3.2189 (1.222 sec/step)\n","I0830 17:41:29.721630 139622356395904 learning.py:507] global step 4899: loss = 3.3018 (1.225 sec/step)\n","I0830 17:41:30.909590 139622356395904 learning.py:507] global step 4900: loss = 3.1558 (1.186 sec/step)\n","I0830 17:41:32.195528 139622356395904 learning.py:507] global step 4901: loss = 4.2809 (1.284 sec/step)\n","I0830 17:41:33.414108 139622356395904 learning.py:507] global step 4902: loss = 2.7713 (1.217 sec/step)\n","I0830 17:41:34.642913 139622356395904 learning.py:507] global step 4903: loss = 2.6298 (1.227 sec/step)\n","I0830 17:41:35.877631 139622356395904 learning.py:507] global step 4904: loss = 3.7685 (1.233 sec/step)\n","I0830 17:41:37.128763 139622356395904 learning.py:507] global step 4905: loss = 2.4587 (1.249 sec/step)\n","I0830 17:41:38.332538 139622356395904 learning.py:507] global step 4906: loss = 3.9102 (1.200 sec/step)\n","I0830 17:41:39.551436 139622356395904 learning.py:507] global step 4907: loss = 2.6925 (1.217 sec/step)\n","I0830 17:41:40.798496 139622356395904 learning.py:507] global step 4908: loss = 3.1585 (1.245 sec/step)\n","I0830 17:41:42.002483 139622356395904 learning.py:507] global step 4909: loss = 4.1068 (1.202 sec/step)\n","I0830 17:41:43.242939 139622356395904 learning.py:507] global step 4910: loss = 3.6947 (1.239 sec/step)\n","I0830 17:41:44.457861 139622356395904 learning.py:507] global step 4911: loss = 4.1393 (1.213 sec/step)\n","I0830 17:41:45.669817 139622356395904 learning.py:507] global step 4912: loss = 5.2743 (1.210 sec/step)\n","I0830 17:41:46.888841 139622356395904 learning.py:507] global step 4913: loss = 3.3822 (1.217 sec/step)\n","I0830 17:41:48.110785 139622356395904 learning.py:507] global step 4914: loss = 3.9442 (1.220 sec/step)\n","I0830 17:41:49.326738 139622356395904 learning.py:507] global step 4915: loss = 3.3867 (1.214 sec/step)\n","I0830 17:41:50.566256 139622356395904 learning.py:507] global step 4916: loss = 3.7340 (1.237 sec/step)\n","I0830 17:41:51.818655 139622356395904 learning.py:507] global step 4917: loss = 2.8286 (1.251 sec/step)\n","I0830 17:41:53.065416 139622356395904 learning.py:507] global step 4918: loss = 2.6205 (1.245 sec/step)\n","I0830 17:41:54.261858 139622356395904 learning.py:507] global step 4919: loss = 3.0164 (1.194 sec/step)\n","I0830 17:41:55.521576 139622356395904 learning.py:507] global step 4920: loss = 3.0785 (1.258 sec/step)\n","I0830 17:41:56.730570 139622356395904 learning.py:507] global step 4921: loss = 2.5878 (1.207 sec/step)\n","I0830 17:41:57.963686 139622356395904 learning.py:507] global step 4922: loss = 3.6792 (1.231 sec/step)\n","I0830 17:41:59.172127 139622356395904 learning.py:507] global step 4923: loss = 3.0576 (1.206 sec/step)\n","I0830 17:42:00.539748 139622356395904 learning.py:507] global step 4924: loss = 2.5187 (1.295 sec/step)\n","I0830 17:42:02.558845 139619299985152 supervisor.py:1050] Recording summary at step 4925.\n","I0830 17:42:02.590555 139622356395904 learning.py:507] global step 4925: loss = 4.7777 (2.049 sec/step)\n","I0830 17:42:03.828558 139622356395904 learning.py:507] global step 4926: loss = 3.0791 (1.236 sec/step)\n","I0830 17:42:05.067222 139622356395904 learning.py:507] global step 4927: loss = 3.8220 (1.235 sec/step)\n","I0830 17:42:06.303598 139622356395904 learning.py:507] global step 4928: loss = 3.2641 (1.235 sec/step)\n","I0830 17:42:07.511112 139622356395904 learning.py:507] global step 4929: loss = 3.4543 (1.206 sec/step)\n","I0830 17:42:08.730811 139622356395904 learning.py:507] global step 4930: loss = 3.2368 (1.218 sec/step)\n","I0830 17:42:09.964680 139622356395904 learning.py:507] global step 4931: loss = 2.8425 (1.232 sec/step)\n","I0830 17:42:11.192646 139622356395904 learning.py:507] global step 4932: loss = 3.4951 (1.226 sec/step)\n","I0830 17:42:12.422918 139622356395904 learning.py:507] global step 4933: loss = 3.3498 (1.228 sec/step)\n","I0830 17:42:13.647735 139622356395904 learning.py:507] global step 4934: loss = 3.1794 (1.223 sec/step)\n","I0830 17:42:14.868535 139622356395904 learning.py:507] global step 4935: loss = 3.5218 (1.219 sec/step)\n","I0830 17:42:16.117666 139622356395904 learning.py:507] global step 4936: loss = 3.5621 (1.247 sec/step)\n","I0830 17:42:17.334628 139622356395904 learning.py:507] global step 4937: loss = 3.6514 (1.215 sec/step)\n","I0830 17:42:18.549551 139622356395904 learning.py:507] global step 4938: loss = 3.0594 (1.213 sec/step)\n","I0830 17:42:19.742444 139622356395904 learning.py:507] global step 4939: loss = 3.1189 (1.191 sec/step)\n","I0830 17:42:20.963531 139622356395904 learning.py:507] global step 4940: loss = 2.7654 (1.219 sec/step)\n","I0830 17:42:22.204602 139622356395904 learning.py:507] global step 4941: loss = 3.3408 (1.239 sec/step)\n","I0830 17:42:23.410910 139622356395904 learning.py:507] global step 4942: loss = 2.9447 (1.204 sec/step)\n","I0830 17:42:24.637658 139622356395904 learning.py:507] global step 4943: loss = 3.6070 (1.225 sec/step)\n","I0830 17:42:25.871542 139622356395904 learning.py:507] global step 4944: loss = 3.4844 (1.232 sec/step)\n","I0830 17:42:27.098148 139622356395904 learning.py:507] global step 4945: loss = 3.1274 (1.225 sec/step)\n","I0830 17:42:28.321493 139622356395904 learning.py:507] global step 4946: loss = 3.1536 (1.221 sec/step)\n","I0830 17:42:29.563127 139622356395904 learning.py:507] global step 4947: loss = 4.7643 (1.240 sec/step)\n","I0830 17:42:30.812430 139622356395904 learning.py:507] global step 4948: loss = 2.3938 (1.247 sec/step)\n","I0830 17:42:32.013528 139622356395904 learning.py:507] global step 4949: loss = 4.0022 (1.199 sec/step)\n","I0830 17:42:33.238555 139622356395904 learning.py:507] global step 4950: loss = 3.2269 (1.223 sec/step)\n","I0830 17:42:34.442155 139622356395904 learning.py:507] global step 4951: loss = 4.0576 (1.202 sec/step)\n","I0830 17:42:35.654590 139622356395904 learning.py:507] global step 4952: loss = 3.5011 (1.211 sec/step)\n","I0830 17:42:36.875195 139622356395904 learning.py:507] global step 4953: loss = 2.8233 (1.218 sec/step)\n","I0830 17:42:38.114009 139622356395904 learning.py:507] global step 4954: loss = 2.9281 (1.237 sec/step)\n","I0830 17:42:39.299664 139622356395904 learning.py:507] global step 4955: loss = 3.9514 (1.184 sec/step)\n","I0830 17:42:40.552468 139622356395904 learning.py:507] global step 4956: loss = 2.7040 (1.251 sec/step)\n","I0830 17:42:41.780621 139622356395904 learning.py:507] global step 4957: loss = 4.4903 (1.226 sec/step)\n","I0830 17:42:42.987150 139622356395904 learning.py:507] global step 4958: loss = 2.5557 (1.205 sec/step)\n","I0830 17:42:44.186355 139622356395904 learning.py:507] global step 4959: loss = 2.9493 (1.197 sec/step)\n","I0830 17:42:45.402645 139622356395904 learning.py:507] global step 4960: loss = 3.1903 (1.215 sec/step)\n","I0830 17:42:46.619846 139622356395904 learning.py:507] global step 4961: loss = 2.6395 (1.215 sec/step)\n","I0830 17:42:47.834100 139622356395904 learning.py:507] global step 4962: loss = 3.8423 (1.212 sec/step)\n","I0830 17:42:49.064786 139622356395904 learning.py:507] global step 4963: loss = 3.6523 (1.229 sec/step)\n","I0830 17:42:50.285963 139622356395904 learning.py:507] global step 4964: loss = 4.5167 (1.219 sec/step)\n","I0830 17:42:51.481348 139622356395904 learning.py:507] global step 4965: loss = 3.2085 (1.193 sec/step)\n","I0830 17:42:52.698200 139622356395904 learning.py:507] global step 4966: loss = 3.9246 (1.215 sec/step)\n","I0830 17:42:53.929605 139622356395904 learning.py:507] global step 4967: loss = 3.3698 (1.229 sec/step)\n","I0830 17:42:55.179361 139622356395904 learning.py:507] global step 4968: loss = 3.6079 (1.248 sec/step)\n","I0830 17:42:56.383605 139622356395904 learning.py:507] global step 4969: loss = 3.1914 (1.202 sec/step)\n","I0830 17:42:57.674922 139622356395904 learning.py:507] global step 4970: loss = 2.9777 (1.290 sec/step)\n","I0830 17:42:58.947513 139622356395904 learning.py:507] global step 4971: loss = 3.6235 (1.270 sec/step)\n","I0830 17:43:00.190582 139622356395904 learning.py:507] global step 4972: loss = 3.4591 (1.241 sec/step)\n","I0830 17:43:01.451338 139622356395904 learning.py:507] global step 4973: loss = 3.3760 (1.259 sec/step)\n","I0830 17:43:02.660033 139622356395904 learning.py:507] global step 4974: loss = 2.5240 (1.207 sec/step)\n","I0830 17:43:03.895673 139622356395904 learning.py:507] global step 4975: loss = 2.6998 (1.234 sec/step)\n","I0830 17:43:05.154378 139622356395904 learning.py:507] global step 4976: loss = 3.0682 (1.257 sec/step)\n","I0830 17:43:06.359109 139622356395904 learning.py:507] global step 4977: loss = 2.6373 (1.203 sec/step)\n","I0830 17:43:07.592031 139622356395904 learning.py:507] global step 4978: loss = 3.1683 (1.231 sec/step)\n","I0830 17:43:08.823971 139622356395904 learning.py:507] global step 4979: loss = 2.8156 (1.230 sec/step)\n","I0830 17:43:10.059155 139622356395904 learning.py:507] global step 4980: loss = 4.2472 (1.233 sec/step)\n","I0830 17:43:11.308953 139622356395904 learning.py:507] global step 4981: loss = 2.5258 (1.248 sec/step)\n","I0830 17:43:12.537430 139622356395904 learning.py:507] global step 4982: loss = 2.5498 (1.226 sec/step)\n","I0830 17:43:13.790042 139622356395904 learning.py:507] global step 4983: loss = 3.5529 (1.251 sec/step)\n","I0830 17:43:14.993189 139622356395904 learning.py:507] global step 4984: loss = 2.8387 (1.201 sec/step)\n","I0830 17:43:16.248718 139622356395904 learning.py:507] global step 4985: loss = 3.4419 (1.254 sec/step)\n","I0830 17:43:17.447839 139622356395904 learning.py:507] global step 4986: loss = 4.0428 (1.197 sec/step)\n","I0830 17:43:18.646330 139622356395904 learning.py:507] global step 4987: loss = 4.6914 (1.197 sec/step)\n","I0830 17:43:19.875203 139622356395904 learning.py:507] global step 4988: loss = 4.6201 (1.227 sec/step)\n","I0830 17:43:21.110804 139622356395904 learning.py:507] global step 4989: loss = 3.3262 (1.234 sec/step)\n","I0830 17:43:22.335479 139622356395904 learning.py:507] global step 4990: loss = 3.6085 (1.223 sec/step)\n","I0830 17:43:23.588043 139622356395904 learning.py:507] global step 4991: loss = 2.7376 (1.251 sec/step)\n","I0830 17:43:24.806555 139622356395904 learning.py:507] global step 4992: loss = 4.7132 (1.217 sec/step)\n","I0830 17:43:26.046983 139622356395904 learning.py:507] global step 4993: loss = 2.8163 (1.238 sec/step)\n","I0830 17:43:27.274535 139622356395904 learning.py:507] global step 4994: loss = 2.5378 (1.226 sec/step)\n","I0830 17:43:28.488593 139622356395904 learning.py:507] global step 4995: loss = 3.0403 (1.212 sec/step)\n","I0830 17:43:29.722992 139622356395904 learning.py:507] global step 4996: loss = 3.0502 (1.233 sec/step)\n","I0830 17:43:30.958477 139622356395904 learning.py:507] global step 4997: loss = 5.5031 (1.234 sec/step)\n","I0830 17:43:32.162704 139622356395904 learning.py:507] global step 4998: loss = 3.1222 (1.201 sec/step)\n","I0830 17:43:33.371661 139622356395904 learning.py:507] global step 4999: loss = 3.8026 (1.207 sec/step)\n","I0830 17:43:34.571541 139622356395904 learning.py:507] global step 5000: loss = 3.4423 (1.198 sec/step)\n","I0830 17:43:35.761338 139622356395904 learning.py:507] global step 5001: loss = 3.9789 (1.188 sec/step)\n","I0830 17:43:37.004739 139622356395904 learning.py:507] global step 5002: loss = 3.2763 (1.242 sec/step)\n","I0830 17:43:38.195566 139622356395904 learning.py:507] global step 5003: loss = 2.7258 (1.189 sec/step)\n","I0830 17:43:39.439979 139622356395904 learning.py:507] global step 5004: loss = 2.8337 (1.240 sec/step)\n","I0830 17:43:40.681159 139622356395904 learning.py:507] global step 5005: loss = 2.6339 (1.239 sec/step)\n","I0830 17:43:41.891547 139622356395904 learning.py:507] global step 5006: loss = 3.5480 (1.209 sec/step)\n","I0830 17:43:43.135509 139622356395904 learning.py:507] global step 5007: loss = 2.6277 (1.242 sec/step)\n","I0830 17:43:44.388644 139622356395904 learning.py:507] global step 5008: loss = 2.8859 (1.251 sec/step)\n","I0830 17:43:45.603574 139622356395904 learning.py:507] global step 5009: loss = 2.6978 (1.213 sec/step)\n","I0830 17:43:46.831145 139622356395904 learning.py:507] global step 5010: loss = 3.3748 (1.226 sec/step)\n","I0830 17:43:48.039928 139622356395904 learning.py:507] global step 5011: loss = 3.4383 (1.207 sec/step)\n","I0830 17:43:49.259657 139622356395904 learning.py:507] global step 5012: loss = 4.6326 (1.218 sec/step)\n","I0830 17:43:50.451905 139622356395904 learning.py:507] global step 5013: loss = 3.2386 (1.190 sec/step)\n","I0830 17:43:51.709330 139622356395904 learning.py:507] global step 5014: loss = 2.9224 (1.255 sec/step)\n","I0830 17:43:52.922227 139622356395904 learning.py:507] global step 5015: loss = 3.1478 (1.211 sec/step)\n","I0830 17:43:54.121384 139622356395904 learning.py:507] global step 5016: loss = 3.2686 (1.197 sec/step)\n","I0830 17:43:55.345675 139622356395904 learning.py:507] global step 5017: loss = 3.3616 (1.222 sec/step)\n","I0830 17:43:56.613138 139622356395904 learning.py:507] global step 5018: loss = 3.7948 (1.265 sec/step)\n","I0830 17:43:57.821383 139622356395904 learning.py:507] global step 5019: loss = 3.5892 (1.206 sec/step)\n","I0830 17:43:59.036885 139622356395904 learning.py:507] global step 5020: loss = 3.0243 (1.214 sec/step)\n","I0830 17:44:00.499826 139622356395904 learning.py:507] global step 5021: loss = 3.6273 (1.382 sec/step)\n","I0830 17:44:02.296316 139619299985152 supervisor.py:1050] Recording summary at step 5022.\n","I0830 17:44:02.337253 139622356395904 learning.py:507] global step 5022: loss = 3.3266 (1.835 sec/step)\n","I0830 17:44:03.575129 139622356395904 learning.py:507] global step 5023: loss = 2.8973 (1.236 sec/step)\n","I0830 17:44:04.779021 139622356395904 learning.py:507] global step 5024: loss = 4.0330 (1.202 sec/step)\n","I0830 17:44:06.020338 139622356395904 learning.py:507] global step 5025: loss = 3.3053 (1.239 sec/step)\n","I0830 17:44:07.242201 139622356395904 learning.py:507] global step 5026: loss = 2.6819 (1.220 sec/step)\n","I0830 17:44:08.457593 139622356395904 learning.py:507] global step 5027: loss = 3.7889 (1.214 sec/step)\n","I0830 17:44:09.708588 139622356395904 learning.py:507] global step 5028: loss = 3.6403 (1.249 sec/step)\n","I0830 17:44:10.916557 139622356395904 learning.py:507] global step 5029: loss = 3.1887 (1.206 sec/step)\n","I0830 17:44:12.102602 139622356395904 learning.py:507] global step 5030: loss = 2.9585 (1.184 sec/step)\n","I0830 17:44:13.335602 139622356395904 learning.py:507] global step 5031: loss = 3.5725 (1.231 sec/step)\n","I0830 17:44:14.564369 139622356395904 learning.py:507] global step 5032: loss = 2.7655 (1.227 sec/step)\n","I0830 17:44:15.788506 139622356395904 learning.py:507] global step 5033: loss = 3.0692 (1.222 sec/step)\n","I0830 17:44:17.003465 139622356395904 learning.py:507] global step 5034: loss = 2.5171 (1.213 sec/step)\n","I0830 17:44:18.225523 139622356395904 learning.py:507] global step 5035: loss = 3.1633 (1.220 sec/step)\n","I0830 17:44:19.418279 139622356395904 learning.py:507] global step 5036: loss = 3.1636 (1.191 sec/step)\n","I0830 17:44:20.646804 139622356395904 learning.py:507] global step 5037: loss = 2.6865 (1.227 sec/step)\n","I0830 17:44:21.870327 139622356395904 learning.py:507] global step 5038: loss = 5.2034 (1.222 sec/step)\n","I0830 17:44:23.117428 139622356395904 learning.py:507] global step 5039: loss = 2.7372 (1.245 sec/step)\n","I0830 17:44:24.357179 139622356395904 learning.py:507] global step 5040: loss = 2.9927 (1.238 sec/step)\n","I0830 17:44:25.618857 139622356395904 learning.py:507] global step 5041: loss = 3.7915 (1.260 sec/step)\n","I0830 17:44:26.835460 139622356395904 learning.py:507] global step 5042: loss = 3.6829 (1.215 sec/step)\n","I0830 17:44:28.105777 139622356395904 learning.py:507] global step 5043: loss = 3.6083 (1.269 sec/step)\n","I0830 17:44:29.361741 139622356395904 learning.py:507] global step 5044: loss = 2.4365 (1.254 sec/step)\n","I0830 17:44:30.603489 139622356395904 learning.py:507] global step 5045: loss = 3.3506 (1.240 sec/step)\n","I0830 17:44:31.824747 139622356395904 learning.py:507] global step 5046: loss = 2.9788 (1.219 sec/step)\n","I0830 17:44:33.058382 139622356395904 learning.py:507] global step 5047: loss = 2.6439 (1.232 sec/step)\n","I0830 17:44:34.308614 139622356395904 learning.py:507] global step 5048: loss = 2.7358 (1.248 sec/step)\n","I0830 17:44:35.562329 139622356395904 learning.py:507] global step 5049: loss = 3.5356 (1.252 sec/step)\n","I0830 17:44:36.782319 139622356395904 learning.py:507] global step 5050: loss = 3.2809 (1.218 sec/step)\n","I0830 17:44:38.022881 139622356395904 learning.py:507] global step 5051: loss = 2.4615 (1.239 sec/step)\n","I0830 17:44:39.284029 139622356395904 learning.py:507] global step 5052: loss = 2.9960 (1.259 sec/step)\n","I0830 17:44:40.535795 139622356395904 learning.py:507] global step 5053: loss = 3.1336 (1.250 sec/step)\n","I0830 17:44:41.719659 139622356395904 learning.py:507] global step 5054: loss = 3.2728 (1.182 sec/step)\n","I0830 17:44:42.911000 139622356395904 learning.py:507] global step 5055: loss = 2.9469 (1.190 sec/step)\n","I0830 17:44:44.127028 139622356395904 learning.py:507] global step 5056: loss = 3.2446 (1.214 sec/step)\n","I0830 17:44:45.347600 139622356395904 learning.py:507] global step 5057: loss = 3.5077 (1.218 sec/step)\n","I0830 17:44:46.567785 139622356395904 learning.py:507] global step 5058: loss = 3.2689 (1.218 sec/step)\n","I0830 17:44:47.773130 139622356395904 learning.py:507] global step 5059: loss = 3.0446 (1.203 sec/step)\n","I0830 17:44:48.977212 139622356395904 learning.py:507] global step 5060: loss = 2.7066 (1.202 sec/step)\n","I0830 17:44:50.202905 139622356395904 learning.py:507] global step 5061: loss = 2.8273 (1.224 sec/step)\n","I0830 17:44:51.444963 139622356395904 learning.py:507] global step 5062: loss = 4.0155 (1.240 sec/step)\n","I0830 17:44:52.678227 139622356395904 learning.py:507] global step 5063: loss = 3.3770 (1.231 sec/step)\n","I0830 17:44:53.877001 139622356395904 learning.py:507] global step 5064: loss = 4.1427 (1.197 sec/step)\n","I0830 17:44:55.109688 139622356395904 learning.py:507] global step 5065: loss = 3.3058 (1.231 sec/step)\n","I0830 17:44:56.334785 139622356395904 learning.py:507] global step 5066: loss = 3.0420 (1.223 sec/step)\n","I0830 17:44:57.579921 139622356395904 learning.py:507] global step 5067: loss = 3.7004 (1.244 sec/step)\n","I0830 17:44:58.848197 139622356395904 learning.py:507] global step 5068: loss = 4.4375 (1.266 sec/step)\n","I0830 17:45:00.060551 139622356395904 learning.py:507] global step 5069: loss = 3.6208 (1.210 sec/step)\n","I0830 17:45:01.293852 139622356395904 learning.py:507] global step 5070: loss = 3.1359 (1.231 sec/step)\n","I0830 17:45:02.541743 139622356395904 learning.py:507] global step 5071: loss = 3.4179 (1.246 sec/step)\n","I0830 17:45:03.775494 139622356395904 learning.py:507] global step 5072: loss = 3.1193 (1.232 sec/step)\n","I0830 17:45:05.019538 139622356395904 learning.py:507] global step 5073: loss = 2.9347 (1.242 sec/step)\n","I0830 17:45:06.228219 139622356395904 learning.py:507] global step 5074: loss = 2.8175 (1.207 sec/step)\n","I0830 17:45:07.428703 139622356395904 learning.py:507] global step 5075: loss = 4.1728 (1.199 sec/step)\n","I0830 17:45:08.638713 139622356395904 learning.py:507] global step 5076: loss = 3.2636 (1.208 sec/step)\n","I0830 17:45:09.844494 139622356395904 learning.py:507] global step 5077: loss = 2.9958 (1.204 sec/step)\n","I0830 17:45:11.077751 139622356395904 learning.py:507] global step 5078: loss = 2.6657 (1.231 sec/step)\n","I0830 17:45:12.281146 139622356395904 learning.py:507] global step 5079: loss = 4.6386 (1.201 sec/step)\n","I0830 17:45:13.493719 139622356395904 learning.py:507] global step 5080: loss = 3.5424 (1.211 sec/step)\n","I0830 17:45:14.705223 139622356395904 learning.py:507] global step 5081: loss = 2.8791 (1.210 sec/step)\n","I0830 17:45:15.936173 139622356395904 learning.py:507] global step 5082: loss = 2.7259 (1.229 sec/step)\n","I0830 17:45:17.176084 139622356395904 learning.py:507] global step 5083: loss = 3.5585 (1.238 sec/step)\n","I0830 17:45:18.411821 139622356395904 learning.py:507] global step 5084: loss = 3.6778 (1.234 sec/step)\n","I0830 17:45:19.641846 139622356395904 learning.py:507] global step 5085: loss = 3.5134 (1.228 sec/step)\n","I0830 17:45:20.916355 139622356395904 learning.py:507] global step 5086: loss = 2.6502 (1.273 sec/step)\n","I0830 17:45:22.115144 139622356395904 learning.py:507] global step 5087: loss = 3.5002 (1.197 sec/step)\n","I0830 17:45:23.357280 139622356395904 learning.py:507] global step 5088: loss = 3.3863 (1.240 sec/step)\n","I0830 17:45:24.579168 139622356395904 learning.py:507] global step 5089: loss = 3.3528 (1.220 sec/step)\n","I0830 17:45:25.820374 139622356395904 learning.py:507] global step 5090: loss = 3.1524 (1.239 sec/step)\n","I0830 17:45:27.037721 139622356395904 learning.py:507] global step 5091: loss = 2.7701 (1.215 sec/step)\n","I0830 17:45:28.268467 139622356395904 learning.py:507] global step 5092: loss = 2.5518 (1.229 sec/step)\n","I0830 17:45:29.502007 139622356395904 learning.py:507] global step 5093: loss = 2.7654 (1.232 sec/step)\n","I0830 17:45:30.742461 139622356395904 learning.py:507] global step 5094: loss = 3.2576 (1.238 sec/step)\n","I0830 17:45:31.989156 139622356395904 learning.py:507] global step 5095: loss = 3.0186 (1.245 sec/step)\n","I0830 17:45:33.231424 139622356395904 learning.py:507] global step 5096: loss = 4.1420 (1.240 sec/step)\n","I0830 17:45:34.449222 139622356395904 learning.py:507] global step 5097: loss = 3.3239 (1.216 sec/step)\n","I0830 17:45:35.640655 139622356395904 learning.py:507] global step 5098: loss = 4.3916 (1.190 sec/step)\n","I0830 17:45:36.896967 139622356395904 learning.py:507] global step 5099: loss = 2.7831 (1.254 sec/step)\n","I0830 17:45:38.137807 139622356395904 learning.py:507] global step 5100: loss = 4.7723 (1.239 sec/step)\n","I0830 17:45:39.351344 139622356395904 learning.py:507] global step 5101: loss = 4.0734 (1.211 sec/step)\n","I0830 17:45:40.546014 139622356395904 learning.py:507] global step 5102: loss = 6.3062 (1.193 sec/step)\n","I0830 17:45:41.772294 139622356395904 learning.py:507] global step 5103: loss = 3.2330 (1.224 sec/step)\n","I0830 17:45:42.970106 139622356395904 learning.py:507] global step 5104: loss = 3.8170 (1.196 sec/step)\n","I0830 17:45:44.194135 139622356395904 learning.py:507] global step 5105: loss = 3.5038 (1.222 sec/step)\n","I0830 17:45:45.413428 139622356395904 learning.py:507] global step 5106: loss = 3.2321 (1.217 sec/step)\n","I0830 17:45:46.642222 139622356395904 learning.py:507] global step 5107: loss = 4.2666 (1.227 sec/step)\n","I0830 17:45:47.868245 139622356395904 learning.py:507] global step 5108: loss = 2.8445 (1.224 sec/step)\n","I0830 17:45:49.121196 139622356395904 learning.py:507] global step 5109: loss = 3.4692 (1.251 sec/step)\n","I0830 17:45:50.375038 139622356395904 learning.py:507] global step 5110: loss = 3.9840 (1.252 sec/step)\n","I0830 17:45:51.609871 139622356395904 learning.py:507] global step 5111: loss = 2.5514 (1.233 sec/step)\n","I0830 17:45:52.850395 139622356395904 learning.py:507] global step 5112: loss = 2.8286 (1.238 sec/step)\n","I0830 17:45:54.073924 139622356395904 learning.py:507] global step 5113: loss = 3.2374 (1.222 sec/step)\n","I0830 17:45:55.308696 139622356395904 learning.py:507] global step 5114: loss = 3.6809 (1.233 sec/step)\n","I0830 17:45:56.539333 139622356395904 learning.py:507] global step 5115: loss = 2.8925 (1.229 sec/step)\n","I0830 17:45:57.734416 139622356395904 learning.py:507] global step 5116: loss = 3.3921 (1.193 sec/step)\n","I0830 17:45:58.932713 139622356395904 learning.py:507] global step 5117: loss = 4.0085 (1.196 sec/step)\n","I0830 17:46:00.319571 139622356395904 learning.py:507] global step 5118: loss = 2.9133 (1.281 sec/step)\n","I0830 17:46:02.361214 139619299985152 supervisor.py:1050] Recording summary at step 5119.\n","I0830 17:46:02.397501 139622356395904 learning.py:507] global step 5119: loss = 2.5776 (2.076 sec/step)\n","I0830 17:46:03.621412 139622356395904 learning.py:507] global step 5120: loss = 3.2151 (1.222 sec/step)\n","I0830 17:46:04.874396 139622356395904 learning.py:507] global step 5121: loss = 3.6281 (1.251 sec/step)\n","I0830 17:46:06.130610 139622356395904 learning.py:507] global step 5122: loss = 3.1972 (1.254 sec/step)\n","I0830 17:46:07.348715 139622356395904 learning.py:507] global step 5123: loss = 2.4750 (1.216 sec/step)\n","I0830 17:46:08.582188 139622356395904 learning.py:507] global step 5124: loss = 3.4987 (1.232 sec/step)\n","I0830 17:46:09.808202 139622356395904 learning.py:507] global step 5125: loss = 2.9637 (1.224 sec/step)\n","I0830 17:46:11.051851 139622356395904 learning.py:507] global step 5126: loss = 2.8244 (1.241 sec/step)\n","I0830 17:46:12.274958 139622356395904 learning.py:507] global step 5127: loss = 2.9965 (1.221 sec/step)\n","I0830 17:46:13.504141 139622356395904 learning.py:507] global step 5128: loss = 2.6844 (1.227 sec/step)\n","I0830 17:46:14.707661 139622356395904 learning.py:507] global step 5129: loss = 4.2085 (1.201 sec/step)\n","I0830 17:46:15.910335 139622356395904 learning.py:507] global step 5130: loss = 2.8773 (1.201 sec/step)\n","I0830 17:46:17.129330 139622356395904 learning.py:507] global step 5131: loss = 2.6108 (1.217 sec/step)\n","I0830 17:46:18.338662 139622356395904 learning.py:507] global step 5132: loss = 3.3633 (1.208 sec/step)\n","I0830 17:46:19.565505 139622356395904 learning.py:507] global step 5133: loss = 2.8324 (1.225 sec/step)\n","I0830 17:46:20.791365 139622356395904 learning.py:507] global step 5134: loss = 2.6905 (1.224 sec/step)\n","I0830 17:46:22.036019 139622356395904 learning.py:507] global step 5135: loss = 3.6803 (1.243 sec/step)\n","I0830 17:46:23.282006 139622356395904 learning.py:507] global step 5136: loss = 3.2382 (1.244 sec/step)\n","I0830 17:46:24.475455 139622356395904 learning.py:507] global step 5137: loss = 3.3966 (1.192 sec/step)\n","I0830 17:46:25.656533 139622356395904 learning.py:507] global step 5138: loss = 3.2386 (1.179 sec/step)\n","I0830 17:46:26.844993 139622356395904 learning.py:507] global step 5139: loss = 4.2637 (1.187 sec/step)\n","I0830 17:46:28.042589 139622356395904 learning.py:507] global step 5140: loss = 3.1770 (1.196 sec/step)\n","I0830 17:46:29.284386 139622356395904 learning.py:507] global step 5141: loss = 4.1453 (1.240 sec/step)\n","I0830 17:46:30.503470 139622356395904 learning.py:507] global step 5142: loss = 3.8668 (1.217 sec/step)\n","I0830 17:46:31.720830 139622356395904 learning.py:507] global step 5143: loss = 3.4087 (1.215 sec/step)\n","I0830 17:46:32.912762 139622356395904 learning.py:507] global step 5144: loss = 3.1150 (1.190 sec/step)\n","I0830 17:46:34.139866 139622356395904 learning.py:507] global step 5145: loss = 2.7447 (1.225 sec/step)\n","I0830 17:46:35.375893 139622356395904 learning.py:507] global step 5146: loss = 2.9439 (1.234 sec/step)\n","I0830 17:46:36.607078 139622356395904 learning.py:507] global step 5147: loss = 2.9201 (1.229 sec/step)\n","I0830 17:46:37.814426 139622356395904 learning.py:507] global step 5148: loss = 3.2700 (1.206 sec/step)\n","I0830 17:46:39.032536 139622356395904 learning.py:507] global step 5149: loss = 3.1410 (1.216 sec/step)\n","I0830 17:46:40.249364 139622356395904 learning.py:507] global step 5150: loss = 2.7767 (1.214 sec/step)\n","I0830 17:46:41.471358 139622356395904 learning.py:507] global step 5151: loss = 3.0255 (1.220 sec/step)\n","I0830 17:46:42.722489 139622356395904 learning.py:507] global step 5152: loss = 3.2705 (1.249 sec/step)\n","I0830 17:46:43.951194 139622356395904 learning.py:507] global step 5153: loss = 2.3575 (1.227 sec/step)\n","I0830 17:46:45.180323 139622356395904 learning.py:507] global step 5154: loss = 2.9198 (1.226 sec/step)\n","I0830 17:46:46.372960 139622356395904 learning.py:507] global step 5155: loss = 3.3794 (1.190 sec/step)\n","I0830 17:46:47.581842 139622356395904 learning.py:507] global step 5156: loss = 2.9790 (1.207 sec/step)\n","I0830 17:46:48.814027 139622356395904 learning.py:507] global step 5157: loss = 3.4027 (1.230 sec/step)\n","I0830 17:46:50.041027 139622356395904 learning.py:507] global step 5158: loss = 4.0196 (1.225 sec/step)\n","I0830 17:46:51.279663 139622356395904 learning.py:507] global step 5159: loss = 3.4011 (1.237 sec/step)\n","I0830 17:46:52.530589 139622356395904 learning.py:507] global step 5160: loss = 2.7050 (1.249 sec/step)\n","I0830 17:46:53.770511 139622356395904 learning.py:507] global step 5161: loss = 2.3788 (1.238 sec/step)\n","I0830 17:46:55.016342 139622356395904 learning.py:507] global step 5162: loss = 3.3487 (1.244 sec/step)\n","I0830 17:46:56.228305 139622356395904 learning.py:507] global step 5163: loss = 2.8274 (1.210 sec/step)\n","I0830 17:46:57.443710 139622356395904 learning.py:507] global step 5164: loss = 2.8419 (1.214 sec/step)\n","I0830 17:46:58.680315 139622356395904 learning.py:507] global step 5165: loss = 2.6226 (1.235 sec/step)\n","I0830 17:46:59.888291 139622356395904 learning.py:507] global step 5166: loss = 2.6145 (1.206 sec/step)\n","I0830 17:47:01.148010 139622356395904 learning.py:507] global step 5167: loss = 3.4371 (1.258 sec/step)\n","I0830 17:47:02.372791 139622356395904 learning.py:507] global step 5168: loss = 2.9372 (1.223 sec/step)\n","I0830 17:47:03.603511 139622356395904 learning.py:507] global step 5169: loss = 3.1383 (1.228 sec/step)\n","I0830 17:47:04.812394 139622356395904 learning.py:507] global step 5170: loss = 2.5455 (1.207 sec/step)\n","I0830 17:47:06.031563 139622356395904 learning.py:507] global step 5171: loss = 2.2928 (1.218 sec/step)\n","I0830 17:47:07.263797 139622356395904 learning.py:507] global step 5172: loss = 4.2460 (1.230 sec/step)\n","I0830 17:47:08.459383 139622356395904 learning.py:507] global step 5173: loss = 3.4702 (1.194 sec/step)\n","I0830 17:47:09.705829 139622356395904 learning.py:507] global step 5174: loss = 3.5296 (1.244 sec/step)\n","I0830 17:47:10.935312 139622356395904 learning.py:507] global step 5175: loss = 4.3351 (1.227 sec/step)\n","I0830 17:47:12.177642 139622356395904 learning.py:507] global step 5176: loss = 2.3666 (1.240 sec/step)\n","I0830 17:47:13.377278 139622356395904 learning.py:507] global step 5177: loss = 3.9354 (1.198 sec/step)\n","I0830 17:47:14.599691 139622356395904 learning.py:507] global step 5178: loss = 3.9807 (1.221 sec/step)\n","I0830 17:47:15.810938 139622356395904 learning.py:507] global step 5179: loss = 2.8112 (1.209 sec/step)\n","I0830 17:47:17.038484 139622356395904 learning.py:507] global step 5180: loss = 3.6352 (1.226 sec/step)\n","I0830 17:47:18.240872 139622356395904 learning.py:507] global step 5181: loss = 2.9197 (1.200 sec/step)\n","I0830 17:47:19.446014 139622356395904 learning.py:507] global step 5182: loss = 3.2622 (1.203 sec/step)\n","I0830 17:47:20.659008 139622356395904 learning.py:507] global step 5183: loss = 2.8830 (1.211 sec/step)\n","I0830 17:47:21.926809 139622356395904 learning.py:507] global step 5184: loss = 2.7704 (1.266 sec/step)\n","I0830 17:47:23.169516 139622356395904 learning.py:507] global step 5185: loss = 3.7376 (1.241 sec/step)\n","I0830 17:47:24.393981 139622356395904 learning.py:507] global step 5186: loss = 3.8553 (1.223 sec/step)\n","I0830 17:47:25.626863 139622356395904 learning.py:507] global step 5187: loss = 2.4738 (1.231 sec/step)\n","I0830 17:47:26.881222 139622356395904 learning.py:507] global step 5188: loss = 2.9623 (1.252 sec/step)\n","I0830 17:47:28.115576 139622356395904 learning.py:507] global step 5189: loss = 2.7392 (1.232 sec/step)\n","I0830 17:47:29.374802 139622356395904 learning.py:507] global step 5190: loss = 2.7131 (1.258 sec/step)\n","I0830 17:47:30.622383 139622356395904 learning.py:507] global step 5191: loss = 3.4360 (1.246 sec/step)\n","I0830 17:47:31.813385 139622356395904 learning.py:507] global step 5192: loss = 4.2262 (1.189 sec/step)\n","I0830 17:47:33.029216 139622356395904 learning.py:507] global step 5193: loss = 2.9947 (1.214 sec/step)\n","I0830 17:47:34.253304 139622356395904 learning.py:507] global step 5194: loss = 2.8216 (1.222 sec/step)\n","I0830 17:47:35.483231 139622356395904 learning.py:507] global step 5195: loss = 3.0442 (1.228 sec/step)\n","I0830 17:47:36.711031 139622356395904 learning.py:507] global step 5196: loss = 3.0043 (1.226 sec/step)\n","I0830 17:47:37.957702 139622356395904 learning.py:507] global step 5197: loss = 2.8013 (1.245 sec/step)\n","I0830 17:47:39.215933 139622356395904 learning.py:507] global step 5198: loss = 4.1667 (1.256 sec/step)\n","I0830 17:47:40.416494 139622356395904 learning.py:507] global step 5199: loss = 3.8662 (1.199 sec/step)\n","I0830 17:47:41.661460 139622356395904 learning.py:507] global step 5200: loss = 2.6217 (1.243 sec/step)\n","I0830 17:47:42.866148 139622356395904 learning.py:507] global step 5201: loss = 2.5094 (1.203 sec/step)\n","I0830 17:47:44.100845 139622356395904 learning.py:507] global step 5202: loss = 3.4638 (1.233 sec/step)\n","I0830 17:47:45.300201 139622356395904 learning.py:507] global step 5203: loss = 3.5068 (1.197 sec/step)\n","I0830 17:47:46.518037 139622356395904 learning.py:507] global step 5204: loss = 2.9330 (1.216 sec/step)\n","I0830 17:47:47.704346 139622356395904 learning.py:507] global step 5205: loss = 2.8627 (1.184 sec/step)\n","I0830 17:47:48.951613 139622356395904 learning.py:507] global step 5206: loss = 5.3069 (1.245 sec/step)\n","I0830 17:47:50.189412 139622356395904 learning.py:507] global step 5207: loss = 2.5996 (1.236 sec/step)\n","I0830 17:47:51.441290 139622356395904 learning.py:507] global step 5208: loss = 2.7198 (1.246 sec/step)\n","I0830 17:47:52.683876 139622356395904 learning.py:507] global step 5209: loss = 2.5974 (1.241 sec/step)\n","I0830 17:47:53.936836 139622356395904 learning.py:507] global step 5210: loss = 3.7408 (1.251 sec/step)\n","I0830 17:47:55.179675 139622356395904 learning.py:507] global step 5211: loss = 3.2563 (1.240 sec/step)\n","I0830 17:47:56.428925 139622356395904 learning.py:507] global step 5212: loss = 2.7042 (1.247 sec/step)\n","I0830 17:47:57.670000 139622356395904 learning.py:507] global step 5213: loss = 3.9955 (1.239 sec/step)\n","I0830 17:47:58.909114 139622356395904 learning.py:507] global step 5214: loss = 2.6939 (1.237 sec/step)\n","I0830 17:48:00.339604 139622356395904 learning.py:507] global step 5215: loss = 3.0700 (1.331 sec/step)\n","I0830 17:48:02.343478 139619299985152 supervisor.py:1050] Recording summary at step 5216.\n","I0830 17:48:02.375830 139622356395904 learning.py:507] global step 5216: loss = 3.3392 (2.033 sec/step)\n","I0830 17:48:03.629369 139622356395904 learning.py:507] global step 5217: loss = 3.8881 (1.251 sec/step)\n","I0830 17:48:04.864261 139622356395904 learning.py:507] global step 5218: loss = 2.6451 (1.233 sec/step)\n","I0830 17:48:06.137746 139622356395904 learning.py:507] global step 5219: loss = 3.3377 (1.271 sec/step)\n","I0830 17:48:07.396192 139622356395904 learning.py:507] global step 5220: loss = 3.5193 (1.257 sec/step)\n","I0830 17:48:08.637742 139622356395904 learning.py:507] global step 5221: loss = 2.4737 (1.240 sec/step)\n","I0830 17:48:09.851809 139622356395904 learning.py:507] global step 5222: loss = 4.1053 (1.212 sec/step)\n","I0830 17:48:11.030550 139622356395904 learning.py:507] global step 5223: loss = 2.5035 (1.177 sec/step)\n","I0830 17:48:12.258727 139622356395904 learning.py:507] global step 5224: loss = 2.7422 (1.226 sec/step)\n","I0830 17:48:13.517997 139622356395904 learning.py:507] global step 5225: loss = 2.3829 (1.257 sec/step)\n","I0830 17:48:14.699432 139622356395904 learning.py:507] global step 5226: loss = 2.6238 (1.180 sec/step)\n","I0830 17:48:15.931242 139622356395904 learning.py:507] global step 5227: loss = 2.5944 (1.230 sec/step)\n","I0830 17:48:17.162532 139622356395904 learning.py:507] global step 5228: loss = 3.4106 (1.229 sec/step)\n","I0830 17:48:18.408017 139622356395904 learning.py:507] global step 5229: loss = 2.5935 (1.244 sec/step)\n","I0830 17:48:19.644447 139622356395904 learning.py:507] global step 5230: loss = 2.6779 (1.235 sec/step)\n","I0830 17:48:20.853798 139622356395904 learning.py:507] global step 5231: loss = 3.4868 (1.208 sec/step)\n","I0830 17:48:22.064281 139622356395904 learning.py:507] global step 5232: loss = 3.5036 (1.209 sec/step)\n","I0830 17:48:23.291927 139622356395904 learning.py:507] global step 5233: loss = 2.9424 (1.226 sec/step)\n","I0830 17:48:24.493066 139622356395904 learning.py:507] global step 5234: loss = 3.9465 (1.199 sec/step)\n","I0830 17:48:25.728993 139622356395904 learning.py:507] global step 5235: loss = 2.5383 (1.234 sec/step)\n","I0830 17:48:26.973254 139622356395904 learning.py:507] global step 5236: loss = 3.0567 (1.242 sec/step)\n","I0830 17:48:28.214857 139622356395904 learning.py:507] global step 5237: loss = 3.7473 (1.240 sec/step)\n","I0830 17:48:29.411356 139622356395904 learning.py:507] global step 5238: loss = 3.0563 (1.195 sec/step)\n","I0830 17:48:30.673574 139622356395904 learning.py:507] global step 5239: loss = 3.3406 (1.261 sec/step)\n","I0830 17:48:31.923680 139622356395904 learning.py:507] global step 5240: loss = 3.2373 (1.248 sec/step)\n","I0830 17:48:33.169753 139622356395904 learning.py:507] global step 5241: loss = 3.9467 (1.244 sec/step)\n","I0830 17:48:34.378949 139622356395904 learning.py:507] global step 5242: loss = 3.6916 (1.208 sec/step)\n","I0830 17:48:35.621820 139622356395904 learning.py:507] global step 5243: loss = 4.5183 (1.241 sec/step)\n","I0830 17:48:36.853835 139622356395904 learning.py:507] global step 5244: loss = 3.5572 (1.230 sec/step)\n","I0830 17:48:38.105319 139622356395904 learning.py:507] global step 5245: loss = 2.8988 (1.250 sec/step)\n","I0830 17:48:39.332530 139622356395904 learning.py:507] global step 5246: loss = 2.7219 (1.225 sec/step)\n","I0830 17:48:40.553593 139622356395904 learning.py:507] global step 5247: loss = 4.3276 (1.219 sec/step)\n","I0830 17:48:41.767103 139622356395904 learning.py:507] global step 5248: loss = 2.5671 (1.212 sec/step)\n","I0830 17:48:43.012311 139622356395904 learning.py:507] global step 5249: loss = 3.5944 (1.244 sec/step)\n","I0830 17:48:44.255532 139622356395904 learning.py:507] global step 5250: loss = 2.8911 (1.241 sec/step)\n","I0830 17:48:45.498747 139622356395904 learning.py:507] global step 5251: loss = 2.8229 (1.241 sec/step)\n","I0830 17:48:46.736531 139622356395904 learning.py:507] global step 5252: loss = 2.6479 (1.236 sec/step)\n","I0830 17:48:47.970157 139622356395904 learning.py:507] global step 5253: loss = 2.8527 (1.232 sec/step)\n","I0830 17:48:49.169929 139622356395904 learning.py:507] global step 5254: loss = 2.2989 (1.198 sec/step)\n","I0830 17:48:50.429720 139622356395904 learning.py:507] global step 5255: loss = 3.2565 (1.258 sec/step)\n","I0830 17:48:51.685872 139622356395904 learning.py:507] global step 5256: loss = 2.8284 (1.254 sec/step)\n","I0830 17:48:52.906414 139622356395904 learning.py:507] global step 5257: loss = 2.5476 (1.219 sec/step)\n","I0830 17:48:54.153460 139622356395904 learning.py:507] global step 5258: loss = 2.6842 (1.245 sec/step)\n","I0830 17:48:55.341231 139622356395904 learning.py:507] global step 5259: loss = 2.9740 (1.186 sec/step)\n","I0830 17:48:56.583600 139622356395904 learning.py:507] global step 5260: loss = 3.4626 (1.241 sec/step)\n","I0830 17:48:57.826290 139622356395904 learning.py:507] global step 5261: loss = 3.1883 (1.241 sec/step)\n","I0830 17:48:59.050446 139622356395904 learning.py:507] global step 5262: loss = 2.6528 (1.221 sec/step)\n","I0830 17:49:00.281352 139622356395904 learning.py:507] global step 5263: loss = 3.9988 (1.229 sec/step)\n","I0830 17:49:01.486602 139622356395904 learning.py:507] global step 5264: loss = 3.0264 (1.203 sec/step)\n","I0830 17:49:02.729861 139622356395904 learning.py:507] global step 5265: loss = 3.3584 (1.241 sec/step)\n","I0830 17:49:03.977201 139622356395904 learning.py:507] global step 5266: loss = 3.1784 (1.245 sec/step)\n","I0830 17:49:05.184367 139622356395904 learning.py:507] global step 5267: loss = 2.8749 (1.205 sec/step)\n","I0830 17:49:06.414369 139622356395904 learning.py:507] global step 5268: loss = 4.0697 (1.228 sec/step)\n","I0830 17:49:07.666745 139622356395904 learning.py:507] global step 5269: loss = 3.9084 (1.250 sec/step)\n","I0830 17:49:08.899341 139622356395904 learning.py:507] global step 5270: loss = 3.4113 (1.231 sec/step)\n","I0830 17:49:10.140590 139622356395904 learning.py:507] global step 5271: loss = 3.0148 (1.239 sec/step)\n","I0830 17:49:11.398428 139622356395904 learning.py:507] global step 5272: loss = 4.2709 (1.256 sec/step)\n","I0830 17:49:12.649787 139622356395904 learning.py:507] global step 5273: loss = 3.2466 (1.249 sec/step)\n","I0830 17:49:13.861183 139622356395904 learning.py:507] global step 5274: loss = 3.1094 (1.210 sec/step)\n","I0830 17:49:15.067759 139622356395904 learning.py:507] global step 5275: loss = 2.6721 (1.204 sec/step)\n","I0830 17:49:16.318088 139622356395904 learning.py:507] global step 5276: loss = 2.6634 (1.248 sec/step)\n","I0830 17:49:17.554848 139622356395904 learning.py:507] global step 5277: loss = 2.7267 (1.235 sec/step)\n","I0830 17:49:18.803118 139622356395904 learning.py:507] global step 5278: loss = 3.0997 (1.247 sec/step)\n","I0830 17:49:20.007536 139622356395904 learning.py:507] global step 5279: loss = 3.9152 (1.203 sec/step)\n","I0830 17:49:21.261266 139622356395904 learning.py:507] global step 5280: loss = 4.3516 (1.252 sec/step)\n","I0830 17:49:22.466831 139622356395904 learning.py:507] global step 5281: loss = 2.3359 (1.204 sec/step)\n","I0830 17:49:23.714502 139622356395904 learning.py:507] global step 5282: loss = 3.3459 (1.245 sec/step)\n","I0830 17:49:24.943150 139622356395904 learning.py:507] global step 5283: loss = 2.7630 (1.227 sec/step)\n","I0830 17:49:26.157151 139622356395904 learning.py:507] global step 5284: loss = 3.0168 (1.212 sec/step)\n","I0830 17:49:27.368288 139622356395904 learning.py:507] global step 5285: loss = 3.0829 (1.209 sec/step)\n","I0830 17:49:28.589668 139622356395904 learning.py:507] global step 5286: loss = 2.7551 (1.219 sec/step)\n","I0830 17:49:29.848009 139622356395904 learning.py:507] global step 5287: loss = 2.6703 (1.256 sec/step)\n","I0830 17:49:31.056597 139622356395904 learning.py:507] global step 5288: loss = 4.0791 (1.207 sec/step)\n","I0830 17:49:32.246665 139622356395904 learning.py:507] global step 5289: loss = 2.9783 (1.188 sec/step)\n","I0830 17:49:33.447120 139622356395904 learning.py:507] global step 5290: loss = 3.0876 (1.199 sec/step)\n","I0830 17:49:34.668727 139622356395904 learning.py:507] global step 5291: loss = 3.0229 (1.220 sec/step)\n","I0830 17:49:35.896491 139622356395904 learning.py:507] global step 5292: loss = 4.4363 (1.226 sec/step)\n","I0830 17:49:37.100322 139622356395904 learning.py:507] global step 5293: loss = 4.5800 (1.202 sec/step)\n","I0830 17:49:38.312318 139622356395904 learning.py:507] global step 5294: loss = 3.7175 (1.210 sec/step)\n","I0830 17:49:39.550205 139622356395904 learning.py:507] global step 5295: loss = 3.0201 (1.236 sec/step)\n","I0830 17:49:40.771402 139622356395904 learning.py:507] global step 5296: loss = 3.0679 (1.219 sec/step)\n","I0830 17:49:41.995149 139622356395904 learning.py:507] global step 5297: loss = 3.0333 (1.222 sec/step)\n","I0830 17:49:43.241594 139622356395904 learning.py:507] global step 5298: loss = 4.1984 (1.245 sec/step)\n","I0830 17:49:44.458145 139622356395904 learning.py:507] global step 5299: loss = 3.5148 (1.215 sec/step)\n","I0830 17:49:45.661682 139622356395904 learning.py:507] global step 5300: loss = 2.8807 (1.201 sec/step)\n","I0830 17:49:46.870492 139622356395904 learning.py:507] global step 5301: loss = 4.0944 (1.207 sec/step)\n","I0830 17:49:48.095801 139622356395904 learning.py:507] global step 5302: loss = 3.5424 (1.223 sec/step)\n","I0830 17:49:49.359246 139622356395904 learning.py:507] global step 5303: loss = 2.8962 (1.261 sec/step)\n","I0830 17:49:50.594852 139622356395904 learning.py:507] global step 5304: loss = 2.6019 (1.233 sec/step)\n","I0830 17:49:51.825449 139622356395904 learning.py:507] global step 5305: loss = 2.8534 (1.229 sec/step)\n","I0830 17:49:53.038418 139622356395904 learning.py:507] global step 5306: loss = 3.2909 (1.211 sec/step)\n","I0830 17:49:54.276260 139622356395904 learning.py:507] global step 5307: loss = 3.5777 (1.236 sec/step)\n","I0830 17:49:55.487027 139622356395904 learning.py:507] global step 5308: loss = 4.6376 (1.209 sec/step)\n","I0830 17:49:56.737745 139622356395904 learning.py:507] global step 5309: loss = 3.2784 (1.244 sec/step)\n","I0830 17:49:57.980575 139622356395904 learning.py:507] global step 5310: loss = 3.0551 (1.241 sec/step)\n","I0830 17:49:59.170301 139622356395904 learning.py:507] global step 5311: loss = 2.7516 (1.188 sec/step)\n","I0830 17:49:59.646773 139619316770560 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n","I0830 17:50:01.931219 139622356395904 learning.py:507] global step 5312: loss = 3.1572 (2.637 sec/step)\n","I0830 17:50:01.968336 139619299985152 supervisor.py:1050] Recording summary at step 5312.\n","I0830 17:50:03.478529 139622356395904 learning.py:507] global step 5313: loss = 2.9461 (1.534 sec/step)\n","I0830 17:50:05.117478 139622356395904 learning.py:507] global step 5314: loss = 3.6375 (1.510 sec/step)\n","I0830 17:50:06.318310 139622356395904 learning.py:507] global step 5315: loss = 3.1848 (1.198 sec/step)\n","I0830 17:50:07.573368 139622356395904 learning.py:507] global step 5316: loss = 3.3638 (1.253 sec/step)\n","I0830 17:50:08.826914 139622356395904 learning.py:507] global step 5317: loss = 2.7059 (1.252 sec/step)\n","I0830 17:50:10.077759 139622356395904 learning.py:507] global step 5318: loss = 3.3091 (1.249 sec/step)\n","I0830 17:50:11.308893 139622356395904 learning.py:507] global step 5319: loss = 2.6885 (1.229 sec/step)\n","I0830 17:50:12.509665 139622356395904 learning.py:507] global step 5320: loss = 3.1605 (1.199 sec/step)\n","I0830 17:50:13.751306 139622356395904 learning.py:507] global step 5321: loss = 3.2579 (1.240 sec/step)\n","I0830 17:50:14.953570 139622356395904 learning.py:507] global step 5322: loss = 3.6477 (1.200 sec/step)\n","I0830 17:50:16.186924 139622356395904 learning.py:507] global step 5323: loss = 3.1489 (1.232 sec/step)\n","I0830 17:50:17.394411 139622356395904 learning.py:507] global step 5324: loss = 3.6907 (1.205 sec/step)\n","I0830 17:50:18.607722 139622356395904 learning.py:507] global step 5325: loss = 2.7455 (1.211 sec/step)\n","I0830 17:50:19.842343 139622356395904 learning.py:507] global step 5326: loss = 2.6493 (1.233 sec/step)\n","I0830 17:50:21.065077 139622356395904 learning.py:507] global step 5327: loss = 3.1906 (1.221 sec/step)\n","I0830 17:50:22.290659 139622356395904 learning.py:507] global step 5328: loss = 3.2997 (1.224 sec/step)\n","I0830 17:50:23.487770 139622356395904 learning.py:507] global step 5329: loss = 3.0876 (1.195 sec/step)\n","I0830 17:50:24.696306 139622356395904 learning.py:507] global step 5330: loss = 3.5465 (1.207 sec/step)\n","I0830 17:50:25.923558 139622356395904 learning.py:507] global step 5331: loss = 3.0905 (1.225 sec/step)\n","I0830 17:50:27.141406 139622356395904 learning.py:507] global step 5332: loss = 2.8410 (1.216 sec/step)\n","I0830 17:50:28.383028 139622356395904 learning.py:507] global step 5333: loss = 3.1945 (1.240 sec/step)\n","I0830 17:50:29.584383 139622356395904 learning.py:507] global step 5334: loss = 3.2688 (1.199 sec/step)\n","I0830 17:50:30.791785 139622356395904 learning.py:507] global step 5335: loss = 3.2885 (1.205 sec/step)\n","I0830 17:50:32.041933 139622356395904 learning.py:507] global step 5336: loss = 4.0851 (1.248 sec/step)\n","I0830 17:50:33.238242 139622356395904 learning.py:507] global step 5337: loss = 2.5884 (1.195 sec/step)\n","I0830 17:50:34.459265 139622356395904 learning.py:507] global step 5338: loss = 2.9410 (1.219 sec/step)\n","I0830 17:50:35.698181 139622356395904 learning.py:507] global step 5339: loss = 3.7781 (1.237 sec/step)\n","I0830 17:50:36.916913 139622356395904 learning.py:507] global step 5340: loss = 3.1031 (1.217 sec/step)\n","I0830 17:50:38.163456 139622356395904 learning.py:507] global step 5341: loss = 2.4502 (1.245 sec/step)\n","I0830 17:50:39.417267 139622356395904 learning.py:507] global step 5342: loss = 3.6912 (1.252 sec/step)\n","I0830 17:50:40.632026 139622356395904 learning.py:507] global step 5343: loss = 3.5028 (1.213 sec/step)\n","I0830 17:50:41.878534 139622356395904 learning.py:507] global step 5344: loss = 2.7742 (1.245 sec/step)\n","I0830 17:50:43.103343 139622356395904 learning.py:507] global step 5345: loss = 2.9410 (1.223 sec/step)\n","I0830 17:50:44.290372 139622356395904 learning.py:507] global step 5346: loss = 2.6409 (1.185 sec/step)\n","I0830 17:50:45.498247 139622356395904 learning.py:507] global step 5347: loss = 3.3295 (1.206 sec/step)\n","I0830 17:50:46.696175 139622356395904 learning.py:507] global step 5348: loss = 2.8873 (1.196 sec/step)\n","I0830 17:50:47.928896 139622356395904 learning.py:507] global step 5349: loss = 2.8984 (1.231 sec/step)\n","I0830 17:50:49.161202 139622356395904 learning.py:507] global step 5350: loss = 3.1133 (1.230 sec/step)\n","I0830 17:50:50.376918 139622356395904 learning.py:507] global step 5351: loss = 3.1619 (1.214 sec/step)\n","I0830 17:50:51.587528 139622356395904 learning.py:507] global step 5352: loss = 2.6304 (1.209 sec/step)\n","I0830 17:50:52.811437 139622356395904 learning.py:507] global step 5353: loss = 3.6031 (1.222 sec/step)\n","I0830 17:50:54.041360 139622356395904 learning.py:507] global step 5354: loss = 2.7544 (1.228 sec/step)\n","I0830 17:50:55.271615 139622356395904 learning.py:507] global step 5355: loss = 3.1581 (1.228 sec/step)\n","I0830 17:50:56.515338 139622356395904 learning.py:507] global step 5356: loss = 3.6347 (1.242 sec/step)\n","I0830 17:50:57.743762 139622356395904 learning.py:507] global step 5357: loss = 3.6791 (1.227 sec/step)\n","I0830 17:50:58.975516 139622356395904 learning.py:507] global step 5358: loss = 3.6386 (1.230 sec/step)\n","I0830 17:51:00.206570 139622356395904 learning.py:507] global step 5359: loss = 3.2080 (1.229 sec/step)\n","I0830 17:51:01.432949 139622356395904 learning.py:507] global step 5360: loss = 2.9877 (1.225 sec/step)\n","I0830 17:51:02.684329 139622356395904 learning.py:507] global step 5361: loss = 3.4569 (1.249 sec/step)\n","I0830 17:51:03.905124 139622356395904 learning.py:507] global step 5362: loss = 3.1524 (1.219 sec/step)\n","I0830 17:51:05.106788 139622356395904 learning.py:507] global step 5363: loss = 2.6378 (1.200 sec/step)\n","I0830 17:51:06.288473 139622356395904 learning.py:507] global step 5364: loss = 3.3339 (1.180 sec/step)\n","I0830 17:51:07.526388 139622356395904 learning.py:507] global step 5365: loss = 3.1154 (1.236 sec/step)\n","I0830 17:51:08.743277 139622356395904 learning.py:507] global step 5366: loss = 2.3365 (1.215 sec/step)\n","I0830 17:51:09.991195 139622356395904 learning.py:507] global step 5367: loss = 2.8389 (1.246 sec/step)\n","I0830 17:51:11.209232 139622356395904 learning.py:507] global step 5368: loss = 2.6487 (1.216 sec/step)\n","I0830 17:51:12.463707 139622356395904 learning.py:507] global step 5369: loss = 2.5848 (1.253 sec/step)\n","I0830 17:51:13.680814 139622356395904 learning.py:507] global step 5370: loss = 3.0473 (1.215 sec/step)\n","I0830 17:51:14.902306 139622356395904 learning.py:507] global step 5371: loss = 3.3585 (1.220 sec/step)\n","I0830 17:51:16.105184 139622356395904 learning.py:507] global step 5372: loss = 3.2623 (1.201 sec/step)\n","I0830 17:51:17.330558 139622356395904 learning.py:507] global step 5373: loss = 2.8070 (1.223 sec/step)\n","I0830 17:51:18.549206 139622356395904 learning.py:507] global step 5374: loss = 3.0744 (1.217 sec/step)\n","I0830 17:51:19.752679 139622356395904 learning.py:507] global step 5375: loss = 3.0954 (1.201 sec/step)\n","I0830 17:51:20.973620 139622356395904 learning.py:507] global step 5376: loss = 2.3906 (1.219 sec/step)\n","I0830 17:51:22.225339 139622356395904 learning.py:507] global step 5377: loss = 2.7716 (1.248 sec/step)\n","I0830 17:51:23.459346 139622356395904 learning.py:507] global step 5378: loss = 3.4140 (1.232 sec/step)\n","I0830 17:51:24.710602 139622356395904 learning.py:507] global step 5379: loss = 3.0205 (1.249 sec/step)\n","I0830 17:51:25.950620 139622356395904 learning.py:507] global step 5380: loss = 3.0466 (1.238 sec/step)\n","I0830 17:51:27.206613 139622356395904 learning.py:507] global step 5381: loss = 3.1683 (1.254 sec/step)\n","I0830 17:51:28.430188 139622356395904 learning.py:507] global step 5382: loss = 3.1406 (1.220 sec/step)\n","I0830 17:51:29.629911 139622356395904 learning.py:507] global step 5383: loss = 2.6452 (1.198 sec/step)\n","I0830 17:51:30.846733 139622356395904 learning.py:507] global step 5384: loss = 2.9319 (1.215 sec/step)\n","I0830 17:51:32.097099 139622356395904 learning.py:507] global step 5385: loss = 3.0216 (1.249 sec/step)\n","I0830 17:51:33.329611 139622356395904 learning.py:507] global step 5386: loss = 3.3096 (1.230 sec/step)\n","I0830 17:51:34.551008 139622356395904 learning.py:507] global step 5387: loss = 2.9376 (1.219 sec/step)\n","I0830 17:51:35.797163 139622356395904 learning.py:507] global step 5388: loss = 3.8972 (1.244 sec/step)\n","I0830 17:51:37.006515 139622356395904 learning.py:507] global step 5389: loss = 3.3024 (1.207 sec/step)\n","I0830 17:51:38.210210 139622356395904 learning.py:507] global step 5390: loss = 2.7971 (1.201 sec/step)\n","I0830 17:51:39.429474 139622356395904 learning.py:507] global step 5391: loss = 4.1705 (1.217 sec/step)\n","I0830 17:51:40.641906 139622356395904 learning.py:507] global step 5392: loss = 2.4209 (1.211 sec/step)\n","I0830 17:51:41.867210 139622356395904 learning.py:507] global step 5393: loss = 2.5595 (1.223 sec/step)\n","I0830 17:51:43.074527 139622356395904 learning.py:507] global step 5394: loss = 2.8713 (1.205 sec/step)\n","I0830 17:51:44.311110 139622356395904 learning.py:507] global step 5395: loss = 3.3025 (1.235 sec/step)\n","I0830 17:51:45.531300 139622356395904 learning.py:507] global step 5396: loss = 2.8946 (1.219 sec/step)\n","I0830 17:51:46.753472 139622356395904 learning.py:507] global step 5397: loss = 3.4616 (1.221 sec/step)\n","I0830 17:51:47.966828 139622356395904 learning.py:507] global step 5398: loss = 2.6406 (1.212 sec/step)\n","I0830 17:51:49.199208 139622356395904 learning.py:507] global step 5399: loss = 4.0413 (1.231 sec/step)\n","I0830 17:51:50.413437 139622356395904 learning.py:507] global step 5400: loss = 3.3119 (1.212 sec/step)\n","I0830 17:51:51.640790 139622356395904 learning.py:507] global step 5401: loss = 3.3242 (1.225 sec/step)\n","I0830 17:51:52.892960 139622356395904 learning.py:507] global step 5402: loss = 2.4003 (1.250 sec/step)\n","I0830 17:51:54.112604 139622356395904 learning.py:507] global step 5403: loss = 3.9905 (1.218 sec/step)\n","I0830 17:51:55.349860 139622356395904 learning.py:507] global step 5404: loss = 3.6929 (1.235 sec/step)\n","I0830 17:51:56.529168 139622356395904 learning.py:507] global step 5405: loss = 3.9907 (1.177 sec/step)\n","I0830 17:51:57.777907 139622356395904 learning.py:507] global step 5406: loss = 2.5533 (1.247 sec/step)\n","I0830 17:51:58.976552 139622356395904 learning.py:507] global step 5407: loss = 2.9053 (1.197 sec/step)\n","I0830 17:52:00.297184 139622356395904 learning.py:507] global step 5408: loss = 3.2507 (1.260 sec/step)\n","I0830 17:52:02.327195 139619299985152 supervisor.py:1050] Recording summary at step 5409.\n","I0830 17:52:02.365491 139622356395904 learning.py:507] global step 5409: loss = 3.5820 (2.066 sec/step)\n","I0830 17:52:03.568328 139622356395904 learning.py:507] global step 5410: loss = 2.4819 (1.201 sec/step)\n","I0830 17:52:04.774577 139622356395904 learning.py:507] global step 5411: loss = 3.7930 (1.204 sec/step)\n","I0830 17:52:06.008178 139622356395904 learning.py:507] global step 5412: loss = 2.9850 (1.232 sec/step)\n","I0830 17:52:07.244679 139622356395904 learning.py:507] global step 5413: loss = 3.0711 (1.235 sec/step)\n","I0830 17:52:08.484595 139622356395904 learning.py:507] global step 5414: loss = 2.6677 (1.238 sec/step)\n","I0830 17:52:09.736876 139622356395904 learning.py:507] global step 5415: loss = 3.1531 (1.249 sec/step)\n","I0830 17:52:10.970412 139622356395904 learning.py:507] global step 5416: loss = 2.2486 (1.231 sec/step)\n","I0830 17:52:12.187103 139622356395904 learning.py:507] global step 5417: loss = 3.8037 (1.215 sec/step)\n","I0830 17:52:13.429303 139622356395904 learning.py:507] global step 5418: loss = 2.7214 (1.240 sec/step)\n","I0830 17:52:14.625559 139622356395904 learning.py:507] global step 5419: loss = 3.4945 (1.194 sec/step)\n","I0830 17:52:15.844137 139622356395904 learning.py:507] global step 5420: loss = 3.0892 (1.217 sec/step)\n","I0830 17:52:17.052645 139622356395904 learning.py:507] global step 5421: loss = 3.3780 (1.207 sec/step)\n","I0830 17:52:18.256809 139622356395904 learning.py:507] global step 5422: loss = 4.2480 (1.202 sec/step)\n","I0830 17:52:19.497677 139622356395904 learning.py:507] global step 5423: loss = 3.0897 (1.239 sec/step)\n","I0830 17:52:20.730120 139622356395904 learning.py:507] global step 5424: loss = 2.4649 (1.231 sec/step)\n","I0830 17:52:21.965734 139622356395904 learning.py:507] global step 5425: loss = 2.1619 (1.234 sec/step)\n","I0830 17:52:23.183780 139622356395904 learning.py:507] global step 5426: loss = 3.1310 (1.216 sec/step)\n","I0830 17:52:24.396861 139622356395904 learning.py:507] global step 5427: loss = 3.1826 (1.211 sec/step)\n","I0830 17:52:25.591325 139622356395904 learning.py:507] global step 5428: loss = 3.9856 (1.193 sec/step)\n","I0830 17:52:26.808974 139622356395904 learning.py:507] global step 5429: loss = 3.1879 (1.216 sec/step)\n","I0830 17:52:28.033686 139622356395904 learning.py:507] global step 5430: loss = 3.5867 (1.223 sec/step)\n","I0830 17:52:29.251230 139622356395904 learning.py:507] global step 5431: loss = 3.4246 (1.216 sec/step)\n","I0830 17:52:30.445574 139622356395904 learning.py:507] global step 5432: loss = 2.3952 (1.193 sec/step)\n","I0830 17:52:31.683768 139622356395904 learning.py:507] global step 5433: loss = 2.8851 (1.237 sec/step)\n","I0830 17:52:32.890829 139622356395904 learning.py:507] global step 5434: loss = 2.8192 (1.205 sec/step)\n","I0830 17:52:34.114867 139622356395904 learning.py:507] global step 5435: loss = 2.5941 (1.222 sec/step)\n","I0830 17:52:35.342865 139622356395904 learning.py:507] global step 5436: loss = 4.1603 (1.226 sec/step)\n","I0830 17:52:36.562206 139622356395904 learning.py:507] global step 5437: loss = 4.2402 (1.218 sec/step)\n","I0830 17:52:37.785972 139622356395904 learning.py:507] global step 5438: loss = 2.8744 (1.222 sec/step)\n","I0830 17:52:39.020172 139622356395904 learning.py:507] global step 5439: loss = 3.3431 (1.232 sec/step)\n","I0830 17:52:40.256340 139622356395904 learning.py:507] global step 5440: loss = 3.6532 (1.234 sec/step)\n","I0830 17:52:41.517676 139622356395904 learning.py:507] global step 5441: loss = 2.5314 (1.260 sec/step)\n","I0830 17:52:42.747477 139622356395904 learning.py:507] global step 5442: loss = 2.6927 (1.228 sec/step)\n","I0830 17:52:43.958262 139622356395904 learning.py:507] global step 5443: loss = 2.9965 (1.209 sec/step)\n","I0830 17:52:45.227560 139622356395904 learning.py:507] global step 5444: loss = 3.0532 (1.267 sec/step)\n","I0830 17:52:46.431620 139622356395904 learning.py:507] global step 5445: loss = 3.0129 (1.202 sec/step)\n","I0830 17:52:47.627542 139622356395904 learning.py:507] global step 5446: loss = 3.3141 (1.194 sec/step)\n","I0830 17:52:48.808191 139622356395904 learning.py:507] global step 5447: loss = 2.2312 (1.179 sec/step)\n","I0830 17:52:50.061991 139622356395904 learning.py:507] global step 5448: loss = 4.0827 (1.252 sec/step)\n","I0830 17:52:51.327304 139622356395904 learning.py:507] global step 5449: loss = 3.4409 (1.263 sec/step)\n","I0830 17:52:52.531498 139622356395904 learning.py:507] global step 5450: loss = 3.1154 (1.202 sec/step)\n","I0830 17:52:53.788279 139622356395904 learning.py:507] global step 5451: loss = 2.8146 (1.255 sec/step)\n","I0830 17:52:55.012816 139622356395904 learning.py:507] global step 5452: loss = 2.8072 (1.223 sec/step)\n","I0830 17:52:56.255344 139622356395904 learning.py:507] global step 5453: loss = 2.6925 (1.241 sec/step)\n","I0830 17:52:57.484272 139622356395904 learning.py:507] global step 5454: loss = 3.2598 (1.227 sec/step)\n","I0830 17:52:58.725392 139622356395904 learning.py:507] global step 5455: loss = 3.5988 (1.239 sec/step)\n","I0830 17:52:59.931103 139622356395904 learning.py:507] global step 5456: loss = 3.5327 (1.204 sec/step)\n","I0830 17:53:01.174163 139622356395904 learning.py:507] global step 5457: loss = 3.0685 (1.241 sec/step)\n","I0830 17:53:02.383433 139622356395904 learning.py:507] global step 5458: loss = 2.4545 (1.207 sec/step)\n","I0830 17:53:03.574276 139622356395904 learning.py:507] global step 5459: loss = 3.2358 (1.189 sec/step)\n","I0830 17:53:04.817820 139622356395904 learning.py:507] global step 5460: loss = 3.5134 (1.242 sec/step)\n","I0830 17:53:06.035128 139622356395904 learning.py:507] global step 5461: loss = 2.8573 (1.215 sec/step)\n","I0830 17:53:07.242447 139622356395904 learning.py:507] global step 5462: loss = 4.1535 (1.205 sec/step)\n","I0830 17:53:08.457945 139622356395904 learning.py:507] global step 5463: loss = 3.8350 (1.213 sec/step)\n","I0830 17:53:09.690186 139622356395904 learning.py:507] global step 5464: loss = 3.9314 (1.229 sec/step)\n","I0830 17:53:10.904635 139622356395904 learning.py:507] global step 5465: loss = 2.8213 (1.212 sec/step)\n","I0830 17:53:12.113841 139622356395904 learning.py:507] global step 5466: loss = 3.4284 (1.207 sec/step)\n","I0830 17:53:13.420166 139622356395904 learning.py:507] global step 5467: loss = 3.0768 (1.304 sec/step)\n","I0830 17:53:14.654873 139622356395904 learning.py:507] global step 5468: loss = 2.6414 (1.233 sec/step)\n","I0830 17:53:15.915648 139622356395904 learning.py:507] global step 5469: loss = 3.7174 (1.259 sec/step)\n","I0830 17:53:17.136876 139622356395904 learning.py:507] global step 5470: loss = 2.7507 (1.219 sec/step)\n","I0830 17:53:18.387603 139622356395904 learning.py:507] global step 5471: loss = 3.0789 (1.248 sec/step)\n","I0830 17:53:19.592445 139622356395904 learning.py:507] global step 5472: loss = 3.3877 (1.203 sec/step)\n","I0830 17:53:20.829766 139622356395904 learning.py:507] global step 5473: loss = 2.7926 (1.236 sec/step)\n","I0830 17:53:22.050440 139622356395904 learning.py:507] global step 5474: loss = 2.5966 (1.219 sec/step)\n","I0830 17:53:23.320300 139622356395904 learning.py:507] global step 5475: loss = 2.7158 (1.268 sec/step)\n","I0830 17:53:24.552520 139622356395904 learning.py:507] global step 5476: loss = 2.6079 (1.230 sec/step)\n","I0830 17:53:25.806735 139622356395904 learning.py:507] global step 5477: loss = 2.8708 (1.252 sec/step)\n","I0830 17:53:27.036103 139622356395904 learning.py:507] global step 5478: loss = 2.8161 (1.228 sec/step)\n","I0830 17:53:28.233475 139622356395904 learning.py:507] global step 5479: loss = 3.1717 (1.195 sec/step)\n","I0830 17:53:29.478393 139622356395904 learning.py:507] global step 5480: loss = 3.0822 (1.243 sec/step)\n","I0830 17:53:30.672848 139622356395904 learning.py:507] global step 5481: loss = 2.6072 (1.193 sec/step)\n","I0830 17:53:31.930449 139622356395904 learning.py:507] global step 5482: loss = 3.1512 (1.255 sec/step)\n","I0830 17:53:33.154716 139622356395904 learning.py:507] global step 5483: loss = 3.7772 (1.222 sec/step)\n","I0830 17:53:34.331119 139622356395904 learning.py:507] global step 5484: loss = 3.2862 (1.174 sec/step)\n","I0830 17:53:35.544358 139622356395904 learning.py:507] global step 5485: loss = 3.1673 (1.211 sec/step)\n","I0830 17:53:36.797913 139622356395904 learning.py:507] global step 5486: loss = 3.0827 (1.252 sec/step)\n","I0830 17:53:38.050250 139622356395904 learning.py:507] global step 5487: loss = 3.4227 (1.250 sec/step)\n","I0830 17:53:39.274327 139622356395904 learning.py:507] global step 5488: loss = 2.7862 (1.222 sec/step)\n","I0830 17:53:40.483861 139622356395904 learning.py:507] global step 5489: loss = 3.4023 (1.208 sec/step)\n","I0830 17:53:41.706525 139622356395904 learning.py:507] global step 5490: loss = 2.4452 (1.221 sec/step)\n","I0830 17:53:42.931933 139622356395904 learning.py:507] global step 5491: loss = 3.6230 (1.223 sec/step)\n","I0830 17:53:44.191338 139622356395904 learning.py:507] global step 5492: loss = 3.0790 (1.258 sec/step)\n","I0830 17:53:45.408694 139622356395904 learning.py:507] global step 5493: loss = 3.1444 (1.216 sec/step)\n","I0830 17:53:46.610966 139622356395904 learning.py:507] global step 5494: loss = 2.6373 (1.200 sec/step)\n","I0830 17:53:47.816900 139622356395904 learning.py:507] global step 5495: loss = 4.4012 (1.204 sec/step)\n","I0830 17:53:49.031765 139622356395904 learning.py:507] global step 5496: loss = 2.5736 (1.213 sec/step)\n","I0830 17:53:50.223954 139622356395904 learning.py:507] global step 5497: loss = 3.4551 (1.190 sec/step)\n","I0830 17:53:51.428846 139622356395904 learning.py:507] global step 5498: loss = 4.4775 (1.203 sec/step)\n","I0830 17:53:52.644601 139622356395904 learning.py:507] global step 5499: loss = 2.6753 (1.214 sec/step)\n","I0830 17:53:53.848788 139622356395904 learning.py:507] global step 5500: loss = 3.5841 (1.202 sec/step)\n","I0830 17:53:55.084216 139622356395904 learning.py:507] global step 5501: loss = 2.4543 (1.234 sec/step)\n","I0830 17:53:56.325170 139622356395904 learning.py:507] global step 5502: loss = 3.6402 (1.239 sec/step)\n","I0830 17:53:57.582235 139622356395904 learning.py:507] global step 5503: loss = 2.8446 (1.255 sec/step)\n","I0830 17:53:58.778885 139622356395904 learning.py:507] global step 5504: loss = 3.2363 (1.194 sec/step)\n","I0830 17:54:00.041189 139622356395904 learning.py:507] global step 5505: loss = 2.7273 (1.260 sec/step)\n","I0830 17:54:02.140393 139619299985152 supervisor.py:1050] Recording summary at step 5506.\n","I0830 17:54:02.160509 139622356395904 learning.py:507] global step 5506: loss = 2.4084 (2.118 sec/step)\n","I0830 17:54:03.399037 139622356395904 learning.py:507] global step 5507: loss = 2.7038 (1.237 sec/step)\n","I0830 17:54:04.642581 139622356395904 learning.py:507] global step 5508: loss = 2.2826 (1.242 sec/step)\n","I0830 17:54:05.878153 139622356395904 learning.py:507] global step 5509: loss = 4.1923 (1.234 sec/step)\n","I0830 17:54:07.092906 139622356395904 learning.py:507] global step 5510: loss = 3.8897 (1.213 sec/step)\n","I0830 17:54:08.295204 139622356395904 learning.py:507] global step 5511: loss = 3.1255 (1.200 sec/step)\n","I0830 17:54:09.568382 139622356395904 learning.py:507] global step 5512: loss = 2.5475 (1.270 sec/step)\n","I0830 17:54:10.772722 139622356395904 learning.py:507] global step 5513: loss = 4.1837 (1.202 sec/step)\n","I0830 17:54:12.034916 139622356395904 learning.py:507] global step 5514: loss = 3.5152 (1.260 sec/step)\n","I0830 17:54:13.266566 139622356395904 learning.py:507] global step 5515: loss = 2.7748 (1.230 sec/step)\n","I0830 17:54:14.484776 139622356395904 learning.py:507] global step 5516: loss = 2.8031 (1.216 sec/step)\n","I0830 17:54:15.697369 139622356395904 learning.py:507] global step 5517: loss = 2.7909 (1.211 sec/step)\n","I0830 17:54:16.921159 139622356395904 learning.py:507] global step 5518: loss = 3.4709 (1.222 sec/step)\n","I0830 17:54:18.218249 139622356395904 learning.py:507] global step 5519: loss = 2.8592 (1.295 sec/step)\n","I0830 17:54:19.426208 139622356395904 learning.py:507] global step 5520: loss = 3.7192 (1.205 sec/step)\n","I0830 17:54:20.663435 139622356395904 learning.py:507] global step 5521: loss = 3.4417 (1.235 sec/step)\n","I0830 17:54:21.886578 139622356395904 learning.py:507] global step 5522: loss = 3.5894 (1.221 sec/step)\n","I0830 17:54:23.101721 139622356395904 learning.py:507] global step 5523: loss = 2.7192 (1.213 sec/step)\n","I0830 17:54:24.322155 139622356395904 learning.py:507] global step 5524: loss = 3.8325 (1.219 sec/step)\n","I0830 17:54:25.505985 139622356395904 learning.py:507] global step 5525: loss = 2.6663 (1.182 sec/step)\n","I0830 17:54:26.727596 139622356395904 learning.py:507] global step 5526: loss = 3.0124 (1.220 sec/step)\n","I0830 17:54:27.956163 139622356395904 learning.py:507] global step 5527: loss = 2.7929 (1.227 sec/step)\n","I0830 17:54:29.154474 139622356395904 learning.py:507] global step 5528: loss = 3.3107 (1.196 sec/step)\n","I0830 17:54:30.376611 139622356395904 learning.py:507] global step 5529: loss = 3.1554 (1.220 sec/step)\n","I0830 17:54:31.593900 139622356395904 learning.py:507] global step 5530: loss = 2.6484 (1.215 sec/step)\n","I0830 17:54:32.819985 139622356395904 learning.py:507] global step 5531: loss = 2.9310 (1.224 sec/step)\n","I0830 17:54:34.027811 139622356395904 learning.py:507] global step 5532: loss = 2.8522 (1.206 sec/step)\n","I0830 17:54:35.258261 139622356395904 learning.py:507] global step 5533: loss = 2.5728 (1.229 sec/step)\n","I0830 17:54:36.460676 139622356395904 learning.py:507] global step 5534: loss = 2.5833 (1.201 sec/step)\n","I0830 17:54:37.678235 139622356395904 learning.py:507] global step 5535: loss = 2.7370 (1.216 sec/step)\n","I0830 17:54:38.913904 139622356395904 learning.py:507] global step 5536: loss = 3.1434 (1.234 sec/step)\n","I0830 17:54:40.108030 139622356395904 learning.py:507] global step 5537: loss = 3.4475 (1.192 sec/step)\n","I0830 17:54:41.325696 139622356395904 learning.py:507] global step 5538: loss = 3.1415 (1.216 sec/step)\n","I0830 17:54:42.560849 139622356395904 learning.py:507] global step 5539: loss = 2.6295 (1.233 sec/step)\n","I0830 17:54:43.796649 139622356395904 learning.py:507] global step 5540: loss = 2.5895 (1.234 sec/step)\n","I0830 17:54:45.031231 139622356395904 learning.py:507] global step 5541: loss = 4.1653 (1.233 sec/step)\n","I0830 17:54:46.252165 139622356395904 learning.py:507] global step 5542: loss = 2.6735 (1.219 sec/step)\n","I0830 17:54:47.469419 139622356395904 learning.py:507] global step 5543: loss = 2.7501 (1.215 sec/step)\n","I0830 17:54:48.669093 139622356395904 learning.py:507] global step 5544: loss = 3.4072 (1.198 sec/step)\n","I0830 17:54:49.877735 139622356395904 learning.py:507] global step 5545: loss = 2.3763 (1.207 sec/step)\n","I0830 17:54:51.094630 139622356395904 learning.py:507] global step 5546: loss = 3.2819 (1.215 sec/step)\n","I0830 17:54:52.304356 139622356395904 learning.py:507] global step 5547: loss = 3.0251 (1.208 sec/step)\n","I0830 17:54:53.563960 139622356395904 learning.py:507] global step 5548: loss = 2.9970 (1.258 sec/step)\n","I0830 17:54:54.744987 139622356395904 learning.py:507] global step 5549: loss = 2.9601 (1.179 sec/step)\n","I0830 17:54:55.947204 139622356395904 learning.py:507] global step 5550: loss = 3.1337 (1.201 sec/step)\n","I0830 17:54:57.166636 139622356395904 learning.py:507] global step 5551: loss = 2.6020 (1.218 sec/step)\n","I0830 17:54:58.397215 139622356395904 learning.py:507] global step 5552: loss = 2.9538 (1.229 sec/step)\n","I0830 17:54:59.637377 139622356395904 learning.py:507] global step 5553: loss = 4.7799 (1.238 sec/step)\n","I0830 17:55:00.853218 139622356395904 learning.py:507] global step 5554: loss = 2.9858 (1.214 sec/step)\n","I0830 17:55:02.073225 139622356395904 learning.py:507] global step 5555: loss = 3.4521 (1.218 sec/step)\n","I0830 17:55:03.267302 139622356395904 learning.py:507] global step 5556: loss = 4.4057 (1.192 sec/step)\n","I0830 17:55:04.501013 139622356395904 learning.py:507] global step 5557: loss = 2.5414 (1.232 sec/step)\n","I0830 17:55:05.688687 139622356395904 learning.py:507] global step 5558: loss = 3.1935 (1.186 sec/step)\n","I0830 17:55:06.908248 139622356395904 learning.py:507] global step 5559: loss = 3.2059 (1.218 sec/step)\n","I0830 17:55:08.140186 139622356395904 learning.py:507] global step 5560: loss = 3.1421 (1.230 sec/step)\n","I0830 17:55:09.363427 139622356395904 learning.py:507] global step 5561: loss = 3.1449 (1.221 sec/step)\n","I0830 17:55:10.552511 139622356395904 learning.py:507] global step 5562: loss = 3.0199 (1.187 sec/step)\n","I0830 17:55:11.815509 139622356395904 learning.py:507] global step 5563: loss = 2.7013 (1.261 sec/step)\n","I0830 17:55:13.006939 139622356395904 learning.py:507] global step 5564: loss = 3.5790 (1.189 sec/step)\n","I0830 17:55:14.241779 139622356395904 learning.py:507] global step 5565: loss = 3.2226 (1.233 sec/step)\n","I0830 17:55:15.441494 139622356395904 learning.py:507] global step 5566: loss = 3.6036 (1.198 sec/step)\n","I0830 17:55:16.665465 139622356395904 learning.py:507] global step 5567: loss = 3.1812 (1.222 sec/step)\n","I0830 17:55:17.868303 139622356395904 learning.py:507] global step 5568: loss = 3.3771 (1.201 sec/step)\n","I0830 17:55:19.077006 139622356395904 learning.py:507] global step 5569: loss = 3.2722 (1.207 sec/step)\n","I0830 17:55:20.307535 139622356395904 learning.py:507] global step 5570: loss = 3.1849 (1.229 sec/step)\n","I0830 17:55:21.511660 139622356395904 learning.py:507] global step 5571: loss = 2.8602 (1.202 sec/step)\n","I0830 17:55:22.719032 139622356395904 learning.py:507] global step 5572: loss = 3.3937 (1.206 sec/step)\n","I0830 17:55:23.967995 139622356395904 learning.py:507] global step 5573: loss = 3.4378 (1.247 sec/step)\n","I0830 17:55:25.199111 139622356395904 learning.py:507] global step 5574: loss = 3.3820 (1.229 sec/step)\n","I0830 17:55:26.404092 139622356395904 learning.py:507] global step 5575: loss = 2.9771 (1.203 sec/step)\n","I0830 17:55:27.614468 139622356395904 learning.py:507] global step 5576: loss = 3.0692 (1.208 sec/step)\n","I0830 17:55:28.835425 139622356395904 learning.py:507] global step 5577: loss = 3.2949 (1.219 sec/step)\n","I0830 17:55:30.087678 139622356395904 learning.py:507] global step 5578: loss = 2.8914 (1.250 sec/step)\n","I0830 17:55:31.318005 139622356395904 learning.py:507] global step 5579: loss = 2.8189 (1.229 sec/step)\n","I0830 17:55:32.547474 139622356395904 learning.py:507] global step 5580: loss = 3.1634 (1.228 sec/step)\n","I0830 17:55:33.797432 139622356395904 learning.py:507] global step 5581: loss = 2.2688 (1.248 sec/step)\n","I0830 17:55:35.030729 139622356395904 learning.py:507] global step 5582: loss = 2.5620 (1.231 sec/step)\n","I0830 17:55:36.259104 139622356395904 learning.py:507] global step 5583: loss = 2.9829 (1.226 sec/step)\n","I0830 17:55:37.497602 139622356395904 learning.py:507] global step 5584: loss = 2.9772 (1.237 sec/step)\n","I0830 17:55:38.714396 139622356395904 learning.py:507] global step 5585: loss = 3.0296 (1.215 sec/step)\n","I0830 17:55:39.903647 139622356395904 learning.py:507] global step 5586: loss = 2.6546 (1.187 sec/step)\n","I0830 17:55:41.143506 139622356395904 learning.py:507] global step 5587: loss = 3.0613 (1.238 sec/step)\n","I0830 17:55:42.386408 139622356395904 learning.py:507] global step 5588: loss = 3.2323 (1.240 sec/step)\n","I0830 17:55:43.611496 139622356395904 learning.py:507] global step 5589: loss = 3.2905 (1.223 sec/step)\n","I0830 17:55:44.836551 139622356395904 learning.py:507] global step 5590: loss = 3.1824 (1.223 sec/step)\n","I0830 17:55:46.057608 139622356395904 learning.py:507] global step 5591: loss = 2.7369 (1.219 sec/step)\n","I0830 17:55:47.271478 139622356395904 learning.py:507] global step 5592: loss = 4.2535 (1.212 sec/step)\n","I0830 17:55:48.496388 139622356395904 learning.py:507] global step 5593: loss = 4.2904 (1.223 sec/step)\n","I0830 17:55:49.739034 139622356395904 learning.py:507] global step 5594: loss = 3.6193 (1.241 sec/step)\n","I0830 17:55:50.951212 139622356395904 learning.py:507] global step 5595: loss = 2.7987 (1.210 sec/step)\n","I0830 17:55:52.147209 139622356395904 learning.py:507] global step 5596: loss = 2.8460 (1.194 sec/step)\n","I0830 17:55:53.353405 139622356395904 learning.py:507] global step 5597: loss = 2.8382 (1.204 sec/step)\n","I0830 17:55:54.564413 139622356395904 learning.py:507] global step 5598: loss = 4.0975 (1.209 sec/step)\n","I0830 17:55:55.828476 139622356395904 learning.py:507] global step 5599: loss = 2.8664 (1.262 sec/step)\n","I0830 17:55:57.076511 139622356395904 learning.py:507] global step 5600: loss = 3.8167 (1.246 sec/step)\n","I0830 17:55:58.298950 139622356395904 learning.py:507] global step 5601: loss = 4.5005 (1.219 sec/step)\n","I0830 17:55:59.496638 139622356395904 learning.py:507] global step 5602: loss = 3.7317 (1.195 sec/step)\n","I0830 17:56:01.818092 139622356395904 learning.py:507] global step 5603: loss = 4.1845 (1.842 sec/step)\n","I0830 17:56:01.869026 139619299985152 supervisor.py:1050] Recording summary at step 5603.\n","I0830 17:56:03.050781 139622356395904 learning.py:507] global step 5604: loss = 5.4598 (1.231 sec/step)\n","I0830 17:56:04.272949 139622356395904 learning.py:507] global step 5605: loss = 2.6892 (1.220 sec/step)\n","I0830 17:56:05.520689 139622356395904 learning.py:507] global step 5606: loss = 2.5760 (1.246 sec/step)\n","I0830 17:56:06.767750 139622356395904 learning.py:507] global step 5607: loss = 2.9557 (1.245 sec/step)\n","I0830 17:56:07.991993 139622356395904 learning.py:507] global step 5608: loss = 2.8483 (1.222 sec/step)\n","I0830 17:56:09.207191 139622356395904 learning.py:507] global step 5609: loss = 2.3429 (1.213 sec/step)\n","I0830 17:56:10.462397 139622356395904 learning.py:507] global step 5610: loss = 2.4054 (1.253 sec/step)\n","I0830 17:56:11.716747 139622356395904 learning.py:507] global step 5611: loss = 3.1597 (1.252 sec/step)\n","I0830 17:56:12.929848 139622356395904 learning.py:507] global step 5612: loss = 2.3366 (1.211 sec/step)\n","I0830 17:56:14.141651 139622356395904 learning.py:507] global step 5613: loss = 3.6222 (1.210 sec/step)\n","I0830 17:56:15.347091 139622356395904 learning.py:507] global step 5614: loss = 3.2861 (1.203 sec/step)\n","I0830 17:56:16.562920 139622356395904 learning.py:507] global step 5615: loss = 2.9693 (1.214 sec/step)\n","I0830 17:56:17.774066 139622356395904 learning.py:507] global step 5616: loss = 3.8114 (1.209 sec/step)\n","I0830 17:56:18.975442 139622356395904 learning.py:507] global step 5617: loss = 2.6250 (1.199 sec/step)\n","I0830 17:56:20.187708 139622356395904 learning.py:507] global step 5618: loss = 3.5271 (1.210 sec/step)\n","I0830 17:56:21.407218 139622356395904 learning.py:507] global step 5619: loss = 4.3148 (1.218 sec/step)\n","I0830 17:56:22.602225 139622356395904 learning.py:507] global step 5620: loss = 2.6170 (1.193 sec/step)\n","I0830 17:56:23.836256 139622356395904 learning.py:507] global step 5621: loss = 2.9882 (1.232 sec/step)\n","I0830 17:56:25.028507 139622356395904 learning.py:507] global step 5622: loss = 4.4560 (1.190 sec/step)\n","I0830 17:56:26.233071 139622356395904 learning.py:507] global step 5623: loss = 3.4161 (1.203 sec/step)\n","I0830 17:56:27.422553 139622356395904 learning.py:507] global step 5624: loss = 2.7437 (1.188 sec/step)\n","I0830 17:56:28.614663 139622356395904 learning.py:507] global step 5625: loss = 2.8051 (1.190 sec/step)\n","I0830 17:56:29.847604 139622356395904 learning.py:507] global step 5626: loss = 4.0108 (1.231 sec/step)\n","I0830 17:56:31.095239 139622356395904 learning.py:507] global step 5627: loss = 2.6962 (1.246 sec/step)\n","I0830 17:56:32.308725 139622356395904 learning.py:507] global step 5628: loss = 2.2910 (1.212 sec/step)\n","I0830 17:56:33.516357 139622356395904 learning.py:507] global step 5629: loss = 3.6066 (1.206 sec/step)\n","I0830 17:56:34.708683 139622356395904 learning.py:507] global step 5630: loss = 3.5194 (1.190 sec/step)\n","I0830 17:56:35.903278 139622356395904 learning.py:507] global step 5631: loss = 2.7140 (1.193 sec/step)\n","I0830 17:56:37.142835 139622356395904 learning.py:507] global step 5632: loss = 3.9348 (1.238 sec/step)\n","I0830 17:56:38.364001 139622356395904 learning.py:507] global step 5633: loss = 3.1991 (1.219 sec/step)\n","I0830 17:56:39.577718 139622356395904 learning.py:507] global step 5634: loss = 3.6344 (1.212 sec/step)\n","I0830 17:56:40.768772 139622356395904 learning.py:507] global step 5635: loss = 2.6274 (1.189 sec/step)\n","I0830 17:56:41.969354 139622356395904 learning.py:507] global step 5636: loss = 2.6843 (1.199 sec/step)\n","I0830 17:56:43.214511 139622356395904 learning.py:507] global step 5637: loss = 2.9399 (1.244 sec/step)\n","I0830 17:56:44.444219 139622356395904 learning.py:507] global step 5638: loss = 3.8698 (1.228 sec/step)\n","I0830 17:56:45.657287 139622356395904 learning.py:507] global step 5639: loss = 2.9023 (1.211 sec/step)\n","I0830 17:56:46.907092 139622356395904 learning.py:507] global step 5640: loss = 2.8445 (1.248 sec/step)\n","I0830 17:56:48.126948 139622356395904 learning.py:507] global step 5641: loss = 2.5885 (1.218 sec/step)\n","I0830 17:56:49.350393 139622356395904 learning.py:507] global step 5642: loss = 2.6817 (1.222 sec/step)\n","I0830 17:56:50.590116 139622356395904 learning.py:507] global step 5643: loss = 2.6091 (1.238 sec/step)\n","I0830 17:56:51.838217 139622356395904 learning.py:507] global step 5644: loss = 2.8580 (1.246 sec/step)\n","I0830 17:56:53.044989 139622356395904 learning.py:507] global step 5645: loss = 3.9496 (1.205 sec/step)\n","I0830 17:56:54.282885 139622356395904 learning.py:507] global step 5646: loss = 2.5817 (1.236 sec/step)\n","I0830 17:56:55.502426 139622356395904 learning.py:507] global step 5647: loss = 4.5271 (1.218 sec/step)\n","I0830 17:56:56.716671 139622356395904 learning.py:507] global step 5648: loss = 3.5858 (1.213 sec/step)\n","I0830 17:56:57.954954 139622356395904 learning.py:507] global step 5649: loss = 4.0921 (1.237 sec/step)\n","I0830 17:56:59.170106 139622356395904 learning.py:507] global step 5650: loss = 3.5382 (1.213 sec/step)\n","I0830 17:57:00.386972 139622356395904 learning.py:507] global step 5651: loss = 2.3964 (1.215 sec/step)\n","I0830 17:57:01.635488 139622356395904 learning.py:507] global step 5652: loss = 3.1438 (1.247 sec/step)\n","I0830 17:57:02.901196 139622356395904 learning.py:507] global step 5653: loss = 2.9175 (1.264 sec/step)\n","I0830 17:57:04.148809 139622356395904 learning.py:507] global step 5654: loss = 2.9038 (1.246 sec/step)\n","I0830 17:57:05.335997 139622356395904 learning.py:507] global step 5655: loss = 2.8833 (1.185 sec/step)\n","I0830 17:57:06.551961 139622356395904 learning.py:507] global step 5656: loss = 2.8025 (1.214 sec/step)\n","I0830 17:57:07.758685 139622356395904 learning.py:507] global step 5657: loss = 3.1910 (1.205 sec/step)\n","I0830 17:57:08.971557 139622356395904 learning.py:507] global step 5658: loss = 2.8030 (1.211 sec/step)\n","I0830 17:57:10.209317 139622356395904 learning.py:507] global step 5659: loss = 4.2406 (1.236 sec/step)\n","I0830 17:57:11.424113 139622356395904 learning.py:507] global step 5660: loss = 3.4304 (1.213 sec/step)\n","I0830 17:57:12.642532 139622356395904 learning.py:507] global step 5661: loss = 2.8044 (1.217 sec/step)\n","I0830 17:57:13.852758 139622356395904 learning.py:507] global step 5662: loss = 3.4974 (1.208 sec/step)\n","I0830 17:57:15.034324 139622356395904 learning.py:507] global step 5663: loss = 2.8916 (1.180 sec/step)\n","I0830 17:57:16.287410 139622356395904 learning.py:507] global step 5664: loss = 3.4515 (1.251 sec/step)\n","I0830 17:57:17.503404 139622356395904 learning.py:507] global step 5665: loss = 2.8580 (1.214 sec/step)\n","I0830 17:57:18.737301 139622356395904 learning.py:507] global step 5666: loss = 2.4230 (1.232 sec/step)\n","I0830 17:57:19.929647 139622356395904 learning.py:507] global step 5667: loss = 3.0162 (1.190 sec/step)\n","I0830 17:57:21.150113 139622356395904 learning.py:507] global step 5668: loss = 3.2801 (1.219 sec/step)\n","I0830 17:57:22.383167 139622356395904 learning.py:507] global step 5669: loss = 2.8318 (1.231 sec/step)\n","I0830 17:57:23.596460 139622356395904 learning.py:507] global step 5670: loss = 3.0398 (1.211 sec/step)\n","I0830 17:57:24.839622 139622356395904 learning.py:507] global step 5671: loss = 3.2394 (1.239 sec/step)\n","I0830 17:57:26.069128 139622356395904 learning.py:507] global step 5672: loss = 2.9130 (1.227 sec/step)\n","I0830 17:57:27.290416 139622356395904 learning.py:507] global step 5673: loss = 3.6747 (1.219 sec/step)\n","I0830 17:57:28.541852 139622356395904 learning.py:507] global step 5674: loss = 2.5851 (1.249 sec/step)\n","I0830 17:57:29.760365 139622356395904 learning.py:507] global step 5675: loss = 2.4917 (1.217 sec/step)\n","I0830 17:57:30.954842 139622356395904 learning.py:507] global step 5676: loss = 3.0394 (1.192 sec/step)\n","I0830 17:57:32.148374 139622356395904 learning.py:507] global step 5677: loss = 3.4487 (1.192 sec/step)\n","I0830 17:57:33.385539 139622356395904 learning.py:507] global step 5678: loss = 3.3574 (1.235 sec/step)\n","I0830 17:57:34.608758 139622356395904 learning.py:507] global step 5679: loss = 2.6045 (1.221 sec/step)\n","I0830 17:57:35.847293 139622356395904 learning.py:507] global step 5680: loss = 3.0596 (1.237 sec/step)\n","I0830 17:57:37.082667 139622356395904 learning.py:507] global step 5681: loss = 2.8450 (1.233 sec/step)\n","I0830 17:57:38.297321 139622356395904 learning.py:507] global step 5682: loss = 3.7211 (1.213 sec/step)\n","I0830 17:57:39.527481 139622356395904 learning.py:507] global step 5683: loss = 3.9535 (1.228 sec/step)\n","I0830 17:57:40.765854 139622356395904 learning.py:507] global step 5684: loss = 3.7699 (1.236 sec/step)\n","I0830 17:57:41.985882 139622356395904 learning.py:507] global step 5685: loss = 2.9686 (1.218 sec/step)\n","I0830 17:57:43.235472 139622356395904 learning.py:507] global step 5686: loss = 3.8302 (1.248 sec/step)\n","I0830 17:57:44.465534 139622356395904 learning.py:507] global step 5687: loss = 3.3882 (1.228 sec/step)\n","I0830 17:57:45.671930 139622356395904 learning.py:507] global step 5688: loss = 3.4972 (1.204 sec/step)\n","I0830 17:57:46.881689 139622356395904 learning.py:507] global step 5689: loss = 3.7323 (1.208 sec/step)\n","I0830 17:57:48.125115 139622356395904 learning.py:507] global step 5690: loss = 3.1325 (1.241 sec/step)\n","I0830 17:57:49.368333 139622356395904 learning.py:507] global step 5691: loss = 2.2425 (1.241 sec/step)\n","I0830 17:57:50.642312 139622356395904 learning.py:507] global step 5692: loss = 4.2464 (1.272 sec/step)\n","I0830 17:57:51.874734 139622356395904 learning.py:507] global step 5693: loss = 2.5982 (1.231 sec/step)\n","I0830 17:57:53.055968 139622356395904 learning.py:507] global step 5694: loss = 3.5415 (1.180 sec/step)\n","I0830 17:57:54.271320 139622356395904 learning.py:507] global step 5695: loss = 3.0542 (1.214 sec/step)\n","I0830 17:57:55.489402 139622356395904 learning.py:507] global step 5696: loss = 3.2210 (1.216 sec/step)\n","I0830 17:57:56.693202 139622356395904 learning.py:507] global step 5697: loss = 2.7418 (1.202 sec/step)\n","I0830 17:57:57.907039 139622356395904 learning.py:507] global step 5698: loss = 3.0229 (1.212 sec/step)\n","I0830 17:57:59.110922 139622356395904 learning.py:507] global step 5699: loss = 3.1845 (1.202 sec/step)\n","I0830 17:58:00.529835 139622356395904 learning.py:507] global step 5700: loss = 3.9484 (1.379 sec/step)\n","I0830 17:58:02.298903 139619299985152 supervisor.py:1050] Recording summary at step 5700.\n","I0830 17:58:02.345150 139622356395904 learning.py:507] global step 5701: loss = 3.8563 (1.812 sec/step)\n","I0830 17:58:03.575005 139622356395904 learning.py:507] global step 5702: loss = 3.2308 (1.228 sec/step)\n","I0830 17:58:04.819664 139622356395904 learning.py:507] global step 5703: loss = 2.3913 (1.243 sec/step)\n","I0830 17:58:06.040989 139622356395904 learning.py:507] global step 5704: loss = 3.1411 (1.220 sec/step)\n","I0830 17:58:07.293151 139622356395904 learning.py:507] global step 5705: loss = 2.7038 (1.250 sec/step)\n","I0830 17:58:08.530453 139622356395904 learning.py:507] global step 5706: loss = 3.0415 (1.235 sec/step)\n","I0830 17:58:09.731682 139622356395904 learning.py:507] global step 5707: loss = 3.0341 (1.199 sec/step)\n","I0830 17:58:10.969106 139622356395904 learning.py:507] global step 5708: loss = 3.1762 (1.235 sec/step)\n","I0830 17:58:12.190517 139622356395904 learning.py:507] global step 5709: loss = 3.8155 (1.219 sec/step)\n","I0830 17:58:13.430543 139622356395904 learning.py:507] global step 5710: loss = 3.0367 (1.238 sec/step)\n","I0830 17:58:14.677806 139622356395904 learning.py:507] global step 5711: loss = 2.6990 (1.245 sec/step)\n","I0830 17:58:15.910844 139622356395904 learning.py:507] global step 5712: loss = 3.1236 (1.231 sec/step)\n","I0830 17:58:17.148978 139622356395904 learning.py:507] global step 5713: loss = 2.2736 (1.236 sec/step)\n","I0830 17:58:18.361916 139622356395904 learning.py:507] global step 5714: loss = 2.7134 (1.211 sec/step)\n","I0830 17:58:19.589256 139622356395904 learning.py:507] global step 5715: loss = 3.1991 (1.225 sec/step)\n","I0830 17:58:20.779955 139622356395904 learning.py:507] global step 5716: loss = 3.6814 (1.189 sec/step)\n","I0830 17:58:22.026519 139622356395904 learning.py:507] global step 5717: loss = 3.8008 (1.245 sec/step)\n","I0830 17:58:23.246438 139622356395904 learning.py:507] global step 5718: loss = 2.7383 (1.218 sec/step)\n","I0830 17:58:24.480319 139622356395904 learning.py:507] global step 5719: loss = 2.8428 (1.232 sec/step)\n","I0830 17:58:25.692324 139622356395904 learning.py:507] global step 5720: loss = 3.4401 (1.210 sec/step)\n","I0830 17:58:26.928514 139622356395904 learning.py:507] global step 5721: loss = 3.4600 (1.234 sec/step)\n","I0830 17:58:28.108166 139622356395904 learning.py:507] global step 5722: loss = 3.6003 (1.178 sec/step)\n","I0830 17:58:29.332022 139622356395904 learning.py:507] global step 5723: loss = 3.6865 (1.221 sec/step)\n","I0830 17:58:30.553510 139622356395904 learning.py:507] global step 5724: loss = 3.8393 (1.219 sec/step)\n","I0830 17:58:31.785603 139622356395904 learning.py:507] global step 5725: loss = 3.6172 (1.230 sec/step)\n","I0830 17:58:33.010147 139622356395904 learning.py:507] global step 5726: loss = 3.1045 (1.222 sec/step)\n","I0830 17:58:34.233252 139622356395904 learning.py:507] global step 5727: loss = 2.4245 (1.221 sec/step)\n","I0830 17:58:35.462765 139622356395904 learning.py:507] global step 5728: loss = 3.0051 (1.228 sec/step)\n","I0830 17:58:36.695842 139622356395904 learning.py:507] global step 5729: loss = 3.0044 (1.231 sec/step)\n","I0830 17:58:37.917102 139622356395904 learning.py:507] global step 5730: loss = 4.0957 (1.219 sec/step)\n","I0830 17:58:39.122849 139622356395904 learning.py:507] global step 5731: loss = 2.9264 (1.204 sec/step)\n","I0830 17:58:40.340427 139622356395904 learning.py:507] global step 5732: loss = 2.6863 (1.216 sec/step)\n","I0830 17:58:41.554471 139622356395904 learning.py:507] global step 5733: loss = 2.7399 (1.212 sec/step)\n","I0830 17:58:42.750176 139622356395904 learning.py:507] global step 5734: loss = 3.6216 (1.194 sec/step)\n","I0830 17:58:43.981792 139622356395904 learning.py:507] global step 5735: loss = 2.4649 (1.230 sec/step)\n","I0830 17:58:45.243231 139622356395904 learning.py:507] global step 5736: loss = 2.7341 (1.259 sec/step)\n","I0830 17:58:46.470904 139622356395904 learning.py:507] global step 5737: loss = 4.6927 (1.226 sec/step)\n","I0830 17:58:47.662629 139622356395904 learning.py:507] global step 5738: loss = 3.3306 (1.190 sec/step)\n","I0830 17:58:48.876186 139622356395904 learning.py:507] global step 5739: loss = 3.2410 (1.212 sec/step)\n","I0830 17:58:50.122804 139622356395904 learning.py:507] global step 5740: loss = 2.6183 (1.245 sec/step)\n","I0830 17:58:51.344969 139622356395904 learning.py:507] global step 5741: loss = 2.6603 (1.220 sec/step)\n","I0830 17:58:52.571303 139622356395904 learning.py:507] global step 5742: loss = 3.1193 (1.224 sec/step)\n","I0830 17:58:53.816371 139622356395904 learning.py:507] global step 5743: loss = 3.2082 (1.243 sec/step)\n","I0830 17:58:55.042120 139622356395904 learning.py:507] global step 5744: loss = 2.7286 (1.224 sec/step)\n","I0830 17:58:56.273807 139622356395904 learning.py:507] global step 5745: loss = 2.6578 (1.230 sec/step)\n","I0830 17:58:57.499642 139622356395904 learning.py:507] global step 5746: loss = 3.0572 (1.224 sec/step)\n","I0830 17:58:58.744436 139622356395904 learning.py:507] global step 5747: loss = 2.4910 (1.243 sec/step)\n","I0830 17:58:59.946597 139622356395904 learning.py:507] global step 5748: loss = 4.0457 (1.200 sec/step)\n","I0830 17:59:01.179457 139622356395904 learning.py:507] global step 5749: loss = 3.5485 (1.231 sec/step)\n","I0830 17:59:02.396129 139622356395904 learning.py:507] global step 5750: loss = 4.1941 (1.215 sec/step)\n","I0830 17:59:03.633495 139622356395904 learning.py:507] global step 5751: loss = 3.4587 (1.235 sec/step)\n","I0830 17:59:04.865597 139622356395904 learning.py:507] global step 5752: loss = 3.1176 (1.230 sec/step)\n","I0830 17:59:06.085107 139622356395904 learning.py:507] global step 5753: loss = 3.1156 (1.217 sec/step)\n","I0830 17:59:07.312700 139622356395904 learning.py:507] global step 5754: loss = 2.7053 (1.225 sec/step)\n","I0830 17:59:08.544371 139622356395904 learning.py:507] global step 5755: loss = 3.6314 (1.230 sec/step)\n","I0830 17:59:09.766307 139622356395904 learning.py:507] global step 5756: loss = 3.1679 (1.220 sec/step)\n","I0830 17:59:10.975978 139622356395904 learning.py:507] global step 5757: loss = 2.8302 (1.208 sec/step)\n","I0830 17:59:12.196597 139622356395904 learning.py:507] global step 5758: loss = 2.8899 (1.219 sec/step)\n","I0830 17:59:13.397261 139622356395904 learning.py:507] global step 5759: loss = 3.1787 (1.199 sec/step)\n","I0830 17:59:14.653336 139622356395904 learning.py:507] global step 5760: loss = 3.7425 (1.254 sec/step)\n","I0830 17:59:15.871949 139622356395904 learning.py:507] global step 5761: loss = 3.1249 (1.217 sec/step)\n","I0830 17:59:17.061705 139622356395904 learning.py:507] global step 5762: loss = 2.3230 (1.188 sec/step)\n","I0830 17:59:18.265804 139622356395904 learning.py:507] global step 5763: loss = 2.4830 (1.202 sec/step)\n","I0830 17:59:19.491348 139622356395904 learning.py:507] global step 5764: loss = 4.0623 (1.224 sec/step)\n","I0830 17:59:20.718076 139622356395904 learning.py:507] global step 5765: loss = 2.4631 (1.225 sec/step)\n","I0830 17:59:21.964180 139622356395904 learning.py:507] global step 5766: loss = 2.6828 (1.244 sec/step)\n","I0830 17:59:23.180504 139622356395904 learning.py:507] global step 5767: loss = 2.9129 (1.214 sec/step)\n","I0830 17:59:24.432461 139622356395904 learning.py:507] global step 5768: loss = 2.8272 (1.250 sec/step)\n","I0830 17:59:25.636518 139622356395904 learning.py:507] global step 5769: loss = 2.5829 (1.202 sec/step)\n","I0830 17:59:26.876851 139622356395904 learning.py:507] global step 5770: loss = 3.4985 (1.238 sec/step)\n","I0830 17:59:28.108216 139622356395904 learning.py:507] global step 5771: loss = 2.5207 (1.229 sec/step)\n","I0830 17:59:29.352969 139622356395904 learning.py:507] global step 5772: loss = 3.4152 (1.243 sec/step)\n","I0830 17:59:30.545873 139622356395904 learning.py:507] global step 5773: loss = 3.0920 (1.191 sec/step)\n","I0830 17:59:31.766715 139622356395904 learning.py:507] global step 5774: loss = 2.8432 (1.219 sec/step)\n","I0830 17:59:33.001966 139622356395904 learning.py:507] global step 5775: loss = 4.0406 (1.233 sec/step)\n","I0830 17:59:34.244182 139622356395904 learning.py:507] global step 5776: loss = 2.7300 (1.240 sec/step)\n","I0830 17:59:35.483096 139622356395904 learning.py:507] global step 5777: loss = 2.7264 (1.237 sec/step)\n","I0830 17:59:36.699208 139622356395904 learning.py:507] global step 5778: loss = 2.6055 (1.213 sec/step)\n","I0830 17:59:37.927624 139622356395904 learning.py:507] global step 5779: loss = 3.4710 (1.226 sec/step)\n","I0830 17:59:39.132150 139622356395904 learning.py:507] global step 5780: loss = 2.9907 (1.202 sec/step)\n","I0830 17:59:40.342743 139622356395904 learning.py:507] global step 5781: loss = 2.9593 (1.208 sec/step)\n","I0830 17:59:41.563126 139622356395904 learning.py:507] global step 5782: loss = 3.3717 (1.218 sec/step)\n","I0830 17:59:42.845404 139622356395904 learning.py:507] global step 5783: loss = 3.3150 (1.280 sec/step)\n","I0830 17:59:44.095797 139622356395904 learning.py:507] global step 5784: loss = 3.1494 (1.248 sec/step)\n","I0830 17:59:45.273893 139622356395904 learning.py:507] global step 5785: loss = 3.2348 (1.176 sec/step)\n","I0830 17:59:46.514222 139622356395904 learning.py:507] global step 5786: loss = 3.1634 (1.239 sec/step)\n","I0830 17:59:47.750161 139622356395904 learning.py:507] global step 5787: loss = 3.0606 (1.234 sec/step)\n","I0830 17:59:49.015408 139622356395904 learning.py:507] global step 5788: loss = 2.7629 (1.263 sec/step)\n","I0830 17:59:50.213033 139622356395904 learning.py:507] global step 5789: loss = 2.5186 (1.196 sec/step)\n","I0830 17:59:51.431003 139622356395904 learning.py:507] global step 5790: loss = 3.4683 (1.216 sec/step)\n","I0830 17:59:52.669238 139622356395904 learning.py:507] global step 5791: loss = 3.8654 (1.236 sec/step)\n","I0830 17:59:53.900786 139622356395904 learning.py:507] global step 5792: loss = 2.3835 (1.230 sec/step)\n","I0830 17:59:55.141927 139622356395904 learning.py:507] global step 5793: loss = 2.6828 (1.239 sec/step)\n","I0830 17:59:56.391657 139622356395904 learning.py:507] global step 5794: loss = 2.8934 (1.248 sec/step)\n","I0830 17:59:57.596593 139622356395904 learning.py:507] global step 5795: loss = 3.1522 (1.203 sec/step)\n","I0830 17:59:58.833736 139622356395904 learning.py:507] global step 5796: loss = 2.5969 (1.235 sec/step)\n","I0830 17:59:59.645701 139619316770560 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n","I0830 18:00:00.647255 139622356395904 learning.py:507] global step 5797: loss = 3.5739 (1.795 sec/step)\n","I0830 18:00:02.356350 139619299985152 supervisor.py:1050] Recording summary at step 5797.\n","I0830 18:00:03.153685 139622356395904 learning.py:507] global step 5798: loss = 3.1837 (2.398 sec/step)\n","I0830 18:00:04.830252 139622356395904 learning.py:507] global step 5799: loss = 2.8529 (1.560 sec/step)\n","I0830 18:00:06.022925 139622356395904 learning.py:507] global step 5800: loss = 2.8370 (1.190 sec/step)\n","I0830 18:00:07.242730 139622356395904 learning.py:507] global step 5801: loss = 3.1569 (1.218 sec/step)\n","I0830 18:00:08.486156 139622356395904 learning.py:507] global step 5802: loss = 2.5240 (1.242 sec/step)\n","I0830 18:00:09.692789 139622356395904 learning.py:507] global step 5803: loss = 3.0499 (1.205 sec/step)\n","I0830 18:00:10.967022 139622356395904 learning.py:507] global step 5804: loss = 3.1301 (1.272 sec/step)\n","I0830 18:00:12.211251 139622356395904 learning.py:507] global step 5805: loss = 4.1319 (1.242 sec/step)\n","I0830 18:00:13.460584 139622356395904 learning.py:507] global step 5806: loss = 3.2969 (1.248 sec/step)\n","I0830 18:00:14.726539 139622356395904 learning.py:507] global step 5807: loss = 3.9206 (1.264 sec/step)\n","I0830 18:00:15.977849 139622356395904 learning.py:507] global step 5808: loss = 2.5658 (1.249 sec/step)\n","I0830 18:00:17.204259 139622356395904 learning.py:507] global step 5809: loss = 2.5297 (1.224 sec/step)\n","I0830 18:00:18.462342 139622356395904 learning.py:507] global step 5810: loss = 3.3435 (1.256 sec/step)\n","I0830 18:00:19.703476 139622356395904 learning.py:507] global step 5811: loss = 3.2195 (1.239 sec/step)\n","I0830 18:00:20.925420 139622356395904 learning.py:507] global step 5812: loss = 2.5662 (1.220 sec/step)\n","I0830 18:00:22.167131 139622356395904 learning.py:507] global step 5813: loss = 2.7867 (1.239 sec/step)\n","I0830 18:00:23.364299 139622356395904 learning.py:507] global step 5814: loss = 2.6518 (1.195 sec/step)\n","I0830 18:00:24.579305 139622356395904 learning.py:507] global step 5815: loss = 3.0131 (1.213 sec/step)\n","I0830 18:00:25.880559 139622356395904 learning.py:507] global step 5816: loss = 3.1910 (1.299 sec/step)\n","I0830 18:00:27.113256 139622356395904 learning.py:507] global step 5817: loss = 2.5947 (1.231 sec/step)\n","I0830 18:00:28.294583 139622356395904 learning.py:507] global step 5818: loss = 2.6445 (1.180 sec/step)\n","I0830 18:00:29.530508 139622356395904 learning.py:507] global step 5819: loss = 2.7611 (1.234 sec/step)\n","I0830 18:00:30.775242 139622356395904 learning.py:507] global step 5820: loss = 3.7824 (1.243 sec/step)\n","I0830 18:00:32.003108 139622356395904 learning.py:507] global step 5821: loss = 3.4706 (1.226 sec/step)\n","I0830 18:00:33.252330 139622356395904 learning.py:507] global step 5822: loss = 2.6052 (1.247 sec/step)\n","I0830 18:00:34.464454 139622356395904 learning.py:507] global step 5823: loss = 3.4154 (1.210 sec/step)\n","I0830 18:00:35.690736 139622356395904 learning.py:507] global step 5824: loss = 2.5999 (1.224 sec/step)\n","I0830 18:00:36.881574 139622356395904 learning.py:507] global step 5825: loss = 2.9410 (1.189 sec/step)\n","I0830 18:00:38.091391 139622356395904 learning.py:507] global step 5826: loss = 2.6466 (1.208 sec/step)\n","I0830 18:00:39.337132 139622356395904 learning.py:507] global step 5827: loss = 2.7075 (1.244 sec/step)\n","I0830 18:00:40.548532 139622356395904 learning.py:507] global step 5828: loss = 2.7788 (1.209 sec/step)\n","I0830 18:00:41.788485 139622356395904 learning.py:507] global step 5829: loss = 2.9690 (1.238 sec/step)\n","I0830 18:00:42.995849 139622356395904 learning.py:507] global step 5830: loss = 2.7932 (1.206 sec/step)\n","I0830 18:00:44.208662 139622356395904 learning.py:507] global step 5831: loss = 3.2245 (1.211 sec/step)\n","I0830 18:00:45.458629 139622356395904 learning.py:507] global step 5832: loss = 3.3082 (1.248 sec/step)\n","I0830 18:00:46.667511 139622356395904 learning.py:507] global step 5833: loss = 2.8313 (1.207 sec/step)\n","I0830 18:00:47.876515 139622356395904 learning.py:507] global step 5834: loss = 2.8967 (1.207 sec/step)\n","I0830 18:00:49.082446 139622356395904 learning.py:507] global step 5835: loss = 3.5136 (1.204 sec/step)\n","I0830 18:00:50.308927 139622356395904 learning.py:507] global step 5836: loss = 3.0180 (1.224 sec/step)\n","I0830 18:00:51.525943 139622356395904 learning.py:507] global step 5837: loss = 2.5498 (1.215 sec/step)\n","I0830 18:00:52.777188 139622356395904 learning.py:507] global step 5838: loss = 2.6718 (1.249 sec/step)\n","I0830 18:00:54.025636 139622356395904 learning.py:507] global step 5839: loss = 2.3521 (1.246 sec/step)\n","I0830 18:00:55.229773 139622356395904 learning.py:507] global step 5840: loss = 3.3070 (1.201 sec/step)\n","I0830 18:00:56.475202 139622356395904 learning.py:507] global step 5841: loss = 4.0111 (1.243 sec/step)\n","I0830 18:00:57.705469 139622356395904 learning.py:507] global step 5842: loss = 2.6219 (1.228 sec/step)\n","I0830 18:00:58.920528 139622356395904 learning.py:507] global step 5843: loss = 3.2931 (1.213 sec/step)\n","I0830 18:01:00.155422 139622356395904 learning.py:507] global step 5844: loss = 3.2769 (1.233 sec/step)\n","I0830 18:01:01.395206 139622356395904 learning.py:507] global step 5845: loss = 3.3164 (1.238 sec/step)\n","I0830 18:01:02.570691 139622356395904 learning.py:507] global step 5846: loss = 2.4970 (1.174 sec/step)\n","I0830 18:01:03.808587 139622356395904 learning.py:507] global step 5847: loss = 4.0435 (1.236 sec/step)\n","I0830 18:01:05.071352 139622356395904 learning.py:507] global step 5848: loss = 2.9391 (1.261 sec/step)\n","I0830 18:01:06.322400 139622356395904 learning.py:507] global step 5849: loss = 2.8503 (1.249 sec/step)\n","I0830 18:01:07.542637 139622356395904 learning.py:507] global step 5850: loss = 3.8024 (1.218 sec/step)\n","I0830 18:01:08.764611 139622356395904 learning.py:507] global step 5851: loss = 2.9071 (1.220 sec/step)\n","I0830 18:01:10.023963 139622356395904 learning.py:507] global step 5852: loss = 2.6959 (1.258 sec/step)\n","I0830 18:01:11.246667 139622356395904 learning.py:507] global step 5853: loss = 2.8017 (1.221 sec/step)\n","I0830 18:01:12.448578 139622356395904 learning.py:507] global step 5854: loss = 3.3389 (1.200 sec/step)\n","I0830 18:01:13.689690 139622356395904 learning.py:507] global step 5855: loss = 3.0814 (1.239 sec/step)\n","I0830 18:01:14.895156 139622356395904 learning.py:507] global step 5856: loss = 2.9523 (1.204 sec/step)\n","I0830 18:01:16.137227 139622356395904 learning.py:507] global step 5857: loss = 3.2086 (1.240 sec/step)\n","I0830 18:01:17.364830 139622356395904 learning.py:507] global step 5858: loss = 2.9285 (1.226 sec/step)\n","I0830 18:01:18.604593 139622356395904 learning.py:507] global step 5859: loss = 3.0153 (1.238 sec/step)\n","I0830 18:01:19.815621 139622356395904 learning.py:507] global step 5860: loss = 2.5734 (1.209 sec/step)\n","I0830 18:01:21.041304 139622356395904 learning.py:507] global step 5861: loss = 2.5814 (1.224 sec/step)\n","I0830 18:01:22.266317 139622356395904 learning.py:507] global step 5862: loss = 4.1764 (1.223 sec/step)\n","I0830 18:01:23.510780 139622356395904 learning.py:507] global step 5863: loss = 3.2562 (1.243 sec/step)\n","I0830 18:01:24.760387 139622356395904 learning.py:507] global step 5864: loss = 3.1729 (1.248 sec/step)\n","I0830 18:01:25.994642 139622356395904 learning.py:507] global step 5865: loss = 3.3102 (1.230 sec/step)\n","I0830 18:01:27.182993 139622356395904 learning.py:507] global step 5866: loss = 2.8068 (1.186 sec/step)\n","I0830 18:01:28.354256 139622356395904 learning.py:507] global step 5867: loss = 3.7602 (1.170 sec/step)\n","I0830 18:01:29.547192 139622356395904 learning.py:507] global step 5868: loss = 3.0380 (1.191 sec/step)\n","I0830 18:01:30.767459 139622356395904 learning.py:507] global step 5869: loss = 4.2674 (1.218 sec/step)\n","I0830 18:01:31.987925 139622356395904 learning.py:507] global step 5870: loss = 2.6323 (1.218 sec/step)\n","I0830 18:01:33.229093 139622356395904 learning.py:507] global step 5871: loss = 2.7126 (1.239 sec/step)\n","I0830 18:01:34.466462 139622356395904 learning.py:507] global step 5872: loss = 3.0449 (1.235 sec/step)\n","I0830 18:01:35.711707 139622356395904 learning.py:507] global step 5873: loss = 3.7957 (1.243 sec/step)\n","I0830 18:01:36.933702 139622356395904 learning.py:507] global step 5874: loss = 2.7605 (1.220 sec/step)\n","I0830 18:01:38.148514 139622356395904 learning.py:507] global step 5875: loss = 3.3359 (1.213 sec/step)\n","I0830 18:01:39.367170 139622356395904 learning.py:507] global step 5876: loss = 2.6233 (1.217 sec/step)\n","I0830 18:01:40.615162 139622356395904 learning.py:507] global step 5877: loss = 3.3406 (1.246 sec/step)\n","I0830 18:01:41.819129 139622356395904 learning.py:507] global step 5878: loss = 3.3293 (1.202 sec/step)\n","I0830 18:01:43.058424 139622356395904 learning.py:507] global step 5879: loss = 2.7271 (1.238 sec/step)\n","I0830 18:01:44.270891 139622356395904 learning.py:507] global step 5880: loss = 3.3041 (1.210 sec/step)\n","I0830 18:01:45.504693 139622356395904 learning.py:507] global step 5881: loss = 4.7140 (1.232 sec/step)\n","I0830 18:01:46.762171 139622356395904 learning.py:507] global step 5882: loss = 2.9411 (1.256 sec/step)\n","I0830 18:01:47.980169 139622356395904 learning.py:507] global step 5883: loss = 2.6781 (1.216 sec/step)\n","I0830 18:01:49.226652 139622356395904 learning.py:507] global step 5884: loss = 2.1427 (1.241 sec/step)\n","I0830 18:01:50.463152 139622356395904 learning.py:507] global step 5885: loss = 3.2730 (1.235 sec/step)\n","I0830 18:01:51.679848 139622356395904 learning.py:507] global step 5886: loss = 2.7294 (1.215 sec/step)\n","I0830 18:01:52.850924 139622356395904 learning.py:507] global step 5887: loss = 4.2994 (1.169 sec/step)\n","I0830 18:01:54.065777 139622356395904 learning.py:507] global step 5888: loss = 2.9220 (1.213 sec/step)\n","I0830 18:01:55.318433 139622356395904 learning.py:507] global step 5889: loss = 2.3087 (1.251 sec/step)\n","I0830 18:01:56.534281 139622356395904 learning.py:507] global step 5890: loss = 2.9645 (1.214 sec/step)\n","I0830 18:01:57.783426 139622356395904 learning.py:507] global step 5891: loss = 2.9159 (1.245 sec/step)\n","I0830 18:01:59.041286 139622356395904 learning.py:507] global step 5892: loss = 4.2944 (1.256 sec/step)\n","I0830 18:02:00.467312 139622356395904 learning.py:507] global step 5893: loss = 3.4060 (1.359 sec/step)\n","I0830 18:02:02.175328 139622356395904 learning.py:507] global step 5894: loss = 3.3351 (1.588 sec/step)\n","I0830 18:02:02.944730 139619299985152 supervisor.py:1050] Recording summary at step 5894.\n","I0830 18:02:03.479786 139622356395904 learning.py:507] global step 5895: loss = 3.9647 (1.302 sec/step)\n","I0830 18:02:04.706110 139622356395904 learning.py:507] global step 5896: loss = 3.2948 (1.224 sec/step)\n","I0830 18:02:05.923089 139622356395904 learning.py:507] global step 5897: loss = 2.8872 (1.215 sec/step)\n","I0830 18:02:07.146470 139622356395904 learning.py:507] global step 5898: loss = 4.9587 (1.221 sec/step)\n","I0830 18:02:08.378733 139622356395904 learning.py:507] global step 5899: loss = 2.5459 (1.230 sec/step)\n","I0830 18:02:09.569386 139622356395904 learning.py:507] global step 5900: loss = 2.7162 (1.189 sec/step)\n","I0830 18:02:10.814746 139622356395904 learning.py:507] global step 5901: loss = 3.8786 (1.243 sec/step)\n","I0830 18:02:12.074660 139622356395904 learning.py:507] global step 5902: loss = 3.3164 (1.258 sec/step)\n","I0830 18:02:13.323288 139622356395904 learning.py:507] global step 5903: loss = 2.9334 (1.247 sec/step)\n","I0830 18:02:14.587672 139622356395904 learning.py:507] global step 5904: loss = 3.1756 (1.262 sec/step)\n","I0830 18:02:15.795272 139622356395904 learning.py:507] global step 5905: loss = 3.1280 (1.206 sec/step)\n","I0830 18:02:17.049828 139622356395904 learning.py:507] global step 5906: loss = 4.3438 (1.253 sec/step)\n","I0830 18:02:18.302456 139622356395904 learning.py:507] global step 5907: loss = 2.6738 (1.251 sec/step)\n","I0830 18:02:19.526643 139622356395904 learning.py:507] global step 5908: loss = 3.6346 (1.222 sec/step)\n","I0830 18:02:20.763415 139622356395904 learning.py:507] global step 5909: loss = 2.8680 (1.235 sec/step)\n","I0830 18:02:22.003281 139622356395904 learning.py:507] global step 5910: loss = 2.4806 (1.238 sec/step)\n","I0830 18:02:23.242484 139622356395904 learning.py:507] global step 5911: loss = 3.6392 (1.237 sec/step)\n","I0830 18:02:24.493619 139622356395904 learning.py:507] global step 5912: loss = 2.7543 (1.249 sec/step)\n","I0830 18:02:25.711814 139622356395904 learning.py:507] global step 5913: loss = 3.1797 (1.216 sec/step)\n","I0830 18:02:26.947202 139622356395904 learning.py:507] global step 5914: loss = 2.5660 (1.225 sec/step)\n","I0830 18:02:28.174223 139622356395904 learning.py:507] global step 5915: loss = 2.4144 (1.225 sec/step)\n","I0830 18:02:29.493327 139622356395904 learning.py:507] global step 5916: loss = 3.1269 (1.317 sec/step)\n","I0830 18:02:30.708224 139622356395904 learning.py:507] global step 5917: loss = 3.9549 (1.213 sec/step)\n","I0830 18:02:31.961677 139622356395904 learning.py:507] global step 5918: loss = 3.8222 (1.252 sec/step)\n","I0830 18:02:33.180096 139622356395904 learning.py:507] global step 5919: loss = 3.9873 (1.216 sec/step)\n","I0830 18:02:34.391141 139622356395904 learning.py:507] global step 5920: loss = 3.0797 (1.209 sec/step)\n","I0830 18:02:35.622603 139622356395904 learning.py:507] global step 5921: loss = 2.8965 (1.230 sec/step)\n","I0830 18:02:36.833350 139622356395904 learning.py:507] global step 5922: loss = 2.5825 (1.209 sec/step)\n","I0830 18:02:38.053113 139622356395904 learning.py:507] global step 5923: loss = 2.7458 (1.218 sec/step)\n","I0830 18:02:39.291168 139622356395904 learning.py:507] global step 5924: loss = 2.4415 (1.236 sec/step)\n","I0830 18:02:40.521467 139622356395904 learning.py:507] global step 5925: loss = 2.3045 (1.229 sec/step)\n","I0830 18:02:41.724947 139622356395904 learning.py:507] global step 5926: loss = 3.1652 (1.202 sec/step)\n","I0830 18:02:42.974231 139622356395904 learning.py:507] global step 5927: loss = 3.1958 (1.247 sec/step)\n","I0830 18:02:44.191991 139622356395904 learning.py:507] global step 5928: loss = 4.4702 (1.216 sec/step)\n","I0830 18:02:45.372946 139622356395904 learning.py:507] global step 5929: loss = 4.9706 (1.179 sec/step)\n","I0830 18:02:46.593206 139622356395904 learning.py:507] global step 5930: loss = 2.2923 (1.218 sec/step)\n","I0830 18:02:47.804921 139622356395904 learning.py:507] global step 5931: loss = 2.7136 (1.210 sec/step)\n","I0830 18:02:49.058788 139622356395904 learning.py:507] global step 5932: loss = 2.8134 (1.252 sec/step)\n","I0830 18:02:50.305941 139622356395904 learning.py:507] global step 5933: loss = 3.6517 (1.245 sec/step)\n","I0830 18:02:51.533786 139622356395904 learning.py:507] global step 5934: loss = 3.1955 (1.226 sec/step)\n","I0830 18:02:52.718523 139622356395904 learning.py:507] global step 5935: loss = 3.3268 (1.183 sec/step)\n","I0830 18:02:53.951873 139622356395904 learning.py:507] global step 5936: loss = 3.3706 (1.232 sec/step)\n","I0830 18:02:55.164963 139622356395904 learning.py:507] global step 5937: loss = 3.1833 (1.211 sec/step)\n","I0830 18:02:56.396863 139622356395904 learning.py:507] global step 5938: loss = 2.8539 (1.230 sec/step)\n","I0830 18:02:57.644147 139622356395904 learning.py:507] global step 5939: loss = 2.9954 (1.246 sec/step)\n","I0830 18:02:58.856568 139622356395904 learning.py:507] global step 5940: loss = 3.4728 (1.211 sec/step)\n","I0830 18:03:00.084917 139622356395904 learning.py:507] global step 5941: loss = 2.7284 (1.226 sec/step)\n","I0830 18:03:01.322216 139622356395904 learning.py:507] global step 5942: loss = 2.4196 (1.235 sec/step)\n","I0830 18:03:02.518869 139622356395904 learning.py:507] global step 5943: loss = 3.2723 (1.195 sec/step)\n","I0830 18:03:03.750413 139622356395904 learning.py:507] global step 5944: loss = 3.1428 (1.230 sec/step)\n","I0830 18:03:04.948478 139622356395904 learning.py:507] global step 5945: loss = 2.8123 (1.196 sec/step)\n","I0830 18:03:06.182619 139622356395904 learning.py:507] global step 5946: loss = 3.4943 (1.232 sec/step)\n","I0830 18:03:07.408478 139622356395904 learning.py:507] global step 5947: loss = 2.6948 (1.224 sec/step)\n","I0830 18:03:08.655250 139622356395904 learning.py:507] global step 5948: loss = 3.3901 (1.245 sec/step)\n","I0830 18:03:09.901495 139622356395904 learning.py:507] global step 5949: loss = 2.7910 (1.244 sec/step)\n","I0830 18:03:11.122376 139622356395904 learning.py:507] global step 5950: loss = 3.2224 (1.219 sec/step)\n","I0830 18:03:12.324970 139622356395904 learning.py:507] global step 5951: loss = 3.3821 (1.201 sec/step)\n","I0830 18:03:13.515397 139622356395904 learning.py:507] global step 5952: loss = 3.1420 (1.188 sec/step)\n","I0830 18:03:14.744122 139622356395904 learning.py:507] global step 5953: loss = 3.5315 (1.227 sec/step)\n","I0830 18:03:15.987206 139622356395904 learning.py:507] global step 5954: loss = 2.5267 (1.241 sec/step)\n","I0830 18:03:17.237766 139622356395904 learning.py:507] global step 5955: loss = 2.9991 (1.247 sec/step)\n","I0830 18:03:18.486677 139622356395904 learning.py:507] global step 5956: loss = 3.5266 (1.246 sec/step)\n","I0830 18:03:19.691966 139622356395904 learning.py:507] global step 5957: loss = 3.7407 (1.203 sec/step)\n","I0830 18:03:20.901269 139622356395904 learning.py:507] global step 5958: loss = 3.1966 (1.207 sec/step)\n","I0830 18:03:22.163583 139622356395904 learning.py:507] global step 5959: loss = 5.2685 (1.261 sec/step)\n","I0830 18:03:23.394662 139622356395904 learning.py:507] global step 5960: loss = 2.4601 (1.229 sec/step)\n","I0830 18:03:24.617471 139622356395904 learning.py:507] global step 5961: loss = 2.8632 (1.221 sec/step)\n","I0830 18:03:25.857586 139622356395904 learning.py:507] global step 5962: loss = 2.5222 (1.238 sec/step)\n","I0830 18:03:27.086772 139622356395904 learning.py:507] global step 5963: loss = 3.1017 (1.227 sec/step)\n","I0830 18:03:28.306685 139622356395904 learning.py:507] global step 5964: loss = 2.7209 (1.218 sec/step)\n","I0830 18:03:29.545003 139622356395904 learning.py:507] global step 5965: loss = 3.3521 (1.236 sec/step)\n","I0830 18:03:30.760024 139622356395904 learning.py:507] global step 5966: loss = 3.1048 (1.213 sec/step)\n","I0830 18:03:31.993989 139622356395904 learning.py:507] global step 5967: loss = 3.9965 (1.232 sec/step)\n","I0830 18:03:33.233995 139622356395904 learning.py:507] global step 5968: loss = 3.3263 (1.238 sec/step)\n","I0830 18:03:34.485036 139622356395904 learning.py:507] global step 5969: loss = 2.5198 (1.249 sec/step)\n","I0830 18:03:35.722232 139622356395904 learning.py:507] global step 5970: loss = 3.8062 (1.235 sec/step)\n","I0830 18:03:36.973621 139622356395904 learning.py:507] global step 5971: loss = 2.8139 (1.249 sec/step)\n","I0830 18:03:38.180147 139622356395904 learning.py:507] global step 5972: loss = 2.5923 (1.204 sec/step)\n","I0830 18:03:39.433936 139622356395904 learning.py:507] global step 5973: loss = 3.0141 (1.252 sec/step)\n","I0830 18:03:40.676915 139622356395904 learning.py:507] global step 5974: loss = 3.2585 (1.241 sec/step)\n","I0830 18:03:41.929038 139622356395904 learning.py:507] global step 5975: loss = 3.1923 (1.247 sec/step)\n","I0830 18:03:43.129504 139622356395904 learning.py:507] global step 5976: loss = 3.2025 (1.199 sec/step)\n","I0830 18:03:44.353071 139622356395904 learning.py:507] global step 5977: loss = 3.0370 (1.222 sec/step)\n","I0830 18:03:45.587121 139622356395904 learning.py:507] global step 5978: loss = 2.5663 (1.232 sec/step)\n","I0830 18:03:46.813893 139622356395904 learning.py:507] global step 5979: loss = 2.7475 (1.225 sec/step)\n","I0830 18:03:48.037037 139622356395904 learning.py:507] global step 5980: loss = 2.7578 (1.221 sec/step)\n","I0830 18:03:49.244290 139622356395904 learning.py:507] global step 5981: loss = 2.5127 (1.205 sec/step)\n","I0830 18:03:50.496019 139622356395904 learning.py:507] global step 5982: loss = 2.8861 (1.250 sec/step)\n","I0830 18:03:51.710772 139622356395904 learning.py:507] global step 5983: loss = 4.0669 (1.213 sec/step)\n","I0830 18:03:52.946863 139622356395904 learning.py:507] global step 5984: loss = 3.4755 (1.234 sec/step)\n","I0830 18:03:54.203799 139622356395904 learning.py:507] global step 5985: loss = 2.5788 (1.255 sec/step)\n","I0830 18:03:55.415844 139622356395904 learning.py:507] global step 5986: loss = 2.6828 (1.210 sec/step)\n","I0830 18:03:56.588122 139622356395904 learning.py:507] global step 5987: loss = 3.4322 (1.171 sec/step)\n","I0830 18:03:57.800626 139622356395904 learning.py:507] global step 5988: loss = 3.6450 (1.210 sec/step)\n","I0830 18:03:59.021911 139622356395904 learning.py:507] global step 5989: loss = 2.8557 (1.219 sec/step)\n","I0830 18:04:00.456082 139622356395904 learning.py:507] global step 5990: loss = 4.1415 (1.368 sec/step)\n","I0830 18:04:02.413327 139619299985152 supervisor.py:1050] Recording summary at step 5991.\n","I0830 18:04:02.455370 139622356395904 learning.py:507] global step 5991: loss = 3.3519 (1.997 sec/step)\n","I0830 18:04:03.647459 139622356395904 learning.py:507] global step 5992: loss = 2.8948 (1.190 sec/step)\n","I0830 18:04:04.842927 139622356395904 learning.py:507] global step 5993: loss = 2.5946 (1.194 sec/step)\n","I0830 18:04:06.089175 139622356395904 learning.py:507] global step 5994: loss = 3.2710 (1.244 sec/step)\n","I0830 18:04:07.329166 139622356395904 learning.py:507] global step 5995: loss = 3.2809 (1.238 sec/step)\n","I0830 18:04:08.548290 139622356395904 learning.py:507] global step 5996: loss = 2.9235 (1.217 sec/step)\n","I0830 18:04:09.767529 139622356395904 learning.py:507] global step 5997: loss = 2.6854 (1.217 sec/step)\n","I0830 18:04:11.015570 139622356395904 learning.py:507] global step 5998: loss = 3.1811 (1.246 sec/step)\n","I0830 18:04:12.254245 139622356395904 learning.py:507] global step 5999: loss = 2.4935 (1.237 sec/step)\n","I0830 18:04:13.473954 139622356395904 learning.py:507] global step 6000: loss = 3.0728 (1.218 sec/step)\n","I0830 18:04:14.674726 139622356395904 learning.py:507] global step 6001: loss = 2.8078 (1.199 sec/step)\n","I0830 18:04:15.864991 139622356395904 learning.py:507] global step 6002: loss = 2.8036 (1.189 sec/step)\n","I0830 18:04:17.084728 139622356395904 learning.py:507] global step 6003: loss = 3.1727 (1.217 sec/step)\n","I0830 18:04:18.294947 139622356395904 learning.py:507] global step 6004: loss = 3.3356 (1.208 sec/step)\n","I0830 18:04:19.509827 139622356395904 learning.py:507] global step 6005: loss = 3.0758 (1.213 sec/step)\n","I0830 18:04:20.706516 139622356395904 learning.py:507] global step 6006: loss = 3.2893 (1.195 sec/step)\n","I0830 18:04:21.900508 139622356395904 learning.py:507] global step 6007: loss = 2.3785 (1.192 sec/step)\n","I0830 18:04:23.122692 139622356395904 learning.py:507] global step 6008: loss = 2.7445 (1.220 sec/step)\n","I0830 18:04:24.357176 139622356395904 learning.py:507] global step 6009: loss = 3.1879 (1.233 sec/step)\n","I0830 18:04:25.600295 139622356395904 learning.py:507] global step 6010: loss = 3.0677 (1.241 sec/step)\n","I0830 18:04:26.804726 139622356395904 learning.py:507] global step 6011: loss = 2.4016 (1.202 sec/step)\n","I0830 18:04:28.056030 139622356395904 learning.py:507] global step 6012: loss = 2.3924 (1.249 sec/step)\n","I0830 18:04:29.259986 139622356395904 learning.py:507] global step 6013: loss = 2.3273 (1.202 sec/step)\n","I0830 18:04:30.445857 139622356395904 learning.py:507] global step 6014: loss = 3.1285 (1.184 sec/step)\n","I0830 18:04:31.625735 139622356395904 learning.py:507] global step 6015: loss = 3.8257 (1.178 sec/step)\n","I0830 18:04:32.829747 139622356395904 learning.py:507] global step 6016: loss = 2.7452 (1.202 sec/step)\n","I0830 18:04:34.076948 139622356395904 learning.py:507] global step 6017: loss = 2.6458 (1.245 sec/step)\n","I0830 18:04:35.308109 139622356395904 learning.py:507] global step 6018: loss = 2.8203 (1.229 sec/step)\n","I0830 18:04:36.493441 139622356395904 learning.py:507] global step 6019: loss = 2.5556 (1.183 sec/step)\n","I0830 18:04:37.692235 139622356395904 learning.py:507] global step 6020: loss = 2.7318 (1.197 sec/step)\n","I0830 18:04:38.895586 139622356395904 learning.py:507] global step 6021: loss = 3.9626 (1.201 sec/step)\n","I0830 18:04:40.146118 139622356395904 learning.py:507] global step 6022: loss = 2.5260 (1.248 sec/step)\n","I0830 18:04:41.376097 139622356395904 learning.py:507] global step 6023: loss = 2.2892 (1.228 sec/step)\n","I0830 18:04:42.644922 139622356395904 learning.py:507] global step 6024: loss = 2.6923 (1.267 sec/step)\n","I0830 18:04:43.853608 139622356395904 learning.py:507] global step 6025: loss = 2.4121 (1.207 sec/step)\n","I0830 18:04:45.076373 139622356395904 learning.py:507] global step 6026: loss = 3.1356 (1.221 sec/step)\n","I0830 18:04:46.311292 139622356395904 learning.py:507] global step 6027: loss = 2.3446 (1.232 sec/step)\n","I0830 18:04:47.536512 139622356395904 learning.py:507] global step 6028: loss = 4.1080 (1.223 sec/step)\n","I0830 18:04:48.724379 139622356395904 learning.py:507] global step 6029: loss = 3.1992 (1.186 sec/step)\n","I0830 18:04:49.938011 139622356395904 learning.py:507] global step 6030: loss = 3.0019 (1.212 sec/step)\n","I0830 18:04:51.179531 139622356395904 learning.py:507] global step 6031: loss = 3.2519 (1.240 sec/step)\n","I0830 18:04:52.400027 139622356395904 learning.py:507] global step 6032: loss = 2.2800 (1.219 sec/step)\n","I0830 18:04:53.604324 139622356395904 learning.py:507] global step 6033: loss = 3.2656 (1.202 sec/step)\n","I0830 18:04:54.831253 139622356395904 learning.py:507] global step 6034: loss = 3.2433 (1.225 sec/step)\n","I0830 18:04:56.066673 139622356395904 learning.py:507] global step 6035: loss = 2.9055 (1.234 sec/step)\n","I0830 18:04:57.249588 139622356395904 learning.py:507] global step 6036: loss = 2.3270 (1.181 sec/step)\n","I0830 18:04:58.458560 139622356395904 learning.py:507] global step 6037: loss = 3.0418 (1.207 sec/step)\n","I0830 18:04:59.667807 139622356395904 learning.py:507] global step 6038: loss = 2.3221 (1.207 sec/step)\n","I0830 18:05:00.874568 139622356395904 learning.py:507] global step 6039: loss = 2.5014 (1.205 sec/step)\n","I0830 18:05:02.122581 139622356395904 learning.py:507] global step 6040: loss = 2.5029 (1.246 sec/step)\n","I0830 18:05:03.351730 139622356395904 learning.py:507] global step 6041: loss = 2.9512 (1.227 sec/step)\n","I0830 18:05:04.572689 139622356395904 learning.py:507] global step 6042: loss = 2.5602 (1.219 sec/step)\n","I0830 18:05:05.824399 139622356395904 learning.py:507] global step 6043: loss = 3.5547 (1.250 sec/step)\n","I0830 18:05:07.016998 139622356395904 learning.py:507] global step 6044: loss = 3.3928 (1.191 sec/step)\n","I0830 18:05:08.234642 139622356395904 learning.py:507] global step 6045: loss = 2.9346 (1.216 sec/step)\n","I0830 18:05:09.428938 139622356395904 learning.py:507] global step 6046: loss = 2.7569 (1.192 sec/step)\n","I0830 18:05:10.648144 139622356395904 learning.py:507] global step 6047: loss = 3.3376 (1.217 sec/step)\n","I0830 18:05:11.875421 139622356395904 learning.py:507] global step 6048: loss = 2.8230 (1.225 sec/step)\n","I0830 18:05:13.099862 139622356395904 learning.py:507] global step 6049: loss = 2.7999 (1.222 sec/step)\n","I0830 18:05:14.312407 139622356395904 learning.py:507] global step 6050: loss = 3.9791 (1.211 sec/step)\n","I0830 18:05:15.541377 139622356395904 learning.py:507] global step 6051: loss = 3.0666 (1.227 sec/step)\n","I0830 18:05:16.749363 139622356395904 learning.py:507] global step 6052: loss = 2.8659 (1.206 sec/step)\n","I0830 18:05:17.981436 139622356395904 learning.py:507] global step 6053: loss = 3.3923 (1.230 sec/step)\n","I0830 18:05:19.204705 139622356395904 learning.py:507] global step 6054: loss = 3.4563 (1.221 sec/step)\n","I0830 18:05:20.449265 139622356395904 learning.py:507] global step 6055: loss = 4.0841 (1.243 sec/step)\n","I0830 18:05:21.683275 139622356395904 learning.py:507] global step 6056: loss = 3.1378 (1.232 sec/step)\n","I0830 18:05:22.917897 139622356395904 learning.py:507] global step 6057: loss = 2.5050 (1.231 sec/step)\n","I0830 18:05:24.180869 139622356395904 learning.py:507] global step 6058: loss = 2.9152 (1.257 sec/step)\n","I0830 18:05:25.391158 139622356395904 learning.py:507] global step 6059: loss = 3.0155 (1.208 sec/step)\n","I0830 18:05:26.619970 139622356395904 learning.py:507] global step 6060: loss = 2.5552 (1.227 sec/step)\n","I0830 18:05:27.867431 139622356395904 learning.py:507] global step 6061: loss = 2.9285 (1.246 sec/step)\n","I0830 18:05:29.085237 139622356395904 learning.py:507] global step 6062: loss = 2.7921 (1.216 sec/step)\n","I0830 18:05:30.310897 139622356395904 learning.py:507] global step 6063: loss = 2.2948 (1.218 sec/step)\n","I0830 18:05:31.519442 139622356395904 learning.py:507] global step 6064: loss = 3.2538 (1.207 sec/step)\n","I0830 18:05:32.710421 139622356395904 learning.py:507] global step 6065: loss = 3.6036 (1.189 sec/step)\n","I0830 18:05:33.926697 139622356395904 learning.py:507] global step 6066: loss = 2.2627 (1.214 sec/step)\n","I0830 18:05:35.180431 139622356395904 learning.py:507] global step 6067: loss = 2.8164 (1.252 sec/step)\n","I0830 18:05:36.382840 139622356395904 learning.py:507] global step 6068: loss = 3.1447 (1.201 sec/step)\n","I0830 18:05:37.587649 139622356395904 learning.py:507] global step 6069: loss = 3.5569 (1.203 sec/step)\n","I0830 18:05:38.817390 139622356395904 learning.py:507] global step 6070: loss = 3.0696 (1.228 sec/step)\n","I0830 18:05:40.037836 139622356395904 learning.py:507] global step 6071: loss = 3.0470 (1.219 sec/step)\n","I0830 18:05:41.276409 139622356395904 learning.py:507] global step 6072: loss = 3.3388 (1.236 sec/step)\n","I0830 18:05:42.515863 139622356395904 learning.py:507] global step 6073: loss = 2.9963 (1.237 sec/step)\n","I0830 18:05:43.730893 139622356395904 learning.py:507] global step 6074: loss = 4.2539 (1.213 sec/step)\n","I0830 18:05:44.953767 139622356395904 learning.py:507] global step 6075: loss = 3.1780 (1.221 sec/step)\n","I0830 18:05:46.193674 139622356395904 learning.py:507] global step 6076: loss = 3.3468 (1.235 sec/step)\n","I0830 18:05:47.413911 139622356395904 learning.py:507] global step 6077: loss = 3.4449 (1.218 sec/step)\n","I0830 18:05:48.636510 139622356395904 learning.py:507] global step 6078: loss = 3.3356 (1.221 sec/step)\n","I0830 18:05:49.865342 139622356395904 learning.py:507] global step 6079: loss = 2.2032 (1.227 sec/step)\n","I0830 18:05:51.112284 139622356395904 learning.py:507] global step 6080: loss = 3.5541 (1.245 sec/step)\n","I0830 18:05:52.329553 139622356395904 learning.py:507] global step 6081: loss = 3.2872 (1.215 sec/step)\n","I0830 18:05:53.546169 139622356395904 learning.py:507] global step 6082: loss = 2.7345 (1.215 sec/step)\n","I0830 18:05:54.782210 139622356395904 learning.py:507] global step 6083: loss = 2.6719 (1.234 sec/step)\n","I0830 18:05:56.036569 139622356395904 learning.py:507] global step 6084: loss = 3.2106 (1.253 sec/step)\n","I0830 18:05:57.266980 139622356395904 learning.py:507] global step 6085: loss = 2.6470 (1.229 sec/step)\n","I0830 18:05:58.501670 139622356395904 learning.py:507] global step 6086: loss = 3.2024 (1.232 sec/step)\n","I0830 18:05:59.756181 139622356395904 learning.py:507] global step 6087: loss = 3.2546 (1.247 sec/step)\n","I0830 18:06:01.821856 139619299985152 supervisor.py:1050] Recording summary at step 6088.\n","I0830 18:06:01.854329 139622356395904 learning.py:507] global step 6088: loss = 2.3930 (2.096 sec/step)\n","I0830 18:06:03.053169 139622356395904 learning.py:507] global step 6089: loss = 2.7331 (1.197 sec/step)\n","I0830 18:06:04.308617 139622356395904 learning.py:507] global step 6090: loss = 2.4044 (1.254 sec/step)\n","I0830 18:06:05.530167 139622356395904 learning.py:507] global step 6091: loss = 3.7393 (1.220 sec/step)\n","I0830 18:06:06.760984 139622356395904 learning.py:507] global step 6092: loss = 2.7766 (1.229 sec/step)\n","I0830 18:06:08.007823 139622356395904 learning.py:507] global step 6093: loss = 3.3680 (1.245 sec/step)\n","I0830 18:06:09.223083 139622356395904 learning.py:507] global step 6094: loss = 2.3721 (1.213 sec/step)\n","I0830 18:06:10.455443 139622356395904 learning.py:507] global step 6095: loss = 2.7908 (1.231 sec/step)\n","I0830 18:06:11.678177 139622356395904 learning.py:507] global step 6096: loss = 2.3698 (1.220 sec/step)\n","I0830 18:06:12.887593 139622356395904 learning.py:507] global step 6097: loss = 2.9238 (1.207 sec/step)\n","I0830 18:06:14.144757 139622356395904 learning.py:507] global step 6098: loss = 3.4566 (1.255 sec/step)\n","I0830 18:06:15.369524 139622356395904 learning.py:507] global step 6099: loss = 2.7643 (1.223 sec/step)\n","I0830 18:06:16.625174 139622356395904 learning.py:507] global step 6100: loss = 3.1383 (1.253 sec/step)\n","I0830 18:06:17.863881 139622356395904 learning.py:507] global step 6101: loss = 2.9322 (1.237 sec/step)\n","I0830 18:06:19.125019 139622356395904 learning.py:507] global step 6102: loss = 2.6158 (1.259 sec/step)\n","I0830 18:06:20.319569 139622356395904 learning.py:507] global step 6103: loss = 3.4143 (1.193 sec/step)\n","I0830 18:06:21.555847 139622356395904 learning.py:507] global step 6104: loss = 3.8658 (1.234 sec/step)\n","I0830 18:06:22.788916 139622356395904 learning.py:507] global step 6105: loss = 2.3714 (1.231 sec/step)\n","I0830 18:06:24.044354 139622356395904 learning.py:507] global step 6106: loss = 2.2996 (1.254 sec/step)\n","I0830 18:06:25.272382 139622356395904 learning.py:507] global step 6107: loss = 3.4254 (1.226 sec/step)\n","I0830 18:06:26.493799 139622356395904 learning.py:507] global step 6108: loss = 3.0614 (1.219 sec/step)\n","I0830 18:06:27.734698 139622356395904 learning.py:507] global step 6109: loss = 3.3778 (1.239 sec/step)\n","I0830 18:06:28.945831 139622356395904 learning.py:507] global step 6110: loss = 3.0371 (1.209 sec/step)\n","I0830 18:06:30.171038 139622356395904 learning.py:507] global step 6111: loss = 3.5929 (1.223 sec/step)\n","I0830 18:06:31.367717 139622356395904 learning.py:507] global step 6112: loss = 2.8307 (1.195 sec/step)\n","I0830 18:06:32.596130 139622356395904 learning.py:507] global step 6113: loss = 3.5651 (1.227 sec/step)\n","I0830 18:06:33.787842 139622356395904 learning.py:507] global step 6114: loss = 2.4130 (1.190 sec/step)\n","I0830 18:06:35.052204 139622356395904 learning.py:507] global step 6115: loss = 2.7137 (1.263 sec/step)\n","I0830 18:06:36.284199 139622356395904 learning.py:507] global step 6116: loss = 3.4133 (1.229 sec/step)\n","I0830 18:06:37.506082 139622356395904 learning.py:507] global step 6117: loss = 4.0627 (1.220 sec/step)\n","I0830 18:06:38.696449 139622356395904 learning.py:507] global step 6118: loss = 2.4253 (1.188 sec/step)\n","I0830 18:06:39.924890 139622356395904 learning.py:507] global step 6119: loss = 2.5536 (1.226 sec/step)\n","I0830 18:06:41.132206 139622356395904 learning.py:507] global step 6120: loss = 2.2174 (1.205 sec/step)\n","I0830 18:06:42.396531 139622356395904 learning.py:507] global step 6121: loss = 3.0566 (1.263 sec/step)\n","I0830 18:06:43.645441 139622356395904 learning.py:507] global step 6122: loss = 2.8091 (1.247 sec/step)\n","I0830 18:06:44.862733 139622356395904 learning.py:507] global step 6123: loss = 2.2589 (1.216 sec/step)\n","I0830 18:06:46.113662 139622356395904 learning.py:507] global step 6124: loss = 2.8165 (1.249 sec/step)\n","I0830 18:06:47.315266 139622356395904 learning.py:507] global step 6125: loss = 3.7060 (1.200 sec/step)\n","I0830 18:06:48.495597 139622356395904 learning.py:507] global step 6126: loss = 3.1083 (1.178 sec/step)\n","I0830 18:06:49.700356 139622356395904 learning.py:507] global step 6127: loss = 2.6327 (1.203 sec/step)\n","I0830 18:06:50.909790 139622356395904 learning.py:507] global step 6128: loss = 3.7331 (1.207 sec/step)\n","I0830 18:06:52.126312 139622356395904 learning.py:507] global step 6129: loss = 3.5050 (1.214 sec/step)\n","I0830 18:06:53.372406 139622356395904 learning.py:507] global step 6130: loss = 2.7293 (1.244 sec/step)\n","I0830 18:06:54.611984 139622356395904 learning.py:507] global step 6131: loss = 2.7078 (1.236 sec/step)\n","I0830 18:06:55.869748 139622356395904 learning.py:507] global step 6132: loss = 2.9385 (1.256 sec/step)\n","I0830 18:06:57.072176 139622356395904 learning.py:507] global step 6133: loss = 3.1439 (1.200 sec/step)\n","I0830 18:06:58.311324 139622356395904 learning.py:507] global step 6134: loss = 3.2637 (1.237 sec/step)\n","I0830 18:06:59.516194 139622356395904 learning.py:507] global step 6135: loss = 2.8596 (1.203 sec/step)\n","I0830 18:07:00.713793 139622356395904 learning.py:507] global step 6136: loss = 3.3087 (1.196 sec/step)\n","I0830 18:07:01.927690 139622356395904 learning.py:507] global step 6137: loss = 2.7609 (1.212 sec/step)\n","I0830 18:07:03.115545 139622356395904 learning.py:507] global step 6138: loss = 2.4386 (1.186 sec/step)\n","I0830 18:07:04.325676 139622356395904 learning.py:507] global step 6139: loss = 2.4882 (1.208 sec/step)\n","I0830 18:07:05.578157 139622356395904 learning.py:507] global step 6140: loss = 2.7668 (1.251 sec/step)\n","I0830 18:07:06.835026 139622356395904 learning.py:507] global step 6141: loss = 3.3105 (1.255 sec/step)\n","I0830 18:07:08.055198 139622356395904 learning.py:507] global step 6142: loss = 3.0697 (1.218 sec/step)\n","I0830 18:07:09.280972 139622356395904 learning.py:507] global step 6143: loss = 3.7580 (1.224 sec/step)\n","I0830 18:07:10.495865 139622356395904 learning.py:507] global step 6144: loss = 2.9800 (1.213 sec/step)\n","I0830 18:07:11.703856 139622356395904 learning.py:507] global step 6145: loss = 3.5224 (1.206 sec/step)\n","I0830 18:07:12.960323 139622356395904 learning.py:507] global step 6146: loss = 2.4851 (1.255 sec/step)\n","I0830 18:07:14.207422 139622356395904 learning.py:507] global step 6147: loss = 4.1783 (1.245 sec/step)\n","I0830 18:07:15.442141 139622356395904 learning.py:507] global step 6148: loss = 2.8619 (1.233 sec/step)\n","I0830 18:07:16.674081 139622356395904 learning.py:507] global step 6149: loss = 3.0232 (1.230 sec/step)\n","I0830 18:07:17.909631 139622356395904 learning.py:507] global step 6150: loss = 2.6620 (1.234 sec/step)\n","I0830 18:07:19.172557 139622356395904 learning.py:507] global step 6151: loss = 2.7866 (1.261 sec/step)\n","I0830 18:07:20.430366 139622356395904 learning.py:507] global step 6152: loss = 3.5768 (1.256 sec/step)\n","I0830 18:07:21.664088 139622356395904 learning.py:507] global step 6153: loss = 2.2410 (1.232 sec/step)\n","I0830 18:07:22.891893 139622356395904 learning.py:507] global step 6154: loss = 3.6360 (1.226 sec/step)\n","I0830 18:07:24.118355 139622356395904 learning.py:507] global step 6155: loss = 2.8116 (1.225 sec/step)\n","I0830 18:07:25.370619 139622356395904 learning.py:507] global step 6156: loss = 3.4030 (1.250 sec/step)\n","I0830 18:07:26.588920 139622356395904 learning.py:507] global step 6157: loss = 4.7289 (1.216 sec/step)\n","I0830 18:07:27.810355 139622356395904 learning.py:507] global step 6158: loss = 2.5456 (1.220 sec/step)\n","I0830 18:07:29.022902 139622356395904 learning.py:507] global step 6159: loss = 2.8539 (1.210 sec/step)\n","I0830 18:07:30.228986 139622356395904 learning.py:507] global step 6160: loss = 3.5923 (1.204 sec/step)\n","I0830 18:07:31.431542 139622356395904 learning.py:507] global step 6161: loss = 3.0976 (1.200 sec/step)\n","I0830 18:07:32.659709 139622356395904 learning.py:507] global step 6162: loss = 2.8689 (1.226 sec/step)\n","I0830 18:07:33.897096 139622356395904 learning.py:507] global step 6163: loss = 2.3257 (1.235 sec/step)\n","I0830 18:07:35.130959 139622356395904 learning.py:507] global step 6164: loss = 2.1903 (1.232 sec/step)\n","I0830 18:07:36.311433 139622356395904 learning.py:507] global step 6165: loss = 2.9268 (1.179 sec/step)\n","I0830 18:07:37.561501 139622356395904 learning.py:507] global step 6166: loss = 3.4779 (1.248 sec/step)\n","I0830 18:07:38.795456 139622356395904 learning.py:507] global step 6167: loss = 3.4854 (1.232 sec/step)\n","I0830 18:07:40.038425 139622356395904 learning.py:507] global step 6168: loss = 3.4204 (1.241 sec/step)\n","I0830 18:07:41.254511 139622356395904 learning.py:507] global step 6169: loss = 2.5153 (1.214 sec/step)\n","I0830 18:07:42.472684 139622356395904 learning.py:507] global step 6170: loss = 3.1943 (1.216 sec/step)\n","I0830 18:07:43.709261 139622356395904 learning.py:507] global step 6171: loss = 2.4676 (1.235 sec/step)\n","I0830 18:07:44.942556 139622356395904 learning.py:507] global step 6172: loss = 2.8582 (1.231 sec/step)\n","I0830 18:07:46.161161 139622356395904 learning.py:507] global step 6173: loss = 2.8398 (1.217 sec/step)\n","I0830 18:07:47.362805 139622356395904 learning.py:507] global step 6174: loss = 2.5727 (1.200 sec/step)\n","I0830 18:07:48.557482 139622356395904 learning.py:507] global step 6175: loss = 2.9981 (1.193 sec/step)\n","I0830 18:07:49.753561 139622356395904 learning.py:507] global step 6176: loss = 2.7840 (1.194 sec/step)\n","I0830 18:07:51.023754 139622356395904 learning.py:507] global step 6177: loss = 3.8556 (1.268 sec/step)\n","I0830 18:07:52.236435 139622356395904 learning.py:507] global step 6178: loss = 2.9324 (1.211 sec/step)\n","I0830 18:07:53.443485 139622356395904 learning.py:507] global step 6179: loss = 2.8950 (1.205 sec/step)\n","I0830 18:07:54.666913 139622356395904 learning.py:507] global step 6180: loss = 2.7355 (1.221 sec/step)\n","I0830 18:07:55.913532 139622356395904 learning.py:507] global step 6181: loss = 2.9268 (1.245 sec/step)\n","I0830 18:07:57.117563 139622356395904 learning.py:507] global step 6182: loss = 3.1953 (1.202 sec/step)\n","I0830 18:07:58.356739 139622356395904 learning.py:507] global step 6183: loss = 2.5836 (1.237 sec/step)\n","I0830 18:07:59.588752 139622356395904 learning.py:507] global step 6184: loss = 3.2625 (1.230 sec/step)\n","I0830 18:08:01.880516 139619299985152 supervisor.py:1050] Recording summary at step 6185.\n","I0830 18:08:01.894941 139622356395904 learning.py:507] global step 6185: loss = 3.9761 (2.304 sec/step)\n","I0830 18:08:03.150888 139622356395904 learning.py:507] global step 6186: loss = 2.5811 (1.254 sec/step)\n","I0830 18:08:04.386445 139622356395904 learning.py:507] global step 6187: loss = 2.9612 (1.233 sec/step)\n","I0830 18:08:05.632635 139622356395904 learning.py:507] global step 6188: loss = 3.3146 (1.244 sec/step)\n","I0830 18:08:06.824169 139622356395904 learning.py:507] global step 6189: loss = 2.6088 (1.189 sec/step)\n","I0830 18:08:08.070693 139622356395904 learning.py:507] global step 6190: loss = 3.5714 (1.244 sec/step)\n","I0830 18:08:09.294705 139622356395904 learning.py:507] global step 6191: loss = 3.5533 (1.222 sec/step)\n","I0830 18:08:10.542155 139622356395904 learning.py:507] global step 6192: loss = 3.1707 (1.246 sec/step)\n","I0830 18:08:11.787526 139622356395904 learning.py:507] global step 6193: loss = 2.3626 (1.244 sec/step)\n","I0830 18:08:12.966760 139622356395904 learning.py:507] global step 6194: loss = 3.8475 (1.177 sec/step)\n","I0830 18:08:14.169114 139622356395904 learning.py:507] global step 6195: loss = 3.0754 (1.200 sec/step)\n","I0830 18:08:15.401149 139622356395904 learning.py:507] global step 6196: loss = 2.2256 (1.230 sec/step)\n","I0830 18:08:16.656363 139622356395904 learning.py:507] global step 6197: loss = 2.4426 (1.253 sec/step)\n","I0830 18:08:17.894033 139622356395904 learning.py:507] global step 6198: loss = 3.3713 (1.236 sec/step)\n","I0830 18:08:19.122859 139622356395904 learning.py:507] global step 6199: loss = 2.6467 (1.227 sec/step)\n","I0830 18:08:20.328515 139622356395904 learning.py:507] global step 6200: loss = 2.7575 (1.204 sec/step)\n","I0830 18:08:21.563895 139622356395904 learning.py:507] global step 6201: loss = 2.2167 (1.233 sec/step)\n","I0830 18:08:22.773990 139622356395904 learning.py:507] global step 6202: loss = 3.6329 (1.208 sec/step)\n","I0830 18:08:24.011254 139622356395904 learning.py:507] global step 6203: loss = 3.3198 (1.236 sec/step)\n","I0830 18:08:25.207892 139622356395904 learning.py:507] global step 6204: loss = 3.4989 (1.195 sec/step)\n","I0830 18:08:26.445459 139622356395904 learning.py:507] global step 6205: loss = 3.2047 (1.236 sec/step)\n","I0830 18:08:27.682251 139622356395904 learning.py:507] global step 6206: loss = 2.2433 (1.235 sec/step)\n","I0830 18:08:28.911191 139622356395904 learning.py:507] global step 6207: loss = 2.6148 (1.227 sec/step)\n","I0830 18:08:30.138179 139622356395904 learning.py:507] global step 6208: loss = 2.8716 (1.225 sec/step)\n","I0830 18:08:31.387911 139622356395904 learning.py:507] global step 6209: loss = 2.5160 (1.248 sec/step)\n","I0830 18:08:32.636485 139622356395904 learning.py:507] global step 6210: loss = 2.4958 (1.247 sec/step)\n","I0830 18:08:33.884993 139622356395904 learning.py:507] global step 6211: loss = 2.5462 (1.247 sec/step)\n","I0830 18:08:35.114950 139622356395904 learning.py:507] global step 6212: loss = 2.8989 (1.228 sec/step)\n","I0830 18:08:36.376234 139622356395904 learning.py:507] global step 6213: loss = 2.5800 (1.259 sec/step)\n","I0830 18:08:37.589088 139622356395904 learning.py:507] global step 6214: loss = 4.2322 (1.211 sec/step)\n","I0830 18:08:38.799559 139622356395904 learning.py:507] global step 6215: loss = 3.1677 (1.209 sec/step)\n","I0830 18:08:40.033813 139622356395904 learning.py:507] global step 6216: loss = 2.7617 (1.232 sec/step)\n","I0830 18:08:41.261871 139622356395904 learning.py:507] global step 6217: loss = 2.9337 (1.226 sec/step)\n","I0830 18:08:42.492074 139622356395904 learning.py:507] global step 6218: loss = 2.5473 (1.228 sec/step)\n","I0830 18:08:43.699292 139622356395904 learning.py:507] global step 6219: loss = 2.7791 (1.205 sec/step)\n","I0830 18:08:44.914562 139622356395904 learning.py:507] global step 6220: loss = 2.9464 (1.213 sec/step)\n","I0830 18:08:46.149231 139622356395904 learning.py:507] global step 6221: loss = 3.2678 (1.233 sec/step)\n","I0830 18:08:47.380226 139622356395904 learning.py:507] global step 6222: loss = 3.1123 (1.229 sec/step)\n","I0830 18:08:48.610376 139622356395904 learning.py:507] global step 6223: loss = 2.2932 (1.228 sec/step)\n","I0830 18:08:49.857500 139622356395904 learning.py:507] global step 6224: loss = 2.9317 (1.245 sec/step)\n","I0830 18:08:51.092565 139622356395904 learning.py:507] global step 6225: loss = 2.6436 (1.233 sec/step)\n","I0830 18:08:52.299297 139622356395904 learning.py:507] global step 6226: loss = 2.9198 (1.205 sec/step)\n","I0830 18:08:53.496160 139622356395904 learning.py:507] global step 6227: loss = 2.6994 (1.195 sec/step)\n","I0830 18:08:54.721982 139622356395904 learning.py:507] global step 6228: loss = 2.8284 (1.224 sec/step)\n","I0830 18:08:55.927016 139622356395904 learning.py:507] global step 6229: loss = 2.9458 (1.203 sec/step)\n","I0830 18:08:57.143615 139622356395904 learning.py:507] global step 6230: loss = 3.6771 (1.215 sec/step)\n","I0830 18:08:58.386694 139622356395904 learning.py:507] global step 6231: loss = 2.4397 (1.241 sec/step)\n","I0830 18:08:59.603108 139622356395904 learning.py:507] global step 6232: loss = 4.0006 (1.214 sec/step)\n","I0830 18:09:00.870682 139622356395904 learning.py:507] global step 6233: loss = 2.7424 (1.266 sec/step)\n","I0830 18:09:02.101705 139622356395904 learning.py:507] global step 6234: loss = 2.1755 (1.229 sec/step)\n","I0830 18:09:03.368375 139622356395904 learning.py:507] global step 6235: loss = 2.8962 (1.265 sec/step)\n","I0830 18:09:04.583899 139622356395904 learning.py:507] global step 6236: loss = 2.2274 (1.214 sec/step)\n","I0830 18:09:05.822466 139622356395904 learning.py:507] global step 6237: loss = 2.4504 (1.237 sec/step)\n","I0830 18:09:07.094739 139622356395904 learning.py:507] global step 6238: loss = 3.2795 (1.270 sec/step)\n","I0830 18:09:08.324288 139622356395904 learning.py:507] global step 6239: loss = 3.5173 (1.228 sec/step)\n","I0830 18:09:09.609459 139622356395904 learning.py:507] global step 6240: loss = 2.5953 (1.278 sec/step)\n","I0830 18:09:10.857666 139622356395904 learning.py:507] global step 6241: loss = 2.7199 (1.246 sec/step)\n","I0830 18:09:12.102488 139622356395904 learning.py:507] global step 6242: loss = 2.8715 (1.243 sec/step)\n","I0830 18:09:13.294188 139622356395904 learning.py:507] global step 6243: loss = 2.9598 (1.189 sec/step)\n","I0830 18:09:14.508624 139622356395904 learning.py:507] global step 6244: loss = 2.6174 (1.213 sec/step)\n","I0830 18:09:15.761530 139622356395904 learning.py:507] global step 6245: loss = 2.1436 (1.251 sec/step)\n","I0830 18:09:16.979840 139622356395904 learning.py:507] global step 6246: loss = 3.1353 (1.216 sec/step)\n","I0830 18:09:18.243649 139622356395904 learning.py:507] global step 6247: loss = 3.1120 (1.262 sec/step)\n","I0830 18:09:19.465144 139622356395904 learning.py:507] global step 6248: loss = 2.6456 (1.220 sec/step)\n","I0830 18:09:20.691317 139622356395904 learning.py:507] global step 6249: loss = 2.8110 (1.224 sec/step)\n","I0830 18:09:21.917971 139622356395904 learning.py:507] global step 6250: loss = 2.9805 (1.225 sec/step)\n","I0830 18:09:23.126754 139622356395904 learning.py:507] global step 6251: loss = 3.2798 (1.207 sec/step)\n","I0830 18:09:24.353511 139622356395904 learning.py:507] global step 6252: loss = 2.4992 (1.225 sec/step)\n","I0830 18:09:25.592997 139622356395904 learning.py:507] global step 6253: loss = 2.9103 (1.237 sec/step)\n","I0830 18:09:26.830085 139622356395904 learning.py:507] global step 6254: loss = 2.7243 (1.235 sec/step)\n","I0830 18:09:28.093281 139622356395904 learning.py:507] global step 6255: loss = 2.8942 (1.261 sec/step)\n","I0830 18:09:29.325256 139622356395904 learning.py:507] global step 6256: loss = 3.0321 (1.230 sec/step)\n","I0830 18:09:30.570143 139622356395904 learning.py:507] global step 6257: loss = 2.4199 (1.243 sec/step)\n","I0830 18:09:31.813330 139622356395904 learning.py:507] global step 6258: loss = 2.7618 (1.241 sec/step)\n","I0830 18:09:33.058944 139622356395904 learning.py:507] global step 6259: loss = 3.6249 (1.244 sec/step)\n","I0830 18:09:34.297248 139622356395904 learning.py:507] global step 6260: loss = 3.0012 (1.236 sec/step)\n","I0830 18:09:35.541429 139622356395904 learning.py:507] global step 6261: loss = 4.1991 (1.242 sec/step)\n","I0830 18:09:36.773298 139622356395904 learning.py:507] global step 6262: loss = 2.4409 (1.230 sec/step)\n","I0830 18:09:38.027223 139622356395904 learning.py:507] global step 6263: loss = 2.8225 (1.252 sec/step)\n","I0830 18:09:39.232885 139622356395904 learning.py:507] global step 6264: loss = 3.2663 (1.204 sec/step)\n","I0830 18:09:40.455033 139622356395904 learning.py:507] global step 6265: loss = 2.9393 (1.220 sec/step)\n","I0830 18:09:41.657751 139622356395904 learning.py:507] global step 6266: loss = 2.5970 (1.201 sec/step)\n","I0830 18:09:42.866894 139622356395904 learning.py:507] global step 6267: loss = 3.0427 (1.207 sec/step)\n","I0830 18:09:44.085230 139622356395904 learning.py:507] global step 6268: loss = 2.5675 (1.217 sec/step)\n","I0830 18:09:45.319449 139622356395904 learning.py:507] global step 6269: loss = 3.6816 (1.232 sec/step)\n","I0830 18:09:46.522848 139622356395904 learning.py:507] global step 6270: loss = 3.3878 (1.201 sec/step)\n","I0830 18:09:47.742256 139622356395904 learning.py:507] global step 6271: loss = 2.5728 (1.217 sec/step)\n","I0830 18:09:48.977999 139622356395904 learning.py:507] global step 6272: loss = 3.6783 (1.234 sec/step)\n","I0830 18:09:50.215483 139622356395904 learning.py:507] global step 6273: loss = 2.2567 (1.235 sec/step)\n","I0830 18:09:51.436212 139622356395904 learning.py:507] global step 6274: loss = 3.3839 (1.219 sec/step)\n","I0830 18:09:52.662927 139622356395904 learning.py:507] global step 6275: loss = 2.6453 (1.225 sec/step)\n","I0830 18:09:53.891744 139622356395904 learning.py:507] global step 6276: loss = 2.3808 (1.227 sec/step)\n","I0830 18:09:55.096934 139622356395904 learning.py:507] global step 6277: loss = 2.2299 (1.203 sec/step)\n","I0830 18:09:56.313126 139622356395904 learning.py:507] global step 6278: loss = 3.5609 (1.214 sec/step)\n","I0830 18:09:57.505335 139622356395904 learning.py:507] global step 6279: loss = 3.1242 (1.190 sec/step)\n","I0830 18:09:58.738633 139622356395904 learning.py:507] global step 6280: loss = 2.8595 (1.231 sec/step)\n","I0830 18:09:59.650868 139619316770560 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n","I0830 18:10:00.002440 139622356395904 learning.py:507] global step 6281: loss = 2.7153 (1.257 sec/step)\n","I0830 18:10:02.684862 139619299985152 supervisor.py:1050] Recording summary at step 6282.\n","I0830 18:10:02.731630 139622356395904 learning.py:507] global step 6282: loss = 3.1033 (2.722 sec/step)\n","I0830 18:10:04.499642 139622356395904 learning.py:507] global step 6283: loss = 2.5933 (1.761 sec/step)\n","I0830 18:10:05.912085 139622356395904 learning.py:507] global step 6284: loss = 2.5743 (1.404 sec/step)\n","I0830 18:10:07.140345 139622356395904 learning.py:507] global step 6285: loss = 3.2842 (1.227 sec/step)\n","I0830 18:10:08.389692 139622356395904 learning.py:507] global step 6286: loss = 2.5512 (1.247 sec/step)\n","I0830 18:10:09.615418 139622356395904 learning.py:507] global step 6287: loss = 2.2984 (1.224 sec/step)\n","I0830 18:10:10.824451 139622356395904 learning.py:507] global step 6288: loss = 3.1378 (1.207 sec/step)\n","I0830 18:10:12.070959 139622356395904 learning.py:507] global step 6289: loss = 2.8838 (1.245 sec/step)\n","I0830 18:10:13.300134 139622356395904 learning.py:507] global step 6290: loss = 2.6571 (1.227 sec/step)\n","I0830 18:10:14.504233 139622356395904 learning.py:507] global step 6291: loss = 2.6714 (1.202 sec/step)\n","I0830 18:10:15.716708 139622356395904 learning.py:507] global step 6292: loss = 2.3945 (1.211 sec/step)\n","I0830 18:10:16.984878 139622356395904 learning.py:507] global step 6293: loss = 2.4850 (1.266 sec/step)\n","I0830 18:10:18.220805 139622356395904 learning.py:507] global step 6294: loss = 2.6740 (1.234 sec/step)\n","I0830 18:10:19.474802 139622356395904 learning.py:507] global step 6295: loss = 3.5665 (1.252 sec/step)\n","I0830 18:10:20.730318 139622356395904 learning.py:507] global step 6296: loss = 4.0690 (1.254 sec/step)\n","I0830 18:10:21.925786 139622356395904 learning.py:507] global step 6297: loss = 2.5473 (1.194 sec/step)\n","I0830 18:10:23.121926 139622356395904 learning.py:507] global step 6298: loss = 2.9363 (1.194 sec/step)\n","I0830 18:10:24.348229 139622356395904 learning.py:507] global step 6299: loss = 2.3525 (1.224 sec/step)\n","I0830 18:10:25.552806 139622356395904 learning.py:507] global step 6300: loss = 2.9173 (1.203 sec/step)\n","I0830 18:10:26.768337 139622356395904 learning.py:507] global step 6301: loss = 3.3163 (1.214 sec/step)\n","I0830 18:10:28.005249 139622356395904 learning.py:507] global step 6302: loss = 2.3755 (1.235 sec/step)\n","I0830 18:10:29.218729 139622356395904 learning.py:507] global step 6303: loss = 2.4379 (1.211 sec/step)\n","I0830 18:10:30.420486 139622356395904 learning.py:507] global step 6304: loss = 2.5644 (1.200 sec/step)\n","I0830 18:10:31.656422 139622356395904 learning.py:507] global step 6305: loss = 2.8502 (1.234 sec/step)\n","I0830 18:10:32.896224 139622356395904 learning.py:507] global step 6306: loss = 3.2613 (1.238 sec/step)\n","I0830 18:10:34.093394 139622356395904 learning.py:507] global step 6307: loss = 2.9438 (1.195 sec/step)\n","I0830 18:10:35.301473 139622356395904 learning.py:507] global step 6308: loss = 3.3014 (1.206 sec/step)\n","I0830 18:10:36.516944 139622356395904 learning.py:507] global step 6309: loss = 3.1267 (1.213 sec/step)\n","I0830 18:10:37.749552 139622356395904 learning.py:507] global step 6310: loss = 3.3541 (1.231 sec/step)\n","I0830 18:10:38.993713 139622356395904 learning.py:507] global step 6311: loss = 3.5022 (1.242 sec/step)\n","I0830 18:10:40.233021 139622356395904 learning.py:507] global step 6312: loss = 2.6547 (1.237 sec/step)\n","I0830 18:10:41.436589 139622356395904 learning.py:507] global step 6313: loss = 2.4871 (1.201 sec/step)\n","I0830 18:10:42.644379 139622356395904 learning.py:507] global step 6314: loss = 2.8034 (1.205 sec/step)\n","I0830 18:10:43.851912 139622356395904 learning.py:507] global step 6315: loss = 2.4716 (1.206 sec/step)\n","I0830 18:10:45.067479 139622356395904 learning.py:507] global step 6316: loss = 3.7687 (1.213 sec/step)\n","I0830 18:10:46.283664 139622356395904 learning.py:507] global step 6317: loss = 3.1362 (1.214 sec/step)\n","I0830 18:10:47.490098 139622356395904 learning.py:507] global step 6318: loss = 2.6711 (1.205 sec/step)\n","I0830 18:10:48.681608 139622356395904 learning.py:507] global step 6319: loss = 2.4828 (1.189 sec/step)\n","I0830 18:10:49.870896 139622356395904 learning.py:507] global step 6320: loss = 2.7699 (1.187 sec/step)\n","I0830 18:10:51.100748 139622356395904 learning.py:507] global step 6321: loss = 3.4599 (1.228 sec/step)\n","I0830 18:10:52.318311 139622356395904 learning.py:507] global step 6322: loss = 3.5150 (1.216 sec/step)\n","I0830 18:10:53.524930 139622356395904 learning.py:507] global step 6323: loss = 2.5227 (1.204 sec/step)\n","I0830 18:10:54.734597 139622356395904 learning.py:507] global step 6324: loss = 3.0151 (1.206 sec/step)\n","I0830 18:10:55.969953 139622356395904 learning.py:507] global step 6325: loss = 2.0867 (1.233 sec/step)\n","I0830 18:10:57.202105 139622356395904 learning.py:507] global step 6326: loss = 3.8013 (1.230 sec/step)\n","I0830 18:10:58.416837 139622356395904 learning.py:507] global step 6327: loss = 2.6957 (1.212 sec/step)\n","I0830 18:10:59.641161 139622356395904 learning.py:507] global step 6328: loss = 3.0604 (1.222 sec/step)\n","I0830 18:11:00.928787 139622356395904 learning.py:507] global step 6329: loss = 2.6854 (1.286 sec/step)\n","I0830 18:11:02.144422 139622356395904 learning.py:507] global step 6330: loss = 2.8193 (1.214 sec/step)\n","I0830 18:11:03.366926 139622356395904 learning.py:507] global step 6331: loss = 2.8350 (1.221 sec/step)\n","I0830 18:11:04.586570 139622356395904 learning.py:507] global step 6332: loss = 4.4619 (1.218 sec/step)\n","I0830 18:11:05.797419 139622356395904 learning.py:507] global step 6333: loss = 2.3922 (1.209 sec/step)\n","I0830 18:11:07.024695 139622356395904 learning.py:507] global step 6334: loss = 2.6704 (1.225 sec/step)\n","I0830 18:11:08.229060 139622356395904 learning.py:507] global step 6335: loss = 2.4935 (1.203 sec/step)\n","I0830 18:11:09.459775 139622356395904 learning.py:507] global step 6336: loss = 3.2100 (1.229 sec/step)\n","I0830 18:11:10.684592 139622356395904 learning.py:507] global step 6337: loss = 2.9181 (1.223 sec/step)\n","I0830 18:11:11.925375 139622356395904 learning.py:507] global step 6338: loss = 3.4157 (1.239 sec/step)\n","I0830 18:11:13.174478 139622356395904 learning.py:507] global step 6339: loss = 2.4797 (1.247 sec/step)\n","I0830 18:11:14.390777 139622356395904 learning.py:507] global step 6340: loss = 3.7505 (1.214 sec/step)\n","I0830 18:11:15.608721 139622356395904 learning.py:507] global step 6341: loss = 3.3632 (1.216 sec/step)\n","I0830 18:11:16.843523 139622356395904 learning.py:507] global step 6342: loss = 2.5669 (1.233 sec/step)\n","I0830 18:11:18.074118 139622356395904 learning.py:507] global step 6343: loss = 4.0961 (1.228 sec/step)\n","I0830 18:11:19.343274 139622356395904 learning.py:507] global step 6344: loss = 2.8531 (1.267 sec/step)\n","I0830 18:11:20.607160 139622356395904 learning.py:507] global step 6345: loss = 3.7102 (1.262 sec/step)\n","I0830 18:11:21.817451 139622356395904 learning.py:507] global step 6346: loss = 3.5437 (1.208 sec/step)\n","I0830 18:11:23.057968 139622356395904 learning.py:507] global step 6347: loss = 2.5919 (1.236 sec/step)\n","I0830 18:11:24.267772 139622356395904 learning.py:507] global step 6348: loss = 3.3495 (1.208 sec/step)\n","I0830 18:11:25.507960 139622356395904 learning.py:507] global step 6349: loss = 2.7577 (1.238 sec/step)\n","I0830 18:11:26.714762 139622356395904 learning.py:507] global step 6350: loss = 2.5819 (1.205 sec/step)\n","I0830 18:11:27.920502 139622356395904 learning.py:507] global step 6351: loss = 2.4895 (1.204 sec/step)\n","I0830 18:11:29.156210 139622356395904 learning.py:507] global step 6352: loss = 2.5788 (1.234 sec/step)\n","I0830 18:11:30.393607 139622356395904 learning.py:507] global step 6353: loss = 4.2870 (1.235 sec/step)\n","I0830 18:11:31.640309 139622356395904 learning.py:507] global step 6354: loss = 2.6061 (1.245 sec/step)\n","I0830 18:11:32.883466 139622356395904 learning.py:507] global step 6355: loss = 3.8234 (1.241 sec/step)\n","I0830 18:11:34.119160 139622356395904 learning.py:507] global step 6356: loss = 3.6512 (1.234 sec/step)\n","I0830 18:11:35.316364 139622356395904 learning.py:507] global step 6357: loss = 3.0687 (1.195 sec/step)\n","I0830 18:11:36.518332 139622356395904 learning.py:507] global step 6358: loss = 2.6385 (1.200 sec/step)\n","I0830 18:11:37.762835 139622356395904 learning.py:507] global step 6359: loss = 2.6210 (1.243 sec/step)\n","I0830 18:11:38.998625 139622356395904 learning.py:507] global step 6360: loss = 3.2503 (1.234 sec/step)\n","I0830 18:11:40.191543 139622356395904 learning.py:507] global step 6361: loss = 3.2878 (1.191 sec/step)\n","I0830 18:11:41.442774 139622356395904 learning.py:507] global step 6362: loss = 2.5480 (1.250 sec/step)\n","I0830 18:11:42.671916 139622356395904 learning.py:507] global step 6363: loss = 2.1407 (1.227 sec/step)\n","I0830 18:11:43.924961 139622356395904 learning.py:507] global step 6364: loss = 3.4400 (1.251 sec/step)\n","I0830 18:11:45.140026 139622356395904 learning.py:507] global step 6365: loss = 3.0940 (1.213 sec/step)\n","I0830 18:11:46.359106 139622356395904 learning.py:507] global step 6366: loss = 2.6389 (1.217 sec/step)\n","I0830 18:11:47.561033 139622356395904 learning.py:507] global step 6367: loss = 3.2158 (1.200 sec/step)\n","I0830 18:11:48.794410 139622356395904 learning.py:507] global step 6368: loss = 2.8528 (1.231 sec/step)\n","I0830 18:11:50.003957 139622356395904 learning.py:507] global step 6369: loss = 3.4502 (1.208 sec/step)\n","I0830 18:11:51.221423 139622356395904 learning.py:507] global step 6370: loss = 3.4844 (1.215 sec/step)\n","I0830 18:11:52.444938 139622356395904 learning.py:507] global step 6371: loss = 3.8733 (1.222 sec/step)\n","I0830 18:11:53.666158 139622356395904 learning.py:507] global step 6372: loss = 2.7723 (1.219 sec/step)\n","I0830 18:11:54.878588 139622356395904 learning.py:507] global step 6373: loss = 2.1928 (1.211 sec/step)\n","I0830 18:11:56.077044 139622356395904 learning.py:507] global step 6374: loss = 2.9300 (1.196 sec/step)\n","I0830 18:11:57.299726 139622356395904 learning.py:507] global step 6375: loss = 2.4610 (1.221 sec/step)\n","I0830 18:11:58.524633 139622356395904 learning.py:507] global step 6376: loss = 4.3762 (1.223 sec/step)\n","I0830 18:11:59.821878 139622356395904 learning.py:507] global step 6377: loss = 3.4745 (1.285 sec/step)\n","I0830 18:12:01.979981 139619299985152 supervisor.py:1050] Recording summary at step 6378.\n","I0830 18:12:01.998178 139622356395904 learning.py:507] global step 6378: loss = 3.4563 (2.170 sec/step)\n","I0830 18:12:03.235292 139622356395904 learning.py:507] global step 6379: loss = 3.7211 (1.235 sec/step)\n","I0830 18:12:04.462150 139622356395904 learning.py:507] global step 6380: loss = 3.0187 (1.225 sec/step)\n","I0830 18:12:05.708436 139622356395904 learning.py:507] global step 6381: loss = 3.2989 (1.245 sec/step)\n","I0830 18:12:06.920592 139622356395904 learning.py:507] global step 6382: loss = 3.4967 (1.210 sec/step)\n","I0830 18:12:08.141330 139622356395904 learning.py:507] global step 6383: loss = 3.4251 (1.219 sec/step)\n","I0830 18:12:09.332587 139622356395904 learning.py:507] global step 6384: loss = 3.3616 (1.189 sec/step)\n","I0830 18:12:10.559942 139622356395904 learning.py:507] global step 6385: loss = 2.7440 (1.225 sec/step)\n","I0830 18:12:11.794523 139622356395904 learning.py:507] global step 6386: loss = 4.0373 (1.233 sec/step)\n","I0830 18:12:13.003738 139622356395904 learning.py:507] global step 6387: loss = 3.1692 (1.207 sec/step)\n","I0830 18:12:14.210496 139622356395904 learning.py:507] global step 6388: loss = 3.2226 (1.205 sec/step)\n","I0830 18:12:15.436040 139622356395904 learning.py:507] global step 6389: loss = 3.3398 (1.223 sec/step)\n","I0830 18:12:16.654933 139622356395904 learning.py:507] global step 6390: loss = 2.8845 (1.217 sec/step)\n","I0830 18:12:17.891433 139622356395904 learning.py:507] global step 6391: loss = 3.4533 (1.234 sec/step)\n","I0830 18:12:19.156216 139622356395904 learning.py:507] global step 6392: loss = 3.0420 (1.260 sec/step)\n","I0830 18:12:20.384537 139622356395904 learning.py:507] global step 6393: loss = 3.0380 (1.226 sec/step)\n","I0830 18:12:21.578815 139622356395904 learning.py:507] global step 6394: loss = 3.2308 (1.192 sec/step)\n","I0830 18:12:22.810219 139622356395904 learning.py:507] global step 6395: loss = 2.7995 (1.230 sec/step)\n","I0830 18:12:24.029325 139622356395904 learning.py:507] global step 6396: loss = 2.6537 (1.217 sec/step)\n","I0830 18:12:25.280891 139622356395904 learning.py:507] global step 6397: loss = 2.8933 (1.250 sec/step)\n","I0830 18:12:26.487554 139622356395904 learning.py:507] global step 6398: loss = 2.7293 (1.205 sec/step)\n","I0830 18:12:27.697952 139622356395904 learning.py:507] global step 6399: loss = 2.7291 (1.208 sec/step)\n","I0830 18:12:28.895268 139622356395904 learning.py:507] global step 6400: loss = 2.7115 (1.196 sec/step)\n","I0830 18:12:30.114224 139622356395904 learning.py:507] global step 6401: loss = 2.9623 (1.217 sec/step)\n","I0830 18:12:31.326795 139622356395904 learning.py:507] global step 6402: loss = 3.0034 (1.211 sec/step)\n","I0830 18:12:32.546247 139622356395904 learning.py:507] global step 6403: loss = 2.8500 (1.218 sec/step)\n","I0830 18:12:33.727154 139622356395904 learning.py:507] global step 6404: loss = 4.3413 (1.179 sec/step)\n","I0830 18:12:34.958872 139622356395904 learning.py:507] global step 6405: loss = 2.9714 (1.230 sec/step)\n","I0830 18:12:36.141341 139622356395904 learning.py:507] global step 6406: loss = 2.4943 (1.181 sec/step)\n","I0830 18:12:37.335836 139622356395904 learning.py:507] global step 6407: loss = 3.2327 (1.192 sec/step)\n","I0830 18:12:38.531322 139622356395904 learning.py:507] global step 6408: loss = 4.0060 (1.193 sec/step)\n","I0830 18:12:39.737938 139622356395904 learning.py:507] global step 6409: loss = 2.5584 (1.205 sec/step)\n","I0830 18:12:40.928937 139622356395904 learning.py:507] global step 6410: loss = 2.4157 (1.189 sec/step)\n","I0830 18:12:42.163977 139622356395904 learning.py:507] global step 6411: loss = 2.1512 (1.233 sec/step)\n","I0830 18:12:43.376013 139622356395904 learning.py:507] global step 6412: loss = 2.6498 (1.210 sec/step)\n","I0830 18:12:44.603973 139622356395904 learning.py:507] global step 6413: loss = 3.5335 (1.226 sec/step)\n","I0830 18:12:45.829022 139622356395904 learning.py:507] global step 6414: loss = 2.3805 (1.223 sec/step)\n","I0830 18:12:47.041327 139622356395904 learning.py:507] global step 6415: loss = 3.8178 (1.210 sec/step)\n","I0830 18:12:48.318294 139622356395904 learning.py:507] global step 6416: loss = 2.7739 (1.275 sec/step)\n","I0830 18:12:49.541162 139622356395904 learning.py:507] global step 6417: loss = 3.0706 (1.221 sec/step)\n","I0830 18:12:50.768931 139622356395904 learning.py:507] global step 6418: loss = 2.7994 (1.223 sec/step)\n","I0830 18:12:51.997875 139622356395904 learning.py:507] global step 6419: loss = 3.2829 (1.227 sec/step)\n","I0830 18:12:53.209161 139622356395904 learning.py:507] global step 6420: loss = 2.6958 (1.210 sec/step)\n","I0830 18:12:54.409547 139622356395904 learning.py:507] global step 6421: loss = 2.4500 (1.199 sec/step)\n","I0830 18:12:55.651851 139622356395904 learning.py:507] global step 6422: loss = 2.2441 (1.240 sec/step)\n","I0830 18:12:56.855585 139622356395904 learning.py:507] global step 6423: loss = 4.4412 (1.202 sec/step)\n","I0830 18:12:58.054291 139622356395904 learning.py:507] global step 6424: loss = 3.0943 (1.197 sec/step)\n","I0830 18:12:59.311913 139622356395904 learning.py:507] global step 6425: loss = 2.5086 (1.256 sec/step)\n","I0830 18:13:00.544819 139622356395904 learning.py:507] global step 6426: loss = 3.1386 (1.231 sec/step)\n","I0830 18:13:01.781738 139622356395904 learning.py:507] global step 6427: loss = 2.4059 (1.235 sec/step)\n","I0830 18:13:03.026667 139622356395904 learning.py:507] global step 6428: loss = 2.9435 (1.243 sec/step)\n","I0830 18:13:04.255825 139622356395904 learning.py:507] global step 6429: loss = 3.0486 (1.227 sec/step)\n","I0830 18:13:05.482827 139622356395904 learning.py:507] global step 6430: loss = 2.9140 (1.225 sec/step)\n","I0830 18:13:06.708579 139622356395904 learning.py:507] global step 6431: loss = 4.4702 (1.224 sec/step)\n","I0830 18:13:07.916202 139622356395904 learning.py:507] global step 6432: loss = 2.9722 (1.206 sec/step)\n","I0830 18:13:09.101146 139622356395904 learning.py:507] global step 6433: loss = 2.8375 (1.183 sec/step)\n","I0830 18:13:10.337392 139622356395904 learning.py:507] global step 6434: loss = 3.7128 (1.234 sec/step)\n","I0830 18:13:11.568305 139622356395904 learning.py:507] global step 6435: loss = 2.1198 (1.228 sec/step)\n","I0830 18:13:12.781157 139622356395904 learning.py:507] global step 6436: loss = 2.5174 (1.211 sec/step)\n","I0830 18:13:14.063852 139622356395904 learning.py:507] global step 6437: loss = 2.7024 (1.281 sec/step)\n","I0830 18:13:15.277069 139622356395904 learning.py:507] global step 6438: loss = 3.1606 (1.211 sec/step)\n","I0830 18:13:16.508723 139622356395904 learning.py:507] global step 6439: loss = 3.3406 (1.230 sec/step)\n","I0830 18:13:17.725540 139622356395904 learning.py:507] global step 6440: loss = 2.8091 (1.215 sec/step)\n","I0830 18:13:18.948639 139622356395904 learning.py:507] global step 6441: loss = 3.4910 (1.221 sec/step)\n","I0830 18:13:20.155832 139622356395904 learning.py:507] global step 6442: loss = 3.5171 (1.205 sec/step)\n","I0830 18:13:21.355525 139622356395904 learning.py:507] global step 6443: loss = 4.9287 (1.198 sec/step)\n","I0830 18:13:22.558362 139622356395904 learning.py:507] global step 6444: loss = 3.1429 (1.201 sec/step)\n","I0830 18:13:23.788417 139622356395904 learning.py:507] global step 6445: loss = 3.1014 (1.228 sec/step)\n","I0830 18:13:25.070593 139622356395904 learning.py:507] global step 6446: loss = 2.8106 (1.280 sec/step)\n","I0830 18:13:26.280839 139622356395904 learning.py:507] global step 6447: loss = 2.6925 (1.209 sec/step)\n","I0830 18:13:27.487254 139622356395904 learning.py:507] global step 6448: loss = 3.8160 (1.205 sec/step)\n","I0830 18:13:28.700639 139622356395904 learning.py:507] global step 6449: loss = 2.4275 (1.211 sec/step)\n","I0830 18:13:29.945896 139622356395904 learning.py:507] global step 6450: loss = 2.1458 (1.243 sec/step)\n","I0830 18:13:31.159526 139622356395904 learning.py:507] global step 6451: loss = 2.6913 (1.212 sec/step)\n","I0830 18:13:32.362454 139622356395904 learning.py:507] global step 6452: loss = 2.8309 (1.201 sec/step)\n","I0830 18:13:33.595828 139622356395904 learning.py:507] global step 6453: loss = 3.1701 (1.231 sec/step)\n","I0830 18:13:34.788594 139622356395904 learning.py:507] global step 6454: loss = 3.4191 (1.191 sec/step)\n","I0830 18:13:36.022426 139622356395904 learning.py:507] global step 6455: loss = 3.6143 (1.232 sec/step)\n","I0830 18:13:37.243600 139622356395904 learning.py:507] global step 6456: loss = 2.5803 (1.219 sec/step)\n","I0830 18:13:38.485505 139622356395904 learning.py:507] global step 6457: loss = 2.9195 (1.240 sec/step)\n","I0830 18:13:39.668799 139622356395904 learning.py:507] global step 6458: loss = 2.8462 (1.181 sec/step)\n","I0830 18:13:40.848970 139622356395904 learning.py:507] global step 6459: loss = 2.8921 (1.178 sec/step)\n","I0830 18:13:42.065841 139622356395904 learning.py:507] global step 6460: loss = 2.5773 (1.215 sec/step)\n","I0830 18:13:43.285786 139622356395904 learning.py:507] global step 6461: loss = 3.2827 (1.218 sec/step)\n","I0830 18:13:44.480938 139622356395904 learning.py:507] global step 6462: loss = 2.8006 (1.193 sec/step)\n","I0830 18:13:45.729744 139622356395904 learning.py:507] global step 6463: loss = 3.8873 (1.247 sec/step)\n","I0830 18:13:46.929114 139622356395904 learning.py:507] global step 6464: loss = 3.4563 (1.198 sec/step)\n","I0830 18:13:48.145537 139622356395904 learning.py:507] global step 6465: loss = 3.8965 (1.215 sec/step)\n","I0830 18:13:49.330376 139622356395904 learning.py:507] global step 6466: loss = 3.1539 (1.183 sec/step)\n","I0830 18:13:50.585519 139622356395904 learning.py:507] global step 6467: loss = 3.2566 (1.253 sec/step)\n","I0830 18:13:51.806765 139622356395904 learning.py:507] global step 6468: loss = 2.3635 (1.219 sec/step)\n","I0830 18:13:53.007768 139622356395904 learning.py:507] global step 6469: loss = 2.9554 (1.199 sec/step)\n","I0830 18:13:54.220913 139622356395904 learning.py:507] global step 6470: loss = 3.0206 (1.211 sec/step)\n","I0830 18:13:55.456579 139622356395904 learning.py:507] global step 6471: loss = 3.8717 (1.234 sec/step)\n","I0830 18:13:56.720373 139622356395904 learning.py:507] global step 6472: loss = 3.4059 (1.262 sec/step)\n","I0830 18:13:57.914012 139622356395904 learning.py:507] global step 6473: loss = 2.8829 (1.192 sec/step)\n","I0830 18:13:59.118398 139622356395904 learning.py:507] global step 6474: loss = 2.7939 (1.202 sec/step)\n","I0830 18:14:00.477488 139622356395904 learning.py:507] global step 6475: loss = 2.8122 (1.347 sec/step)\n","I0830 18:14:02.405447 139622356395904 learning.py:507] global step 6476: loss = 2.2697 (1.925 sec/step)\n","I0830 18:14:02.465623 139619299985152 supervisor.py:1050] Recording summary at step 6476.\n","I0830 18:14:03.680768 139622356395904 learning.py:507] global step 6477: loss = 2.9756 (1.273 sec/step)\n","I0830 18:14:04.906481 139622356395904 learning.py:507] global step 6478: loss = 3.0518 (1.224 sec/step)\n","I0830 18:14:06.143544 139622356395904 learning.py:507] global step 6479: loss = 2.7586 (1.235 sec/step)\n","I0830 18:14:07.365751 139622356395904 learning.py:507] global step 6480: loss = 2.6196 (1.220 sec/step)\n","I0830 18:14:08.587618 139622356395904 learning.py:507] global step 6481: loss = 2.9657 (1.220 sec/step)\n","I0830 18:14:09.839424 139622356395904 learning.py:507] global step 6482: loss = 2.6967 (1.250 sec/step)\n","I0830 18:14:11.058965 139622356395904 learning.py:507] global step 6483: loss = 2.7524 (1.218 sec/step)\n","I0830 18:14:12.298989 139622356395904 learning.py:507] global step 6484: loss = 3.6093 (1.238 sec/step)\n","I0830 18:14:13.507432 139622356395904 learning.py:507] global step 6485: loss = 2.9599 (1.207 sec/step)\n","I0830 18:14:14.688981 139622356395904 learning.py:507] global step 6486: loss = 2.3810 (1.180 sec/step)\n","I0830 18:14:15.928266 139622356395904 learning.py:507] global step 6487: loss = 2.8115 (1.237 sec/step)\n","I0830 18:14:17.124782 139622356395904 learning.py:507] global step 6488: loss = 2.5632 (1.195 sec/step)\n","I0830 18:14:18.371544 139622356395904 learning.py:507] global step 6489: loss = 2.5496 (1.245 sec/step)\n","I0830 18:14:19.615230 139622356395904 learning.py:507] global step 6490: loss = 3.3873 (1.242 sec/step)\n","I0830 18:14:20.840924 139622356395904 learning.py:507] global step 6491: loss = 3.1896 (1.223 sec/step)\n","I0830 18:14:22.062650 139622356395904 learning.py:507] global step 6492: loss = 3.8515 (1.220 sec/step)\n","I0830 18:14:23.285840 139622356395904 learning.py:507] global step 6493: loss = 3.3847 (1.221 sec/step)\n","I0830 18:14:24.547392 139622356395904 learning.py:507] global step 6494: loss = 3.1428 (1.260 sec/step)\n","I0830 18:14:25.741477 139622356395904 learning.py:507] global step 6495: loss = 2.7438 (1.192 sec/step)\n","I0830 18:14:26.973988 139622356395904 learning.py:507] global step 6496: loss = 2.5175 (1.231 sec/step)\n","I0830 18:14:28.233239 139622356395904 learning.py:507] global step 6497: loss = 2.2790 (1.257 sec/step)\n","I0830 18:14:29.465729 139622356395904 learning.py:507] global step 6498: loss = 3.1818 (1.231 sec/step)\n","I0830 18:14:30.712423 139622356395904 learning.py:507] global step 6499: loss = 2.9846 (1.245 sec/step)\n","I0830 18:14:31.933367 139622356395904 learning.py:507] global step 6500: loss = 1.9864 (1.219 sec/step)\n","I0830 18:14:33.132899 139622356395904 learning.py:507] global step 6501: loss = 3.1723 (1.198 sec/step)\n","I0830 18:14:34.360933 139622356395904 learning.py:507] global step 6502: loss = 3.2457 (1.226 sec/step)\n","I0830 18:14:35.564943 139622356395904 learning.py:507] global step 6503: loss = 2.6553 (1.202 sec/step)\n","I0830 18:14:36.825873 139622356395904 learning.py:507] global step 6504: loss = 2.9936 (1.259 sec/step)\n","I0830 18:14:38.036855 139622356395904 learning.py:507] global step 6505: loss = 2.9349 (1.209 sec/step)\n","I0830 18:14:39.235306 139622356395904 learning.py:507] global step 6506: loss = 3.4301 (1.197 sec/step)\n","I0830 18:14:40.447741 139622356395904 learning.py:507] global step 6507: loss = 2.1897 (1.210 sec/step)\n","I0830 18:14:41.678712 139622356395904 learning.py:507] global step 6508: loss = 2.9392 (1.229 sec/step)\n","I0830 18:14:42.886402 139622356395904 learning.py:507] global step 6509: loss = 3.4953 (1.206 sec/step)\n","I0830 18:14:44.108042 139622356395904 learning.py:507] global step 6510: loss = 2.5017 (1.220 sec/step)\n","I0830 18:14:45.348994 139622356395904 learning.py:507] global step 6511: loss = 4.3311 (1.239 sec/step)\n","I0830 18:14:46.560231 139622356395904 learning.py:507] global step 6512: loss = 4.8165 (1.209 sec/step)\n","I0830 18:14:47.802088 139622356395904 learning.py:507] global step 6513: loss = 2.8786 (1.240 sec/step)\n","I0830 18:14:49.007678 139622356395904 learning.py:507] global step 6514: loss = 2.7594 (1.203 sec/step)\n","I0830 18:14:50.247953 139622356395904 learning.py:507] global step 6515: loss = 2.7169 (1.238 sec/step)\n","I0830 18:14:51.451004 139622356395904 learning.py:507] global step 6516: loss = 2.4485 (1.201 sec/step)\n","I0830 18:14:52.695997 139622356395904 learning.py:507] global step 6517: loss = 2.7803 (1.240 sec/step)\n","I0830 18:14:53.924115 139622356395904 learning.py:507] global step 6518: loss = 2.7994 (1.226 sec/step)\n","I0830 18:14:55.126897 139622356395904 learning.py:507] global step 6519: loss = 2.1645 (1.201 sec/step)\n","I0830 18:14:56.348226 139622356395904 learning.py:507] global step 6520: loss = 2.2194 (1.219 sec/step)\n","I0830 18:14:57.570945 139622356395904 learning.py:507] global step 6521: loss = 2.2680 (1.221 sec/step)\n","I0830 18:14:58.778963 139622356395904 learning.py:507] global step 6522: loss = 2.5727 (1.206 sec/step)\n","I0830 18:14:59.978622 139622356395904 learning.py:507] global step 6523: loss = 3.0291 (1.198 sec/step)\n","I0830 18:15:01.162506 139622356395904 learning.py:507] global step 6524: loss = 2.1945 (1.181 sec/step)\n","I0830 18:15:02.370183 139622356395904 learning.py:507] global step 6525: loss = 2.3836 (1.206 sec/step)\n","I0830 18:15:03.583315 139622356395904 learning.py:507] global step 6526: loss = 2.7329 (1.211 sec/step)\n","I0830 18:15:04.804696 139622356395904 learning.py:507] global step 6527: loss = 3.0864 (1.220 sec/step)\n","I0830 18:15:06.058525 139622356395904 learning.py:507] global step 6528: loss = 3.2536 (1.252 sec/step)\n","I0830 18:15:07.265183 139622356395904 learning.py:507] global step 6529: loss = 2.5901 (1.205 sec/step)\n","I0830 18:15:08.458567 139622356395904 learning.py:507] global step 6530: loss = 4.2935 (1.192 sec/step)\n","I0830 18:15:09.646752 139622356395904 learning.py:507] global step 6531: loss = 3.9090 (1.186 sec/step)\n","I0830 18:15:10.860279 139622356395904 learning.py:507] global step 6532: loss = 2.8226 (1.212 sec/step)\n","I0830 18:15:12.053603 139622356395904 learning.py:507] global step 6533: loss = 2.6822 (1.190 sec/step)\n","I0830 18:15:13.273447 139622356395904 learning.py:507] global step 6534: loss = 2.4664 (1.218 sec/step)\n","I0830 18:15:14.501096 139622356395904 learning.py:507] global step 6535: loss = 2.9530 (1.226 sec/step)\n","I0830 18:15:15.714112 139622356395904 learning.py:507] global step 6536: loss = 2.4656 (1.211 sec/step)\n","I0830 18:15:16.922734 139622356395904 learning.py:507] global step 6537: loss = 2.5745 (1.207 sec/step)\n","I0830 18:15:18.159385 139622356395904 learning.py:507] global step 6538: loss = 3.2580 (1.235 sec/step)\n","I0830 18:15:19.367854 139622356395904 learning.py:507] global step 6539: loss = 2.5291 (1.206 sec/step)\n","I0830 18:15:20.594037 139622356395904 learning.py:507] global step 6540: loss = 4.3805 (1.223 sec/step)\n","I0830 18:15:21.836547 139622356395904 learning.py:507] global step 6541: loss = 2.4767 (1.240 sec/step)\n","I0830 18:15:23.012738 139622356395904 learning.py:507] global step 6542: loss = 2.7193 (1.175 sec/step)\n","I0830 18:15:24.245527 139622356395904 learning.py:507] global step 6543: loss = 2.8105 (1.231 sec/step)\n","I0830 18:15:25.444540 139622356395904 learning.py:507] global step 6544: loss = 2.5405 (1.197 sec/step)\n","I0830 18:15:26.678543 139622356395904 learning.py:507] global step 6545: loss = 3.4037 (1.232 sec/step)\n","I0830 18:15:27.858943 139622356395904 learning.py:507] global step 6546: loss = 2.3417 (1.178 sec/step)\n","I0830 18:15:29.049507 139622356395904 learning.py:507] global step 6547: loss = 4.2327 (1.189 sec/step)\n","I0830 18:15:30.255853 139622356395904 learning.py:507] global step 6548: loss = 3.2434 (1.204 sec/step)\n","I0830 18:15:31.518723 139622356395904 learning.py:507] global step 6549: loss = 3.1651 (1.261 sec/step)\n","I0830 18:15:32.753168 139622356395904 learning.py:507] global step 6550: loss = 2.6318 (1.233 sec/step)\n","I0830 18:15:33.946241 139622356395904 learning.py:507] global step 6551: loss = 3.0108 (1.191 sec/step)\n","I0830 18:15:35.154452 139622356395904 learning.py:507] global step 6552: loss = 2.9046 (1.206 sec/step)\n","I0830 18:15:36.383596 139622356395904 learning.py:507] global step 6553: loss = 3.2691 (1.227 sec/step)\n","I0830 18:15:37.591794 139622356395904 learning.py:507] global step 6554: loss = 2.3937 (1.206 sec/step)\n","I0830 18:15:38.817561 139622356395904 learning.py:507] global step 6555: loss = 3.5514 (1.224 sec/step)\n","I0830 18:15:40.033934 139622356395904 learning.py:507] global step 6556: loss = 2.9246 (1.214 sec/step)\n","I0830 18:15:41.282331 139622356395904 learning.py:507] global step 6557: loss = 2.4220 (1.247 sec/step)\n","I0830 18:15:42.501379 139622356395904 learning.py:507] global step 6558: loss = 3.0370 (1.217 sec/step)\n","I0830 18:15:43.726616 139622356395904 learning.py:507] global step 6559: loss = 3.3016 (1.223 sec/step)\n","I0830 18:15:44.949316 139622356395904 learning.py:507] global step 6560: loss = 3.6109 (1.221 sec/step)\n","I0830 18:15:46.180426 139622356395904 learning.py:507] global step 6561: loss = 3.2794 (1.229 sec/step)\n","I0830 18:15:47.402166 139622356395904 learning.py:507] global step 6562: loss = 4.1743 (1.220 sec/step)\n","I0830 18:15:48.611869 139622356395904 learning.py:507] global step 6563: loss = 2.6834 (1.208 sec/step)\n","I0830 18:15:49.796234 139622356395904 learning.py:507] global step 6564: loss = 2.7099 (1.182 sec/step)\n","I0830 18:15:51.029343 139622356395904 learning.py:507] global step 6565: loss = 2.1674 (1.231 sec/step)\n","I0830 18:15:52.226288 139622356395904 learning.py:507] global step 6566: loss = 2.8847 (1.195 sec/step)\n","I0830 18:15:53.435713 139622356395904 learning.py:507] global step 6567: loss = 3.4218 (1.208 sec/step)\n","I0830 18:15:54.637775 139622356395904 learning.py:507] global step 6568: loss = 2.7959 (1.200 sec/step)\n","I0830 18:15:55.830776 139622356395904 learning.py:507] global step 6569: loss = 4.1813 (1.191 sec/step)\n","I0830 18:15:57.048889 139622356395904 learning.py:507] global step 6570: loss = 2.8984 (1.216 sec/step)\n","I0830 18:15:58.262697 139622356395904 learning.py:507] global step 6571: loss = 3.0534 (1.212 sec/step)\n","I0830 18:15:59.521369 139622356395904 learning.py:507] global step 6572: loss = 3.3738 (1.257 sec/step)\n","I0830 18:16:01.257137 139622356395904 learning.py:507] global step 6573: loss = 3.2590 (1.654 sec/step)\n","I0830 18:16:02.436496 139619299985152 supervisor.py:1050] Recording summary at step 6573.\n","I0830 18:16:02.945521 139622356395904 learning.py:507] global step 6574: loss = 2.8066 (1.624 sec/step)\n","I0830 18:16:04.123179 139622356395904 learning.py:507] global step 6575: loss = 2.9378 (1.176 sec/step)\n","I0830 18:16:05.314368 139622356395904 learning.py:507] global step 6576: loss = 2.6345 (1.189 sec/step)\n","I0830 18:16:06.535972 139622356395904 learning.py:507] global step 6577: loss = 3.6219 (1.219 sec/step)\n","I0830 18:16:07.764802 139622356395904 learning.py:507] global step 6578: loss = 2.4355 (1.227 sec/step)\n","I0830 18:16:08.976547 139622356395904 learning.py:507] global step 6579: loss = 2.1572 (1.208 sec/step)\n","I0830 18:16:10.200313 139622356395904 learning.py:507] global step 6580: loss = 2.2507 (1.218 sec/step)\n","I0830 18:16:11.442273 139622356395904 learning.py:507] global step 6581: loss = 3.1187 (1.240 sec/step)\n","I0830 18:16:12.675863 139622356395904 learning.py:507] global step 6582: loss = 2.2069 (1.232 sec/step)\n","I0830 18:16:13.894366 139622356395904 learning.py:507] global step 6583: loss = 2.9995 (1.217 sec/step)\n","I0830 18:16:15.142956 139622356395904 learning.py:507] global step 6584: loss = 3.1212 (1.247 sec/step)\n","I0830 18:16:16.369001 139622356395904 learning.py:507] global step 6585: loss = 2.9016 (1.224 sec/step)\n","I0830 18:16:17.569683 139622356395904 learning.py:507] global step 6586: loss = 3.1441 (1.199 sec/step)\n","I0830 18:16:18.772920 139622356395904 learning.py:507] global step 6587: loss = 2.9488 (1.201 sec/step)\n","I0830 18:16:20.022216 139622356395904 learning.py:507] global step 6588: loss = 2.5131 (1.247 sec/step)\n","I0830 18:16:21.268252 139622356395904 learning.py:507] global step 6589: loss = 2.6146 (1.244 sec/step)\n","I0830 18:16:22.530511 139622356395904 learning.py:507] global step 6590: loss = 3.2161 (1.260 sec/step)\n","I0830 18:16:23.705010 139622356395904 learning.py:507] global step 6591: loss = 2.7326 (1.173 sec/step)\n","I0830 18:16:24.924522 139622356395904 learning.py:507] global step 6592: loss = 2.6555 (1.218 sec/step)\n","I0830 18:16:26.109019 139622356395904 learning.py:507] global step 6593: loss = 3.4226 (1.182 sec/step)\n","I0830 18:16:27.338717 139622356395904 learning.py:507] global step 6594: loss = 2.5361 (1.228 sec/step)\n","I0830 18:16:28.596627 139622356395904 learning.py:507] global step 6595: loss = 2.5655 (1.256 sec/step)\n","I0830 18:16:29.799978 139622356395904 learning.py:507] global step 6596: loss = 2.5167 (1.202 sec/step)\n","I0830 18:16:30.991438 139622356395904 learning.py:507] global step 6597: loss = 2.4082 (1.189 sec/step)\n","I0830 18:16:32.216766 139622356395904 learning.py:507] global step 6598: loss = 2.5657 (1.223 sec/step)\n","I0830 18:16:33.463843 139622356395904 learning.py:507] global step 6599: loss = 3.4818 (1.245 sec/step)\n","I0830 18:16:34.677850 139622356395904 learning.py:507] global step 6600: loss = 3.1322 (1.212 sec/step)\n","I0830 18:16:35.880588 139622356395904 learning.py:507] global step 6601: loss = 2.2453 (1.201 sec/step)\n","I0830 18:16:37.073968 139622356395904 learning.py:507] global step 6602: loss = 3.4211 (1.191 sec/step)\n","I0830 18:16:38.292708 139622356395904 learning.py:507] global step 6603: loss = 2.8633 (1.217 sec/step)\n","I0830 18:16:39.490462 139622356395904 learning.py:507] global step 6604: loss = 2.5852 (1.196 sec/step)\n","I0830 18:16:40.742297 139622356395904 learning.py:507] global step 6605: loss = 4.1749 (1.250 sec/step)\n","I0830 18:16:41.975207 139622356395904 learning.py:507] global step 6606: loss = 3.3447 (1.231 sec/step)\n","I0830 18:16:43.211824 139622356395904 learning.py:507] global step 6607: loss = 2.8406 (1.235 sec/step)\n","I0830 18:16:44.417759 139622356395904 learning.py:507] global step 6608: loss = 2.7590 (1.201 sec/step)\n","I0830 18:16:45.700719 139622356395904 learning.py:507] global step 6609: loss = 2.5652 (1.281 sec/step)\n","I0830 18:16:46.951210 139622356395904 learning.py:507] global step 6610: loss = 3.0202 (1.249 sec/step)\n","I0830 18:16:48.139383 139622356395904 learning.py:507] global step 6611: loss = 2.8209 (1.186 sec/step)\n","I0830 18:16:49.435009 139622356395904 learning.py:507] global step 6612: loss = 2.7096 (1.294 sec/step)\n","I0830 18:16:50.657282 139622356395904 learning.py:507] global step 6613: loss = 3.0384 (1.221 sec/step)\n","I0830 18:16:51.854177 139622356395904 learning.py:507] global step 6614: loss = 2.7064 (1.195 sec/step)\n","I0830 18:16:53.053676 139622356395904 learning.py:507] global step 6615: loss = 2.6059 (1.198 sec/step)\n","I0830 18:16:54.252637 139622356395904 learning.py:507] global step 6616: loss = 2.4894 (1.197 sec/step)\n","I0830 18:16:55.466485 139622356395904 learning.py:507] global step 6617: loss = 2.7800 (1.212 sec/step)\n","I0830 18:16:56.651182 139622356395904 learning.py:507] global step 6618: loss = 3.0782 (1.183 sec/step)\n","I0830 18:16:57.869719 139622356395904 learning.py:507] global step 6619: loss = 2.5620 (1.217 sec/step)\n","I0830 18:16:59.047691 139622356395904 learning.py:507] global step 6620: loss = 2.9091 (1.176 sec/step)\n","I0830 18:17:00.315601 139622356395904 learning.py:507] global step 6621: loss = 3.2495 (1.266 sec/step)\n","I0830 18:17:01.529196 139622356395904 learning.py:507] global step 6622: loss = 2.5154 (1.212 sec/step)\n","I0830 18:17:02.718996 139622356395904 learning.py:507] global step 6623: loss = 2.5004 (1.188 sec/step)\n","I0830 18:17:03.961448 139622356395904 learning.py:507] global step 6624: loss = 2.3677 (1.241 sec/step)\n","I0830 18:17:05.143330 139622356395904 learning.py:507] global step 6625: loss = 3.8122 (1.180 sec/step)\n","I0830 18:17:06.351399 139622356395904 learning.py:507] global step 6626: loss = 3.0363 (1.206 sec/step)\n","I0830 18:17:07.553777 139622356395904 learning.py:507] global step 6627: loss = 2.4342 (1.201 sec/step)\n","I0830 18:17:08.759863 139622356395904 learning.py:507] global step 6628: loss = 2.3944 (1.204 sec/step)\n","I0830 18:17:09.969977 139622356395904 learning.py:507] global step 6629: loss = 2.6582 (1.208 sec/step)\n","I0830 18:17:11.177285 139622356395904 learning.py:507] global step 6630: loss = 4.1403 (1.206 sec/step)\n","I0830 18:17:12.422897 139622356395904 learning.py:507] global step 6631: loss = 2.7436 (1.244 sec/step)\n","I0830 18:17:13.620800 139622356395904 learning.py:507] global step 6632: loss = 2.6742 (1.196 sec/step)\n","I0830 18:17:14.827721 139622356395904 learning.py:507] global step 6633: loss = 3.0990 (1.205 sec/step)\n","I0830 18:17:16.044951 139622356395904 learning.py:507] global step 6634: loss = 2.2687 (1.216 sec/step)\n","I0830 18:17:17.288172 139622356395904 learning.py:507] global step 6635: loss = 3.1773 (1.242 sec/step)\n","I0830 18:17:18.530478 139622356395904 learning.py:507] global step 6636: loss = 3.6852 (1.240 sec/step)\n","I0830 18:17:19.766757 139622356395904 learning.py:507] global step 6637: loss = 2.7251 (1.234 sec/step)\n","I0830 18:17:20.971733 139622356395904 learning.py:507] global step 6638: loss = 2.9085 (1.203 sec/step)\n","I0830 18:17:22.214775 139622356395904 learning.py:507] global step 6639: loss = 2.4904 (1.241 sec/step)\n","I0830 18:17:23.439946 139622356395904 learning.py:507] global step 6640: loss = 2.6042 (1.224 sec/step)\n","I0830 18:17:24.667016 139622356395904 learning.py:507] global step 6641: loss = 3.6800 (1.225 sec/step)\n","I0830 18:17:25.913525 139622356395904 learning.py:507] global step 6642: loss = 3.9820 (1.245 sec/step)\n","I0830 18:17:27.125135 139622356395904 learning.py:507] global step 6643: loss = 3.4779 (1.210 sec/step)\n","I0830 18:17:28.348309 139622356395904 learning.py:507] global step 6644: loss = 2.7472 (1.221 sec/step)\n","I0830 18:17:29.576610 139622356395904 learning.py:507] global step 6645: loss = 2.6154 (1.226 sec/step)\n","I0830 18:17:30.833989 139622356395904 learning.py:507] global step 6646: loss = 3.1179 (1.256 sec/step)\n","I0830 18:17:32.057839 139622356395904 learning.py:507] global step 6647: loss = 3.7010 (1.222 sec/step)\n","I0830 18:17:33.310500 139622356395904 learning.py:507] global step 6648: loss = 2.5004 (1.251 sec/step)\n","I0830 18:17:34.516869 139622356395904 learning.py:507] global step 6649: loss = 3.8517 (1.204 sec/step)\n","I0830 18:17:35.719483 139622356395904 learning.py:507] global step 6650: loss = 2.5198 (1.201 sec/step)\n","I0830 18:17:36.909487 139622356395904 learning.py:507] global step 6651: loss = 2.9347 (1.188 sec/step)\n","I0830 18:17:38.175398 139622356395904 learning.py:507] global step 6652: loss = 2.6664 (1.264 sec/step)\n","I0830 18:17:39.380011 139622356395904 learning.py:507] global step 6653: loss = 2.9763 (1.203 sec/step)\n","I0830 18:17:40.605293 139622356395904 learning.py:507] global step 6654: loss = 2.8972 (1.224 sec/step)\n","I0830 18:17:41.851514 139622356395904 learning.py:507] global step 6655: loss = 2.8286 (1.244 sec/step)\n","I0830 18:17:43.081237 139622356395904 learning.py:507] global step 6656: loss = 3.4016 (1.228 sec/step)\n","I0830 18:17:44.293828 139622356395904 learning.py:507] global step 6657: loss = 3.4694 (1.210 sec/step)\n","I0830 18:17:45.492721 139622356395904 learning.py:507] global step 6658: loss = 2.3799 (1.197 sec/step)\n","I0830 18:17:46.735483 139622356395904 learning.py:507] global step 6659: loss = 2.4436 (1.241 sec/step)\n","I0830 18:17:47.940779 139622356395904 learning.py:507] global step 6660: loss = 3.7610 (1.203 sec/step)\n","I0830 18:17:49.164874 139622356395904 learning.py:507] global step 6661: loss = 2.4866 (1.222 sec/step)\n","I0830 18:17:50.406325 139622356395904 learning.py:507] global step 6662: loss = 4.9261 (1.239 sec/step)\n","I0830 18:17:51.607805 139622356395904 learning.py:507] global step 6663: loss = 2.4493 (1.200 sec/step)\n","I0830 18:17:52.785470 139622356395904 learning.py:507] global step 6664: loss = 3.1000 (1.176 sec/step)\n","I0830 18:17:53.979218 139622356395904 learning.py:507] global step 6665: loss = 2.6160 (1.192 sec/step)\n","I0830 18:17:55.190935 139622356395904 learning.py:507] global step 6666: loss = 3.3076 (1.210 sec/step)\n","I0830 18:17:56.424792 139622356395904 learning.py:507] global step 6667: loss = 2.7552 (1.232 sec/step)\n","I0830 18:17:57.674152 139622356395904 learning.py:507] global step 6668: loss = 2.3813 (1.247 sec/step)\n","I0830 18:17:58.880959 139622356395904 learning.py:507] global step 6669: loss = 3.2080 (1.205 sec/step)\n","I0830 18:18:00.257023 139622356395904 learning.py:507] global step 6670: loss = 3.6246 (1.286 sec/step)\n","I0830 18:18:01.866604 139619299985152 supervisor.py:1050] Recording summary at step 6670.\n","I0830 18:18:02.298174 139622356395904 learning.py:507] global step 6671: loss = 3.3631 (2.038 sec/step)\n","I0830 18:18:03.540225 139622356395904 learning.py:507] global step 6672: loss = 3.0499 (1.240 sec/step)\n","I0830 18:18:04.779618 139622356395904 learning.py:507] global step 6673: loss = 3.1576 (1.237 sec/step)\n","I0830 18:18:05.996563 139622356395904 learning.py:507] global step 6674: loss = 2.5069 (1.215 sec/step)\n","I0830 18:18:07.237862 139622356395904 learning.py:507] global step 6675: loss = 2.7009 (1.239 sec/step)\n","I0830 18:18:08.450753 139622356395904 learning.py:507] global step 6676: loss = 2.6150 (1.211 sec/step)\n","I0830 18:18:09.666812 139622356395904 learning.py:507] global step 6677: loss = 3.0003 (1.211 sec/step)\n","I0830 18:18:10.865178 139622356395904 learning.py:507] global step 6678: loss = 3.2729 (1.196 sec/step)\n","I0830 18:18:12.087743 139622356395904 learning.py:507] global step 6679: loss = 3.9786 (1.220 sec/step)\n","I0830 18:18:13.307932 139622356395904 learning.py:507] global step 6680: loss = 2.8770 (1.218 sec/step)\n","I0830 18:18:14.535824 139622356395904 learning.py:507] global step 6681: loss = 2.3675 (1.226 sec/step)\n","I0830 18:18:15.730811 139622356395904 learning.py:507] global step 6682: loss = 3.4516 (1.193 sec/step)\n","I0830 18:18:16.932992 139622356395904 learning.py:507] global step 6683: loss = 3.6223 (1.200 sec/step)\n","I0830 18:18:18.175583 139622356395904 learning.py:507] global step 6684: loss = 2.8888 (1.240 sec/step)\n","I0830 18:18:19.421490 139622356395904 learning.py:507] global step 6685: loss = 2.8824 (1.244 sec/step)\n","I0830 18:18:20.633152 139622356395904 learning.py:507] global step 6686: loss = 2.8772 (1.210 sec/step)\n","I0830 18:18:21.892165 139622356395904 learning.py:507] global step 6687: loss = 2.3685 (1.257 sec/step)\n","I0830 18:18:23.135940 139622356395904 learning.py:507] global step 6688: loss = 2.6847 (1.242 sec/step)\n","I0830 18:18:24.358374 139622356395904 learning.py:507] global step 6689: loss = 2.6372 (1.221 sec/step)\n","I0830 18:18:25.614000 139622356395904 learning.py:507] global step 6690: loss = 2.6833 (1.254 sec/step)\n","I0830 18:18:26.838032 139622356395904 learning.py:507] global step 6691: loss = 2.6946 (1.222 sec/step)\n","I0830 18:18:28.052934 139622356395904 learning.py:507] global step 6692: loss = 3.5211 (1.213 sec/step)\n","I0830 18:18:29.290147 139622356395904 learning.py:507] global step 6693: loss = 3.4669 (1.235 sec/step)\n","I0830 18:18:30.472121 139622356395904 learning.py:507] global step 6694: loss = 4.1693 (1.180 sec/step)\n","I0830 18:18:31.722877 139622356395904 learning.py:507] global step 6695: loss = 2.3361 (1.249 sec/step)\n","I0830 18:18:32.956677 139622356395904 learning.py:507] global step 6696: loss = 2.3967 (1.232 sec/step)\n","I0830 18:18:34.176485 139622356395904 learning.py:507] global step 6697: loss = 4.0064 (1.218 sec/step)\n","I0830 18:18:35.408446 139622356395904 learning.py:507] global step 6698: loss = 3.1193 (1.230 sec/step)\n","I0830 18:18:36.605815 139622356395904 learning.py:507] global step 6699: loss = 2.2999 (1.196 sec/step)\n","I0830 18:18:37.814361 139622356395904 learning.py:507] global step 6700: loss = 2.4160 (1.207 sec/step)\n","I0830 18:18:39.012376 139622356395904 learning.py:507] global step 6701: loss = 2.8393 (1.196 sec/step)\n","I0830 18:18:40.243638 139622356395904 learning.py:507] global step 6702: loss = 2.9466 (1.229 sec/step)\n","I0830 18:18:41.447462 139622356395904 learning.py:507] global step 6703: loss = 2.8270 (1.202 sec/step)\n","I0830 18:18:42.654262 139622356395904 learning.py:507] global step 6704: loss = 2.8108 (1.205 sec/step)\n","I0830 18:18:43.886206 139622356395904 learning.py:507] global step 6705: loss = 2.3758 (1.230 sec/step)\n","I0830 18:18:45.087240 139622356395904 learning.py:507] global step 6706: loss = 3.0903 (1.199 sec/step)\n","I0830 18:18:46.310062 139622356395904 learning.py:507] global step 6707: loss = 2.1792 (1.221 sec/step)\n","I0830 18:18:47.514395 139622356395904 learning.py:507] global step 6708: loss = 2.4086 (1.203 sec/step)\n","I0830 18:18:48.736848 139622356395904 learning.py:507] global step 6709: loss = 3.7084 (1.220 sec/step)\n","I0830 18:18:49.959648 139622356395904 learning.py:507] global step 6710: loss = 2.7286 (1.221 sec/step)\n","I0830 18:18:51.167418 139622356395904 learning.py:507] global step 6711: loss = 2.4518 (1.206 sec/step)\n","I0830 18:18:52.426010 139622356395904 learning.py:507] global step 6712: loss = 2.7426 (1.256 sec/step)\n","I0830 18:18:53.642524 139622356395904 learning.py:507] global step 6713: loss = 2.1805 (1.215 sec/step)\n","I0830 18:18:54.865106 139622356395904 learning.py:507] global step 6714: loss = 2.2939 (1.219 sec/step)\n","I0830 18:18:56.079112 139622356395904 learning.py:507] global step 6715: loss = 3.8897 (1.212 sec/step)\n","I0830 18:18:57.340176 139622356395904 learning.py:507] global step 6716: loss = 2.9196 (1.259 sec/step)\n","I0830 18:18:58.570100 139622356395904 learning.py:507] global step 6717: loss = 2.3278 (1.228 sec/step)\n","I0830 18:18:59.768710 139622356395904 learning.py:507] global step 6718: loss = 2.8914 (1.197 sec/step)\n","I0830 18:19:00.984187 139622356395904 learning.py:507] global step 6719: loss = 3.4331 (1.213 sec/step)\n","I0830 18:19:02.189687 139622356395904 learning.py:507] global step 6720: loss = 2.3050 (1.203 sec/step)\n","I0830 18:19:03.392515 139622356395904 learning.py:507] global step 6721: loss = 2.7430 (1.200 sec/step)\n","I0830 18:19:04.611688 139622356395904 learning.py:507] global step 6722: loss = 3.0333 (1.217 sec/step)\n","I0830 18:19:05.840941 139622356395904 learning.py:507] global step 6723: loss = 3.5069 (1.227 sec/step)\n","I0830 18:19:07.060213 139622356395904 learning.py:507] global step 6724: loss = 3.2079 (1.218 sec/step)\n","I0830 18:19:08.300583 139622356395904 learning.py:507] global step 6725: loss = 2.4679 (1.239 sec/step)\n","I0830 18:19:09.533337 139622356395904 learning.py:507] global step 6726: loss = 2.8301 (1.231 sec/step)\n","I0830 18:19:10.791601 139622356395904 learning.py:507] global step 6727: loss = 2.3518 (1.256 sec/step)\n","I0830 18:19:12.019912 139622356395904 learning.py:507] global step 6728: loss = 2.2728 (1.223 sec/step)\n","I0830 18:19:13.253236 139622356395904 learning.py:507] global step 6729: loss = 3.7063 (1.231 sec/step)\n","I0830 18:19:14.474091 139622356395904 learning.py:507] global step 6730: loss = 2.8786 (1.219 sec/step)\n","I0830 18:19:15.667694 139622356395904 learning.py:507] global step 6731: loss = 2.8186 (1.192 sec/step)\n","I0830 18:19:16.910139 139622356395904 learning.py:507] global step 6732: loss = 3.1237 (1.241 sec/step)\n","I0830 18:19:18.143752 139622356395904 learning.py:507] global step 6733: loss = 2.3683 (1.231 sec/step)\n","I0830 18:19:19.341149 139622356395904 learning.py:507] global step 6734: loss = 2.8915 (1.195 sec/step)\n","I0830 18:19:20.591298 139622356395904 learning.py:507] global step 6735: loss = 2.2140 (1.248 sec/step)\n","I0830 18:19:21.824953 139622356395904 learning.py:507] global step 6736: loss = 3.8507 (1.231 sec/step)\n","I0830 18:19:23.050229 139622356395904 learning.py:507] global step 6737: loss = 2.6326 (1.223 sec/step)\n","I0830 18:19:24.270762 139622356395904 learning.py:507] global step 6738: loss = 3.2840 (1.219 sec/step)\n","I0830 18:19:25.513110 139622356395904 learning.py:507] global step 6739: loss = 2.8737 (1.240 sec/step)\n","I0830 18:19:26.711253 139622356395904 learning.py:507] global step 6740: loss = 2.6497 (1.196 sec/step)\n","I0830 18:19:27.930853 139622356395904 learning.py:507] global step 6741: loss = 2.9562 (1.218 sec/step)\n","I0830 18:19:29.144876 139622356395904 learning.py:507] global step 6742: loss = 2.9978 (1.212 sec/step)\n","I0830 18:19:30.382359 139622356395904 learning.py:507] global step 6743: loss = 3.3488 (1.236 sec/step)\n","I0830 18:19:31.584324 139622356395904 learning.py:507] global step 6744: loss = 2.5473 (1.200 sec/step)\n","I0830 18:19:32.785571 139622356395904 learning.py:507] global step 6745: loss = 2.3663 (1.199 sec/step)\n","I0830 18:19:33.983245 139622356395904 learning.py:507] global step 6746: loss = 2.1494 (1.196 sec/step)\n","I0830 18:19:35.206946 139622356395904 learning.py:507] global step 6747: loss = 2.4582 (1.222 sec/step)\n","I0830 18:19:36.439681 139622356395904 learning.py:507] global step 6748: loss = 3.5108 (1.231 sec/step)\n","I0830 18:19:37.651572 139622356395904 learning.py:507] global step 6749: loss = 2.6527 (1.210 sec/step)\n","I0830 18:19:38.872787 139622356395904 learning.py:507] global step 6750: loss = 3.1025 (1.219 sec/step)\n","I0830 18:19:40.099294 139622356395904 learning.py:507] global step 6751: loss = 2.6712 (1.225 sec/step)\n","I0830 18:19:41.302587 139622356395904 learning.py:507] global step 6752: loss = 3.1056 (1.201 sec/step)\n","I0830 18:19:42.515627 139622356395904 learning.py:507] global step 6753: loss = 3.0314 (1.211 sec/step)\n","I0830 18:19:43.737234 139622356395904 learning.py:507] global step 6754: loss = 2.5935 (1.220 sec/step)\n","I0830 18:19:44.985736 139622356395904 learning.py:507] global step 6755: loss = 2.4640 (1.246 sec/step)\n","I0830 18:19:46.186438 139622356395904 learning.py:507] global step 6756: loss = 2.4340 (1.199 sec/step)\n","I0830 18:19:47.385852 139622356395904 learning.py:507] global step 6757: loss = 2.8518 (1.197 sec/step)\n","I0830 18:19:48.588406 139622356395904 learning.py:507] global step 6758: loss = 3.2054 (1.201 sec/step)\n","I0830 18:19:49.787356 139622356395904 learning.py:507] global step 6759: loss = 2.9553 (1.197 sec/step)\n","I0830 18:19:50.984672 139622356395904 learning.py:507] global step 6760: loss = 2.6356 (1.196 sec/step)\n","I0830 18:19:52.186609 139622356395904 learning.py:507] global step 6761: loss = 2.7424 (1.200 sec/step)\n","I0830 18:19:53.390436 139622356395904 learning.py:507] global step 6762: loss = 3.3131 (1.202 sec/step)\n","I0830 18:19:54.578967 139622356395904 learning.py:507] global step 6763: loss = 2.9694 (1.187 sec/step)\n","I0830 18:19:55.773497 139622356395904 learning.py:507] global step 6764: loss = 2.7059 (1.193 sec/step)\n","I0830 18:19:56.971835 139622356395904 learning.py:507] global step 6765: loss = 3.2431 (1.197 sec/step)\n","I0830 18:19:58.224497 139622356395904 learning.py:507] global step 6766: loss = 2.5205 (1.251 sec/step)\n","I0830 18:19:59.418071 139622356395904 learning.py:507] global step 6767: loss = 2.6062 (1.192 sec/step)\n","I0830 18:19:59.646550 139619316770560 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n","I0830 18:20:02.160493 139619299985152 supervisor.py:1050] Recording summary at step 6768.\n","I0830 18:20:02.185402 139622356395904 learning.py:507] global step 6768: loss = 2.9515 (2.742 sec/step)\n","I0830 18:20:03.763383 139622356395904 learning.py:507] global step 6769: loss = 3.1455 (1.560 sec/step)\n","I0830 18:20:05.297448 139622356395904 learning.py:507] global step 6770: loss = 2.8693 (1.527 sec/step)\n","I0830 18:20:06.541337 139622356395904 learning.py:507] global step 6771: loss = 2.4525 (1.242 sec/step)\n","I0830 18:20:07.755774 139622356395904 learning.py:507] global step 6772: loss = 3.0536 (1.213 sec/step)\n","I0830 18:20:08.963962 139622356395904 learning.py:507] global step 6773: loss = 2.4545 (1.205 sec/step)\n","I0830 18:20:10.154155 139622356395904 learning.py:507] global step 6774: loss = 2.6403 (1.186 sec/step)\n","I0830 18:20:11.362419 139622356395904 learning.py:507] global step 6775: loss = 3.4423 (1.206 sec/step)\n","I0830 18:20:12.573868 139622356395904 learning.py:507] global step 6776: loss = 3.0849 (1.210 sec/step)\n","I0830 18:20:13.792736 139622356395904 learning.py:507] global step 6777: loss = 2.6593 (1.217 sec/step)\n","I0830 18:20:14.982308 139622356395904 learning.py:507] global step 6778: loss = 3.1446 (1.188 sec/step)\n","I0830 18:20:16.200365 139622356395904 learning.py:507] global step 6779: loss = 2.9263 (1.216 sec/step)\n","I0830 18:20:17.432114 139622356395904 learning.py:507] global step 6780: loss = 2.6433 (1.230 sec/step)\n","I0830 18:20:18.632999 139622356395904 learning.py:507] global step 6781: loss = 2.7346 (1.199 sec/step)\n","I0830 18:20:19.841999 139622356395904 learning.py:507] global step 6782: loss = 2.7248 (1.207 sec/step)\n","I0830 18:20:21.081268 139622356395904 learning.py:507] global step 6783: loss = 2.7282 (1.237 sec/step)\n","I0830 18:20:22.313604 139622356395904 learning.py:507] global step 6784: loss = 2.5866 (1.230 sec/step)\n","I0830 18:20:23.543180 139622356395904 learning.py:507] global step 6785: loss = 2.4953 (1.228 sec/step)\n","I0830 18:20:24.755212 139622356395904 learning.py:507] global step 6786: loss = 3.2172 (1.210 sec/step)\n","I0830 18:20:25.962156 139622356395904 learning.py:507] global step 6787: loss = 2.8572 (1.205 sec/step)\n","I0830 18:20:27.188210 139622356395904 learning.py:507] global step 6788: loss = 3.4215 (1.224 sec/step)\n","I0830 18:20:28.425658 139622356395904 learning.py:507] global step 6789: loss = 3.6649 (1.235 sec/step)\n","I0830 18:20:29.650230 139622356395904 learning.py:507] global step 6790: loss = 3.2291 (1.223 sec/step)\n","I0830 18:20:30.846958 139622356395904 learning.py:507] global step 6791: loss = 2.8333 (1.195 sec/step)\n","I0830 18:20:32.047160 139622356395904 learning.py:507] global step 6792: loss = 2.4729 (1.198 sec/step)\n","I0830 18:20:33.255034 139622356395904 learning.py:507] global step 6793: loss = 3.0219 (1.206 sec/step)\n","I0830 18:20:34.474936 139622356395904 learning.py:507] global step 6794: loss = 2.2246 (1.218 sec/step)\n","I0830 18:20:35.668162 139622356395904 learning.py:507] global step 6795: loss = 2.8910 (1.191 sec/step)\n","I0830 18:20:36.873224 139622356395904 learning.py:507] global step 6796: loss = 4.0736 (1.203 sec/step)\n","I0830 18:20:38.072112 139622356395904 learning.py:507] global step 6797: loss = 2.5862 (1.197 sec/step)\n","I0830 18:20:39.282067 139622356395904 learning.py:507] global step 6798: loss = 4.8159 (1.208 sec/step)\n","I0830 18:20:40.450960 139622356395904 learning.py:507] global step 6799: loss = 2.8716 (1.167 sec/step)\n","I0830 18:20:41.696240 139622356395904 learning.py:507] global step 6800: loss = 3.1433 (1.243 sec/step)\n","I0830 18:20:42.908825 139622356395904 learning.py:507] global step 6801: loss = 2.3540 (1.211 sec/step)\n","I0830 18:20:44.105216 139622356395904 learning.py:507] global step 6802: loss = 2.7030 (1.194 sec/step)\n","I0830 18:20:45.306331 139622356395904 learning.py:507] global step 6803: loss = 3.1941 (1.199 sec/step)\n","I0830 18:20:46.492034 139622356395904 learning.py:507] global step 6804: loss = 2.6700 (1.184 sec/step)\n","I0830 18:20:47.715030 139622356395904 learning.py:507] global step 6805: loss = 2.2560 (1.221 sec/step)\n","I0830 18:20:48.928655 139622356395904 learning.py:507] global step 6806: loss = 4.6465 (1.211 sec/step)\n","I0830 18:20:50.102467 139622356395904 learning.py:507] global step 6807: loss = 2.8543 (1.172 sec/step)\n","I0830 18:20:51.313524 139622356395904 learning.py:507] global step 6808: loss = 2.6874 (1.209 sec/step)\n","I0830 18:20:52.522539 139622356395904 learning.py:507] global step 6809: loss = 2.5509 (1.207 sec/step)\n","I0830 18:20:53.695589 139622356395904 learning.py:507] global step 6810: loss = 2.6814 (1.171 sec/step)\n","I0830 18:20:54.902264 139622356395904 learning.py:507] global step 6811: loss = 3.3340 (1.205 sec/step)\n","I0830 18:20:56.099972 139622356395904 learning.py:507] global step 6812: loss = 2.2580 (1.196 sec/step)\n","I0830 18:20:57.282716 139622356395904 learning.py:507] global step 6813: loss = 2.4814 (1.181 sec/step)\n","I0830 18:20:58.473741 139622356395904 learning.py:507] global step 6814: loss = 2.5186 (1.189 sec/step)\n","I0830 18:20:59.691459 139622356395904 learning.py:507] global step 6815: loss = 2.7664 (1.216 sec/step)\n","I0830 18:21:00.881987 139622356395904 learning.py:507] global step 6816: loss = 3.8870 (1.189 sec/step)\n","I0830 18:21:02.089475 139622356395904 learning.py:507] global step 6817: loss = 3.1434 (1.206 sec/step)\n","I0830 18:21:03.301016 139622356395904 learning.py:507] global step 6818: loss = 2.3325 (1.209 sec/step)\n","I0830 18:21:04.502255 139622356395904 learning.py:507] global step 6819: loss = 3.9984 (1.199 sec/step)\n","I0830 18:21:05.720900 139622356395904 learning.py:507] global step 6820: loss = 2.1524 (1.217 sec/step)\n","I0830 18:21:06.899660 139622356395904 learning.py:507] global step 6821: loss = 3.1501 (1.177 sec/step)\n","I0830 18:21:08.086808 139622356395904 learning.py:507] global step 6822: loss = 3.9346 (1.185 sec/step)\n","I0830 18:21:09.302158 139622356395904 learning.py:507] global step 6823: loss = 2.9858 (1.214 sec/step)\n","I0830 18:21:10.508773 139622356395904 learning.py:507] global step 6824: loss = 2.8847 (1.205 sec/step)\n","I0830 18:21:11.712737 139622356395904 learning.py:507] global step 6825: loss = 2.6026 (1.202 sec/step)\n","I0830 18:21:12.895539 139622356395904 learning.py:507] global step 6826: loss = 3.7369 (1.179 sec/step)\n","I0830 18:21:14.122103 139622356395904 learning.py:507] global step 6827: loss = 4.2797 (1.223 sec/step)\n","I0830 18:21:15.327938 139622356395904 learning.py:507] global step 6828: loss = 2.4789 (1.204 sec/step)\n","I0830 18:21:16.582739 139622356395904 learning.py:507] global step 6829: loss = 3.1682 (1.253 sec/step)\n","I0830 18:21:17.785434 139622356395904 learning.py:507] global step 6830: loss = 2.8156 (1.201 sec/step)\n","I0830 18:21:19.037698 139622356395904 learning.py:507] global step 6831: loss = 3.0114 (1.250 sec/step)\n","I0830 18:21:20.237915 139622356395904 learning.py:507] global step 6832: loss = 2.6575 (1.198 sec/step)\n","I0830 18:21:21.459749 139622356395904 learning.py:507] global step 6833: loss = 2.4174 (1.220 sec/step)\n","I0830 18:21:22.682334 139622356395904 learning.py:507] global step 6834: loss = 3.7022 (1.221 sec/step)\n","I0830 18:21:23.884301 139622356395904 learning.py:507] global step 6835: loss = 2.7040 (1.200 sec/step)\n","I0830 18:21:25.120558 139622356395904 learning.py:507] global step 6836: loss = 2.7622 (1.235 sec/step)\n","I0830 18:21:26.347379 139622356395904 learning.py:507] global step 6837: loss = 4.3204 (1.225 sec/step)\n","I0830 18:21:27.569319 139622356395904 learning.py:507] global step 6838: loss = 2.3673 (1.220 sec/step)\n","I0830 18:21:28.800329 139622356395904 learning.py:507] global step 6839: loss = 3.0929 (1.229 sec/step)\n","I0830 18:21:30.003729 139622356395904 learning.py:507] global step 6840: loss = 3.0872 (1.202 sec/step)\n","I0830 18:21:31.214813 139622356395904 learning.py:507] global step 6841: loss = 3.3055 (1.209 sec/step)\n","I0830 18:21:32.481192 139622356395904 learning.py:507] global step 6842: loss = 2.9237 (1.264 sec/step)\n","I0830 18:21:33.672837 139622356395904 learning.py:507] global step 6843: loss = 3.2571 (1.190 sec/step)\n","I0830 18:21:34.872039 139622356395904 learning.py:507] global step 6844: loss = 2.3030 (1.197 sec/step)\n","I0830 18:21:36.079921 139622356395904 learning.py:507] global step 6845: loss = 3.4384 (1.206 sec/step)\n","I0830 18:21:37.322680 139622356395904 learning.py:507] global step 6846: loss = 3.0755 (1.240 sec/step)\n","I0830 18:21:38.567012 139622356395904 learning.py:507] global step 6847: loss = 2.5417 (1.243 sec/step)\n","I0830 18:21:39.752157 139622356395904 learning.py:507] global step 6848: loss = 2.8806 (1.184 sec/step)\n","I0830 18:21:40.963844 139622356395904 learning.py:507] global step 6849: loss = 3.6443 (1.210 sec/step)\n","I0830 18:21:42.187484 139622356395904 learning.py:507] global step 6850: loss = 2.5370 (1.222 sec/step)\n","I0830 18:21:43.414723 139622356395904 learning.py:507] global step 6851: loss = 3.1422 (1.225 sec/step)\n","I0830 18:21:44.616253 139622356395904 learning.py:507] global step 6852: loss = 2.8674 (1.200 sec/step)\n","I0830 18:21:45.814507 139622356395904 learning.py:507] global step 6853: loss = 2.7123 (1.196 sec/step)\n","I0830 18:21:47.027500 139622356395904 learning.py:507] global step 6854: loss = 2.1642 (1.211 sec/step)\n","I0830 18:21:48.215279 139622356395904 learning.py:507] global step 6855: loss = 3.6870 (1.186 sec/step)\n","I0830 18:21:49.447434 139622356395904 learning.py:507] global step 6856: loss = 3.1210 (1.230 sec/step)\n","I0830 18:21:50.674484 139622356395904 learning.py:507] global step 6857: loss = 2.5561 (1.225 sec/step)\n","I0830 18:21:51.897825 139622356395904 learning.py:507] global step 6858: loss = 2.2183 (1.221 sec/step)\n","I0830 18:21:53.121969 139622356395904 learning.py:507] global step 6859: loss = 2.5517 (1.222 sec/step)\n","I0830 18:21:54.304798 139622356395904 learning.py:507] global step 6860: loss = 3.3649 (1.181 sec/step)\n","I0830 18:21:55.528539 139622356395904 learning.py:507] global step 6861: loss = 3.1317 (1.222 sec/step)\n","I0830 18:21:56.763983 139622356395904 learning.py:507] global step 6862: loss = 2.4847 (1.234 sec/step)\n","I0830 18:21:57.970272 139622356395904 learning.py:507] global step 6863: loss = 3.3492 (1.205 sec/step)\n","I0830 18:21:59.180625 139622356395904 learning.py:507] global step 6864: loss = 3.7669 (1.208 sec/step)\n","I0830 18:22:00.610100 139622356395904 learning.py:507] global step 6865: loss = 3.9192 (1.424 sec/step)\n","I0830 18:22:02.421526 139619299985152 supervisor.py:1050] Recording summary at step 6866.\n","I0830 18:22:02.458502 139622356395904 learning.py:507] global step 6866: loss = 2.1871 (1.840 sec/step)\n","I0830 18:22:03.687267 139622356395904 learning.py:507] global step 6867: loss = 3.0776 (1.227 sec/step)\n","I0830 18:22:04.865352 139622356395904 learning.py:507] global step 6868: loss = 2.5263 (1.176 sec/step)\n","I0830 18:22:06.064042 139622356395904 learning.py:507] global step 6869: loss = 3.1348 (1.196 sec/step)\n","I0830 18:22:07.288189 139622356395904 learning.py:507] global step 6870: loss = 2.7536 (1.222 sec/step)\n","I0830 18:22:08.471391 139622356395904 learning.py:507] global step 6871: loss = 2.9867 (1.182 sec/step)\n","I0830 18:22:09.679635 139622356395904 learning.py:507] global step 6872: loss = 2.7864 (1.207 sec/step)\n","I0830 18:22:10.883138 139622356395904 learning.py:507] global step 6873: loss = 2.2908 (1.202 sec/step)\n","I0830 18:22:12.079256 139622356395904 learning.py:507] global step 6874: loss = 2.4758 (1.194 sec/step)\n","I0830 18:22:13.289685 139622356395904 learning.py:507] global step 6875: loss = 3.1444 (1.209 sec/step)\n","I0830 18:22:14.520000 139622356395904 learning.py:507] global step 6876: loss = 2.8593 (1.229 sec/step)\n","I0830 18:22:15.769161 139622356395904 learning.py:507] global step 6877: loss = 3.0045 (1.247 sec/step)\n","I0830 18:22:16.984759 139622356395904 learning.py:507] global step 6878: loss = 2.4682 (1.214 sec/step)\n","I0830 18:22:18.215861 139622356395904 learning.py:507] global step 6879: loss = 2.9178 (1.229 sec/step)\n","I0830 18:22:19.458341 139622356395904 learning.py:507] global step 6880: loss = 2.1157 (1.241 sec/step)\n","I0830 18:22:20.674595 139622356395904 learning.py:507] global step 6881: loss = 2.5505 (1.215 sec/step)\n","I0830 18:22:21.911472 139622356395904 learning.py:507] global step 6882: loss = 3.0016 (1.234 sec/step)\n","I0830 18:22:23.149415 139622356395904 learning.py:507] global step 6883: loss = 2.8558 (1.236 sec/step)\n","I0830 18:22:24.360309 139622356395904 learning.py:507] global step 6884: loss = 3.2249 (1.209 sec/step)\n","I0830 18:22:25.618754 139622356395904 learning.py:507] global step 6885: loss = 2.4342 (1.256 sec/step)\n","I0830 18:22:26.838076 139622356395904 learning.py:507] global step 6886: loss = 3.0190 (1.217 sec/step)\n","I0830 18:22:28.094518 139622356395904 learning.py:507] global step 6887: loss = 2.7560 (1.253 sec/step)\n","I0830 18:22:29.316861 139622356395904 learning.py:507] global step 6888: loss = 3.3357 (1.220 sec/step)\n","I0830 18:22:30.499446 139622356395904 learning.py:507] global step 6889: loss = 2.8598 (1.181 sec/step)\n","I0830 18:22:31.719345 139622356395904 learning.py:507] global step 6890: loss = 2.1013 (1.218 sec/step)\n","I0830 18:22:32.936402 139622356395904 learning.py:507] global step 6891: loss = 2.5334 (1.215 sec/step)\n","I0830 18:22:34.132005 139622356395904 learning.py:507] global step 6892: loss = 2.9885 (1.194 sec/step)\n","I0830 18:22:35.346195 139622356395904 learning.py:507] global step 6893: loss = 2.5919 (1.212 sec/step)\n","I0830 18:22:36.553520 139622356395904 learning.py:507] global step 6894: loss = 3.1480 (1.206 sec/step)\n","I0830 18:22:37.787515 139622356395904 learning.py:507] global step 6895: loss = 2.9205 (1.232 sec/step)\n","I0830 18:22:39.028312 139622356395904 learning.py:507] global step 6896: loss = 2.6608 (1.239 sec/step)\n","I0830 18:22:40.271476 139622356395904 learning.py:507] global step 6897: loss = 2.8702 (1.242 sec/step)\n","I0830 18:22:41.463156 139622356395904 learning.py:507] global step 6898: loss = 2.7269 (1.190 sec/step)\n","I0830 18:22:42.690304 139622356395904 learning.py:507] global step 6899: loss = 3.6273 (1.225 sec/step)\n","I0830 18:22:43.877110 139622356395904 learning.py:507] global step 6900: loss = 3.1456 (1.185 sec/step)\n","I0830 18:22:45.059587 139622356395904 learning.py:507] global step 6901: loss = 3.7562 (1.181 sec/step)\n","I0830 18:22:46.302381 139622356395904 learning.py:507] global step 6902: loss = 3.8816 (1.241 sec/step)\n","I0830 18:22:47.509418 139622356395904 learning.py:507] global step 6903: loss = 2.0785 (1.205 sec/step)\n","I0830 18:22:48.725552 139622356395904 learning.py:507] global step 6904: loss = 2.8646 (1.214 sec/step)\n","I0830 18:22:49.958545 139622356395904 learning.py:507] global step 6905: loss = 2.7122 (1.231 sec/step)\n","I0830 18:22:51.191847 139622356395904 learning.py:507] global step 6906: loss = 3.0432 (1.231 sec/step)\n","I0830 18:22:52.403508 139622356395904 learning.py:507] global step 6907: loss = 2.6574 (1.210 sec/step)\n","I0830 18:22:53.605684 139622356395904 learning.py:507] global step 6908: loss = 3.7909 (1.200 sec/step)\n","I0830 18:22:54.810876 139622356395904 learning.py:507] global step 6909: loss = 3.6900 (1.203 sec/step)\n","I0830 18:22:56.023322 139622356395904 learning.py:507] global step 6910: loss = 3.0286 (1.210 sec/step)\n","I0830 18:22:57.250291 139622356395904 learning.py:507] global step 6911: loss = 3.1129 (1.225 sec/step)\n","I0830 18:22:58.466780 139622356395904 learning.py:507] global step 6912: loss = 2.5945 (1.215 sec/step)\n","I0830 18:22:59.710378 139622356395904 learning.py:507] global step 6913: loss = 2.5405 (1.242 sec/step)\n","I0830 18:23:00.923760 139622356395904 learning.py:507] global step 6914: loss = 3.1837 (1.211 sec/step)\n","I0830 18:23:02.146803 139622356395904 learning.py:507] global step 6915: loss = 2.3012 (1.221 sec/step)\n","I0830 18:23:03.376880 139622356395904 learning.py:507] global step 6916: loss = 2.6276 (1.228 sec/step)\n","I0830 18:23:04.577219 139622356395904 learning.py:507] global step 6917: loss = 2.8526 (1.199 sec/step)\n","I0830 18:23:05.779003 139622356395904 learning.py:507] global step 6918: loss = 4.1915 (1.197 sec/step)\n","I0830 18:23:07.013699 139622356395904 learning.py:507] global step 6919: loss = 2.6805 (1.233 sec/step)\n","I0830 18:23:08.254137 139622356395904 learning.py:507] global step 6920: loss = 2.7182 (1.237 sec/step)\n","I0830 18:23:09.439702 139622356395904 learning.py:507] global step 6921: loss = 3.0524 (1.184 sec/step)\n","I0830 18:23:10.659823 139622356395904 learning.py:507] global step 6922: loss = 2.9306 (1.218 sec/step)\n","I0830 18:23:11.861074 139622356395904 learning.py:507] global step 6923: loss = 3.3086 (1.199 sec/step)\n","I0830 18:23:13.064894 139622356395904 learning.py:507] global step 6924: loss = 2.9960 (1.202 sec/step)\n","I0830 18:23:14.269842 139622356395904 learning.py:507] global step 6925: loss = 2.3127 (1.203 sec/step)\n","I0830 18:23:15.455847 139622356395904 learning.py:507] global step 6926: loss = 2.5862 (1.184 sec/step)\n","I0830 18:23:16.693153 139622356395904 learning.py:507] global step 6927: loss = 2.0550 (1.235 sec/step)\n","I0830 18:23:17.864397 139622356395904 learning.py:507] global step 6928: loss = 3.0222 (1.169 sec/step)\n","I0830 18:23:19.106281 139622356395904 learning.py:507] global step 6929: loss = 2.3098 (1.240 sec/step)\n","I0830 18:23:20.370808 139622356395904 learning.py:507] global step 6930: loss = 2.8480 (1.263 sec/step)\n","I0830 18:23:21.617579 139622356395904 learning.py:507] global step 6931: loss = 2.0787 (1.245 sec/step)\n","I0830 18:23:22.877630 139622356395904 learning.py:507] global step 6932: loss = 1.9660 (1.258 sec/step)\n","I0830 18:23:24.099337 139622356395904 learning.py:507] global step 6933: loss = 3.2413 (1.220 sec/step)\n","I0830 18:23:25.323281 139622356395904 learning.py:507] global step 6934: loss = 2.4912 (1.222 sec/step)\n","I0830 18:23:26.520072 139622356395904 learning.py:507] global step 6935: loss = 2.8299 (1.195 sec/step)\n","I0830 18:23:27.734273 139622356395904 learning.py:507] global step 6936: loss = 3.2145 (1.212 sec/step)\n","I0830 18:23:28.945271 139622356395904 learning.py:507] global step 6937: loss = 2.5399 (1.209 sec/step)\n","I0830 18:23:30.164647 139622356395904 learning.py:507] global step 6938: loss = 2.1610 (1.217 sec/step)\n","I0830 18:23:31.373229 139622356395904 learning.py:507] global step 6939: loss = 2.7427 (1.207 sec/step)\n","I0830 18:23:32.584363 139622356395904 learning.py:507] global step 6940: loss = 3.1560 (1.209 sec/step)\n","I0830 18:23:33.826587 139622356395904 learning.py:507] global step 6941: loss = 2.2961 (1.240 sec/step)\n","I0830 18:23:35.029787 139622356395904 learning.py:507] global step 6942: loss = 2.1752 (1.201 sec/step)\n","I0830 18:23:36.246089 139622356395904 learning.py:507] global step 6943: loss = 2.4658 (1.214 sec/step)\n","I0830 18:23:37.431364 139622356395904 learning.py:507] global step 6944: loss = 2.5163 (1.184 sec/step)\n","I0830 18:23:38.614614 139622356395904 learning.py:507] global step 6945: loss = 3.5916 (1.181 sec/step)\n","I0830 18:23:39.828762 139622356395904 learning.py:507] global step 6946: loss = 2.5401 (1.212 sec/step)\n","I0830 18:23:41.039212 139622356395904 learning.py:507] global step 6947: loss = 2.4857 (1.209 sec/step)\n","I0830 18:23:42.237448 139622356395904 learning.py:507] global step 6948: loss = 3.2331 (1.197 sec/step)\n","I0830 18:23:43.472949 139622356395904 learning.py:507] global step 6949: loss = 4.0879 (1.234 sec/step)\n","I0830 18:23:44.654371 139622356395904 learning.py:507] global step 6950: loss = 3.0069 (1.179 sec/step)\n","I0830 18:23:45.875909 139622356395904 learning.py:507] global step 6951: loss = 2.6070 (1.220 sec/step)\n","I0830 18:23:47.109411 139622356395904 learning.py:507] global step 6952: loss = 2.0767 (1.232 sec/step)\n","I0830 18:23:48.332994 139622356395904 learning.py:507] global step 6953: loss = 2.7560 (1.222 sec/step)\n","I0830 18:23:49.545703 139622356395904 learning.py:507] global step 6954: loss = 2.6420 (1.211 sec/step)\n","I0830 18:23:50.775847 139622356395904 learning.py:507] global step 6955: loss = 2.3208 (1.228 sec/step)\n","I0830 18:23:51.975952 139622356395904 learning.py:507] global step 6956: loss = 2.4104 (1.198 sec/step)\n","I0830 18:23:53.231869 139622356395904 learning.py:507] global step 6957: loss = 3.7060 (1.254 sec/step)\n","I0830 18:23:54.466908 139622356395904 learning.py:507] global step 6958: loss = 3.4449 (1.233 sec/step)\n","I0830 18:23:55.688320 139622356395904 learning.py:507] global step 6959: loss = 2.9379 (1.218 sec/step)\n","I0830 18:23:56.905884 139622356395904 learning.py:507] global step 6960: loss = 2.3031 (1.215 sec/step)\n","I0830 18:23:58.132003 139622356395904 learning.py:507] global step 6961: loss = 3.0046 (1.224 sec/step)\n","I0830 18:23:59.322377 139622356395904 learning.py:507] global step 6962: loss = 2.9299 (1.188 sec/step)\n","I0830 18:24:00.578569 139622356395904 learning.py:507] global step 6963: loss = 3.1991 (1.245 sec/step)\n","I0830 18:24:02.655686 139619299985152 supervisor.py:1050] Recording summary at step 6964.\n","I0830 18:24:02.673257 139622356395904 learning.py:507] global step 6964: loss = 3.0684 (2.067 sec/step)\n","I0830 18:24:03.895536 139622356395904 learning.py:507] global step 6965: loss = 3.0494 (1.215 sec/step)\n","I0830 18:24:05.066074 139622356395904 learning.py:507] global step 6966: loss = 3.6586 (1.169 sec/step)\n","I0830 18:24:06.261010 139622356395904 learning.py:507] global step 6967: loss = 3.7975 (1.193 sec/step)\n","I0830 18:24:07.479661 139622356395904 learning.py:507] global step 6968: loss = 2.1292 (1.217 sec/step)\n","I0830 18:24:08.705332 139622356395904 learning.py:507] global step 6969: loss = 2.9644 (1.224 sec/step)\n","I0830 18:24:09.933156 139622356395904 learning.py:507] global step 6970: loss = 2.0899 (1.226 sec/step)\n","I0830 18:24:11.130455 139622356395904 learning.py:507] global step 6971: loss = 2.5022 (1.195 sec/step)\n","I0830 18:24:12.314174 139622356395904 learning.py:507] global step 6972: loss = 3.9589 (1.182 sec/step)\n","I0830 18:24:13.550418 139622356395904 learning.py:507] global step 6973: loss = 2.4083 (1.235 sec/step)\n","I0830 18:24:14.775799 139622356395904 learning.py:507] global step 6974: loss = 2.6502 (1.223 sec/step)\n","I0830 18:24:15.994422 139622356395904 learning.py:507] global step 6975: loss = 4.7023 (1.216 sec/step)\n","I0830 18:24:17.211758 139622356395904 learning.py:507] global step 6976: loss = 3.2174 (1.215 sec/step)\n","I0830 18:24:18.399317 139622356395904 learning.py:507] global step 6977: loss = 2.6608 (1.186 sec/step)\n","I0830 18:24:19.645017 139622356395904 learning.py:507] global step 6978: loss = 2.8116 (1.244 sec/step)\n","I0830 18:24:20.874963 139622356395904 learning.py:507] global step 6979: loss = 3.6051 (1.228 sec/step)\n","I0830 18:24:22.099007 139622356395904 learning.py:507] global step 6980: loss = 2.3170 (1.222 sec/step)\n","I0830 18:24:23.305815 139622356395904 learning.py:507] global step 6981: loss = 3.1524 (1.205 sec/step)\n","I0830 18:24:24.571934 139622356395904 learning.py:507] global step 6982: loss = 3.3617 (1.264 sec/step)\n","I0830 18:24:25.778248 139622356395904 learning.py:507] global step 6983: loss = 2.5114 (1.204 sec/step)\n","I0830 18:24:26.985003 139622356395904 learning.py:507] global step 6984: loss = 2.2968 (1.205 sec/step)\n","I0830 18:24:28.208979 139622356395904 learning.py:507] global step 6985: loss = 2.8019 (1.222 sec/step)\n","I0830 18:24:29.426511 139622356395904 learning.py:507] global step 6986: loss = 2.3259 (1.216 sec/step)\n","I0830 18:24:30.660797 139622356395904 learning.py:507] global step 6987: loss = 2.7174 (1.232 sec/step)\n","I0830 18:24:31.868494 139622356395904 learning.py:507] global step 6988: loss = 2.9173 (1.205 sec/step)\n","I0830 18:24:33.083451 139622356395904 learning.py:507] global step 6989: loss = 2.5880 (1.213 sec/step)\n","I0830 18:24:34.315087 139622356395904 learning.py:507] global step 6990: loss = 3.8782 (1.230 sec/step)\n","I0830 18:24:35.513811 139622356395904 learning.py:507] global step 6991: loss = 2.9568 (1.197 sec/step)\n","I0830 18:24:36.745409 139622356395904 learning.py:507] global step 6992: loss = 2.8391 (1.230 sec/step)\n","I0830 18:24:37.984278 139622356395904 learning.py:507] global step 6993: loss = 2.5563 (1.237 sec/step)\n","I0830 18:24:39.176153 139622356395904 learning.py:507] global step 6994: loss = 2.9710 (1.190 sec/step)\n","I0830 18:24:40.357129 139622356395904 learning.py:507] global step 6995: loss = 3.5319 (1.179 sec/step)\n","I0830 18:24:41.528122 139622356395904 learning.py:507] global step 6996: loss = 3.0685 (1.169 sec/step)\n","I0830 18:24:42.726735 139622356395904 learning.py:507] global step 6997: loss = 2.5630 (1.197 sec/step)\n","I0830 18:24:43.938179 139622356395904 learning.py:507] global step 6998: loss = 3.2756 (1.210 sec/step)\n","I0830 18:24:45.154422 139622356395904 learning.py:507] global step 6999: loss = 2.5799 (1.214 sec/step)\n","I0830 18:24:46.353469 139622356395904 learning.py:507] global step 7000: loss = 3.6671 (1.197 sec/step)\n","I0830 18:24:47.537616 139622356395904 learning.py:507] global step 7001: loss = 3.8408 (1.182 sec/step)\n","I0830 18:24:48.749226 139622356395904 learning.py:507] global step 7002: loss = 2.3390 (1.210 sec/step)\n","I0830 18:24:49.969329 139622356395904 learning.py:507] global step 7003: loss = 3.3332 (1.218 sec/step)\n","I0830 18:24:51.202522 139622356395904 learning.py:507] global step 7004: loss = 3.7433 (1.231 sec/step)\n","I0830 18:24:52.396629 139622356395904 learning.py:507] global step 7005: loss = 2.5894 (1.192 sec/step)\n","I0830 18:24:53.636624 139622356395904 learning.py:507] global step 7006: loss = 2.4759 (1.238 sec/step)\n","I0830 18:24:54.874089 139622356395904 learning.py:507] global step 7007: loss = 4.0150 (1.236 sec/step)\n","I0830 18:24:56.109261 139622356395904 learning.py:507] global step 7008: loss = 3.1343 (1.233 sec/step)\n","I0830 18:24:57.330408 139622356395904 learning.py:507] global step 7009: loss = 2.7263 (1.219 sec/step)\n","I0830 18:24:58.567215 139622356395904 learning.py:507] global step 7010: loss = 3.0608 (1.235 sec/step)\n","I0830 18:24:59.786000 139622356395904 learning.py:507] global step 7011: loss = 3.4172 (1.217 sec/step)\n","I0830 18:25:00.996418 139622356395904 learning.py:507] global step 7012: loss = 3.4009 (1.208 sec/step)\n","I0830 18:25:02.231984 139622356395904 learning.py:507] global step 7013: loss = 3.6243 (1.234 sec/step)\n","I0830 18:25:03.463016 139622356395904 learning.py:507] global step 7014: loss = 2.4307 (1.229 sec/step)\n","I0830 18:25:04.674519 139622356395904 learning.py:507] global step 7015: loss = 2.8322 (1.210 sec/step)\n","I0830 18:25:05.901182 139622356395904 learning.py:507] global step 7016: loss = 2.5886 (1.225 sec/step)\n","I0830 18:25:07.119732 139622356395904 learning.py:507] global step 7017: loss = 3.5779 (1.217 sec/step)\n","I0830 18:25:08.370521 139622356395904 learning.py:507] global step 7018: loss = 3.4272 (1.249 sec/step)\n","I0830 18:25:09.612364 139622356395904 learning.py:507] global step 7019: loss = 2.2441 (1.240 sec/step)\n","I0830 18:25:10.800888 139622356395904 learning.py:507] global step 7020: loss = 2.5712 (1.186 sec/step)\n","I0830 18:25:11.998538 139622356395904 learning.py:507] global step 7021: loss = 2.5476 (1.196 sec/step)\n","I0830 18:25:13.218827 139622356395904 learning.py:507] global step 7022: loss = 2.9604 (1.218 sec/step)\n","I0830 18:25:14.442076 139622356395904 learning.py:507] global step 7023: loss = 2.9049 (1.221 sec/step)\n","I0830 18:25:15.669580 139622356395904 learning.py:507] global step 7024: loss = 2.4784 (1.226 sec/step)\n","I0830 18:25:16.910958 139622356395904 learning.py:507] global step 7025: loss = 4.4029 (1.240 sec/step)\n","I0830 18:25:18.113622 139622356395904 learning.py:507] global step 7026: loss = 3.7888 (1.201 sec/step)\n","I0830 18:25:19.327240 139622356395904 learning.py:507] global step 7027: loss = 2.4317 (1.212 sec/step)\n","I0830 18:25:20.542677 139622356395904 learning.py:507] global step 7028: loss = 2.7143 (1.213 sec/step)\n","I0830 18:25:21.787711 139622356395904 learning.py:507] global step 7029: loss = 2.4843 (1.243 sec/step)\n","I0830 18:25:23.013904 139622356395904 learning.py:507] global step 7030: loss = 2.2090 (1.224 sec/step)\n","I0830 18:25:24.251173 139622356395904 learning.py:507] global step 7031: loss = 2.2033 (1.232 sec/step)\n","I0830 18:25:25.476269 139622356395904 learning.py:507] global step 7032: loss = 2.8017 (1.222 sec/step)\n","I0830 18:25:26.668337 139622356395904 learning.py:507] global step 7033: loss = 3.2548 (1.190 sec/step)\n","I0830 18:25:27.890600 139622356395904 learning.py:507] global step 7034: loss = 2.8729 (1.221 sec/step)\n","I0830 18:25:29.090258 139622356395904 learning.py:507] global step 7035: loss = 3.4159 (1.198 sec/step)\n","I0830 18:25:30.342578 139622356395904 learning.py:507] global step 7036: loss = 2.3408 (1.250 sec/step)\n","I0830 18:25:31.542846 139622356395904 learning.py:507] global step 7037: loss = 3.3383 (1.198 sec/step)\n","I0830 18:25:32.776867 139622356395904 learning.py:507] global step 7038: loss = 3.7507 (1.232 sec/step)\n","I0830 18:25:33.991991 139622356395904 learning.py:507] global step 7039: loss = 3.2703 (1.213 sec/step)\n","I0830 18:25:35.177839 139622356395904 learning.py:507] global step 7040: loss = 2.4422 (1.184 sec/step)\n","I0830 18:25:36.390554 139622356395904 learning.py:507] global step 7041: loss = 2.7687 (1.208 sec/step)\n","I0830 18:25:37.641005 139622356395904 learning.py:507] global step 7042: loss = 4.6406 (1.246 sec/step)\n","I0830 18:25:38.864946 139622356395904 learning.py:507] global step 7043: loss = 3.7507 (1.222 sec/step)\n","I0830 18:25:40.094777 139622356395904 learning.py:507] global step 7044: loss = 2.9419 (1.228 sec/step)\n","I0830 18:25:41.286966 139622356395904 learning.py:507] global step 7045: loss = 3.8105 (1.190 sec/step)\n","I0830 18:25:42.532383 139622356395904 learning.py:507] global step 7046: loss = 2.5632 (1.244 sec/step)\n","I0830 18:25:43.758507 139622356395904 learning.py:507] global step 7047: loss = 2.7590 (1.224 sec/step)\n","I0830 18:25:44.940974 139622356395904 learning.py:507] global step 7048: loss = 3.0970 (1.181 sec/step)\n","I0830 18:25:46.192283 139622356395904 learning.py:507] global step 7049: loss = 3.5529 (1.249 sec/step)\n","I0830 18:25:47.416755 139622356395904 learning.py:507] global step 7050: loss = 2.6459 (1.223 sec/step)\n","I0830 18:25:48.611515 139622356395904 learning.py:507] global step 7051: loss = 2.6104 (1.193 sec/step)\n","I0830 18:25:49.828249 139622356395904 learning.py:507] global step 7052: loss = 2.5868 (1.214 sec/step)\n","I0830 18:25:51.059208 139622356395904 learning.py:507] global step 7053: loss = 2.6109 (1.229 sec/step)\n","I0830 18:25:52.249016 139622356395904 learning.py:507] global step 7054: loss = 2.8146 (1.188 sec/step)\n","I0830 18:25:53.445030 139622356395904 learning.py:507] global step 7055: loss = 3.6346 (1.194 sec/step)\n","I0830 18:25:54.667833 139622356395904 learning.py:507] global step 7056: loss = 2.7945 (1.221 sec/step)\n","I0830 18:25:55.870728 139622356395904 learning.py:507] global step 7057: loss = 3.2561 (1.201 sec/step)\n","I0830 18:25:57.062315 139622356395904 learning.py:507] global step 7058: loss = 3.2536 (1.190 sec/step)\n","I0830 18:25:58.243040 139622356395904 learning.py:507] global step 7059: loss = 2.2554 (1.179 sec/step)\n","I0830 18:25:59.455256 139622356395904 learning.py:507] global step 7060: loss = 3.3731 (1.210 sec/step)\n","I0830 18:26:01.460986 139622356395904 learning.py:507] global step 7061: loss = 2.4073 (1.874 sec/step)\n","I0830 18:26:02.126221 139619299985152 supervisor.py:1050] Recording summary at step 7061.\n","I0830 18:26:02.715619 139622356395904 learning.py:507] global step 7062: loss = 2.3684 (1.253 sec/step)\n","I0830 18:26:03.958607 139622356395904 learning.py:507] global step 7063: loss = 2.1319 (1.241 sec/step)\n","I0830 18:26:05.190266 139622356395904 learning.py:507] global step 7064: loss = 2.6728 (1.230 sec/step)\n","I0830 18:26:06.447203 139622356395904 learning.py:507] global step 7065: loss = 2.4408 (1.255 sec/step)\n","I0830 18:26:07.649603 139622356395904 learning.py:507] global step 7066: loss = 2.9410 (1.200 sec/step)\n","I0830 18:26:08.863757 139622356395904 learning.py:507] global step 7067: loss = 2.5502 (1.212 sec/step)\n","I0830 18:26:10.071486 139622356395904 learning.py:507] global step 7068: loss = 2.9054 (1.206 sec/step)\n","I0830 18:26:11.287160 139622356395904 learning.py:507] global step 7069: loss = 3.3681 (1.214 sec/step)\n","I0830 18:26:12.482826 139622356395904 learning.py:507] global step 7070: loss = 3.1093 (1.194 sec/step)\n","I0830 18:26:13.716165 139622356395904 learning.py:507] global step 7071: loss = 2.3869 (1.230 sec/step)\n","I0830 18:26:14.963039 139622356395904 learning.py:507] global step 7072: loss = 2.3634 (1.242 sec/step)\n","I0830 18:26:16.190391 139622356395904 learning.py:507] global step 7073: loss = 2.4082 (1.226 sec/step)\n","I0830 18:26:17.414407 139622356395904 learning.py:507] global step 7074: loss = 2.3903 (1.222 sec/step)\n","I0830 18:26:18.619488 139622356395904 learning.py:507] global step 7075: loss = 2.3590 (1.201 sec/step)\n","I0830 18:26:19.834599 139622356395904 learning.py:507] global step 7076: loss = 2.6941 (1.213 sec/step)\n","I0830 18:26:21.048553 139622356395904 learning.py:507] global step 7077: loss = 3.2833 (1.212 sec/step)\n","I0830 18:26:22.286575 139622356395904 learning.py:507] global step 7078: loss = 2.4436 (1.236 sec/step)\n","I0830 18:26:23.509587 139622356395904 learning.py:507] global step 7079: loss = 1.9078 (1.221 sec/step)\n","I0830 18:26:24.754710 139622356395904 learning.py:507] global step 7080: loss = 2.2251 (1.243 sec/step)\n","I0830 18:26:26.022253 139622356395904 learning.py:507] global step 7081: loss = 3.3513 (1.266 sec/step)\n","I0830 18:26:27.231950 139622356395904 learning.py:507] global step 7082: loss = 3.1773 (1.208 sec/step)\n","I0830 18:26:28.449305 139622356395904 learning.py:507] global step 7083: loss = 2.3749 (1.215 sec/step)\n","I0830 18:26:29.668284 139622356395904 learning.py:507] global step 7084: loss = 2.8502 (1.217 sec/step)\n","I0830 18:26:30.868405 139622356395904 learning.py:507] global step 7085: loss = 3.2947 (1.198 sec/step)\n","I0830 18:26:32.091446 139622356395904 learning.py:507] global step 7086: loss = 2.6945 (1.221 sec/step)\n","I0830 18:26:33.335472 139622356395904 learning.py:507] global step 7087: loss = 2.4088 (1.242 sec/step)\n","I0830 18:26:34.563171 139622356395904 learning.py:507] global step 7088: loss = 2.7990 (1.226 sec/step)\n","I0830 18:26:35.784803 139622356395904 learning.py:507] global step 7089: loss = 4.0882 (1.220 sec/step)\n","I0830 18:26:37.006758 139622356395904 learning.py:507] global step 7090: loss = 2.3441 (1.220 sec/step)\n","I0830 18:26:38.242709 139622356395904 learning.py:507] global step 7091: loss = 2.7808 (1.234 sec/step)\n","I0830 18:26:39.436338 139622356395904 learning.py:507] global step 7092: loss = 3.3254 (1.192 sec/step)\n","I0830 18:26:40.648270 139622356395904 learning.py:507] global step 7093: loss = 2.9296 (1.210 sec/step)\n","I0830 18:26:41.845237 139622356395904 learning.py:507] global step 7094: loss = 2.7924 (1.195 sec/step)\n","I0830 18:26:43.085834 139622356395904 learning.py:507] global step 7095: loss = 5.4777 (1.239 sec/step)\n","I0830 18:26:44.324206 139622356395904 learning.py:507] global step 7096: loss = 4.7086 (1.236 sec/step)\n","I0830 18:26:45.555482 139622356395904 learning.py:507] global step 7097: loss = 3.1468 (1.229 sec/step)\n","I0830 18:26:46.790478 139622356395904 learning.py:507] global step 7098: loss = 3.0347 (1.233 sec/step)\n","I0830 18:26:47.975034 139622356395904 learning.py:507] global step 7099: loss = 2.8916 (1.183 sec/step)\n","I0830 18:26:49.169892 139622356395904 learning.py:507] global step 7100: loss = 2.4678 (1.193 sec/step)\n","I0830 18:26:50.395895 139622356395904 learning.py:507] global step 7101: loss = 2.2630 (1.224 sec/step)\n","I0830 18:26:51.645434 139622356395904 learning.py:507] global step 7102: loss = 3.0051 (1.248 sec/step)\n","I0830 18:26:52.925857 139622356395904 learning.py:507] global step 7103: loss = 4.1321 (1.279 sec/step)\n","I0830 18:26:54.179401 139622356395904 learning.py:507] global step 7104: loss = 3.5679 (1.252 sec/step)\n","I0830 18:26:55.448420 139622356395904 learning.py:507] global step 7105: loss = 3.2301 (1.267 sec/step)\n","I0830 18:26:56.676616 139622356395904 learning.py:507] global step 7106: loss = 3.9386 (1.226 sec/step)\n","I0830 18:26:57.906697 139622356395904 learning.py:507] global step 7107: loss = 2.3631 (1.228 sec/step)\n","I0830 18:26:59.128183 139622356395904 learning.py:507] global step 7108: loss = 2.7016 (1.219 sec/step)\n","I0830 18:27:00.322641 139622356395904 learning.py:507] global step 7109: loss = 2.2941 (1.192 sec/step)\n","I0830 18:27:01.548095 139622356395904 learning.py:507] global step 7110: loss = 4.6833 (1.223 sec/step)\n","I0830 18:27:02.750334 139622356395904 learning.py:507] global step 7111: loss = 3.7152 (1.200 sec/step)\n","I0830 18:27:03.960844 139622356395904 learning.py:507] global step 7112: loss = 3.2868 (1.209 sec/step)\n","I0830 18:27:05.165069 139622356395904 learning.py:507] global step 7113: loss = 3.3858 (1.202 sec/step)\n","I0830 18:27:06.376875 139622356395904 learning.py:507] global step 7114: loss = 2.6287 (1.210 sec/step)\n","I0830 18:27:07.572963 139622356395904 learning.py:507] global step 7115: loss = 2.8267 (1.194 sec/step)\n","I0830 18:27:08.783779 139622356395904 learning.py:507] global step 7116: loss = 2.7963 (1.209 sec/step)\n","I0830 18:27:10.033496 139622356395904 learning.py:507] global step 7117: loss = 3.0359 (1.248 sec/step)\n","I0830 18:27:11.235780 139622356395904 learning.py:507] global step 7118: loss = 2.9065 (1.200 sec/step)\n","I0830 18:27:12.439163 139622356395904 learning.py:507] global step 7119: loss = 2.0061 (1.201 sec/step)\n","I0830 18:27:13.682953 139622356395904 learning.py:507] global step 7120: loss = 2.5101 (1.242 sec/step)\n","I0830 18:27:14.882493 139622356395904 learning.py:507] global step 7121: loss = 2.5485 (1.198 sec/step)\n","I0830 18:27:16.113992 139622356395904 learning.py:507] global step 7122: loss = 3.5704 (1.229 sec/step)\n","I0830 18:27:17.366514 139622356395904 learning.py:507] global step 7123: loss = 2.2174 (1.251 sec/step)\n","I0830 18:27:18.610865 139622356395904 learning.py:507] global step 7124: loss = 3.5757 (1.242 sec/step)\n","I0830 18:27:19.811108 139622356395904 learning.py:507] global step 7125: loss = 3.1507 (1.198 sec/step)\n","I0830 18:27:21.041460 139622356395904 learning.py:507] global step 7126: loss = 2.7150 (1.228 sec/step)\n","I0830 18:27:22.281714 139622356395904 learning.py:507] global step 7127: loss = 2.8087 (1.238 sec/step)\n","I0830 18:27:23.538039 139622356395904 learning.py:507] global step 7128: loss = 2.4914 (1.255 sec/step)\n","I0830 18:27:24.817208 139622356395904 learning.py:507] global step 7129: loss = 2.7525 (1.277 sec/step)\n","I0830 18:27:26.061710 139622356395904 learning.py:507] global step 7130: loss = 2.5879 (1.243 sec/step)\n","I0830 18:27:27.271391 139622356395904 learning.py:507] global step 7131: loss = 3.5353 (1.208 sec/step)\n","I0830 18:27:28.461592 139622356395904 learning.py:507] global step 7132: loss = 2.6221 (1.188 sec/step)\n","I0830 18:27:29.682363 139622356395904 learning.py:507] global step 7133: loss = 3.5090 (1.219 sec/step)\n","I0830 18:27:30.903672 139622356395904 learning.py:507] global step 7134: loss = 2.4235 (1.218 sec/step)\n","I0830 18:27:32.112205 139622356395904 learning.py:507] global step 7135: loss = 2.8551 (1.207 sec/step)\n","I0830 18:27:33.370093 139622356395904 learning.py:507] global step 7136: loss = 3.5237 (1.256 sec/step)\n","I0830 18:27:34.599774 139622356395904 learning.py:507] global step 7137: loss = 2.7655 (1.228 sec/step)\n","I0830 18:27:35.811813 139622356395904 learning.py:507] global step 7138: loss = 3.2298 (1.210 sec/step)\n","I0830 18:27:37.030882 139622356395904 learning.py:507] global step 7139: loss = 3.9706 (1.217 sec/step)\n","I0830 18:27:38.311190 139622356395904 learning.py:507] global step 7140: loss = 3.2957 (1.278 sec/step)\n","I0830 18:27:39.535438 139622356395904 learning.py:507] global step 7141: loss = 2.1500 (1.222 sec/step)\n","I0830 18:27:40.750201 139622356395904 learning.py:507] global step 7142: loss = 3.1447 (1.213 sec/step)\n","I0830 18:27:41.975254 139622356395904 learning.py:507] global step 7143: loss = 2.4132 (1.223 sec/step)\n","I0830 18:27:43.201092 139622356395904 learning.py:507] global step 7144: loss = 3.3564 (1.224 sec/step)\n","I0830 18:27:44.433467 139622356395904 learning.py:507] global step 7145: loss = 2.7401 (1.230 sec/step)\n","I0830 18:27:45.665116 139622356395904 learning.py:507] global step 7146: loss = 2.3489 (1.230 sec/step)\n","I0830 18:27:46.859120 139622356395904 learning.py:507] global step 7147: loss = 2.4847 (1.192 sec/step)\n","I0830 18:27:48.099398 139622356395904 learning.py:507] global step 7148: loss = 2.5876 (1.239 sec/step)\n","I0830 18:27:49.329543 139622356395904 learning.py:507] global step 7149: loss = 2.7433 (1.228 sec/step)\n","I0830 18:27:50.562418 139622356395904 learning.py:507] global step 7150: loss = 3.1974 (1.231 sec/step)\n","I0830 18:27:51.760192 139622356395904 learning.py:507] global step 7151: loss = 3.7211 (1.196 sec/step)\n","I0830 18:27:53.002696 139622356395904 learning.py:507] global step 7152: loss = 2.7534 (1.241 sec/step)\n","I0830 18:27:54.217710 139622356395904 learning.py:507] global step 7153: loss = 2.2070 (1.213 sec/step)\n","I0830 18:27:55.489465 139622356395904 learning.py:507] global step 7154: loss = 2.3100 (1.270 sec/step)\n","I0830 18:27:56.698354 139622356395904 learning.py:507] global step 7155: loss = 4.3451 (1.207 sec/step)\n","I0830 18:27:57.907381 139622356395904 learning.py:507] global step 7156: loss = 3.9106 (1.207 sec/step)\n","I0830 18:27:59.105854 139622356395904 learning.py:507] global step 7157: loss = 2.7981 (1.196 sec/step)\n","I0830 18:28:00.403194 139622356395904 learning.py:507] global step 7158: loss = 2.7745 (1.296 sec/step)\n","I0830 18:28:02.301715 139622356395904 learning.py:507] global step 7159: loss = 2.5604 (1.814 sec/step)\n","I0830 18:28:02.555211 139619299985152 supervisor.py:1050] Recording summary at step 7159.\n","I0830 18:28:03.529765 139622356395904 learning.py:507] global step 7160: loss = 2.8488 (1.226 sec/step)\n","I0830 18:28:04.771821 139622356395904 learning.py:507] global step 7161: loss = 2.6255 (1.240 sec/step)\n","I0830 18:28:06.007351 139622356395904 learning.py:507] global step 7162: loss = 3.9574 (1.234 sec/step)\n","I0830 18:28:07.203711 139622356395904 learning.py:507] global step 7163: loss = 2.8932 (1.194 sec/step)\n","I0830 18:28:08.424101 139622356395904 learning.py:507] global step 7164: loss = 2.1981 (1.219 sec/step)\n","I0830 18:28:09.660742 139622356395904 learning.py:507] global step 7165: loss = 2.5544 (1.235 sec/step)\n","I0830 18:28:10.851258 139622356395904 learning.py:507] global step 7166: loss = 2.1138 (1.189 sec/step)\n","I0830 18:28:12.067561 139622356395904 learning.py:507] global step 7167: loss = 2.0309 (1.214 sec/step)\n","I0830 18:28:13.326400 139622356395904 learning.py:507] global step 7168: loss = 2.7682 (1.256 sec/step)\n","I0830 18:28:14.537419 139622356395904 learning.py:507] global step 7169: loss = 3.2456 (1.208 sec/step)\n","I0830 18:28:15.787530 139622356395904 learning.py:507] global step 7170: loss = 2.3849 (1.248 sec/step)\n","I0830 18:28:17.017145 139622356395904 learning.py:507] global step 7171: loss = 3.0525 (1.228 sec/step)\n","I0830 18:28:18.229408 139622356395904 learning.py:507] global step 7172: loss = 3.5542 (1.210 sec/step)\n","I0830 18:28:19.458837 139622356395904 learning.py:507] global step 7173: loss = 2.0903 (1.227 sec/step)\n","I0830 18:28:20.693483 139622356395904 learning.py:507] global step 7174: loss = 3.6107 (1.232 sec/step)\n","I0830 18:28:21.871544 139622356395904 learning.py:507] global step 7175: loss = 2.2748 (1.176 sec/step)\n","I0830 18:28:23.091130 139622356395904 learning.py:507] global step 7176: loss = 2.3784 (1.218 sec/step)\n","I0830 18:28:24.345593 139622356395904 learning.py:507] global step 7177: loss = 2.5396 (1.252 sec/step)\n","I0830 18:28:25.573920 139622356395904 learning.py:507] global step 7178: loss = 3.8138 (1.227 sec/step)\n","I0830 18:28:26.814674 139622356395904 learning.py:507] global step 7179: loss = 2.3897 (1.239 sec/step)\n","I0830 18:28:28.081658 139622356395904 learning.py:507] global step 7180: loss = 3.0254 (1.265 sec/step)\n","I0830 18:28:29.307475 139622356395904 learning.py:507] global step 7181: loss = 2.2700 (1.224 sec/step)\n","I0830 18:28:30.528815 139622356395904 learning.py:507] global step 7182: loss = 2.5536 (1.219 sec/step)\n","I0830 18:28:31.757871 139622356395904 learning.py:507] global step 7183: loss = 3.1413 (1.227 sec/step)\n","I0830 18:28:32.945478 139622356395904 learning.py:507] global step 7184: loss = 3.2240 (1.186 sec/step)\n","I0830 18:28:34.180562 139622356395904 learning.py:507] global step 7185: loss = 2.5188 (1.233 sec/step)\n","I0830 18:28:35.427285 139622356395904 learning.py:507] global step 7186: loss = 3.0226 (1.245 sec/step)\n","I0830 18:28:36.654363 139622356395904 learning.py:507] global step 7187: loss = 3.0081 (1.225 sec/step)\n","I0830 18:28:37.866326 139622356395904 learning.py:507] global step 7188: loss = 3.3007 (1.210 sec/step)\n","I0830 18:28:39.074776 139622356395904 learning.py:507] global step 7189: loss = 2.7092 (1.205 sec/step)\n","I0830 18:28:40.319682 139622356395904 learning.py:507] global step 7190: loss = 2.8980 (1.242 sec/step)\n","I0830 18:28:41.527092 139622356395904 learning.py:507] global step 7191: loss = 2.9496 (1.206 sec/step)\n","I0830 18:28:42.789633 139622356395904 learning.py:507] global step 7192: loss = 2.6832 (1.261 sec/step)\n","I0830 18:28:44.011802 139622356395904 learning.py:507] global step 7193: loss = 3.0390 (1.220 sec/step)\n","I0830 18:28:45.212528 139622356395904 learning.py:507] global step 7194: loss = 2.8634 (1.199 sec/step)\n","I0830 18:28:46.434938 139622356395904 learning.py:507] global step 7195: loss = 3.1300 (1.221 sec/step)\n","I0830 18:28:47.666649 139622356395904 learning.py:507] global step 7196: loss = 2.6909 (1.230 sec/step)\n","I0830 18:28:48.888400 139622356395904 learning.py:507] global step 7197: loss = 2.6418 (1.220 sec/step)\n","I0830 18:28:50.117997 139622356395904 learning.py:507] global step 7198: loss = 3.1507 (1.228 sec/step)\n","I0830 18:28:51.342575 139622356395904 learning.py:507] global step 7199: loss = 3.5042 (1.223 sec/step)\n","I0830 18:28:52.584398 139622356395904 learning.py:507] global step 7200: loss = 2.8465 (1.240 sec/step)\n","I0830 18:28:53.778968 139622356395904 learning.py:507] global step 7201: loss = 2.7582 (1.192 sec/step)\n","I0830 18:28:55.038374 139622356395904 learning.py:507] global step 7202: loss = 3.0008 (1.258 sec/step)\n","I0830 18:28:56.263486 139622356395904 learning.py:507] global step 7203: loss = 3.8064 (1.223 sec/step)\n","I0830 18:28:57.480551 139622356395904 learning.py:507] global step 7204: loss = 3.3361 (1.215 sec/step)\n","I0830 18:28:58.704434 139622356395904 learning.py:507] global step 7205: loss = 3.2032 (1.222 sec/step)\n","I0830 18:28:59.932239 139622356395904 learning.py:507] global step 7206: loss = 2.8657 (1.226 sec/step)\n","I0830 18:29:01.133171 139622356395904 learning.py:507] global step 7207: loss = 2.9889 (1.199 sec/step)\n","I0830 18:29:02.378216 139622356395904 learning.py:507] global step 7208: loss = 2.4209 (1.243 sec/step)\n","I0830 18:29:03.634324 139622356395904 learning.py:507] global step 7209: loss = 2.5546 (1.254 sec/step)\n","I0830 18:29:04.836596 139622356395904 learning.py:507] global step 7210: loss = 2.3284 (1.200 sec/step)\n","I0830 18:29:06.038315 139622356395904 learning.py:507] global step 7211: loss = 1.9475 (1.199 sec/step)\n","I0830 18:29:07.289839 139622356395904 learning.py:507] global step 7212: loss = 3.1876 (1.250 sec/step)\n","I0830 18:29:08.532564 139622356395904 learning.py:507] global step 7213: loss = 2.6923 (1.241 sec/step)\n","I0830 18:29:09.781489 139622356395904 learning.py:507] global step 7214: loss = 2.7308 (1.247 sec/step)\n","I0830 18:29:10.999988 139622356395904 learning.py:507] global step 7215: loss = 3.0545 (1.216 sec/step)\n","I0830 18:29:12.209764 139622356395904 learning.py:507] global step 7216: loss = 2.3319 (1.208 sec/step)\n","I0830 18:29:13.425584 139622356395904 learning.py:507] global step 7217: loss = 3.9191 (1.214 sec/step)\n","I0830 18:29:14.670728 139622356395904 learning.py:507] global step 7218: loss = 3.0028 (1.242 sec/step)\n","I0830 18:29:15.907755 139622356395904 learning.py:507] global step 7219: loss = 2.3081 (1.234 sec/step)\n","I0830 18:29:17.128419 139622356395904 learning.py:507] global step 7220: loss = 2.8609 (1.219 sec/step)\n","I0830 18:29:18.344026 139622356395904 learning.py:507] global step 7221: loss = 2.6328 (1.214 sec/step)\n","I0830 18:29:19.555899 139622356395904 learning.py:507] global step 7222: loss = 2.8187 (1.210 sec/step)\n","I0830 18:29:20.773998 139622356395904 learning.py:507] global step 7223: loss = 3.6095 (1.216 sec/step)\n","I0830 18:29:22.012324 139622356395904 learning.py:507] global step 7224: loss = 2.5347 (1.236 sec/step)\n","I0830 18:29:23.257595 139622356395904 learning.py:507] global step 7225: loss = 2.4726 (1.243 sec/step)\n","I0830 18:29:24.486221 139622356395904 learning.py:507] global step 7226: loss = 3.8406 (1.227 sec/step)\n","I0830 18:29:25.721761 139622356395904 learning.py:507] global step 7227: loss = 2.2627 (1.234 sec/step)\n","I0830 18:29:26.975239 139622356395904 learning.py:507] global step 7228: loss = 3.3115 (1.251 sec/step)\n","I0830 18:29:28.219125 139622356395904 learning.py:507] global step 7229: loss = 3.2001 (1.242 sec/step)\n","I0830 18:29:29.467351 139622356395904 learning.py:507] global step 7230: loss = 3.2655 (1.246 sec/step)\n","I0830 18:29:30.676162 139622356395904 learning.py:507] global step 7231: loss = 3.1252 (1.207 sec/step)\n","I0830 18:29:31.944841 139622356395904 learning.py:507] global step 7232: loss = 2.6767 (1.267 sec/step)\n","I0830 18:29:33.162212 139622356395904 learning.py:507] global step 7233: loss = 2.9108 (1.215 sec/step)\n","I0830 18:29:34.416970 139622356395904 learning.py:507] global step 7234: loss = 2.8546 (1.250 sec/step)\n","I0830 18:29:35.621664 139622356395904 learning.py:507] global step 7235: loss = 2.6218 (1.203 sec/step)\n","I0830 18:29:36.859006 139622356395904 learning.py:507] global step 7236: loss = 3.2384 (1.235 sec/step)\n","I0830 18:29:38.076115 139622356395904 learning.py:507] global step 7237: loss = 2.2291 (1.215 sec/step)\n","I0830 18:29:39.288306 139622356395904 learning.py:507] global step 7238: loss = 2.4826 (1.210 sec/step)\n","I0830 18:29:40.521271 139622356395904 learning.py:507] global step 7239: loss = 2.7995 (1.231 sec/step)\n","I0830 18:29:41.736223 139622356395904 learning.py:507] global step 7240: loss = 5.0276 (1.213 sec/step)\n","I0830 18:29:42.932645 139622356395904 learning.py:507] global step 7241: loss = 2.2924 (1.194 sec/step)\n","I0830 18:29:44.188316 139622356395904 learning.py:507] global step 7242: loss = 2.6552 (1.254 sec/step)\n","I0830 18:29:45.404280 139622356395904 learning.py:507] global step 7243: loss = 4.0322 (1.214 sec/step)\n","I0830 18:29:46.651930 139622356395904 learning.py:507] global step 7244: loss = 2.2734 (1.243 sec/step)\n","I0830 18:29:47.871934 139622356395904 learning.py:507] global step 7245: loss = 3.0533 (1.218 sec/step)\n","I0830 18:29:49.113948 139622356395904 learning.py:507] global step 7246: loss = 2.9008 (1.240 sec/step)\n","I0830 18:29:50.348882 139622356395904 learning.py:507] global step 7247: loss = 2.6480 (1.233 sec/step)\n","I0830 18:29:51.543004 139622356395904 learning.py:507] global step 7248: loss = 2.2818 (1.192 sec/step)\n","I0830 18:29:52.781119 139622356395904 learning.py:507] global step 7249: loss = 2.2160 (1.236 sec/step)\n","I0830 18:29:53.986299 139622356395904 learning.py:507] global step 7250: loss = 2.0910 (1.203 sec/step)\n","I0830 18:29:55.214207 139622356395904 learning.py:507] global step 7251: loss = 2.8240 (1.226 sec/step)\n","I0830 18:29:56.433945 139622356395904 learning.py:507] global step 7252: loss = 2.0054 (1.218 sec/step)\n","I0830 18:29:57.618079 139622356395904 learning.py:507] global step 7253: loss = 2.7247 (1.182 sec/step)\n","I0830 18:29:58.816723 139622356395904 learning.py:507] global step 7254: loss = 2.2331 (1.197 sec/step)\n","I0830 18:29:59.646660 139619316770560 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n","I0830 18:30:00.348494 139622356395904 learning.py:507] global step 7255: loss = 2.7427 (1.528 sec/step)\n","I0830 18:30:03.190125 139622356395904 learning.py:507] global step 7256: loss = 2.5713 (2.833 sec/step)\n","I0830 18:30:03.254309 139619299985152 supervisor.py:1050] Recording summary at step 7256.\n","I0830 18:30:04.848540 139622356395904 learning.py:507] global step 7257: loss = 2.8188 (1.525 sec/step)\n","I0830 18:30:06.078517 139622356395904 learning.py:507] global step 7258: loss = 3.8932 (1.227 sec/step)\n","I0830 18:30:07.316183 139622356395904 learning.py:507] global step 7259: loss = 2.9746 (1.235 sec/step)\n","I0830 18:30:08.533329 139622356395904 learning.py:507] global step 7260: loss = 3.3949 (1.215 sec/step)\n","I0830 18:30:09.757715 139622356395904 learning.py:507] global step 7261: loss = 2.5270 (1.223 sec/step)\n","I0830 18:30:10.970822 139622356395904 learning.py:507] global step 7262: loss = 3.5914 (1.211 sec/step)\n","I0830 18:30:12.228305 139622356395904 learning.py:507] global step 7263: loss = 3.3276 (1.255 sec/step)\n","I0830 18:30:13.487669 139622356395904 learning.py:507] global step 7264: loss = 2.8652 (1.258 sec/step)\n","I0830 18:30:14.744071 139622356395904 learning.py:507] global step 7265: loss = 2.5150 (1.254 sec/step)\n","I0830 18:30:16.026952 139622356395904 learning.py:507] global step 7266: loss = 2.6136 (1.279 sec/step)\n","I0830 18:30:17.216713 139622356395904 learning.py:507] global step 7267: loss = 2.4593 (1.187 sec/step)\n","I0830 18:30:18.490670 139622356395904 learning.py:507] global step 7268: loss = 2.9841 (1.272 sec/step)\n","I0830 18:30:19.760450 139622356395904 learning.py:507] global step 7269: loss = 2.3531 (1.268 sec/step)\n","I0830 18:30:20.974910 139622356395904 learning.py:507] global step 7270: loss = 3.1041 (1.213 sec/step)\n","I0830 18:30:22.233268 139622356395904 learning.py:507] global step 7271: loss = 2.6268 (1.257 sec/step)\n","I0830 18:30:23.466863 139622356395904 learning.py:507] global step 7272: loss = 3.1250 (1.232 sec/step)\n","I0830 18:30:24.686248 139622356395904 learning.py:507] global step 7273: loss = 2.8343 (1.218 sec/step)\n","I0830 18:30:25.917898 139622356395904 learning.py:507] global step 7274: loss = 2.8183 (1.229 sec/step)\n","I0830 18:30:27.162327 139622356395904 learning.py:507] global step 7275: loss = 2.8760 (1.243 sec/step)\n","I0830 18:30:28.374463 139622356395904 learning.py:507] global step 7276: loss = 2.7423 (1.210 sec/step)\n","I0830 18:30:29.595236 139622356395904 learning.py:507] global step 7277: loss = 3.0802 (1.219 sec/step)\n","I0830 18:30:30.831544 139622356395904 learning.py:507] global step 7278: loss = 2.6203 (1.235 sec/step)\n","I0830 18:30:32.064394 139622356395904 learning.py:507] global step 7279: loss = 3.2867 (1.231 sec/step)\n","I0830 18:30:33.314319 139622356395904 learning.py:507] global step 7280: loss = 3.6133 (1.248 sec/step)\n","I0830 18:30:34.549090 139622356395904 learning.py:507] global step 7281: loss = 2.5220 (1.233 sec/step)\n","I0830 18:30:35.762581 139622356395904 learning.py:507] global step 7282: loss = 3.4594 (1.212 sec/step)\n","I0830 18:30:37.023576 139622356395904 learning.py:507] global step 7283: loss = 2.5496 (1.259 sec/step)\n","I0830 18:30:38.261449 139622356395904 learning.py:507] global step 7284: loss = 3.7202 (1.236 sec/step)\n","I0830 18:30:39.503736 139622356395904 learning.py:507] global step 7285: loss = 2.3405 (1.240 sec/step)\n","I0830 18:30:40.729190 139622356395904 learning.py:507] global step 7286: loss = 3.5440 (1.224 sec/step)\n","I0830 18:30:41.948970 139622356395904 learning.py:507] global step 7287: loss = 3.4060 (1.218 sec/step)\n","I0830 18:30:43.183636 139622356395904 learning.py:507] global step 7288: loss = 2.1457 (1.233 sec/step)\n","I0830 18:30:44.392514 139622356395904 learning.py:507] global step 7289: loss = 2.2786 (1.207 sec/step)\n","I0830 18:30:45.626000 139622356395904 learning.py:507] global step 7290: loss = 2.8618 (1.232 sec/step)\n","I0830 18:30:46.834965 139622356395904 learning.py:507] global step 7291: loss = 2.9082 (1.207 sec/step)\n","I0830 18:30:48.054252 139622356395904 learning.py:507] global step 7292: loss = 2.2278 (1.217 sec/step)\n","I0830 18:30:49.278381 139622356395904 learning.py:507] global step 7293: loss = 2.8067 (1.222 sec/step)\n","I0830 18:30:50.524673 139622356395904 learning.py:507] global step 7294: loss = 2.5903 (1.244 sec/step)\n","I0830 18:30:51.756410 139622356395904 learning.py:507] global step 7295: loss = 2.4772 (1.230 sec/step)\n","I0830 18:30:52.954343 139622356395904 learning.py:507] global step 7296: loss = 2.7931 (1.195 sec/step)\n","I0830 18:30:54.175356 139622356395904 learning.py:507] global step 7297: loss = 3.6837 (1.219 sec/step)\n","I0830 18:30:55.381713 139622356395904 learning.py:507] global step 7298: loss = 3.0436 (1.204 sec/step)\n","I0830 18:30:56.633518 139622356395904 learning.py:507] global step 7299: loss = 2.2122 (1.250 sec/step)\n","I0830 18:30:57.853643 139622356395904 learning.py:507] global step 7300: loss = 3.0464 (1.218 sec/step)\n","I0830 18:30:59.071444 139622356395904 learning.py:507] global step 7301: loss = 3.0683 (1.216 sec/step)\n","I0830 18:31:00.320800 139622356395904 learning.py:507] global step 7302: loss = 4.0131 (1.247 sec/step)\n","I0830 18:31:01.567557 139622356395904 learning.py:507] global step 7303: loss = 3.6638 (1.245 sec/step)\n","I0830 18:31:02.771385 139622356395904 learning.py:507] global step 7304: loss = 2.8685 (1.202 sec/step)\n","I0830 18:31:04.026622 139622356395904 learning.py:507] global step 7305: loss = 2.6025 (1.253 sec/step)\n","I0830 18:31:05.252153 139622356395904 learning.py:507] global step 7306: loss = 2.6907 (1.224 sec/step)\n","I0830 18:31:06.464472 139622356395904 learning.py:507] global step 7307: loss = 3.3019 (1.210 sec/step)\n","I0830 18:31:07.672112 139622356395904 learning.py:507] global step 7308: loss = 3.2068 (1.206 sec/step)\n","I0830 18:31:08.910733 139622356395904 learning.py:507] global step 7309: loss = 2.8454 (1.237 sec/step)\n","I0830 18:31:10.134874 139622356395904 learning.py:507] global step 7310: loss = 4.0503 (1.222 sec/step)\n","I0830 18:31:11.395261 139622356395904 learning.py:507] global step 7311: loss = 2.5220 (1.259 sec/step)\n","I0830 18:31:12.634376 139622356395904 learning.py:507] global step 7312: loss = 2.8320 (1.237 sec/step)\n","I0830 18:31:13.878305 139622356395904 learning.py:507] global step 7313: loss = 3.1242 (1.242 sec/step)\n","I0830 18:31:15.089239 139622356395904 learning.py:507] global step 7314: loss = 2.6928 (1.209 sec/step)\n","I0830 18:31:16.325717 139622356395904 learning.py:507] global step 7315: loss = 2.2846 (1.235 sec/step)\n","I0830 18:31:17.574594 139622356395904 learning.py:507] global step 7316: loss = 2.6872 (1.247 sec/step)\n","I0830 18:31:18.792683 139622356395904 learning.py:507] global step 7317: loss = 3.1065 (1.216 sec/step)\n","I0830 18:31:20.025689 139622356395904 learning.py:507] global step 7318: loss = 2.9827 (1.231 sec/step)\n","I0830 18:31:21.278962 139622356395904 learning.py:507] global step 7319: loss = 4.3008 (1.251 sec/step)\n","I0830 18:31:22.538926 139622356395904 learning.py:507] global step 7320: loss = 2.6079 (1.258 sec/step)\n","I0830 18:31:23.791176 139622356395904 learning.py:507] global step 7321: loss = 2.3804 (1.250 sec/step)\n","I0830 18:31:25.029283 139622356395904 learning.py:507] global step 7322: loss = 3.0387 (1.232 sec/step)\n","I0830 18:31:26.243955 139622356395904 learning.py:507] global step 7323: loss = 3.4320 (1.212 sec/step)\n","I0830 18:31:27.501243 139622356395904 learning.py:507] global step 7324: loss = 3.3314 (1.255 sec/step)\n","I0830 18:31:28.727786 139622356395904 learning.py:507] global step 7325: loss = 2.4887 (1.225 sec/step)\n","I0830 18:31:29.986206 139622356395904 learning.py:507] global step 7326: loss = 3.5903 (1.256 sec/step)\n","I0830 18:31:31.216147 139622356395904 learning.py:507] global step 7327: loss = 2.3262 (1.228 sec/step)\n","I0830 18:31:32.461489 139622356395904 learning.py:507] global step 7328: loss = 4.1935 (1.243 sec/step)\n","I0830 18:31:33.684988 139622356395904 learning.py:507] global step 7329: loss = 2.6929 (1.221 sec/step)\n","I0830 18:31:34.936377 139622356395904 learning.py:507] global step 7330: loss = 2.2497 (1.249 sec/step)\n","I0830 18:31:36.181947 139622356395904 learning.py:507] global step 7331: loss = 2.4839 (1.244 sec/step)\n","I0830 18:31:37.405431 139622356395904 learning.py:507] global step 7332: loss = 2.4423 (1.219 sec/step)\n","I0830 18:31:38.641099 139622356395904 learning.py:507] global step 7333: loss = 2.3864 (1.233 sec/step)\n","I0830 18:31:39.880723 139622356395904 learning.py:507] global step 7334: loss = 2.5937 (1.238 sec/step)\n","I0830 18:31:41.123336 139622356395904 learning.py:507] global step 7335: loss = 2.7855 (1.241 sec/step)\n","I0830 18:31:42.314667 139622356395904 learning.py:507] global step 7336: loss = 4.0782 (1.189 sec/step)\n","I0830 18:31:43.571301 139622356395904 learning.py:507] global step 7337: loss = 2.0710 (1.255 sec/step)\n","I0830 18:31:44.792295 139622356395904 learning.py:507] global step 7338: loss = 3.1190 (1.218 sec/step)\n","I0830 18:31:46.001580 139622356395904 learning.py:507] global step 7339: loss = 2.5556 (1.207 sec/step)\n","I0830 18:31:47.220224 139622356395904 learning.py:507] global step 7340: loss = 2.9454 (1.217 sec/step)\n","I0830 18:31:48.418760 139622356395904 learning.py:507] global step 7341: loss = 2.1846 (1.197 sec/step)\n","I0830 18:31:49.662918 139622356395904 learning.py:507] global step 7342: loss = 3.1694 (1.242 sec/step)\n","I0830 18:31:50.902016 139622356395904 learning.py:507] global step 7343: loss = 2.8311 (1.237 sec/step)\n","I0830 18:31:52.140979 139622356395904 learning.py:507] global step 7344: loss = 3.4984 (1.237 sec/step)\n","I0830 18:31:53.372155 139622356395904 learning.py:507] global step 7345: loss = 3.8465 (1.230 sec/step)\n","I0830 18:31:54.592977 139622356395904 learning.py:507] global step 7346: loss = 3.2375 (1.219 sec/step)\n","I0830 18:31:55.806109 139622356395904 learning.py:507] global step 7347: loss = 2.3807 (1.211 sec/step)\n","I0830 18:31:57.045349 139622356395904 learning.py:507] global step 7348: loss = 2.8131 (1.237 sec/step)\n","I0830 18:31:58.247202 139622356395904 learning.py:507] global step 7349: loss = 3.4041 (1.200 sec/step)\n","I0830 18:31:59.424073 139622356395904 learning.py:507] global step 7350: loss = 2.9063 (1.175 sec/step)\n","I0830 18:32:01.538873 139622356395904 learning.py:507] global step 7351: loss = 2.8654 (1.995 sec/step)\n","I0830 18:32:02.090461 139619299985152 supervisor.py:1050] Recording summary at step 7351.\n","I0830 18:32:02.833368 139622356395904 learning.py:507] global step 7352: loss = 3.2147 (1.292 sec/step)\n","I0830 18:32:04.053168 139622356395904 learning.py:507] global step 7353: loss = 2.3742 (1.218 sec/step)\n","I0830 18:32:05.253710 139622356395904 learning.py:507] global step 7354: loss = 2.8326 (1.198 sec/step)\n","I0830 18:32:06.438945 139622356395904 learning.py:507] global step 7355: loss = 3.7170 (1.182 sec/step)\n","I0830 18:32:07.660105 139622356395904 learning.py:507] global step 7356: loss = 3.1871 (1.219 sec/step)\n","I0830 18:32:08.883137 139622356395904 learning.py:507] global step 7357: loss = 2.8108 (1.221 sec/step)\n","I0830 18:32:10.143556 139622356395904 learning.py:507] global step 7358: loss = 4.7132 (1.258 sec/step)\n","I0830 18:32:11.386021 139622356395904 learning.py:507] global step 7359: loss = 2.3589 (1.241 sec/step)\n","I0830 18:32:12.613482 139622356395904 learning.py:507] global step 7360: loss = 2.5105 (1.226 sec/step)\n","I0830 18:32:13.850573 139622356395904 learning.py:507] global step 7361: loss = 3.2734 (1.235 sec/step)\n","I0830 18:32:15.077592 139622356395904 learning.py:507] global step 7362: loss = 3.1518 (1.225 sec/step)\n","I0830 18:32:16.306463 139622356395904 learning.py:507] global step 7363: loss = 3.8135 (1.227 sec/step)\n","I0830 18:32:17.533020 139622356395904 learning.py:507] global step 7364: loss = 3.0739 (1.224 sec/step)\n","I0830 18:32:18.765039 139622356395904 learning.py:507] global step 7365: loss = 3.6939 (1.230 sec/step)\n","I0830 18:32:19.977922 139622356395904 learning.py:507] global step 7366: loss = 3.0110 (1.211 sec/step)\n","I0830 18:32:21.228803 139622356395904 learning.py:507] global step 7367: loss = 3.0195 (1.249 sec/step)\n","I0830 18:32:22.475710 139622356395904 learning.py:507] global step 7368: loss = 2.1954 (1.245 sec/step)\n","I0830 18:32:23.704044 139622356395904 learning.py:507] global step 7369: loss = 2.9991 (1.227 sec/step)\n","I0830 18:32:24.909384 139622356395904 learning.py:507] global step 7370: loss = 2.2380 (1.203 sec/step)\n","I0830 18:32:26.129000 139622356395904 learning.py:507] global step 7371: loss = 2.5770 (1.218 sec/step)\n","I0830 18:32:27.368482 139622356395904 learning.py:507] global step 7372: loss = 3.9548 (1.238 sec/step)\n","I0830 18:32:28.621973 139622356395904 learning.py:507] global step 7373: loss = 2.9337 (1.252 sec/step)\n","I0830 18:32:29.880993 139622356395904 learning.py:507] global step 7374: loss = 2.5314 (1.257 sec/step)\n","I0830 18:32:31.088717 139622356395904 learning.py:507] global step 7375: loss = 2.6496 (1.206 sec/step)\n","I0830 18:32:32.333425 139622356395904 learning.py:507] global step 7376: loss = 2.8456 (1.243 sec/step)\n","I0830 18:32:33.516879 139622356395904 learning.py:507] global step 7377: loss = 3.1858 (1.182 sec/step)\n","I0830 18:32:34.739660 139622356395904 learning.py:507] global step 7378: loss = 2.2078 (1.221 sec/step)\n","I0830 18:32:35.963843 139622356395904 learning.py:507] global step 7379: loss = 2.4749 (1.222 sec/step)\n","I0830 18:32:37.172618 139622356395904 learning.py:507] global step 7380: loss = 3.2196 (1.207 sec/step)\n","I0830 18:32:38.396420 139622356395904 learning.py:507] global step 7381: loss = 2.1776 (1.222 sec/step)\n","I0830 18:32:39.583730 139622356395904 learning.py:507] global step 7382: loss = 3.1224 (1.185 sec/step)\n","I0830 18:32:40.838327 139622356395904 learning.py:507] global step 7383: loss = 2.9407 (1.253 sec/step)\n","I0830 18:32:42.103442 139622356395904 learning.py:507] global step 7384: loss = 2.7372 (1.263 sec/step)\n","I0830 18:32:43.323011 139622356395904 learning.py:507] global step 7385: loss = 3.3917 (1.218 sec/step)\n","I0830 18:32:44.561067 139622356395904 learning.py:507] global step 7386: loss = 3.5705 (1.236 sec/step)\n","I0830 18:32:45.821928 139622356395904 learning.py:507] global step 7387: loss = 3.0410 (1.259 sec/step)\n","I0830 18:32:47.040802 139622356395904 learning.py:507] global step 7388: loss = 3.1604 (1.217 sec/step)\n","I0830 18:32:48.264262 139622356395904 learning.py:507] global step 7389: loss = 3.1526 (1.221 sec/step)\n","I0830 18:32:49.479948 139622356395904 learning.py:507] global step 7390: loss = 2.4760 (1.214 sec/step)\n","I0830 18:32:50.713468 139622356395904 learning.py:507] global step 7391: loss = 3.2538 (1.232 sec/step)\n","I0830 18:32:51.916597 139622356395904 learning.py:507] global step 7392: loss = 3.1185 (1.201 sec/step)\n","I0830 18:32:53.172895 139622356395904 learning.py:507] global step 7393: loss = 2.9237 (1.255 sec/step)\n","I0830 18:32:54.418685 139622356395904 learning.py:507] global step 7394: loss = 3.8054 (1.244 sec/step)\n","I0830 18:32:55.662873 139622356395904 learning.py:507] global step 7395: loss = 2.2638 (1.242 sec/step)\n","I0830 18:32:56.896515 139622356395904 learning.py:507] global step 7396: loss = 3.6608 (1.232 sec/step)\n","I0830 18:32:58.128961 139622356395904 learning.py:507] global step 7397: loss = 2.4690 (1.231 sec/step)\n","I0830 18:32:59.378510 139622356395904 learning.py:507] global step 7398: loss = 2.4097 (1.248 sec/step)\n","I0830 18:33:00.597380 139622356395904 learning.py:507] global step 7399: loss = 3.7302 (1.217 sec/step)\n","I0830 18:33:01.847380 139622356395904 learning.py:507] global step 7400: loss = 2.2353 (1.248 sec/step)\n","I0830 18:33:03.060335 139622356395904 learning.py:507] global step 7401: loss = 3.6571 (1.211 sec/step)\n","I0830 18:33:04.287796 139622356395904 learning.py:507] global step 7402: loss = 3.5579 (1.226 sec/step)\n","I0830 18:33:05.513959 139622356395904 learning.py:507] global step 7403: loss = 2.7407 (1.225 sec/step)\n","I0830 18:33:06.729295 139622356395904 learning.py:507] global step 7404: loss = 2.0921 (1.213 sec/step)\n","I0830 18:33:07.976443 139622356395904 learning.py:507] global step 7405: loss = 2.4067 (1.245 sec/step)\n","I0830 18:33:09.178970 139622356395904 learning.py:507] global step 7406: loss = 3.2791 (1.198 sec/step)\n","I0830 18:33:10.401110 139622356395904 learning.py:507] global step 7407: loss = 2.7024 (1.219 sec/step)\n","I0830 18:33:11.634950 139622356395904 learning.py:507] global step 7408: loss = 1.9554 (1.232 sec/step)\n","I0830 18:33:12.875384 139622356395904 learning.py:507] global step 7409: loss = 3.0209 (1.239 sec/step)\n","I0830 18:33:14.106402 139622356395904 learning.py:507] global step 7410: loss = 3.2607 (1.229 sec/step)\n","I0830 18:33:15.329677 139622356395904 learning.py:507] global step 7411: loss = 3.1832 (1.215 sec/step)\n","I0830 18:33:16.550874 139622356395904 learning.py:507] global step 7412: loss = 2.5069 (1.217 sec/step)\n","I0830 18:33:17.778337 139622356395904 learning.py:507] global step 7413: loss = 3.4782 (1.225 sec/step)\n","I0830 18:33:19.032102 139622356395904 learning.py:507] global step 7414: loss = 2.8971 (1.252 sec/step)\n","I0830 18:33:20.268217 139622356395904 learning.py:507] global step 7415: loss = 2.1414 (1.234 sec/step)\n","I0830 18:33:21.502409 139622356395904 learning.py:507] global step 7416: loss = 2.8811 (1.233 sec/step)\n","I0830 18:33:22.749954 139622356395904 learning.py:507] global step 7417: loss = 2.3147 (1.246 sec/step)\n","I0830 18:33:23.977451 139622356395904 learning.py:507] global step 7418: loss = 2.2108 (1.226 sec/step)\n","I0830 18:33:25.201708 139622356395904 learning.py:507] global step 7419: loss = 2.8950 (1.222 sec/step)\n","I0830 18:33:26.436905 139622356395904 learning.py:507] global step 7420: loss = 3.0412 (1.233 sec/step)\n","I0830 18:33:27.694270 139622356395904 learning.py:507] global step 7421: loss = 3.3025 (1.256 sec/step)\n","I0830 18:33:28.927478 139622356395904 learning.py:507] global step 7422: loss = 2.9135 (1.231 sec/step)\n","I0830 18:33:30.140039 139622356395904 learning.py:507] global step 7423: loss = 3.3092 (1.211 sec/step)\n","I0830 18:33:31.376444 139622356395904 learning.py:507] global step 7424: loss = 3.5618 (1.235 sec/step)\n","I0830 18:33:32.605608 139622356395904 learning.py:507] global step 7425: loss = 2.7161 (1.227 sec/step)\n","I0830 18:33:33.863764 139622356395904 learning.py:507] global step 7426: loss = 2.7141 (1.256 sec/step)\n","I0830 18:33:35.113636 139622356395904 learning.py:507] global step 7427: loss = 2.6915 (1.248 sec/step)\n","I0830 18:33:36.328241 139622356395904 learning.py:507] global step 7428: loss = 2.9357 (1.213 sec/step)\n","I0830 18:33:37.543120 139622356395904 learning.py:507] global step 7429: loss = 3.4925 (1.213 sec/step)\n","I0830 18:33:38.784604 139622356395904 learning.py:507] global step 7430: loss = 2.5949 (1.239 sec/step)\n","I0830 18:33:40.024210 139622356395904 learning.py:507] global step 7431: loss = 3.4339 (1.238 sec/step)\n","I0830 18:33:41.211870 139622356395904 learning.py:507] global step 7432: loss = 3.7638 (1.185 sec/step)\n","I0830 18:33:42.428189 139622356395904 learning.py:507] global step 7433: loss = 2.7305 (1.215 sec/step)\n","I0830 18:33:43.693257 139622356395904 learning.py:507] global step 7434: loss = 2.4607 (1.263 sec/step)\n","I0830 18:33:44.927237 139622356395904 learning.py:507] global step 7435: loss = 2.3435 (1.232 sec/step)\n","I0830 18:33:46.170544 139622356395904 learning.py:507] global step 7436: loss = 2.7619 (1.241 sec/step)\n","I0830 18:33:47.406983 139622356395904 learning.py:507] global step 7437: loss = 2.6942 (1.235 sec/step)\n","I0830 18:33:48.639948 139622356395904 learning.py:507] global step 7438: loss = 2.5643 (1.231 sec/step)\n","I0830 18:33:49.892696 139622356395904 learning.py:507] global step 7439: loss = 2.5644 (1.251 sec/step)\n","I0830 18:33:51.139822 139622356395904 learning.py:507] global step 7440: loss = 2.6417 (1.245 sec/step)\n","I0830 18:33:52.369209 139622356395904 learning.py:507] global step 7441: loss = 2.4017 (1.227 sec/step)\n","I0830 18:33:53.597727 139622356395904 learning.py:507] global step 7442: loss = 4.2490 (1.227 sec/step)\n","I0830 18:33:54.805700 139622356395904 learning.py:507] global step 7443: loss = 2.8921 (1.206 sec/step)\n","I0830 18:33:56.051556 139622356395904 learning.py:507] global step 7444: loss = 2.8660 (1.244 sec/step)\n","I0830 18:33:57.259226 139622356395904 learning.py:507] global step 7445: loss = 2.4722 (1.206 sec/step)\n","I0830 18:33:58.483752 139622356395904 learning.py:507] global step 7446: loss = 2.6482 (1.223 sec/step)\n","I0830 18:33:59.725670 139622356395904 learning.py:507] global step 7447: loss = 4.0952 (1.237 sec/step)\n","I0830 18:34:01.723332 139622356395904 learning.py:507] global step 7448: loss = 2.4874 (1.876 sec/step)\n","I0830 18:34:01.855913 139619299985152 supervisor.py:1050] Recording summary at step 7448.\n","I0830 18:34:03.038336 139622356395904 learning.py:507] global step 7449: loss = 2.8728 (1.193 sec/step)\n","I0830 18:34:04.258508 139622356395904 learning.py:507] global step 7450: loss = 2.5225 (1.218 sec/step)\n","I0830 18:34:05.511865 139622356395904 learning.py:507] global step 7451: loss = 2.4095 (1.252 sec/step)\n","I0830 18:34:06.741183 139622356395904 learning.py:507] global step 7452: loss = 2.9616 (1.227 sec/step)\n","I0830 18:34:07.987271 139622356395904 learning.py:507] global step 7453: loss = 2.5736 (1.244 sec/step)\n","I0830 18:34:09.219712 139622356395904 learning.py:507] global step 7454: loss = 2.7009 (1.231 sec/step)\n","I0830 18:34:10.464971 139622356395904 learning.py:507] global step 7455: loss = 2.9072 (1.243 sec/step)\n","I0830 18:34:11.699102 139622356395904 learning.py:507] global step 7456: loss = 2.4606 (1.232 sec/step)\n","I0830 18:34:12.954228 139622356395904 learning.py:507] global step 7457: loss = 2.9432 (1.253 sec/step)\n","I0830 18:34:14.153327 139622356395904 learning.py:507] global step 7458: loss = 3.1353 (1.197 sec/step)\n","I0830 18:34:15.401090 139622356395904 learning.py:507] global step 7459: loss = 3.1943 (1.246 sec/step)\n","I0830 18:34:16.656534 139622356395904 learning.py:507] global step 7460: loss = 2.4333 (1.254 sec/step)\n","I0830 18:34:17.908684 139622356395904 learning.py:507] global step 7461: loss = 2.8351 (1.250 sec/step)\n","I0830 18:34:19.105684 139622356395904 learning.py:507] global step 7462: loss = 2.7203 (1.195 sec/step)\n","I0830 18:34:20.362374 139622356395904 learning.py:507] global step 7463: loss = 2.7222 (1.255 sec/step)\n","I0830 18:34:21.601166 139622356395904 learning.py:507] global step 7464: loss = 4.1631 (1.237 sec/step)\n","I0830 18:34:22.874636 139622356395904 learning.py:507] global step 7465: loss = 2.5554 (1.272 sec/step)\n","I0830 18:34:24.110503 139622356395904 learning.py:507] global step 7466: loss = 2.6007 (1.234 sec/step)\n","I0830 18:34:25.350778 139622356395904 learning.py:507] global step 7467: loss = 3.4696 (1.238 sec/step)\n","I0830 18:34:26.609138 139622356395904 learning.py:507] global step 7468: loss = 2.6528 (1.256 sec/step)\n","I0830 18:34:27.859287 139622356395904 learning.py:507] global step 7469: loss = 2.8811 (1.248 sec/step)\n","I0830 18:34:29.143868 139622356395904 learning.py:507] global step 7470: loss = 3.2529 (1.283 sec/step)\n","I0830 18:34:30.395988 139622356395904 learning.py:507] global step 7471: loss = 3.5312 (1.250 sec/step)\n","I0830 18:34:31.644982 139622356395904 learning.py:507] global step 7472: loss = 4.0936 (1.247 sec/step)\n","I0830 18:34:32.893506 139622356395904 learning.py:507] global step 7473: loss = 3.1568 (1.247 sec/step)\n","I0830 18:34:34.110024 139622356395904 learning.py:507] global step 7474: loss = 2.2619 (1.214 sec/step)\n","I0830 18:34:35.350778 139622356395904 learning.py:507] global step 7475: loss = 3.0001 (1.239 sec/step)\n","I0830 18:34:36.576627 139622356395904 learning.py:507] global step 7476: loss = 2.5039 (1.224 sec/step)\n","I0830 18:34:37.805189 139622356395904 learning.py:507] global step 7477: loss = 2.5439 (1.226 sec/step)\n","I0830 18:34:39.022744 139622356395904 learning.py:507] global step 7478: loss = 2.6796 (1.215 sec/step)\n","I0830 18:34:40.241621 139622356395904 learning.py:507] global step 7479: loss = 3.4622 (1.217 sec/step)\n","I0830 18:34:41.474668 139622356395904 learning.py:507] global step 7480: loss = 2.5421 (1.231 sec/step)\n","I0830 18:34:42.723920 139622356395904 learning.py:507] global step 7481: loss = 2.6516 (1.247 sec/step)\n","I0830 18:34:43.973998 139622356395904 learning.py:507] global step 7482: loss = 2.7167 (1.248 sec/step)\n","I0830 18:34:45.203113 139622356395904 learning.py:507] global step 7483: loss = 2.8477 (1.227 sec/step)\n","I0830 18:34:46.429271 139622356395904 learning.py:507] global step 7484: loss = 2.5531 (1.224 sec/step)\n","I0830 18:34:47.645872 139622356395904 learning.py:507] global step 7485: loss = 2.5337 (1.215 sec/step)\n","I0830 18:34:48.888229 139622356395904 learning.py:507] global step 7486: loss = 2.0017 (1.240 sec/step)\n","I0830 18:34:50.119294 139622356395904 learning.py:507] global step 7487: loss = 2.2405 (1.229 sec/step)\n","I0830 18:34:51.305624 139622356395904 learning.py:507] global step 7488: loss = 3.3342 (1.184 sec/step)\n","I0830 18:34:52.550126 139622356395904 learning.py:507] global step 7489: loss = 3.2962 (1.241 sec/step)\n","I0830 18:34:53.770432 139622356395904 learning.py:507] global step 7490: loss = 2.3865 (1.219 sec/step)\n","I0830 18:34:55.018137 139622356395904 learning.py:507] global step 7491: loss = 2.8023 (1.245 sec/step)\n","I0830 18:34:56.261491 139622356395904 learning.py:507] global step 7492: loss = 3.2248 (1.241 sec/step)\n","I0830 18:34:57.483746 139622356395904 learning.py:507] global step 7493: loss = 3.5652 (1.220 sec/step)\n","I0830 18:34:58.698791 139622356395904 learning.py:507] global step 7494: loss = 3.0880 (1.213 sec/step)\n","I0830 18:34:59.947781 139622356395904 learning.py:507] global step 7495: loss = 2.7773 (1.247 sec/step)\n","I0830 18:35:01.185898 139622356395904 learning.py:507] global step 7496: loss = 3.3253 (1.236 sec/step)\n","I0830 18:35:02.424818 139622356395904 learning.py:507] global step 7497: loss = 2.7249 (1.237 sec/step)\n","I0830 18:35:03.630193 139622356395904 learning.py:507] global step 7498: loss = 2.6102 (1.202 sec/step)\n","I0830 18:35:04.878461 139622356395904 learning.py:507] global step 7499: loss = 2.8543 (1.246 sec/step)\n","I0830 18:35:06.072928 139622356395904 learning.py:507] global step 7500: loss = 2.7269 (1.193 sec/step)\n","I0830 18:35:07.321571 139622356395904 learning.py:507] global step 7501: loss = 2.8679 (1.247 sec/step)\n","I0830 18:35:08.557889 139622356395904 learning.py:507] global step 7502: loss = 2.1577 (1.234 sec/step)\n","I0830 18:35:09.794162 139622356395904 learning.py:507] global step 7503: loss = 3.2293 (1.233 sec/step)\n","I0830 18:35:11.021405 139622356395904 learning.py:507] global step 7504: loss = 2.3796 (1.225 sec/step)\n","I0830 18:35:12.272439 139622356395904 learning.py:507] global step 7505: loss = 2.8969 (1.246 sec/step)\n","I0830 18:35:13.532737 139622356395904 learning.py:507] global step 7506: loss = 2.8318 (1.257 sec/step)\n","I0830 18:35:14.785006 139622356395904 learning.py:507] global step 7507: loss = 2.2641 (1.250 sec/step)\n","I0830 18:35:16.009711 139622356395904 learning.py:507] global step 7508: loss = 2.4317 (1.222 sec/step)\n","I0830 18:35:17.251245 139622356395904 learning.py:507] global step 7509: loss = 2.6882 (1.239 sec/step)\n","I0830 18:35:18.552680 139622356395904 learning.py:507] global step 7510: loss = 2.3580 (1.300 sec/step)\n","I0830 18:35:19.776585 139622356395904 learning.py:507] global step 7511: loss = 4.4413 (1.222 sec/step)\n","I0830 18:35:20.975465 139622356395904 learning.py:507] global step 7512: loss = 2.7425 (1.197 sec/step)\n","I0830 18:35:22.214744 139622356395904 learning.py:507] global step 7513: loss = 2.2045 (1.237 sec/step)\n","I0830 18:35:23.427138 139622356395904 learning.py:507] global step 7514: loss = 4.1750 (1.210 sec/step)\n","I0830 18:35:24.639123 139622356395904 learning.py:507] global step 7515: loss = 3.1004 (1.210 sec/step)\n","I0830 18:35:25.883073 139622356395904 learning.py:507] global step 7516: loss = 3.1136 (1.242 sec/step)\n","I0830 18:35:27.120943 139622356395904 learning.py:507] global step 7517: loss = 2.9563 (1.236 sec/step)\n","I0830 18:35:28.371372 139622356395904 learning.py:507] global step 7518: loss = 2.5856 (1.246 sec/step)\n","I0830 18:35:29.599163 139622356395904 learning.py:507] global step 7519: loss = 3.7738 (1.226 sec/step)\n","I0830 18:35:30.818136 139622356395904 learning.py:507] global step 7520: loss = 2.7701 (1.217 sec/step)\n","I0830 18:35:32.068751 139622356395904 learning.py:507] global step 7521: loss = 2.8415 (1.249 sec/step)\n","I0830 18:35:33.322071 139622356395904 learning.py:507] global step 7522: loss = 3.1995 (1.251 sec/step)\n","I0830 18:35:34.542637 139622356395904 learning.py:507] global step 7523: loss = 2.5897 (1.218 sec/step)\n","I0830 18:35:35.783874 139622356395904 learning.py:507] global step 7524: loss = 2.7217 (1.239 sec/step)\n","I0830 18:35:37.001998 139622356395904 learning.py:507] global step 7525: loss = 2.6257 (1.216 sec/step)\n","I0830 18:35:38.213906 139622356395904 learning.py:507] global step 7526: loss = 2.9961 (1.210 sec/step)\n","I0830 18:35:39.472115 139622356395904 learning.py:507] global step 7527: loss = 3.5651 (1.256 sec/step)\n","I0830 18:35:40.739600 139622356395904 learning.py:507] global step 7528: loss = 3.0144 (1.263 sec/step)\n","I0830 18:35:41.981701 139622356395904 learning.py:507] global step 7529: loss = 1.8961 (1.240 sec/step)\n","I0830 18:35:43.205254 139622356395904 learning.py:507] global step 7530: loss = 2.3832 (1.222 sec/step)\n","I0830 18:35:44.417227 139622356395904 learning.py:507] global step 7531: loss = 3.1172 (1.210 sec/step)\n","I0830 18:35:45.644975 139622356395904 learning.py:507] global step 7532: loss = 2.8798 (1.225 sec/step)\n","I0830 18:35:46.899182 139622356395904 learning.py:507] global step 7533: loss = 2.5057 (1.252 sec/step)\n","I0830 18:35:48.132262 139622356395904 learning.py:507] global step 7534: loss = 3.5348 (1.231 sec/step)\n","I0830 18:35:49.344184 139622356395904 learning.py:507] global step 7535: loss = 3.5365 (1.209 sec/step)\n","I0830 18:35:50.547629 139622356395904 learning.py:507] global step 7536: loss = 2.9088 (1.201 sec/step)\n","I0830 18:35:51.744503 139622356395904 learning.py:507] global step 7537: loss = 3.1397 (1.195 sec/step)\n","I0830 18:35:53.009232 139622356395904 learning.py:507] global step 7538: loss = 2.2628 (1.263 sec/step)\n","I0830 18:35:54.220032 139622356395904 learning.py:507] global step 7539: loss = 2.2698 (1.209 sec/step)\n","I0830 18:35:55.440650 139622356395904 learning.py:507] global step 7540: loss = 2.5488 (1.219 sec/step)\n","I0830 18:35:56.697756 139622356395904 learning.py:507] global step 7541: loss = 3.2989 (1.255 sec/step)\n","I0830 18:35:57.923578 139622356395904 learning.py:507] global step 7542: loss = 3.2497 (1.224 sec/step)\n","I0830 18:35:59.150833 139622356395904 learning.py:507] global step 7543: loss = 2.9367 (1.225 sec/step)\n","I0830 18:36:00.651746 139622356395904 learning.py:507] global step 7544: loss = 2.6474 (1.470 sec/step)\n","I0830 18:36:02.555161 139619299985152 supervisor.py:1050] Recording summary at step 7545.\n","I0830 18:36:02.592938 139622356395904 learning.py:507] global step 7545: loss = 3.4904 (1.939 sec/step)\n","I0830 18:36:03.819277 139622356395904 learning.py:507] global step 7546: loss = 4.4085 (1.225 sec/step)\n","I0830 18:36:05.077277 139622356395904 learning.py:507] global step 7547: loss = 2.2291 (1.256 sec/step)\n","I0830 18:36:06.296684 139622356395904 learning.py:507] global step 7548: loss = 2.9397 (1.218 sec/step)\n","I0830 18:36:07.558534 139622356395904 learning.py:507] global step 7549: loss = 2.5812 (1.258 sec/step)\n","I0830 18:36:08.777944 139622356395904 learning.py:507] global step 7550: loss = 2.9083 (1.217 sec/step)\n","I0830 18:36:10.031504 139622356395904 learning.py:507] global step 7551: loss = 2.4531 (1.252 sec/step)\n","I0830 18:36:11.276019 139622356395904 learning.py:507] global step 7552: loss = 3.8245 (1.242 sec/step)\n","I0830 18:36:12.501946 139622356395904 learning.py:507] global step 7553: loss = 2.9933 (1.223 sec/step)\n","I0830 18:36:13.770569 139622356395904 learning.py:507] global step 7554: loss = 2.2450 (1.266 sec/step)\n","I0830 18:36:14.984604 139622356395904 learning.py:507] global step 7555: loss = 3.4191 (1.212 sec/step)\n","I0830 18:36:16.213733 139622356395904 learning.py:507] global step 7556: loss = 2.5776 (1.227 sec/step)\n","I0830 18:36:17.440886 139622356395904 learning.py:507] global step 7557: loss = 2.5310 (1.225 sec/step)\n","I0830 18:36:18.710839 139622356395904 learning.py:507] global step 7558: loss = 2.5059 (1.268 sec/step)\n","I0830 18:36:19.956636 139622356395904 learning.py:507] global step 7559: loss = 2.8209 (1.244 sec/step)\n","I0830 18:36:21.182971 139622356395904 learning.py:507] global step 7560: loss = 3.8154 (1.225 sec/step)\n","I0830 18:36:22.412535 139622356395904 learning.py:507] global step 7561: loss = 3.2957 (1.228 sec/step)\n","I0830 18:36:23.667342 139622356395904 learning.py:507] global step 7562: loss = 2.0651 (1.253 sec/step)\n","I0830 18:36:24.891316 139622356395904 learning.py:507] global step 7563: loss = 3.3511 (1.222 sec/step)\n","I0830 18:36:26.154407 139622356395904 learning.py:507] global step 7564: loss = 3.4679 (1.261 sec/step)\n","I0830 18:36:27.359692 139622356395904 learning.py:507] global step 7565: loss = 2.1211 (1.204 sec/step)\n","I0830 18:36:28.579867 139622356395904 learning.py:507] global step 7566: loss = 2.5156 (1.218 sec/step)\n","I0830 18:36:29.797953 139622356395904 learning.py:507] global step 7567: loss = 2.3245 (1.216 sec/step)\n","I0830 18:36:31.071955 139622356395904 learning.py:507] global step 7568: loss = 2.7542 (1.272 sec/step)\n","I0830 18:36:32.311517 139622356395904 learning.py:507] global step 7569: loss = 3.2696 (1.238 sec/step)\n","I0830 18:36:33.540779 139622356395904 learning.py:507] global step 7570: loss = 2.7370 (1.227 sec/step)\n","I0830 18:36:34.811375 139622356395904 learning.py:507] global step 7571: loss = 2.3944 (1.269 sec/step)\n","I0830 18:36:36.053349 139622356395904 learning.py:507] global step 7572: loss = 2.6998 (1.240 sec/step)\n","I0830 18:36:37.295832 139622356395904 learning.py:507] global step 7573: loss = 3.2433 (1.241 sec/step)\n","I0830 18:36:38.551199 139622356395904 learning.py:507] global step 7574: loss = 2.9582 (1.254 sec/step)\n","I0830 18:36:39.755876 139622356395904 learning.py:507] global step 7575: loss = 2.3117 (1.203 sec/step)\n","I0830 18:36:41.008026 139622356395904 learning.py:507] global step 7576: loss = 2.6156 (1.250 sec/step)\n","I0830 18:36:42.249265 139622356395904 learning.py:507] global step 7577: loss = 2.8201 (1.239 sec/step)\n","I0830 18:36:43.473633 139622356395904 learning.py:507] global step 7578: loss = 2.3818 (1.222 sec/step)\n","I0830 18:36:44.713212 139622356395904 learning.py:507] global step 7579: loss = 2.8424 (1.237 sec/step)\n","I0830 18:36:45.921203 139622356395904 learning.py:507] global step 7580: loss = 2.6399 (1.206 sec/step)\n","I0830 18:36:47.153484 139622356395904 learning.py:507] global step 7581: loss = 2.9999 (1.230 sec/step)\n","I0830 18:36:48.415626 139622356395904 learning.py:507] global step 7582: loss = 3.5104 (1.260 sec/step)\n","I0830 18:36:49.660816 139622356395904 learning.py:507] global step 7583: loss = 2.5663 (1.243 sec/step)\n","I0830 18:36:50.890356 139622356395904 learning.py:507] global step 7584: loss = 2.8062 (1.228 sec/step)\n","I0830 18:36:52.119695 139622356395904 learning.py:507] global step 7585: loss = 2.7880 (1.227 sec/step)\n","I0830 18:36:53.330735 139622356395904 learning.py:507] global step 7586: loss = 2.4554 (1.209 sec/step)\n","I0830 18:36:54.552446 139622356395904 learning.py:507] global step 7587: loss = 2.5931 (1.220 sec/step)\n","I0830 18:36:55.807702 139622356395904 learning.py:507] global step 7588: loss = 3.6015 (1.253 sec/step)\n","I0830 18:36:57.017222 139622356395904 learning.py:507] global step 7589: loss = 2.1043 (1.208 sec/step)\n","I0830 18:36:58.251358 139622356395904 learning.py:507] global step 7590: loss = 2.5901 (1.232 sec/step)\n","I0830 18:36:59.460855 139622356395904 learning.py:507] global step 7591: loss = 3.1045 (1.208 sec/step)\n","I0830 18:37:00.716519 139622356395904 learning.py:507] global step 7592: loss = 3.4750 (1.254 sec/step)\n","I0830 18:37:01.924198 139622356395904 learning.py:507] global step 7593: loss = 2.8692 (1.206 sec/step)\n","I0830 18:37:03.163904 139622356395904 learning.py:507] global step 7594: loss = 2.8618 (1.238 sec/step)\n","I0830 18:37:04.365108 139622356395904 learning.py:507] global step 7595: loss = 2.1588 (1.199 sec/step)\n","I0830 18:37:05.607983 139622356395904 learning.py:507] global step 7596: loss = 2.5641 (1.241 sec/step)\n","I0830 18:37:06.859771 139622356395904 learning.py:507] global step 7597: loss = 2.7763 (1.250 sec/step)\n","I0830 18:37:08.136661 139622356395904 learning.py:507] global step 7598: loss = 2.7014 (1.275 sec/step)\n","I0830 18:37:09.365184 139622356395904 learning.py:507] global step 7599: loss = 2.1387 (1.227 sec/step)\n","I0830 18:37:10.624411 139622356395904 learning.py:507] global step 7600: loss = 2.9862 (1.257 sec/step)\n","I0830 18:37:11.884170 139622356395904 learning.py:507] global step 7601: loss = 3.7482 (1.258 sec/step)\n","I0830 18:37:13.141535 139622356395904 learning.py:507] global step 7602: loss = 2.3024 (1.255 sec/step)\n","I0830 18:37:14.373253 139622356395904 learning.py:507] global step 7603: loss = 2.4656 (1.230 sec/step)\n","I0830 18:37:15.572564 139622356395904 learning.py:507] global step 7604: loss = 3.4483 (1.197 sec/step)\n","I0830 18:37:16.811325 139622356395904 learning.py:507] global step 7605: loss = 2.3839 (1.230 sec/step)\n","I0830 18:37:18.045359 139622356395904 learning.py:507] global step 7606: loss = 3.5134 (1.232 sec/step)\n","I0830 18:37:19.287113 139622356395904 learning.py:507] global step 7607: loss = 4.0307 (1.240 sec/step)\n","I0830 18:37:20.509769 139622356395904 learning.py:507] global step 7608: loss = 2.6258 (1.221 sec/step)\n","I0830 18:37:21.712882 139622356395904 learning.py:507] global step 7609: loss = 3.3750 (1.201 sec/step)\n","I0830 18:37:22.937978 139622356395904 learning.py:507] global step 7610: loss = 3.0458 (1.223 sec/step)\n","I0830 18:37:24.197345 139622356395904 learning.py:507] global step 7611: loss = 2.9787 (1.258 sec/step)\n","I0830 18:37:25.422079 139622356395904 learning.py:507] global step 7612: loss = 3.1084 (1.223 sec/step)\n","I0830 18:37:26.672561 139622356395904 learning.py:507] global step 7613: loss = 2.5446 (1.249 sec/step)\n","I0830 18:37:27.913861 139622356395904 learning.py:507] global step 7614: loss = 2.5082 (1.239 sec/step)\n","I0830 18:37:29.114423 139622356395904 learning.py:507] global step 7615: loss = 2.5747 (1.199 sec/step)\n","I0830 18:37:30.340895 139622356395904 learning.py:507] global step 7616: loss = 2.4116 (1.225 sec/step)\n","I0830 18:37:31.656875 139622356395904 learning.py:507] global step 7617: loss = 2.3416 (1.314 sec/step)\n","I0830 18:37:32.934640 139622356395904 learning.py:507] global step 7618: loss = 2.5827 (1.276 sec/step)\n","I0830 18:37:34.197664 139622356395904 learning.py:507] global step 7619: loss = 2.1803 (1.261 sec/step)\n","I0830 18:37:35.423486 139622356395904 learning.py:507] global step 7620: loss = 2.2422 (1.223 sec/step)\n","I0830 18:37:36.696298 139622356395904 learning.py:507] global step 7621: loss = 3.1033 (1.271 sec/step)\n","I0830 18:37:37.953258 139622356395904 learning.py:507] global step 7622: loss = 2.5829 (1.255 sec/step)\n","I0830 18:37:39.196869 139622356395904 learning.py:507] global step 7623: loss = 2.0699 (1.242 sec/step)\n","I0830 18:37:40.447118 139622356395904 learning.py:507] global step 7624: loss = 2.2318 (1.248 sec/step)\n","I0830 18:37:41.717785 139622356395904 learning.py:507] global step 7625: loss = 3.5444 (1.268 sec/step)\n","I0830 18:37:42.950138 139622356395904 learning.py:507] global step 7626: loss = 3.1397 (1.231 sec/step)\n","I0830 18:37:44.235636 139622356395904 learning.py:507] global step 7627: loss = 3.4841 (1.284 sec/step)\n","I0830 18:37:45.429951 139622356395904 learning.py:507] global step 7628: loss = 2.7974 (1.193 sec/step)\n","I0830 18:37:46.702373 139622356395904 learning.py:507] global step 7629: loss = 2.4705 (1.271 sec/step)\n","I0830 18:37:47.910779 139622356395904 learning.py:507] global step 7630: loss = 3.2532 (1.206 sec/step)\n","I0830 18:37:49.205317 139622356395904 learning.py:507] global step 7631: loss = 2.4434 (1.292 sec/step)\n","I0830 18:37:50.428822 139622356395904 learning.py:507] global step 7632: loss = 3.8340 (1.222 sec/step)\n","I0830 18:37:51.685368 139622356395904 learning.py:507] global step 7633: loss = 2.9635 (1.255 sec/step)\n","I0830 18:37:52.931240 139622356395904 learning.py:507] global step 7634: loss = 2.9532 (1.244 sec/step)\n","I0830 18:37:54.175304 139622356395904 learning.py:507] global step 7635: loss = 2.9635 (1.242 sec/step)\n","I0830 18:37:55.377833 139622356395904 learning.py:507] global step 7636: loss = 3.1882 (1.201 sec/step)\n","I0830 18:37:56.605116 139622356395904 learning.py:507] global step 7637: loss = 3.0581 (1.225 sec/step)\n","I0830 18:37:57.804389 139622356395904 learning.py:507] global step 7638: loss = 2.9348 (1.197 sec/step)\n","I0830 18:37:59.024885 139622356395904 learning.py:507] global step 7639: loss = 2.9102 (1.218 sec/step)\n","I0830 18:38:00.397451 139622356395904 learning.py:507] global step 7640: loss = 2.1420 (1.371 sec/step)\n","I0830 18:38:02.297868 139619299985152 supervisor.py:1050] Recording summary at step 7641.\n","I0830 18:38:02.325112 139622356395904 learning.py:507] global step 7641: loss = 2.3725 (1.920 sec/step)\n","I0830 18:38:03.547553 139622356395904 learning.py:507] global step 7642: loss = 3.6393 (1.221 sec/step)\n","I0830 18:38:04.806350 139622356395904 learning.py:507] global step 7643: loss = 2.5320 (1.257 sec/step)\n","I0830 18:38:06.018846 139622356395904 learning.py:507] global step 7644: loss = 2.9437 (1.209 sec/step)\n","I0830 18:38:07.249969 139622356395904 learning.py:507] global step 7645: loss = 3.1821 (1.229 sec/step)\n","I0830 18:38:08.489475 139622356395904 learning.py:507] global step 7646: loss = 3.0066 (1.238 sec/step)\n","I0830 18:38:09.706119 139622356395904 learning.py:507] global step 7647: loss = 2.1718 (1.214 sec/step)\n","I0830 18:38:10.920560 139622356395904 learning.py:507] global step 7648: loss = 2.1970 (1.212 sec/step)\n","I0830 18:38:12.160856 139622356395904 learning.py:507] global step 7649: loss = 4.2187 (1.238 sec/step)\n","I0830 18:38:13.371905 139622356395904 learning.py:507] global step 7650: loss = 2.5059 (1.209 sec/step)\n","I0830 18:38:14.611798 139622356395904 learning.py:507] global step 7651: loss = 2.1714 (1.238 sec/step)\n","I0830 18:38:15.908827 139622356395904 learning.py:507] global step 7652: loss = 2.6916 (1.295 sec/step)\n","I0830 18:38:17.153193 139622356395904 learning.py:507] global step 7653: loss = 2.6498 (1.242 sec/step)\n","I0830 18:38:18.402885 139622356395904 learning.py:507] global step 7654: loss = 2.6504 (1.248 sec/step)\n","I0830 18:38:19.657166 139622356395904 learning.py:507] global step 7655: loss = 2.7834 (1.252 sec/step)\n","I0830 18:38:20.898455 139622356395904 learning.py:507] global step 7656: loss = 2.7046 (1.239 sec/step)\n","I0830 18:38:22.142734 139622356395904 learning.py:507] global step 7657: loss = 2.2250 (1.242 sec/step)\n","I0830 18:38:23.374723 139622356395904 learning.py:507] global step 7658: loss = 2.3378 (1.230 sec/step)\n","I0830 18:38:24.623041 139622356395904 learning.py:507] global step 7659: loss = 4.0743 (1.246 sec/step)\n","I0830 18:38:25.863185 139622356395904 learning.py:507] global step 7660: loss = 3.0931 (1.238 sec/step)\n","I0830 18:38:27.091240 139622356395904 learning.py:507] global step 7661: loss = 2.3933 (1.225 sec/step)\n","I0830 18:38:28.337379 139622356395904 learning.py:507] global step 7662: loss = 3.8581 (1.244 sec/step)\n","I0830 18:38:29.564961 139622356395904 learning.py:507] global step 7663: loss = 3.9757 (1.226 sec/step)\n","I0830 18:38:30.824582 139622356395904 learning.py:507] global step 7664: loss = 2.6551 (1.258 sec/step)\n","I0830 18:38:32.102566 139622356395904 learning.py:507] global step 7665: loss = 2.2925 (1.276 sec/step)\n","I0830 18:38:33.324639 139622356395904 learning.py:507] global step 7666: loss = 2.4530 (1.220 sec/step)\n","I0830 18:38:34.512887 139622356395904 learning.py:507] global step 7667: loss = 4.1637 (1.187 sec/step)\n","I0830 18:38:35.738329 139622356395904 learning.py:507] global step 7668: loss = 2.2423 (1.224 sec/step)\n","I0830 18:38:36.991190 139622356395904 learning.py:507] global step 7669: loss = 2.5969 (1.251 sec/step)\n","I0830 18:38:38.283303 139622356395904 learning.py:507] global step 7670: loss = 2.5015 (1.290 sec/step)\n","I0830 18:38:39.555510 139622356395904 learning.py:507] global step 7671: loss = 2.5365 (1.270 sec/step)\n","I0830 18:38:40.830610 139622356395904 learning.py:507] global step 7672: loss = 2.7177 (1.273 sec/step)\n","I0830 18:38:42.079315 139622356395904 learning.py:507] global step 7673: loss = 2.6302 (1.247 sec/step)\n","I0830 18:38:43.281538 139622356395904 learning.py:507] global step 7674: loss = 2.8233 (1.200 sec/step)\n","I0830 18:38:44.514855 139622356395904 learning.py:507] global step 7675: loss = 2.4991 (1.232 sec/step)\n","I0830 18:38:45.751302 139622356395904 learning.py:507] global step 7676: loss = 3.3818 (1.235 sec/step)\n","I0830 18:38:47.005708 139622356395904 learning.py:507] global step 7677: loss = 2.7605 (1.253 sec/step)\n","I0830 18:38:48.282642 139622356395904 learning.py:507] global step 7678: loss = 2.1854 (1.275 sec/step)\n","I0830 18:38:49.500742 139622356395904 learning.py:507] global step 7679: loss = 3.4903 (1.216 sec/step)\n","I0830 18:38:50.713900 139622356395904 learning.py:507] global step 7680: loss = 3.3921 (1.211 sec/step)\n","I0830 18:38:51.957189 139622356395904 learning.py:507] global step 7681: loss = 3.1900 (1.241 sec/step)\n","I0830 18:38:53.206202 139622356395904 learning.py:507] global step 7682: loss = 2.3906 (1.247 sec/step)\n","I0830 18:38:54.459166 139622356395904 learning.py:507] global step 7683: loss = 3.8479 (1.251 sec/step)\n","I0830 18:38:55.713519 139622356395904 learning.py:507] global step 7684: loss = 2.4604 (1.252 sec/step)\n","I0830 18:38:56.932113 139622356395904 learning.py:507] global step 7685: loss = 3.0204 (1.217 sec/step)\n","I0830 18:38:58.176567 139622356395904 learning.py:507] global step 7686: loss = 2.4392 (1.243 sec/step)\n","I0830 18:38:59.422269 139622356395904 learning.py:507] global step 7687: loss = 3.3916 (1.244 sec/step)\n","I0830 18:39:00.671375 139622356395904 learning.py:507] global step 7688: loss = 2.9351 (1.247 sec/step)\n","I0830 18:39:01.912818 139622356395904 learning.py:507] global step 7689: loss = 2.4948 (1.239 sec/step)\n","I0830 18:39:03.124834 139622356395904 learning.py:507] global step 7690: loss = 3.1822 (1.210 sec/step)\n","I0830 18:39:04.360841 139622356395904 learning.py:507] global step 7691: loss = 2.9288 (1.234 sec/step)\n","I0830 18:39:05.579033 139622356395904 learning.py:507] global step 7692: loss = 2.3503 (1.216 sec/step)\n","I0830 18:39:06.833259 139622356395904 learning.py:507] global step 7693: loss = 3.1410 (1.252 sec/step)\n","I0830 18:39:08.055392 139622356395904 learning.py:507] global step 7694: loss = 2.0328 (1.220 sec/step)\n","I0830 18:39:09.271241 139622356395904 learning.py:507] global step 7695: loss = 2.2585 (1.214 sec/step)\n","I0830 18:39:10.532680 139622356395904 learning.py:507] global step 7696: loss = 3.0611 (1.259 sec/step)\n","I0830 18:39:11.827271 139622356395904 learning.py:507] global step 7697: loss = 2.5536 (1.293 sec/step)\n","I0830 18:39:13.077423 139622356395904 learning.py:507] global step 7698: loss = 2.1206 (1.248 sec/step)\n","I0830 18:39:14.306532 139622356395904 learning.py:507] global step 7699: loss = 3.8852 (1.227 sec/step)\n","I0830 18:39:15.561782 139622356395904 learning.py:507] global step 7700: loss = 2.1532 (1.253 sec/step)\n","I0830 18:39:16.829792 139622356395904 learning.py:507] global step 7701: loss = 2.7147 (1.266 sec/step)\n","I0830 18:39:18.092572 139622356395904 learning.py:507] global step 7702: loss = 2.5638 (1.260 sec/step)\n","I0830 18:39:19.343810 139622356395904 learning.py:507] global step 7703: loss = 2.6851 (1.249 sec/step)\n","I0830 18:39:20.577392 139622356395904 learning.py:507] global step 7704: loss = 3.2858 (1.232 sec/step)\n","I0830 18:39:21.843000 139622356395904 learning.py:507] global step 7705: loss = 3.4274 (1.264 sec/step)\n","I0830 18:39:23.106150 139622356395904 learning.py:507] global step 7706: loss = 2.4622 (1.261 sec/step)\n","I0830 18:39:24.370544 139622356395904 learning.py:507] global step 7707: loss = 2.1306 (1.262 sec/step)\n","I0830 18:39:25.588246 139622356395904 learning.py:507] global step 7708: loss = 3.5719 (1.216 sec/step)\n","I0830 18:39:26.808761 139622356395904 learning.py:507] global step 7709: loss = 3.0532 (1.218 sec/step)\n","I0830 18:39:28.029300 139622356395904 learning.py:507] global step 7710: loss = 4.0400 (1.219 sec/step)\n","I0830 18:39:29.223027 139622356395904 learning.py:507] global step 7711: loss = 2.0973 (1.192 sec/step)\n","I0830 18:39:30.479992 139622356395904 learning.py:507] global step 7712: loss = 2.0876 (1.255 sec/step)\n","I0830 18:39:31.698125 139622356395904 learning.py:507] global step 7713: loss = 2.7519 (1.216 sec/step)\n","I0830 18:39:32.922600 139622356395904 learning.py:507] global step 7714: loss = 3.2455 (1.223 sec/step)\n","I0830 18:39:34.177957 139622356395904 learning.py:507] global step 7715: loss = 2.2074 (1.253 sec/step)\n","I0830 18:39:35.381987 139622356395904 learning.py:507] global step 7716: loss = 3.1391 (1.202 sec/step)\n","I0830 18:39:36.614308 139622356395904 learning.py:507] global step 7717: loss = 2.5695 (1.230 sec/step)\n","I0830 18:39:37.859446 139622356395904 learning.py:507] global step 7718: loss = 3.9515 (1.243 sec/step)\n","I0830 18:39:39.108026 139622356395904 learning.py:507] global step 7719: loss = 2.9139 (1.247 sec/step)\n","I0830 18:39:40.322350 139622356395904 learning.py:507] global step 7720: loss = 2.6789 (1.212 sec/step)\n","I0830 18:39:41.585760 139622356395904 learning.py:507] global step 7721: loss = 2.5974 (1.261 sec/step)\n","I0830 18:39:42.872150 139622356395904 learning.py:507] global step 7722: loss = 2.6593 (1.285 sec/step)\n","I0830 18:39:44.130169 139622356395904 learning.py:507] global step 7723: loss = 1.9804 (1.256 sec/step)\n","I0830 18:39:45.344218 139622356395904 learning.py:507] global step 7724: loss = 3.1008 (1.212 sec/step)\n","I0830 18:39:46.589698 139622356395904 learning.py:507] global step 7725: loss = 2.6081 (1.244 sec/step)\n","I0830 18:39:47.857303 139622356395904 learning.py:507] global step 7726: loss = 3.4070 (1.266 sec/step)\n","I0830 18:39:49.083956 139622356395904 learning.py:507] global step 7727: loss = 2.3785 (1.225 sec/step)\n","I0830 18:39:50.320909 139622356395904 learning.py:507] global step 7728: loss = 3.2152 (1.235 sec/step)\n","I0830 18:39:51.548689 139622356395904 learning.py:507] global step 7729: loss = 2.1238 (1.226 sec/step)\n","I0830 18:39:52.811952 139622356395904 learning.py:507] global step 7730: loss = 3.2961 (1.261 sec/step)\n","I0830 18:39:54.041170 139622356395904 learning.py:507] global step 7731: loss = 4.0729 (1.227 sec/step)\n","I0830 18:39:55.258241 139622356395904 learning.py:507] global step 7732: loss = 2.3037 (1.215 sec/step)\n","I0830 18:39:56.510666 139622356395904 learning.py:507] global step 7733: loss = 3.1496 (1.251 sec/step)\n","I0830 18:39:57.708715 139622356395904 learning.py:507] global step 7734: loss = 3.3993 (1.196 sec/step)\n","I0830 18:39:58.949998 139622356395904 learning.py:507] global step 7735: loss = 2.3415 (1.239 sec/step)\n","I0830 18:39:59.647133 139619316770560 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n","I0830 18:40:01.180335 139622356395904 learning.py:507] global step 7736: loss = 4.7051 (2.185 sec/step)\n","I0830 18:40:03.755451 139619299985152 supervisor.py:1050] Recording summary at step 7737.\n","I0830 18:40:03.767258 139622356395904 learning.py:507] global step 7737: loss = 2.8781 (2.559 sec/step)\n","I0830 18:40:05.169225 139622356395904 learning.py:507] global step 7738: loss = 2.5953 (1.400 sec/step)\n","I0830 18:40:06.443483 139622356395904 learning.py:507] global step 7739: loss = 2.8965 (1.272 sec/step)\n","I0830 18:40:07.664138 139622356395904 learning.py:507] global step 7740: loss = 2.8402 (1.219 sec/step)\n","I0830 18:40:08.903646 139622356395904 learning.py:507] global step 7741: loss = 2.8556 (1.238 sec/step)\n","I0830 18:40:10.145725 139622356395904 learning.py:507] global step 7742: loss = 2.5609 (1.240 sec/step)\n","I0830 18:40:11.382539 139622356395904 learning.py:507] global step 7743: loss = 2.5938 (1.235 sec/step)\n","I0830 18:40:12.579288 139622356395904 learning.py:507] global step 7744: loss = 2.1159 (1.195 sec/step)\n","I0830 18:40:13.850879 139622356395904 learning.py:507] global step 7745: loss = 3.1875 (1.270 sec/step)\n","I0830 18:40:15.070459 139622356395904 learning.py:507] global step 7746: loss = 2.8017 (1.217 sec/step)\n","I0830 18:40:16.270035 139622356395904 learning.py:507] global step 7747: loss = 2.9353 (1.198 sec/step)\n","I0830 18:40:17.492689 139622356395904 learning.py:507] global step 7748: loss = 2.8317 (1.221 sec/step)\n","I0830 18:40:18.734333 139622356395904 learning.py:507] global step 7749: loss = 2.0390 (1.240 sec/step)\n","I0830 18:40:19.992202 139622356395904 learning.py:507] global step 7750: loss = 2.8091 (1.256 sec/step)\n","I0830 18:40:21.247414 139622356395904 learning.py:507] global step 7751: loss = 2.1166 (1.254 sec/step)\n","I0830 18:40:22.511775 139622356395904 learning.py:507] global step 7752: loss = 3.8267 (1.262 sec/step)\n","I0830 18:40:23.707042 139622356395904 learning.py:507] global step 7753: loss = 2.7915 (1.190 sec/step)\n","I0830 18:40:24.920220 139622356395904 learning.py:507] global step 7754: loss = 2.3006 (1.211 sec/step)\n","I0830 18:40:26.146912 139622356395904 learning.py:507] global step 7755: loss = 2.8457 (1.225 sec/step)\n","I0830 18:40:27.389487 139622356395904 learning.py:507] global step 7756: loss = 2.2203 (1.241 sec/step)\n","I0830 18:40:28.618293 139622356395904 learning.py:507] global step 7757: loss = 3.8353 (1.227 sec/step)\n","I0830 18:40:29.809971 139622356395904 learning.py:507] global step 7758: loss = 3.6898 (1.190 sec/step)\n","I0830 18:40:31.031366 139622356395904 learning.py:507] global step 7759: loss = 2.1306 (1.219 sec/step)\n","I0830 18:40:32.287735 139622356395904 learning.py:507] global step 7760: loss = 3.1539 (1.254 sec/step)\n","I0830 18:40:33.554703 139622356395904 learning.py:507] global step 7761: loss = 2.8565 (1.265 sec/step)\n","I0830 18:40:34.792606 139622356395904 learning.py:507] global step 7762: loss = 2.7012 (1.235 sec/step)\n","I0830 18:40:36.033409 139622356395904 learning.py:507] global step 7763: loss = 2.8237 (1.238 sec/step)\n","I0830 18:40:37.273844 139622356395904 learning.py:507] global step 7764: loss = 2.7818 (1.238 sec/step)\n","I0830 18:40:38.498200 139622356395904 learning.py:507] global step 7765: loss = 2.7107 (1.222 sec/step)\n","I0830 18:40:39.730770 139622356395904 learning.py:507] global step 7766: loss = 2.4252 (1.231 sec/step)\n","I0830 18:40:40.965124 139622356395904 learning.py:507] global step 7767: loss = 2.5093 (1.233 sec/step)\n","I0830 18:40:42.170911 139622356395904 learning.py:507] global step 7768: loss = 3.0478 (1.204 sec/step)\n","I0830 18:40:43.461480 139622356395904 learning.py:507] global step 7769: loss = 2.7920 (1.289 sec/step)\n","I0830 18:40:44.661324 139622356395904 learning.py:507] global step 7770: loss = 3.1343 (1.198 sec/step)\n","I0830 18:40:45.885905 139622356395904 learning.py:507] global step 7771: loss = 2.6065 (1.223 sec/step)\n","I0830 18:40:47.115159 139622356395904 learning.py:507] global step 7772: loss = 3.0625 (1.227 sec/step)\n","I0830 18:40:48.325249 139622356395904 learning.py:507] global step 7773: loss = 2.3707 (1.208 sec/step)\n","I0830 18:40:49.554782 139622356395904 learning.py:507] global step 7774: loss = 2.8006 (1.228 sec/step)\n","I0830 18:40:50.745001 139622356395904 learning.py:507] global step 7775: loss = 2.2702 (1.188 sec/step)\n","I0830 18:40:51.993504 139622356395904 learning.py:507] global step 7776: loss = 2.7032 (1.247 sec/step)\n","I0830 18:40:53.216432 139622356395904 learning.py:507] global step 7777: loss = 3.2669 (1.221 sec/step)\n","I0830 18:40:54.484842 139622356395904 learning.py:507] global step 7778: loss = 3.2125 (1.266 sec/step)\n","I0830 18:40:55.736461 139622356395904 learning.py:507] global step 7779: loss = 2.1589 (1.250 sec/step)\n","I0830 18:40:56.962616 139622356395904 learning.py:507] global step 7780: loss = 2.0054 (1.221 sec/step)\n","I0830 18:40:58.231197 139622356395904 learning.py:507] global step 7781: loss = 2.8894 (1.267 sec/step)\n","I0830 18:40:59.487926 139622356395904 learning.py:507] global step 7782: loss = 2.6217 (1.255 sec/step)\n","I0830 18:41:00.750962 139622356395904 learning.py:507] global step 7783: loss = 2.1802 (1.261 sec/step)\n","I0830 18:41:01.952279 139622356395904 learning.py:507] global step 7784: loss = 3.8634 (1.199 sec/step)\n","I0830 18:41:03.171504 139622356395904 learning.py:507] global step 7785: loss = 2.9292 (1.217 sec/step)\n","I0830 18:41:04.397738 139622356395904 learning.py:507] global step 7786: loss = 2.9045 (1.224 sec/step)\n","I0830 18:41:05.626355 139622356395904 learning.py:507] global step 7787: loss = 2.4598 (1.227 sec/step)\n","I0830 18:41:06.825038 139622356395904 learning.py:507] global step 7788: loss = 2.7213 (1.197 sec/step)\n","I0830 18:41:08.021956 139622356395904 learning.py:507] global step 7789: loss = 2.7425 (1.195 sec/step)\n","I0830 18:41:09.250490 139622356395904 learning.py:507] global step 7790: loss = 2.5566 (1.227 sec/step)\n","I0830 18:41:10.479609 139622356395904 learning.py:507] global step 7791: loss = 2.9083 (1.227 sec/step)\n","I0830 18:41:11.689826 139622356395904 learning.py:507] global step 7792: loss = 2.6444 (1.208 sec/step)\n","I0830 18:41:12.896739 139622356395904 learning.py:507] global step 7793: loss = 2.0938 (1.205 sec/step)\n","I0830 18:41:14.165215 139622356395904 learning.py:507] global step 7794: loss = 2.3432 (1.267 sec/step)\n","I0830 18:41:15.399322 139622356395904 learning.py:507] global step 7795: loss = 2.7143 (1.232 sec/step)\n","I0830 18:41:16.657763 139622356395904 learning.py:507] global step 7796: loss = 2.8507 (1.257 sec/step)\n","I0830 18:41:17.908913 139622356395904 learning.py:507] global step 7797: loss = 2.6248 (1.249 sec/step)\n","I0830 18:41:19.136371 139622356395904 learning.py:507] global step 7798: loss = 2.7537 (1.225 sec/step)\n","I0830 18:41:20.381634 139622356395904 learning.py:507] global step 7799: loss = 3.2470 (1.243 sec/step)\n","I0830 18:41:21.589331 139622356395904 learning.py:507] global step 7800: loss = 3.2251 (1.206 sec/step)\n","I0830 18:41:22.808469 139622356395904 learning.py:507] global step 7801: loss = 2.8658 (1.217 sec/step)\n","I0830 18:41:24.108597 139622356395904 learning.py:507] global step 7802: loss = 2.5212 (1.298 sec/step)\n","I0830 18:41:25.339668 139622356395904 learning.py:507] global step 7803: loss = 2.7205 (1.229 sec/step)\n","I0830 18:41:26.544064 139622356395904 learning.py:507] global step 7804: loss = 3.0539 (1.203 sec/step)\n","I0830 18:41:27.760543 139622356395904 learning.py:507] global step 7805: loss = 2.2563 (1.215 sec/step)\n","I0830 18:41:29.031875 139622356395904 learning.py:507] global step 7806: loss = 2.1730 (1.270 sec/step)\n","I0830 18:41:30.289381 139622356395904 learning.py:507] global step 7807: loss = 2.4754 (1.254 sec/step)\n","I0830 18:41:31.531966 139622356395904 learning.py:507] global step 7808: loss = 2.8212 (1.241 sec/step)\n","I0830 18:41:32.777981 139622356395904 learning.py:507] global step 7809: loss = 2.9583 (1.244 sec/step)\n","I0830 18:41:33.990296 139622356395904 learning.py:507] global step 7810: loss = 2.2202 (1.210 sec/step)\n","I0830 18:41:35.241032 139622356395904 learning.py:507] global step 7811: loss = 2.1181 (1.249 sec/step)\n","I0830 18:41:36.470445 139622356395904 learning.py:507] global step 7812: loss = 2.3807 (1.228 sec/step)\n","I0830 18:41:37.711463 139622356395904 learning.py:507] global step 7813: loss = 2.3509 (1.238 sec/step)\n","I0830 18:41:38.934546 139622356395904 learning.py:507] global step 7814: loss = 3.4419 (1.221 sec/step)\n","I0830 18:41:40.139875 139622356395904 learning.py:507] global step 7815: loss = 2.3438 (1.204 sec/step)\n","I0830 18:41:41.342571 139622356395904 learning.py:507] global step 7816: loss = 2.8811 (1.201 sec/step)\n","I0830 18:41:42.574684 139622356395904 learning.py:507] global step 7817: loss = 2.3241 (1.230 sec/step)\n","I0830 18:41:43.812688 139622356395904 learning.py:507] global step 7818: loss = 2.9608 (1.236 sec/step)\n","I0830 18:41:45.023211 139622356395904 learning.py:507] global step 7819: loss = 3.2054 (1.208 sec/step)\n","I0830 18:41:46.235643 139622356395904 learning.py:507] global step 7820: loss = 2.5989 (1.210 sec/step)\n","I0830 18:41:47.487277 139622356395904 learning.py:507] global step 7821: loss = 2.9017 (1.250 sec/step)\n","I0830 18:41:48.689344 139622356395904 learning.py:507] global step 7822: loss = 2.3787 (1.200 sec/step)\n","I0830 18:41:49.930799 139622356395904 learning.py:507] global step 7823: loss = 4.0214 (1.239 sec/step)\n","I0830 18:41:51.148174 139622356395904 learning.py:507] global step 7824: loss = 2.1064 (1.215 sec/step)\n","I0830 18:41:52.376301 139622356395904 learning.py:507] global step 7825: loss = 2.7440 (1.226 sec/step)\n","I0830 18:41:53.589756 139622356395904 learning.py:507] global step 7826: loss = 2.7977 (1.211 sec/step)\n","I0830 18:41:54.824152 139622356395904 learning.py:507] global step 7827: loss = 2.6022 (1.233 sec/step)\n","I0830 18:41:56.105883 139622356395904 learning.py:507] global step 7828: loss = 3.2044 (1.280 sec/step)\n","I0830 18:41:57.326904 139622356395904 learning.py:507] global step 7829: loss = 2.2430 (1.219 sec/step)\n","I0830 18:41:58.559097 139622356395904 learning.py:507] global step 7830: loss = 1.9733 (1.230 sec/step)\n","I0830 18:41:59.826587 139622356395904 learning.py:507] global step 7831: loss = 2.8514 (1.246 sec/step)\n","I0830 18:42:01.901045 139622356395904 learning.py:507] global step 7832: loss = 2.4566 (2.061 sec/step)\n","I0830 18:42:02.156872 139619299985152 supervisor.py:1050] Recording summary at step 7832.\n","I0830 18:42:03.216619 139622356395904 learning.py:507] global step 7833: loss = 2.3004 (1.314 sec/step)\n","I0830 18:42:04.510461 139622356395904 learning.py:507] global step 7834: loss = 2.5162 (1.290 sec/step)\n","I0830 18:42:05.726994 139622356395904 learning.py:507] global step 7835: loss = 3.0639 (1.214 sec/step)\n","I0830 18:42:06.969243 139622356395904 learning.py:507] global step 7836: loss = 2.8088 (1.240 sec/step)\n","I0830 18:42:08.208943 139622356395904 learning.py:507] global step 7837: loss = 2.1847 (1.238 sec/step)\n","I0830 18:42:09.460645 139622356395904 learning.py:507] global step 7838: loss = 3.5136 (1.250 sec/step)\n","I0830 18:42:10.721909 139622356395904 learning.py:507] global step 7839: loss = 2.6464 (1.259 sec/step)\n","I0830 18:42:11.965608 139622356395904 learning.py:507] global step 7840: loss = 2.5432 (1.239 sec/step)\n","I0830 18:42:13.178717 139622356395904 learning.py:507] global step 7841: loss = 2.6636 (1.211 sec/step)\n","I0830 18:42:14.432348 139622356395904 learning.py:507] global step 7842: loss = 3.0330 (1.252 sec/step)\n","I0830 18:42:15.663445 139622356395904 learning.py:507] global step 7843: loss = 2.8133 (1.229 sec/step)\n","I0830 18:42:16.898342 139622356395904 learning.py:507] global step 7844: loss = 2.8492 (1.233 sec/step)\n","I0830 18:42:18.118377 139622356395904 learning.py:507] global step 7845: loss = 2.0968 (1.218 sec/step)\n","I0830 18:42:19.348307 139622356395904 learning.py:507] global step 7846: loss = 2.3458 (1.228 sec/step)\n","I0830 18:42:20.599174 139622356395904 learning.py:507] global step 7847: loss = 2.8760 (1.246 sec/step)\n","I0830 18:42:21.826374 139622356395904 learning.py:507] global step 7848: loss = 2.9948 (1.225 sec/step)\n","I0830 18:42:23.048620 139622356395904 learning.py:507] global step 7849: loss = 2.7251 (1.220 sec/step)\n","I0830 18:42:24.266417 139622356395904 learning.py:507] global step 7850: loss = 2.9509 (1.216 sec/step)\n","I0830 18:42:25.486336 139622356395904 learning.py:507] global step 7851: loss = 2.3207 (1.218 sec/step)\n","I0830 18:42:26.766589 139622356395904 learning.py:507] global step 7852: loss = 2.9956 (1.279 sec/step)\n","I0830 18:42:28.002551 139622356395904 learning.py:507] global step 7853: loss = 2.1135 (1.234 sec/step)\n","I0830 18:42:29.232123 139622356395904 learning.py:507] global step 7854: loss = 2.5947 (1.228 sec/step)\n","I0830 18:42:30.439501 139622356395904 learning.py:507] global step 7855: loss = 3.7586 (1.206 sec/step)\n","I0830 18:42:31.678951 139622356395904 learning.py:507] global step 7856: loss = 4.9242 (1.237 sec/step)\n","I0830 18:42:32.901733 139622356395904 learning.py:507] global step 7857: loss = 2.8550 (1.221 sec/step)\n","I0830 18:42:34.135653 139622356395904 learning.py:507] global step 7858: loss = 2.5956 (1.232 sec/step)\n","I0830 18:42:35.396854 139622356395904 learning.py:507] global step 7859: loss = 2.4889 (1.259 sec/step)\n","I0830 18:42:36.649770 139622356395904 learning.py:507] global step 7860: loss = 2.3900 (1.251 sec/step)\n","I0830 18:42:37.904860 139622356395904 learning.py:507] global step 7861: loss = 2.2896 (1.253 sec/step)\n","I0830 18:42:39.159718 139622356395904 learning.py:507] global step 7862: loss = 2.8887 (1.253 sec/step)\n","I0830 18:42:40.418743 139622356395904 learning.py:507] global step 7863: loss = 3.2997 (1.257 sec/step)\n","I0830 18:42:41.638723 139622356395904 learning.py:507] global step 7864: loss = 1.9419 (1.218 sec/step)\n","I0830 18:42:42.873910 139622356395904 learning.py:507] global step 7865: loss = 3.0768 (1.233 sec/step)\n","I0830 18:42:44.067827 139622356395904 learning.py:507] global step 7866: loss = 3.3632 (1.192 sec/step)\n","I0830 18:42:45.314045 139622356395904 learning.py:507] global step 7867: loss = 3.0774 (1.244 sec/step)\n","I0830 18:42:46.546316 139622356395904 learning.py:507] global step 7868: loss = 2.1656 (1.230 sec/step)\n","I0830 18:42:47.784627 139622356395904 learning.py:507] global step 7869: loss = 2.2228 (1.236 sec/step)\n","I0830 18:42:48.984316 139622356395904 learning.py:507] global step 7870: loss = 2.4607 (1.198 sec/step)\n","I0830 18:42:50.230669 139622356395904 learning.py:507] global step 7871: loss = 2.8799 (1.244 sec/step)\n","I0830 18:42:51.430617 139622356395904 learning.py:507] global step 7872: loss = 2.4452 (1.198 sec/step)\n","I0830 18:42:52.663464 139622356395904 learning.py:507] global step 7873: loss = 2.5187 (1.231 sec/step)\n","I0830 18:42:53.901975 139622356395904 learning.py:507] global step 7874: loss = 2.7890 (1.237 sec/step)\n","I0830 18:42:55.110187 139622356395904 learning.py:507] global step 7875: loss = 3.2331 (1.206 sec/step)\n","I0830 18:42:56.320265 139622356395904 learning.py:507] global step 7876: loss = 2.4705 (1.208 sec/step)\n","I0830 18:42:57.536889 139622356395904 learning.py:507] global step 7877: loss = 2.5814 (1.214 sec/step)\n","I0830 18:42:58.777732 139622356395904 learning.py:507] global step 7878: loss = 2.9684 (1.239 sec/step)\n","I0830 18:43:00.028419 139622356395904 learning.py:507] global step 7879: loss = 2.1032 (1.248 sec/step)\n","I0830 18:43:01.275973 139622356395904 learning.py:507] global step 7880: loss = 2.6352 (1.246 sec/step)\n","I0830 18:43:02.527623 139622356395904 learning.py:507] global step 7881: loss = 2.8023 (1.249 sec/step)\n","I0830 18:43:03.726609 139622356395904 learning.py:507] global step 7882: loss = 2.0044 (1.197 sec/step)\n","I0830 18:43:04.970747 139622356395904 learning.py:507] global step 7883: loss = 2.5305 (1.242 sec/step)\n","I0830 18:43:06.184285 139622356395904 learning.py:507] global step 7884: loss = 2.7053 (1.212 sec/step)\n","I0830 18:43:07.415141 139622356395904 learning.py:507] global step 7885: loss = 2.2529 (1.228 sec/step)\n","I0830 18:43:08.672324 139622356395904 learning.py:507] global step 7886: loss = 2.3726 (1.255 sec/step)\n","I0830 18:43:09.887171 139622356395904 learning.py:507] global step 7887: loss = 2.5420 (1.213 sec/step)\n","I0830 18:43:11.137573 139622356395904 learning.py:507] global step 7888: loss = 2.2242 (1.248 sec/step)\n","I0830 18:43:12.341861 139622356395904 learning.py:507] global step 7889: loss = 2.4798 (1.202 sec/step)\n","I0830 18:43:13.556133 139622356395904 learning.py:507] global step 7890: loss = 2.3911 (1.212 sec/step)\n","I0830 18:43:14.785388 139622356395904 learning.py:507] global step 7891: loss = 2.7551 (1.228 sec/step)\n","I0830 18:43:16.034249 139622356395904 learning.py:507] global step 7892: loss = 2.5001 (1.247 sec/step)\n","I0830 18:43:17.258176 139622356395904 learning.py:507] global step 7893: loss = 2.7729 (1.222 sec/step)\n","I0830 18:43:18.494788 139622356395904 learning.py:507] global step 7894: loss = 2.6573 (1.235 sec/step)\n","I0830 18:43:19.711918 139622356395904 learning.py:507] global step 7895: loss = 2.7327 (1.215 sec/step)\n","I0830 18:43:20.928093 139622356395904 learning.py:507] global step 7896: loss = 2.1531 (1.215 sec/step)\n","I0830 18:43:22.155185 139622356395904 learning.py:507] global step 7897: loss = 2.8677 (1.225 sec/step)\n","I0830 18:43:23.366168 139622356395904 learning.py:507] global step 7898: loss = 2.5417 (1.209 sec/step)\n","I0830 18:43:24.589239 139622356395904 learning.py:507] global step 7899: loss = 2.8251 (1.221 sec/step)\n","I0830 18:43:25.800245 139622356395904 learning.py:507] global step 7900: loss = 3.6538 (1.209 sec/step)\n","I0830 18:43:27.013597 139622356395904 learning.py:507] global step 7901: loss = 2.4861 (1.210 sec/step)\n","I0830 18:43:28.224306 139622356395904 learning.py:507] global step 7902: loss = 3.2238 (1.208 sec/step)\n","I0830 18:43:29.414364 139622356395904 learning.py:507] global step 7903: loss = 3.4501 (1.188 sec/step)\n","I0830 18:43:30.685815 139622356395904 learning.py:507] global step 7904: loss = 2.9933 (1.270 sec/step)\n","I0830 18:43:31.895176 139622356395904 learning.py:507] global step 7905: loss = 2.9794 (1.208 sec/step)\n","I0830 18:43:33.129479 139622356395904 learning.py:507] global step 7906: loss = 3.2414 (1.233 sec/step)\n","I0830 18:43:34.353698 139622356395904 learning.py:507] global step 7907: loss = 2.8822 (1.222 sec/step)\n","I0830 18:43:35.580685 139622356395904 learning.py:507] global step 7908: loss = 4.0887 (1.225 sec/step)\n","I0830 18:43:36.804082 139622356395904 learning.py:507] global step 7909: loss = 2.2531 (1.221 sec/step)\n","I0830 18:43:38.034916 139622356395904 learning.py:507] global step 7910: loss = 2.8163 (1.229 sec/step)\n","I0830 18:43:39.262187 139622356395904 learning.py:507] global step 7911: loss = 4.0825 (1.225 sec/step)\n","I0830 18:43:40.477934 139622356395904 learning.py:507] global step 7912: loss = 2.4797 (1.214 sec/step)\n","I0830 18:43:41.680830 139622356395904 learning.py:507] global step 7913: loss = 2.6808 (1.201 sec/step)\n","I0830 18:43:42.934633 139622356395904 learning.py:507] global step 7914: loss = 3.2551 (1.251 sec/step)\n","I0830 18:43:44.168527 139622356395904 learning.py:507] global step 7915: loss = 2.3976 (1.232 sec/step)\n","I0830 18:43:45.398256 139622356395904 learning.py:507] global step 7916: loss = 2.4891 (1.228 sec/step)\n","I0830 18:43:46.632616 139622356395904 learning.py:507] global step 7917: loss = 3.7436 (1.232 sec/step)\n","I0830 18:43:47.864060 139622356395904 learning.py:507] global step 7918: loss = 3.1172 (1.229 sec/step)\n","I0830 18:43:49.117466 139622356395904 learning.py:507] global step 7919: loss = 2.2540 (1.252 sec/step)\n","I0830 18:43:50.341445 139622356395904 learning.py:507] global step 7920: loss = 3.1510 (1.222 sec/step)\n","I0830 18:43:51.580172 139622356395904 learning.py:507] global step 7921: loss = 2.2632 (1.237 sec/step)\n","I0830 18:43:52.767394 139622356395904 learning.py:507] global step 7922: loss = 2.9016 (1.185 sec/step)\n","I0830 18:43:54.014382 139622356395904 learning.py:507] global step 7923: loss = 2.5439 (1.245 sec/step)\n","I0830 18:43:55.253638 139622356395904 learning.py:507] global step 7924: loss = 2.8118 (1.238 sec/step)\n","I0830 18:43:56.491494 139622356395904 learning.py:507] global step 7925: loss = 2.1586 (1.236 sec/step)\n","I0830 18:43:57.724516 139622356395904 learning.py:507] global step 7926: loss = 3.3122 (1.231 sec/step)\n","I0830 18:43:58.936943 139622356395904 learning.py:507] global step 7927: loss = 2.6509 (1.210 sec/step)\n","I0830 18:44:00.261424 139622356395904 learning.py:507] global step 7928: loss = 2.6316 (1.260 sec/step)\n","I0830 18:44:02.298116 139619299985152 supervisor.py:1050] Recording summary at step 7929.\n","I0830 18:44:02.336016 139622356395904 learning.py:507] global step 7929: loss = 3.3419 (2.070 sec/step)\n","I0830 18:44:03.572302 139622356395904 learning.py:507] global step 7930: loss = 2.3445 (1.234 sec/step)\n","I0830 18:44:04.799324 139622356395904 learning.py:507] global step 7931: loss = 2.4527 (1.225 sec/step)\n","I0830 18:44:06.006542 139622356395904 learning.py:507] global step 7932: loss = 3.5381 (1.205 sec/step)\n","I0830 18:44:07.215140 139622356395904 learning.py:507] global step 7933: loss = 2.5079 (1.206 sec/step)\n","I0830 18:44:08.432006 139622356395904 learning.py:507] global step 7934: loss = 2.4173 (1.215 sec/step)\n","I0830 18:44:09.650024 139622356395904 learning.py:507] global step 7935: loss = 2.7759 (1.216 sec/step)\n","I0830 18:44:10.865653 139622356395904 learning.py:507] global step 7936: loss = 2.4774 (1.214 sec/step)\n","I0830 18:44:12.076889 139622356395904 learning.py:507] global step 7937: loss = 2.3030 (1.209 sec/step)\n","I0830 18:44:13.291034 139622356395904 learning.py:507] global step 7938: loss = 2.6817 (1.212 sec/step)\n","I0830 18:44:14.541602 139622356395904 learning.py:507] global step 7939: loss = 2.5684 (1.248 sec/step)\n","I0830 18:44:15.808506 139622356395904 learning.py:507] global step 7940: loss = 3.2429 (1.265 sec/step)\n","I0830 18:44:17.040757 139622356395904 learning.py:507] global step 7941: loss = 2.7464 (1.229 sec/step)\n","I0830 18:44:18.303690 139622356395904 learning.py:507] global step 7942: loss = 2.9929 (1.261 sec/step)\n","I0830 18:44:19.530211 139622356395904 learning.py:507] global step 7943: loss = 2.5975 (1.225 sec/step)\n","I0830 18:44:20.747751 139622356395904 learning.py:507] global step 7944: loss = 2.3017 (1.216 sec/step)\n","I0830 18:44:21.932216 139622356395904 learning.py:507] global step 7945: loss = 2.5104 (1.183 sec/step)\n","I0830 18:44:23.154447 139622356395904 learning.py:507] global step 7946: loss = 3.1542 (1.220 sec/step)\n","I0830 18:44:24.372720 139622356395904 learning.py:507] global step 7947: loss = 2.0173 (1.216 sec/step)\n","I0830 18:44:25.590735 139622356395904 learning.py:507] global step 7948: loss = 3.8161 (1.216 sec/step)\n","I0830 18:44:26.820077 139622356395904 learning.py:507] global step 7949: loss = 2.5808 (1.227 sec/step)\n","I0830 18:44:28.056363 139622356395904 learning.py:507] global step 7950: loss = 3.5058 (1.234 sec/step)\n","I0830 18:44:29.293284 139622356395904 learning.py:507] global step 7951: loss = 2.9933 (1.235 sec/step)\n","I0830 18:44:30.529979 139622356395904 learning.py:507] global step 7952: loss = 3.0615 (1.235 sec/step)\n","I0830 18:44:31.735650 139622356395904 learning.py:507] global step 7953: loss = 2.2781 (1.204 sec/step)\n","I0830 18:44:32.970951 139622356395904 learning.py:507] global step 7954: loss = 2.8954 (1.233 sec/step)\n","I0830 18:44:34.237393 139622356395904 learning.py:507] global step 7955: loss = 2.6709 (1.265 sec/step)\n","I0830 18:44:35.419709 139622356395904 learning.py:507] global step 7956: loss = 2.8096 (1.180 sec/step)\n","I0830 18:44:36.630936 139622356395904 learning.py:507] global step 7957: loss = 3.3082 (1.209 sec/step)\n","I0830 18:44:37.858892 139622356395904 learning.py:507] global step 7958: loss = 2.6524 (1.226 sec/step)\n","I0830 18:44:39.069212 139622356395904 learning.py:507] global step 7959: loss = 2.9412 (1.208 sec/step)\n","I0830 18:44:40.290631 139622356395904 learning.py:507] global step 7960: loss = 3.3884 (1.219 sec/step)\n","I0830 18:44:41.507945 139622356395904 learning.py:507] global step 7961: loss = 1.9123 (1.216 sec/step)\n","I0830 18:44:42.740174 139622356395904 learning.py:507] global step 7962: loss = 2.8680 (1.230 sec/step)\n","I0830 18:44:43.986156 139622356395904 learning.py:507] global step 7963: loss = 2.6933 (1.244 sec/step)\n","I0830 18:44:45.221273 139622356395904 learning.py:507] global step 7964: loss = 2.6432 (1.233 sec/step)\n","I0830 18:44:46.488990 139622356395904 learning.py:507] global step 7965: loss = 3.1050 (1.266 sec/step)\n","I0830 18:44:47.690643 139622356395904 learning.py:507] global step 7966: loss = 4.4890 (1.200 sec/step)\n","I0830 18:44:48.917591 139622356395904 learning.py:507] global step 7967: loss = 1.9577 (1.225 sec/step)\n","I0830 18:44:50.143137 139622356395904 learning.py:507] global step 7968: loss = 3.6872 (1.224 sec/step)\n","I0830 18:44:51.378083 139622356395904 learning.py:507] global step 7969: loss = 2.0998 (1.233 sec/step)\n","I0830 18:44:52.611103 139622356395904 learning.py:507] global step 7970: loss = 2.4101 (1.231 sec/step)\n","I0830 18:44:53.842432 139622356395904 learning.py:507] global step 7971: loss = 2.0854 (1.229 sec/step)\n","I0830 18:44:55.025940 139622356395904 learning.py:507] global step 7972: loss = 3.0501 (1.182 sec/step)\n","I0830 18:44:56.271838 139622356395904 learning.py:507] global step 7973: loss = 2.0819 (1.244 sec/step)\n","I0830 18:44:57.488866 139622356395904 learning.py:507] global step 7974: loss = 2.0422 (1.215 sec/step)\n","I0830 18:44:58.730339 139622356395904 learning.py:507] global step 7975: loss = 2.3911 (1.240 sec/step)\n","I0830 18:44:59.964010 139622356395904 learning.py:507] global step 7976: loss = 3.5840 (1.232 sec/step)\n","I0830 18:45:01.203479 139622356395904 learning.py:507] global step 7977: loss = 2.2245 (1.237 sec/step)\n","I0830 18:45:02.416082 139622356395904 learning.py:507] global step 7978: loss = 3.0657 (1.210 sec/step)\n","I0830 18:45:03.657306 139622356395904 learning.py:507] global step 7979: loss = 3.1912 (1.239 sec/step)\n","I0830 18:45:04.865662 139622356395904 learning.py:507] global step 7980: loss = 3.1457 (1.207 sec/step)\n","I0830 18:45:06.089410 139622356395904 learning.py:507] global step 7981: loss = 2.4214 (1.222 sec/step)\n","I0830 18:45:07.300037 139622356395904 learning.py:507] global step 7982: loss = 3.0900 (1.208 sec/step)\n","I0830 18:45:08.537809 139622356395904 learning.py:507] global step 7983: loss = 2.5405 (1.236 sec/step)\n","I0830 18:45:09.778952 139622356395904 learning.py:507] global step 7984: loss = 3.2908 (1.239 sec/step)\n","I0830 18:45:11.000124 139622356395904 learning.py:507] global step 7985: loss = 3.6974 (1.219 sec/step)\n","I0830 18:45:12.201978 139622356395904 learning.py:507] global step 7986: loss = 2.5013 (1.200 sec/step)\n","I0830 18:45:13.381791 139622356395904 learning.py:507] global step 7987: loss = 2.2044 (1.178 sec/step)\n","I0830 18:45:14.598732 139622356395904 learning.py:507] global step 7988: loss = 2.8163 (1.215 sec/step)\n","I0830 18:45:15.852003 139622356395904 learning.py:507] global step 7989: loss = 2.5203 (1.252 sec/step)\n","I0830 18:45:17.041953 139622356395904 learning.py:507] global step 7990: loss = 3.3558 (1.188 sec/step)\n","I0830 18:45:18.278196 139622356395904 learning.py:507] global step 7991: loss = 3.1862 (1.234 sec/step)\n","I0830 18:45:19.538287 139622356395904 learning.py:507] global step 7992: loss = 3.3385 (1.258 sec/step)\n","I0830 18:45:20.778514 139622356395904 learning.py:507] global step 7993: loss = 3.0878 (1.238 sec/step)\n","I0830 18:45:21.967881 139622356395904 learning.py:507] global step 7994: loss = 2.1644 (1.187 sec/step)\n","I0830 18:45:23.199905 139622356395904 learning.py:507] global step 7995: loss = 2.0511 (1.230 sec/step)\n","I0830 18:45:24.438864 139622356395904 learning.py:507] global step 7996: loss = 3.5488 (1.237 sec/step)\n","I0830 18:45:25.639541 139622356395904 learning.py:507] global step 7997: loss = 2.2886 (1.198 sec/step)\n","I0830 18:45:26.882616 139622356395904 learning.py:507] global step 7998: loss = 2.3511 (1.241 sec/step)\n","I0830 18:45:28.123208 139622356395904 learning.py:507] global step 7999: loss = 2.5927 (1.239 sec/step)\n","I0830 18:45:29.339268 139622356395904 learning.py:507] global step 8000: loss = 2.1622 (1.214 sec/step)\n","I0830 18:45:30.573644 139622356395904 learning.py:507] global step 8001: loss = 2.4420 (1.233 sec/step)\n","I0830 18:45:31.841254 139622356395904 learning.py:507] global step 8002: loss = 2.9899 (1.266 sec/step)\n","I0830 18:45:33.074487 139622356395904 learning.py:507] global step 8003: loss = 2.8371 (1.231 sec/step)\n","I0830 18:45:34.351624 139622356395904 learning.py:507] global step 8004: loss = 2.6836 (1.273 sec/step)\n","I0830 18:45:35.621757 139622356395904 learning.py:507] global step 8005: loss = 2.0424 (1.265 sec/step)\n","I0830 18:45:36.835574 139622356395904 learning.py:507] global step 8006: loss = 2.3340 (1.212 sec/step)\n","I0830 18:45:38.082025 139622356395904 learning.py:507] global step 8007: loss = 2.9564 (1.244 sec/step)\n","I0830 18:45:39.257168 139622356395904 learning.py:507] global step 8008: loss = 3.3544 (1.173 sec/step)\n","I0830 18:45:40.469264 139622356395904 learning.py:507] global step 8009: loss = 2.7255 (1.210 sec/step)\n","I0830 18:45:41.712755 139622356395904 learning.py:507] global step 8010: loss = 2.4031 (1.242 sec/step)\n","I0830 18:45:42.941777 139622356395904 learning.py:507] global step 8011: loss = 2.3814 (1.227 sec/step)\n","I0830 18:45:44.147668 139622356395904 learning.py:507] global step 8012: loss = 3.3037 (1.203 sec/step)\n","I0830 18:45:45.318041 139622356395904 learning.py:507] global step 8013: loss = 2.5204 (1.168 sec/step)\n","I0830 18:45:46.537528 139622356395904 learning.py:507] global step 8014: loss = 2.7752 (1.217 sec/step)\n","I0830 18:45:47.772494 139622356395904 learning.py:507] global step 8015: loss = 3.7257 (1.233 sec/step)\n","I0830 18:45:48.992602 139622356395904 learning.py:507] global step 8016: loss = 2.2106 (1.218 sec/step)\n","I0830 18:45:50.235347 139622356395904 learning.py:507] global step 8017: loss = 2.5445 (1.241 sec/step)\n","I0830 18:45:51.467608 139622356395904 learning.py:507] global step 8018: loss = 2.2914 (1.230 sec/step)\n","I0830 18:45:52.679491 139622356395904 learning.py:507] global step 8019: loss = 2.4746 (1.210 sec/step)\n","I0830 18:45:53.883851 139622356395904 learning.py:507] global step 8020: loss = 2.4491 (1.203 sec/step)\n","I0830 18:45:55.091904 139622356395904 learning.py:507] global step 8021: loss = 2.1161 (1.206 sec/step)\n","I0830 18:45:56.346163 139622356395904 learning.py:507] global step 8022: loss = 3.3736 (1.252 sec/step)\n","I0830 18:45:57.583568 139622356395904 learning.py:507] global step 8023: loss = 2.7481 (1.235 sec/step)\n","I0830 18:45:58.828804 139622356395904 learning.py:507] global step 8024: loss = 3.3966 (1.243 sec/step)\n","I0830 18:46:00.134961 139622356395904 learning.py:507] global step 8025: loss = 3.1454 (1.240 sec/step)\n","I0830 18:46:02.181555 139619299985152 supervisor.py:1050] Recording summary at step 8025.\n","I0830 18:46:02.235926 139622356395904 learning.py:507] global step 8026: loss = 2.3928 (2.098 sec/step)\n","I0830 18:46:03.485396 139622356395904 learning.py:507] global step 8027: loss = 2.9583 (1.248 sec/step)\n","I0830 18:46:04.694618 139622356395904 learning.py:507] global step 8028: loss = 3.3900 (1.207 sec/step)\n","I0830 18:46:05.879769 139622356395904 learning.py:507] global step 8029: loss = 2.9354 (1.183 sec/step)\n","I0830 18:46:07.116695 139622356395904 learning.py:507] global step 8030: loss = 2.9247 (1.235 sec/step)\n","I0830 18:46:08.322743 139622356395904 learning.py:507] global step 8031: loss = 3.0718 (1.204 sec/step)\n","I0830 18:46:09.540078 139622356395904 learning.py:507] global step 8032: loss = 2.4285 (1.215 sec/step)\n","I0830 18:46:10.788077 139622356395904 learning.py:507] global step 8033: loss = 2.2142 (1.246 sec/step)\n","I0830 18:46:12.023478 139622356395904 learning.py:507] global step 8034: loss = 3.2630 (1.233 sec/step)\n","I0830 18:46:13.231070 139622356395904 learning.py:507] global step 8035: loss = 2.2944 (1.204 sec/step)\n","I0830 18:46:14.458727 139622356395904 learning.py:507] global step 8036: loss = 3.0659 (1.226 sec/step)\n","I0830 18:46:15.677464 139622356395904 learning.py:507] global step 8037: loss = 1.9671 (1.217 sec/step)\n","I0830 18:46:16.898764 139622356395904 learning.py:507] global step 8038: loss = 3.3754 (1.219 sec/step)\n","I0830 18:46:18.142212 139622356395904 learning.py:507] global step 8039: loss = 3.2599 (1.242 sec/step)\n","I0830 18:46:19.397505 139622356395904 learning.py:507] global step 8040: loss = 2.0544 (1.253 sec/step)\n","I0830 18:46:20.610384 139622356395904 learning.py:507] global step 8041: loss = 2.7073 (1.211 sec/step)\n","I0830 18:46:21.820529 139622356395904 learning.py:507] global step 8042: loss = 2.2934 (1.208 sec/step)\n","I0830 18:46:23.016911 139622356395904 learning.py:507] global step 8043: loss = 2.3990 (1.195 sec/step)\n","I0830 18:46:24.233858 139622356395904 learning.py:507] global step 8044: loss = 3.4933 (1.215 sec/step)\n","I0830 18:46:25.479375 139622356395904 learning.py:507] global step 8045: loss = 2.2375 (1.244 sec/step)\n","I0830 18:46:26.675786 139622356395904 learning.py:507] global step 8046: loss = 2.4467 (1.194 sec/step)\n","I0830 18:46:27.897992 139622356395904 learning.py:507] global step 8047: loss = 3.4360 (1.220 sec/step)\n","I0830 18:46:29.145249 139622356395904 learning.py:507] global step 8048: loss = 2.4149 (1.245 sec/step)\n","I0830 18:46:30.390140 139622356395904 learning.py:507] global step 8049: loss = 3.0186 (1.243 sec/step)\n","I0830 18:46:31.647489 139622356395904 learning.py:507] global step 8050: loss = 2.4755 (1.255 sec/step)\n","I0830 18:46:32.845877 139622356395904 learning.py:507] global step 8051: loss = 3.3476 (1.196 sec/step)\n","I0830 18:46:34.077133 139622356395904 learning.py:507] global step 8052: loss = 3.4814 (1.229 sec/step)\n","I0830 18:46:35.369178 139622356395904 learning.py:507] global step 8053: loss = 1.9848 (1.290 sec/step)\n","I0830 18:46:36.625727 139622356395904 learning.py:507] global step 8054: loss = 2.3008 (1.255 sec/step)\n","I0830 18:46:37.842239 139622356395904 learning.py:507] global step 8055: loss = 3.4876 (1.215 sec/step)\n","I0830 18:46:39.072989 139622356395904 learning.py:507] global step 8056: loss = 2.1822 (1.229 sec/step)\n","I0830 18:46:40.316854 139622356395904 learning.py:507] global step 8057: loss = 2.9278 (1.242 sec/step)\n","I0830 18:46:41.567484 139622356395904 learning.py:507] global step 8058: loss = 3.1163 (1.249 sec/step)\n","I0830 18:46:42.829399 139622356395904 learning.py:507] global step 8059: loss = 4.2456 (1.260 sec/step)\n","I0830 18:46:44.039929 139622356395904 learning.py:507] global step 8060: loss = 3.1546 (1.209 sec/step)\n","I0830 18:46:45.274185 139622356395904 learning.py:507] global step 8061: loss = 2.0465 (1.232 sec/step)\n","I0830 18:46:46.493937 139622356395904 learning.py:507] global step 8062: loss = 2.6927 (1.218 sec/step)\n","I0830 18:46:47.724458 139622356395904 learning.py:507] global step 8063: loss = 3.6696 (1.229 sec/step)\n","I0830 18:46:48.961782 139622356395904 learning.py:507] global step 8064: loss = 2.6203 (1.236 sec/step)\n","I0830 18:46:50.179882 139622356395904 learning.py:507] global step 8065: loss = 2.6366 (1.216 sec/step)\n","I0830 18:46:51.420753 139622356395904 learning.py:507] global step 8066: loss = 3.8198 (1.239 sec/step)\n","I0830 18:46:52.636281 139622356395904 learning.py:507] global step 8067: loss = 2.5984 (1.214 sec/step)\n","I0830 18:46:53.883819 139622356395904 learning.py:507] global step 8068: loss = 2.1895 (1.246 sec/step)\n","I0830 18:46:55.093347 139622356395904 learning.py:507] global step 8069: loss = 3.1033 (1.208 sec/step)\n","I0830 18:46:56.311912 139622356395904 learning.py:507] global step 8070: loss = 3.4058 (1.217 sec/step)\n","I0830 18:46:57.563599 139622356395904 learning.py:507] global step 8071: loss = 3.0066 (1.250 sec/step)\n","I0830 18:46:58.783204 139622356395904 learning.py:507] global step 8072: loss = 2.9942 (1.218 sec/step)\n","I0830 18:47:00.024158 139622356395904 learning.py:507] global step 8073: loss = 2.6372 (1.239 sec/step)\n","I0830 18:47:01.289789 139622356395904 learning.py:507] global step 8074: loss = 2.9209 (1.264 sec/step)\n","I0830 18:47:02.532985 139622356395904 learning.py:507] global step 8075: loss = 3.2451 (1.241 sec/step)\n","I0830 18:47:03.786321 139622356395904 learning.py:507] global step 8076: loss = 2.9866 (1.252 sec/step)\n","I0830 18:47:05.012348 139622356395904 learning.py:507] global step 8077: loss = 1.9859 (1.224 sec/step)\n","I0830 18:47:06.222081 139622356395904 learning.py:507] global step 8078: loss = 2.5313 (1.208 sec/step)\n","I0830 18:47:07.450992 139622356395904 learning.py:507] global step 8079: loss = 2.1532 (1.227 sec/step)\n","I0830 18:47:08.682019 139622356395904 learning.py:507] global step 8080: loss = 2.3603 (1.229 sec/step)\n","I0830 18:47:09.905941 139622356395904 learning.py:507] global step 8081: loss = 2.5348 (1.222 sec/step)\n","I0830 18:47:11.116341 139622356395904 learning.py:507] global step 8082: loss = 2.5157 (1.209 sec/step)\n","I0830 18:47:12.341962 139622356395904 learning.py:507] global step 8083: loss = 2.4833 (1.224 sec/step)\n","I0830 18:47:13.605621 139622356395904 learning.py:507] global step 8084: loss = 3.3762 (1.262 sec/step)\n","I0830 18:47:14.793758 139622356395904 learning.py:507] global step 8085: loss = 2.9125 (1.186 sec/step)\n","I0830 18:47:16.042884 139622356395904 learning.py:507] global step 8086: loss = 2.6363 (1.247 sec/step)\n","I0830 18:47:17.264893 139622356395904 learning.py:507] global step 8087: loss = 2.9063 (1.220 sec/step)\n","I0830 18:47:18.483550 139622356395904 learning.py:507] global step 8088: loss = 2.8348 (1.217 sec/step)\n","I0830 18:47:19.678248 139622356395904 learning.py:507] global step 8089: loss = 2.4464 (1.193 sec/step)\n","I0830 18:47:20.909173 139622356395904 learning.py:507] global step 8090: loss = 2.6027 (1.229 sec/step)\n","I0830 18:47:22.123154 139622356395904 learning.py:507] global step 8091: loss = 2.7556 (1.212 sec/step)\n","I0830 18:47:23.368137 139622356395904 learning.py:507] global step 8092: loss = 2.3161 (1.243 sec/step)\n","I0830 18:47:24.597177 139622356395904 learning.py:507] global step 8093: loss = 2.9045 (1.227 sec/step)\n","I0830 18:47:25.847847 139622356395904 learning.py:507] global step 8094: loss = 2.3138 (1.249 sec/step)\n","I0830 18:47:27.055663 139622356395904 learning.py:507] global step 8095: loss = 2.0811 (1.205 sec/step)\n","I0830 18:47:28.253233 139622356395904 learning.py:507] global step 8096: loss = 2.7092 (1.195 sec/step)\n","I0830 18:47:29.479356 139622356395904 learning.py:507] global step 8097: loss = 2.6172 (1.224 sec/step)\n","I0830 18:47:30.685978 139622356395904 learning.py:507] global step 8098: loss = 2.6884 (1.205 sec/step)\n","I0830 18:47:31.937654 139622356395904 learning.py:507] global step 8099: loss = 2.2445 (1.250 sec/step)\n","I0830 18:47:33.142641 139622356395904 learning.py:507] global step 8100: loss = 2.6899 (1.203 sec/step)\n","I0830 18:47:34.359148 139622356395904 learning.py:507] global step 8101: loss = 3.4202 (1.214 sec/step)\n","I0830 18:47:35.595995 139622356395904 learning.py:507] global step 8102: loss = 2.4482 (1.235 sec/step)\n","I0830 18:47:36.842565 139622356395904 learning.py:507] global step 8103: loss = 3.1310 (1.245 sec/step)\n","I0830 18:47:38.077926 139622356395904 learning.py:507] global step 8104: loss = 2.3921 (1.233 sec/step)\n","I0830 18:47:39.297707 139622356395904 learning.py:507] global step 8105: loss = 3.3506 (1.218 sec/step)\n","I0830 18:47:40.517070 139622356395904 learning.py:507] global step 8106: loss = 2.6511 (1.218 sec/step)\n","I0830 18:47:41.745794 139622356395904 learning.py:507] global step 8107: loss = 3.9426 (1.227 sec/step)\n","I0830 18:47:42.977730 139622356395904 learning.py:507] global step 8108: loss = 3.6642 (1.230 sec/step)\n","I0830 18:47:44.236923 139622356395904 learning.py:507] global step 8109: loss = 3.3466 (1.257 sec/step)\n","I0830 18:47:45.497174 139622356395904 learning.py:507] global step 8110: loss = 4.0754 (1.258 sec/step)\n","I0830 18:47:46.723556 139622356395904 learning.py:507] global step 8111: loss = 3.2296 (1.225 sec/step)\n","I0830 18:47:47.924923 139622356395904 learning.py:507] global step 8112: loss = 2.5389 (1.200 sec/step)\n","I0830 18:47:49.156641 139622356395904 learning.py:507] global step 8113: loss = 2.3093 (1.230 sec/step)\n","I0830 18:47:50.383502 139622356395904 learning.py:507] global step 8114: loss = 2.3069 (1.224 sec/step)\n","I0830 18:47:51.634988 139622356395904 learning.py:507] global step 8115: loss = 2.7637 (1.250 sec/step)\n","I0830 18:47:52.868595 139622356395904 learning.py:507] global step 8116: loss = 2.2147 (1.232 sec/step)\n","I0830 18:47:54.093850 139622356395904 learning.py:507] global step 8117: loss = 2.4230 (1.221 sec/step)\n","I0830 18:47:55.337213 139622356395904 learning.py:507] global step 8118: loss = 2.5758 (1.241 sec/step)\n","I0830 18:47:56.552258 139622356395904 learning.py:507] global step 8119: loss = 2.1775 (1.213 sec/step)\n","I0830 18:47:57.820565 139622356395904 learning.py:507] global step 8120: loss = 2.2853 (1.266 sec/step)\n","I0830 18:47:59.018711 139622356395904 learning.py:507] global step 8121: loss = 2.3957 (1.196 sec/step)\n","I0830 18:48:00.281536 139622356395904 learning.py:507] global step 8122: loss = 3.2791 (1.225 sec/step)\n","I0830 18:48:02.291675 139619299985152 supervisor.py:1050] Recording summary at step 8123.\n","I0830 18:48:02.332614 139622356395904 learning.py:507] global step 8123: loss = 2.5466 (2.018 sec/step)\n","I0830 18:48:03.580351 139622356395904 learning.py:507] global step 8124: loss = 2.2147 (1.246 sec/step)\n","I0830 18:48:04.817224 139622356395904 learning.py:507] global step 8125: loss = 2.7717 (1.235 sec/step)\n","I0830 18:48:06.065618 139622356395904 learning.py:507] global step 8126: loss = 2.8970 (1.246 sec/step)\n","I0830 18:48:07.278176 139622356395904 learning.py:507] global step 8127: loss = 2.7797 (1.211 sec/step)\n","I0830 18:48:08.497488 139622356395904 learning.py:507] global step 8128: loss = 2.1695 (1.218 sec/step)\n","I0830 18:48:09.744940 139622356395904 learning.py:507] global step 8129: loss = 1.9688 (1.245 sec/step)\n","I0830 18:48:10.959611 139622356395904 learning.py:507] global step 8130: loss = 2.4329 (1.213 sec/step)\n","I0830 18:48:12.192711 139622356395904 learning.py:507] global step 8131: loss = 4.3119 (1.231 sec/step)\n","I0830 18:48:13.446328 139622356395904 learning.py:507] global step 8132: loss = 2.1893 (1.251 sec/step)\n","I0830 18:48:14.717317 139622356395904 learning.py:507] global step 8133: loss = 2.1570 (1.269 sec/step)\n","I0830 18:48:15.958274 139622356395904 learning.py:507] global step 8134: loss = 2.6457 (1.238 sec/step)\n","I0830 18:48:17.179104 139622356395904 learning.py:507] global step 8135: loss = 2.8169 (1.219 sec/step)\n","I0830 18:48:18.434324 139622356395904 learning.py:507] global step 8136: loss = 3.0541 (1.253 sec/step)\n","I0830 18:48:19.676707 139622356395904 learning.py:507] global step 8137: loss = 3.6081 (1.240 sec/step)\n","I0830 18:48:20.915520 139622356395904 learning.py:507] global step 8138: loss = 2.2827 (1.237 sec/step)\n","I0830 18:48:22.156742 139622356395904 learning.py:507] global step 8139: loss = 2.4673 (1.239 sec/step)\n","I0830 18:48:23.363148 139622356395904 learning.py:507] global step 8140: loss = 2.9940 (1.204 sec/step)\n","I0830 18:48:24.617804 139622356395904 learning.py:507] global step 8141: loss = 2.3063 (1.253 sec/step)\n","I0830 18:48:25.852473 139622356395904 learning.py:507] global step 8142: loss = 2.8172 (1.233 sec/step)\n","I0830 18:48:27.025938 139622356395904 learning.py:507] global step 8143: loss = 3.3000 (1.172 sec/step)\n","I0830 18:48:28.231522 139622356395904 learning.py:507] global step 8144: loss = 2.7654 (1.204 sec/step)\n","I0830 18:48:29.451152 139622356395904 learning.py:507] global step 8145: loss = 2.5923 (1.217 sec/step)\n","I0830 18:48:30.689737 139622356395904 learning.py:507] global step 8146: loss = 2.8223 (1.237 sec/step)\n","I0830 18:48:31.957040 139622356395904 learning.py:507] global step 8147: loss = 2.7935 (1.265 sec/step)\n","I0830 18:48:33.182374 139622356395904 learning.py:507] global step 8148: loss = 2.3605 (1.223 sec/step)\n","I0830 18:48:34.412714 139622356395904 learning.py:507] global step 8149: loss = 2.2286 (1.228 sec/step)\n","I0830 18:48:35.644697 139622356395904 learning.py:507] global step 8150: loss = 3.3920 (1.230 sec/step)\n","I0830 18:48:36.914488 139622356395904 learning.py:507] global step 8151: loss = 2.1982 (1.268 sec/step)\n","I0830 18:48:38.126411 139622356395904 learning.py:507] global step 8152: loss = 2.6894 (1.210 sec/step)\n","I0830 18:48:39.363611 139622356395904 learning.py:507] global step 8153: loss = 3.0702 (1.235 sec/step)\n","I0830 18:48:40.620032 139622356395904 learning.py:507] global step 8154: loss = 3.0570 (1.255 sec/step)\n","I0830 18:48:41.804370 139622356395904 learning.py:507] global step 8155: loss = 2.9531 (1.182 sec/step)\n","I0830 18:48:43.038898 139622356395904 learning.py:507] global step 8156: loss = 2.2508 (1.233 sec/step)\n","I0830 18:48:44.274068 139622356395904 learning.py:507] global step 8157: loss = 3.3010 (1.233 sec/step)\n","I0830 18:48:45.508821 139622356395904 learning.py:507] global step 8158: loss = 3.1286 (1.233 sec/step)\n","I0830 18:48:46.743572 139622356395904 learning.py:507] global step 8159: loss = 2.9600 (1.233 sec/step)\n","I0830 18:48:47.943895 139622356395904 learning.py:507] global step 8160: loss = 3.6886 (1.198 sec/step)\n","I0830 18:48:49.166041 139622356395904 learning.py:507] global step 8161: loss = 3.2448 (1.220 sec/step)\n","I0830 18:48:50.391373 139622356395904 learning.py:507] global step 8162: loss = 2.7044 (1.223 sec/step)\n","I0830 18:48:51.641116 139622356395904 learning.py:507] global step 8163: loss = 2.7668 (1.248 sec/step)\n","I0830 18:48:52.873241 139622356395904 learning.py:507] global step 8164: loss = 2.2683 (1.230 sec/step)\n","I0830 18:48:54.092730 139622356395904 learning.py:507] global step 8165: loss = 2.7336 (1.218 sec/step)\n","I0830 18:48:55.312129 139622356395904 learning.py:507] global step 8166: loss = 2.1214 (1.218 sec/step)\n","I0830 18:48:56.521674 139622356395904 learning.py:507] global step 8167: loss = 2.9873 (1.208 sec/step)\n","I0830 18:48:57.772130 139622356395904 learning.py:507] global step 8168: loss = 2.9523 (1.248 sec/step)\n","I0830 18:48:59.040638 139622356395904 learning.py:507] global step 8169: loss = 2.2171 (1.266 sec/step)\n","I0830 18:49:00.259844 139622356395904 learning.py:507] global step 8170: loss = 3.0601 (1.217 sec/step)\n","I0830 18:49:01.449703 139622356395904 learning.py:507] global step 8171: loss = 1.8490 (1.188 sec/step)\n","I0830 18:49:02.682645 139622356395904 learning.py:507] global step 8172: loss = 3.1652 (1.231 sec/step)\n","I0830 18:49:03.911278 139622356395904 learning.py:507] global step 8173: loss = 2.7921 (1.227 sec/step)\n","I0830 18:49:05.148812 139622356395904 learning.py:507] global step 8174: loss = 2.3349 (1.236 sec/step)\n","I0830 18:49:06.403568 139622356395904 learning.py:507] global step 8175: loss = 4.1393 (1.253 sec/step)\n","I0830 18:49:07.616714 139622356395904 learning.py:507] global step 8176: loss = 2.0301 (1.211 sec/step)\n","I0830 18:49:08.845648 139622356395904 learning.py:507] global step 8177: loss = 2.8485 (1.227 sec/step)\n","I0830 18:49:10.106734 139622356395904 learning.py:507] global step 8178: loss = 1.8102 (1.259 sec/step)\n","I0830 18:49:11.325696 139622356395904 learning.py:507] global step 8179: loss = 2.2997 (1.217 sec/step)\n","I0830 18:49:12.578953 139622356395904 learning.py:507] global step 8180: loss = 2.7898 (1.251 sec/step)\n","I0830 18:49:13.848361 139622356395904 learning.py:507] global step 8181: loss = 2.9569 (1.267 sec/step)\n","I0830 18:49:15.089926 139622356395904 learning.py:507] global step 8182: loss = 2.9859 (1.240 sec/step)\n","I0830 18:49:16.406644 139622356395904 learning.py:507] global step 8183: loss = 3.1523 (1.315 sec/step)\n","I0830 18:49:17.636181 139622356395904 learning.py:507] global step 8184: loss = 3.1667 (1.228 sec/step)\n","I0830 18:49:18.898916 139622356395904 learning.py:507] global step 8185: loss = 2.9282 (1.261 sec/step)\n","I0830 18:49:20.132302 139622356395904 learning.py:507] global step 8186: loss = 4.1386 (1.231 sec/step)\n","I0830 18:49:21.358574 139622356395904 learning.py:507] global step 8187: loss = 2.8946 (1.224 sec/step)\n","I0830 18:49:22.582040 139622356395904 learning.py:507] global step 8188: loss = 2.8889 (1.222 sec/step)\n","I0830 18:49:23.809970 139622356395904 learning.py:507] global step 8189: loss = 3.6192 (1.226 sec/step)\n","I0830 18:49:25.034256 139622356395904 learning.py:507] global step 8190: loss = 2.1189 (1.222 sec/step)\n","I0830 18:49:26.270377 139622356395904 learning.py:507] global step 8191: loss = 2.4185 (1.234 sec/step)\n","I0830 18:49:27.495157 139622356395904 learning.py:507] global step 8192: loss = 2.5531 (1.223 sec/step)\n","I0830 18:49:28.740135 139622356395904 learning.py:507] global step 8193: loss = 3.0037 (1.243 sec/step)\n","I0830 18:49:29.980213 139622356395904 learning.py:507] global step 8194: loss = 3.2575 (1.237 sec/step)\n","I0830 18:49:31.212659 139622356395904 learning.py:507] global step 8195: loss = 2.5037 (1.230 sec/step)\n","I0830 18:49:32.417506 139622356395904 learning.py:507] global step 8196: loss = 2.5676 (1.203 sec/step)\n","I0830 18:49:33.641522 139622356395904 learning.py:507] global step 8197: loss = 2.9700 (1.222 sec/step)\n","I0830 18:49:34.895397 139622356395904 learning.py:507] global step 8198: loss = 2.4673 (1.252 sec/step)\n","I0830 18:49:36.117352 139622356395904 learning.py:507] global step 8199: loss = 2.9194 (1.220 sec/step)\n","I0830 18:49:37.346694 139622356395904 learning.py:507] global step 8200: loss = 2.5438 (1.228 sec/step)\n","I0830 18:49:38.608972 139622356395904 learning.py:507] global step 8201: loss = 3.2122 (1.260 sec/step)\n","I0830 18:49:39.848742 139622356395904 learning.py:507] global step 8202: loss = 2.5989 (1.237 sec/step)\n","I0830 18:49:41.082994 139622356395904 learning.py:507] global step 8203: loss = 3.5487 (1.232 sec/step)\n","I0830 18:49:42.317451 139622356395904 learning.py:507] global step 8204: loss = 1.9352 (1.232 sec/step)\n","I0830 18:49:43.557368 139622356395904 learning.py:507] global step 8205: loss = 2.7133 (1.238 sec/step)\n","I0830 18:49:44.754456 139622356395904 learning.py:507] global step 8206: loss = 2.8670 (1.195 sec/step)\n","I0830 18:49:45.979521 139622356395904 learning.py:507] global step 8207: loss = 2.4068 (1.223 sec/step)\n","I0830 18:49:47.215010 139622356395904 learning.py:507] global step 8208: loss = 2.8835 (1.234 sec/step)\n","I0830 18:49:48.457859 139622356395904 learning.py:507] global step 8209: loss = 2.8424 (1.241 sec/step)\n","I0830 18:49:49.684810 139622356395904 learning.py:507] global step 8210: loss = 2.9955 (1.225 sec/step)\n","I0830 18:49:50.907815 139622356395904 learning.py:507] global step 8211: loss = 3.2839 (1.221 sec/step)\n","I0830 18:49:52.162072 139622356395904 learning.py:507] global step 8212: loss = 3.3009 (1.252 sec/step)\n","I0830 18:49:53.454578 139622356395904 learning.py:507] global step 8213: loss = 3.8609 (1.290 sec/step)\n","I0830 18:49:54.668901 139622356395904 learning.py:507] global step 8214: loss = 2.8023 (1.212 sec/step)\n","I0830 18:49:55.877970 139622356395904 learning.py:507] global step 8215: loss = 2.4314 (1.207 sec/step)\n","I0830 18:49:57.081568 139622356395904 learning.py:507] global step 8216: loss = 2.3344 (1.202 sec/step)\n","I0830 18:49:58.302465 139622356395904 learning.py:507] global step 8217: loss = 3.2568 (1.219 sec/step)\n","I0830 18:49:59.567523 139622356395904 learning.py:507] global step 8218: loss = 2.6887 (1.263 sec/step)\n","I0830 18:49:59.647604 139619316770560 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n","I0830 18:50:02.862770 139622356395904 learning.py:507] global step 8219: loss = 3.4191 (2.948 sec/step)\n","I0830 18:50:02.870114 139619299985152 supervisor.py:1050] Recording summary at step 8219.\n","I0830 18:50:04.566530 139622356395904 learning.py:507] global step 8220: loss = 2.8301 (1.590 sec/step)\n","I0830 18:50:05.773015 139622356395904 learning.py:507] global step 8221: loss = 3.1855 (1.204 sec/step)\n","I0830 18:50:06.995718 139622356395904 learning.py:507] global step 8222: loss = 3.2708 (1.221 sec/step)\n","I0830 18:50:08.208262 139622356395904 learning.py:507] global step 8223: loss = 2.6406 (1.211 sec/step)\n","I0830 18:50:09.416840 139622356395904 learning.py:507] global step 8224: loss = 2.6745 (1.207 sec/step)\n","I0830 18:50:10.642145 139622356395904 learning.py:507] global step 8225: loss = 2.3740 (1.223 sec/step)\n","I0830 18:50:11.891875 139622356395904 learning.py:507] global step 8226: loss = 2.8493 (1.248 sec/step)\n","I0830 18:50:13.146562 139622356395904 learning.py:507] global step 8227: loss = 3.6724 (1.253 sec/step)\n","I0830 18:50:14.391659 139622356395904 learning.py:507] global step 8228: loss = 2.9775 (1.243 sec/step)\n","I0830 18:50:15.624834 139622356395904 learning.py:507] global step 8229: loss = 2.3185 (1.231 sec/step)\n","I0830 18:50:16.880581 139622356395904 learning.py:507] global step 8230: loss = 2.9143 (1.254 sec/step)\n","I0830 18:50:18.090078 139622356395904 learning.py:507] global step 8231: loss = 2.6926 (1.208 sec/step)\n","I0830 18:50:19.346541 139622356395904 learning.py:507] global step 8232: loss = 3.4659 (1.254 sec/step)\n","I0830 18:50:20.581835 139622356395904 learning.py:507] global step 8233: loss = 2.5121 (1.234 sec/step)\n","I0830 18:50:21.818125 139622356395904 learning.py:507] global step 8234: loss = 2.2185 (1.234 sec/step)\n","I0830 18:50:23.071211 139622356395904 learning.py:507] global step 8235: loss = 2.2577 (1.251 sec/step)\n","I0830 18:50:24.299524 139622356395904 learning.py:507] global step 8236: loss = 2.2781 (1.226 sec/step)\n","I0830 18:50:25.560559 139622356395904 learning.py:507] global step 8237: loss = 2.2686 (1.259 sec/step)\n","I0830 18:50:26.809617 139622356395904 learning.py:507] global step 8238: loss = 2.5758 (1.247 sec/step)\n","I0830 18:50:28.032448 139622356395904 learning.py:507] global step 8239: loss = 2.6148 (1.221 sec/step)\n","I0830 18:50:29.310885 139622356395904 learning.py:507] global step 8240: loss = 3.1240 (1.276 sec/step)\n","I0830 18:50:30.514466 139622356395904 learning.py:507] global step 8241: loss = 2.1874 (1.202 sec/step)\n","I0830 18:50:31.753603 139622356395904 learning.py:507] global step 8242: loss = 2.5586 (1.237 sec/step)\n","I0830 18:50:32.981076 139622356395904 learning.py:507] global step 8243: loss = 2.6017 (1.225 sec/step)\n","I0830 18:50:34.196067 139622356395904 learning.py:507] global step 8244: loss = 2.9767 (1.213 sec/step)\n","I0830 18:50:35.435078 139622356395904 learning.py:507] global step 8245: loss = 3.0112 (1.237 sec/step)\n","I0830 18:50:36.691168 139622356395904 learning.py:507] global step 8246: loss = 2.7380 (1.254 sec/step)\n","I0830 18:50:37.897983 139622356395904 learning.py:507] global step 8247: loss = 2.2014 (1.205 sec/step)\n","I0830 18:50:39.134170 139622356395904 learning.py:507] global step 8248: loss = 3.1625 (1.234 sec/step)\n","I0830 18:50:40.381269 139622356395904 learning.py:507] global step 8249: loss = 2.8172 (1.245 sec/step)\n","I0830 18:50:41.611088 139622356395904 learning.py:507] global step 8250: loss = 2.5247 (1.228 sec/step)\n","I0830 18:50:42.817330 139622356395904 learning.py:507] global step 8251: loss = 2.4544 (1.204 sec/step)\n","I0830 18:50:44.057757 139622356395904 learning.py:507] global step 8252: loss = 3.4604 (1.238 sec/step)\n","I0830 18:50:45.293381 139622356395904 learning.py:507] global step 8253: loss = 3.2574 (1.234 sec/step)\n","I0830 18:50:46.526528 139622356395904 learning.py:507] global step 8254: loss = 2.8748 (1.231 sec/step)\n","I0830 18:50:47.746043 139622356395904 learning.py:507] global step 8255: loss = 2.3131 (1.218 sec/step)\n","I0830 18:50:48.971081 139622356395904 learning.py:507] global step 8256: loss = 3.2037 (1.223 sec/step)\n","I0830 18:50:50.194903 139622356395904 learning.py:507] global step 8257: loss = 3.0729 (1.222 sec/step)\n","I0830 18:50:51.437129 139622356395904 learning.py:507] global step 8258: loss = 2.9611 (1.240 sec/step)\n","I0830 18:50:52.681459 139622356395904 learning.py:507] global step 8259: loss = 2.8256 (1.242 sec/step)\n","I0830 18:50:53.880883 139622356395904 learning.py:507] global step 8260: loss = 2.8756 (1.197 sec/step)\n","I0830 18:50:55.097509 139622356395904 learning.py:507] global step 8261: loss = 3.5433 (1.215 sec/step)\n","I0830 18:50:56.328998 139622356395904 learning.py:507] global step 8262: loss = 3.2230 (1.229 sec/step)\n","I0830 18:50:57.570475 139622356395904 learning.py:507] global step 8263: loss = 2.6665 (1.240 sec/step)\n","I0830 18:50:58.838585 139622356395904 learning.py:507] global step 8264: loss = 2.6768 (1.266 sec/step)\n","I0830 18:51:00.048838 139622356395904 learning.py:507] global step 8265: loss = 2.3472 (1.208 sec/step)\n","I0830 18:51:01.291143 139622356395904 learning.py:507] global step 8266: loss = 2.1029 (1.240 sec/step)\n","I0830 18:51:02.517745 139622356395904 learning.py:507] global step 8267: loss = 2.3110 (1.224 sec/step)\n","I0830 18:51:03.734549 139622356395904 learning.py:507] global step 8268: loss = 2.5163 (1.215 sec/step)\n","I0830 18:51:04.943147 139622356395904 learning.py:507] global step 8269: loss = 2.9619 (1.207 sec/step)\n","I0830 18:51:06.230710 139622356395904 learning.py:507] global step 8270: loss = 2.3245 (1.283 sec/step)\n","I0830 18:51:07.488044 139622356395904 learning.py:507] global step 8271: loss = 2.1163 (1.256 sec/step)\n","I0830 18:51:08.714383 139622356395904 learning.py:507] global step 8272: loss = 3.0077 (1.224 sec/step)\n","I0830 18:51:09.957873 139622356395904 learning.py:507] global step 8273: loss = 2.2037 (1.242 sec/step)\n","I0830 18:51:11.202139 139622356395904 learning.py:507] global step 8274: loss = 2.5290 (1.242 sec/step)\n","I0830 18:51:12.425362 139622356395904 learning.py:507] global step 8275: loss = 2.6757 (1.221 sec/step)\n","I0830 18:51:13.658878 139622356395904 learning.py:507] global step 8276: loss = 2.2796 (1.232 sec/step)\n","I0830 18:51:14.886961 139622356395904 learning.py:507] global step 8277: loss = 2.9800 (1.226 sec/step)\n","I0830 18:51:16.097668 139622356395904 learning.py:507] global step 8278: loss = 2.3462 (1.208 sec/step)\n","I0830 18:51:17.347019 139622356395904 learning.py:507] global step 8279: loss = 2.1763 (1.246 sec/step)\n","I0830 18:51:18.559833 139622356395904 learning.py:507] global step 8280: loss = 3.4122 (1.210 sec/step)\n","I0830 18:51:19.771709 139622356395904 learning.py:507] global step 8281: loss = 2.3069 (1.210 sec/step)\n","I0830 18:51:21.001730 139622356395904 learning.py:507] global step 8282: loss = 2.7025 (1.228 sec/step)\n","I0830 18:51:22.258813 139622356395904 learning.py:507] global step 8283: loss = 2.8170 (1.255 sec/step)\n","I0830 18:51:23.469749 139622356395904 learning.py:507] global step 8284: loss = 2.9312 (1.209 sec/step)\n","I0830 18:51:24.687082 139622356395904 learning.py:507] global step 8285: loss = 3.6290 (1.215 sec/step)\n","I0830 18:51:25.904036 139622356395904 learning.py:507] global step 8286: loss = 3.2222 (1.215 sec/step)\n","I0830 18:51:27.122364 139622356395904 learning.py:507] global step 8287: loss = 3.1194 (1.216 sec/step)\n","I0830 18:51:28.325325 139622356395904 learning.py:507] global step 8288: loss = 2.1346 (1.201 sec/step)\n","I0830 18:51:29.524965 139622356395904 learning.py:507] global step 8289: loss = 4.1808 (1.198 sec/step)\n","I0830 18:51:30.779541 139622356395904 learning.py:507] global step 8290: loss = 2.4285 (1.253 sec/step)\n","I0830 18:51:32.004343 139622356395904 learning.py:507] global step 8291: loss = 2.7220 (1.223 sec/step)\n","I0830 18:51:33.235514 139622356395904 learning.py:507] global step 8292: loss = 3.0449 (1.229 sec/step)\n","I0830 18:51:34.463865 139622356395904 learning.py:507] global step 8293: loss = 3.2393 (1.226 sec/step)\n","I0830 18:51:35.706634 139622356395904 learning.py:507] global step 8294: loss = 2.0609 (1.241 sec/step)\n","I0830 18:51:36.944722 139622356395904 learning.py:507] global step 8295: loss = 2.9613 (1.235 sec/step)\n","I0830 18:51:38.147332 139622356395904 learning.py:507] global step 8296: loss = 3.0381 (1.201 sec/step)\n","I0830 18:51:39.345243 139622356395904 learning.py:507] global step 8297: loss = 2.4531 (1.196 sec/step)\n","I0830 18:51:40.598545 139622356395904 learning.py:507] global step 8298: loss = 2.3171 (1.252 sec/step)\n","I0830 18:51:41.857884 139622356395904 learning.py:507] global step 8299: loss = 2.8234 (1.258 sec/step)\n","I0830 18:51:43.096289 139622356395904 learning.py:507] global step 8300: loss = 2.3897 (1.234 sec/step)\n","I0830 18:51:44.360909 139622356395904 learning.py:507] global step 8301: loss = 2.1812 (1.263 sec/step)\n","I0830 18:51:45.616270 139622356395904 learning.py:507] global step 8302: loss = 2.5768 (1.251 sec/step)\n","I0830 18:51:46.852501 139622356395904 learning.py:507] global step 8303: loss = 4.7670 (1.234 sec/step)\n","I0830 18:51:48.086245 139622356395904 learning.py:507] global step 8304: loss = 2.8586 (1.232 sec/step)\n","I0830 18:51:49.337849 139622356395904 learning.py:507] global step 8305: loss = 2.7533 (1.250 sec/step)\n","I0830 18:51:50.549012 139622356395904 learning.py:507] global step 8306: loss = 2.4380 (1.209 sec/step)\n","I0830 18:51:51.784859 139622356395904 learning.py:507] global step 8307: loss = 2.7877 (1.234 sec/step)\n","I0830 18:51:53.024089 139622356395904 learning.py:507] global step 8308: loss = 2.5448 (1.237 sec/step)\n","I0830 18:51:54.235011 139622356395904 learning.py:507] global step 8309: loss = 2.0929 (1.209 sec/step)\n","I0830 18:51:55.439741 139622356395904 learning.py:507] global step 8310: loss = 2.5674 (1.203 sec/step)\n","I0830 18:51:56.662881 139622356395904 learning.py:507] global step 8311: loss = 2.2608 (1.221 sec/step)\n","I0830 18:51:57.872832 139622356395904 learning.py:507] global step 8312: loss = 1.8378 (1.208 sec/step)\n","I0830 18:51:59.090079 139622356395904 learning.py:507] global step 8313: loss = 2.5450 (1.216 sec/step)\n","I0830 18:52:00.544103 139622356395904 learning.py:507] global step 8314: loss = 2.5335 (1.363 sec/step)\n","I0830 18:52:02.461640 139619299985152 supervisor.py:1050] Recording summary at step 8315.\n","I0830 18:52:02.492123 139622356395904 learning.py:507] global step 8315: loss = 2.3724 (1.946 sec/step)\n","I0830 18:52:03.736714 139622356395904 learning.py:507] global step 8316: loss = 2.5331 (1.243 sec/step)\n","I0830 18:52:05.006316 139622356395904 learning.py:507] global step 8317: loss = 2.3935 (1.265 sec/step)\n","I0830 18:52:06.210910 139622356395904 learning.py:507] global step 8318: loss = 2.2679 (1.203 sec/step)\n","I0830 18:52:07.423559 139622356395904 learning.py:507] global step 8319: loss = 2.6509 (1.211 sec/step)\n","I0830 18:52:08.663816 139622356395904 learning.py:507] global step 8320: loss = 2.5762 (1.238 sec/step)\n","I0830 18:52:09.908968 139622356395904 learning.py:507] global step 8321: loss = 3.2369 (1.243 sec/step)\n","I0830 18:52:11.127574 139622356395904 learning.py:507] global step 8322: loss = 3.6469 (1.217 sec/step)\n","I0830 18:52:12.336130 139622356395904 learning.py:507] global step 8323: loss = 2.1327 (1.207 sec/step)\n","I0830 18:52:13.554230 139622356395904 learning.py:507] global step 8324: loss = 2.6450 (1.216 sec/step)\n","I0830 18:52:14.804440 139622356395904 learning.py:507] global step 8325: loss = 2.5031 (1.248 sec/step)\n","I0830 18:52:16.047671 139622356395904 learning.py:507] global step 8326: loss = 2.3903 (1.241 sec/step)\n","I0830 18:52:17.245831 139622356395904 learning.py:507] global step 8327: loss = 2.4033 (1.196 sec/step)\n","I0830 18:52:18.474629 139622356395904 learning.py:507] global step 8328: loss = 2.6731 (1.227 sec/step)\n","I0830 18:52:19.678082 139622356395904 learning.py:507] global step 8329: loss = 2.5248 (1.201 sec/step)\n","I0830 18:52:20.909143 139622356395904 learning.py:507] global step 8330: loss = 2.7315 (1.229 sec/step)\n","I0830 18:52:22.147200 139622356395904 learning.py:507] global step 8331: loss = 2.3721 (1.236 sec/step)\n","I0830 18:52:23.349439 139622356395904 learning.py:507] global step 8332: loss = 1.9219 (1.199 sec/step)\n","I0830 18:52:24.581844 139622356395904 learning.py:507] global step 8333: loss = 3.0429 (1.230 sec/step)\n","I0830 18:52:25.825295 139622356395904 learning.py:507] global step 8334: loss = 4.9468 (1.241 sec/step)\n","I0830 18:52:27.082509 139622356395904 learning.py:507] global step 8335: loss = 2.5049 (1.255 sec/step)\n","I0830 18:52:28.334651 139622356395904 learning.py:507] global step 8336: loss = 4.0699 (1.250 sec/step)\n","I0830 18:52:29.569087 139622356395904 learning.py:507] global step 8337: loss = 2.5229 (1.233 sec/step)\n","I0830 18:52:30.790100 139622356395904 learning.py:507] global step 8338: loss = 2.6217 (1.219 sec/step)\n","I0830 18:52:32.015926 139622356395904 learning.py:507] global step 8339: loss = 2.4230 (1.224 sec/step)\n","I0830 18:52:33.234935 139622356395904 learning.py:507] global step 8340: loss = 3.1408 (1.217 sec/step)\n","I0830 18:52:34.476184 139622356395904 learning.py:507] global step 8341: loss = 2.8016 (1.239 sec/step)\n","I0830 18:52:35.721477 139622356395904 learning.py:507] global step 8342: loss = 2.4017 (1.244 sec/step)\n","I0830 18:52:36.949693 139622356395904 learning.py:507] global step 8343: loss = 2.7436 (1.227 sec/step)\n","I0830 18:52:38.154728 139622356395904 learning.py:507] global step 8344: loss = 2.8428 (1.203 sec/step)\n","I0830 18:52:39.423634 139622356395904 learning.py:507] global step 8345: loss = 3.0603 (1.267 sec/step)\n","I0830 18:52:40.679247 139622356395904 learning.py:507] global step 8346: loss = 2.2376 (1.254 sec/step)\n","I0830 18:52:41.908823 139622356395904 learning.py:507] global step 8347: loss = 2.5646 (1.228 sec/step)\n","I0830 18:52:43.119872 139622356395904 learning.py:507] global step 8348: loss = 2.5802 (1.209 sec/step)\n","I0830 18:52:44.351275 139622356395904 learning.py:507] global step 8349: loss = 1.9835 (1.230 sec/step)\n","I0830 18:52:45.566755 139622356395904 learning.py:507] global step 8350: loss = 2.3597 (1.213 sec/step)\n","I0830 18:52:46.835170 139622356395904 learning.py:507] global step 8351: loss = 2.2088 (1.266 sec/step)\n","I0830 18:52:48.083431 139622356395904 learning.py:507] global step 8352: loss = 3.1346 (1.246 sec/step)\n","I0830 18:52:49.323442 139622356395904 learning.py:507] global step 8353: loss = 2.4891 (1.238 sec/step)\n","I0830 18:52:50.510077 139622356395904 learning.py:507] global step 8354: loss = 3.3335 (1.185 sec/step)\n","I0830 18:52:51.736208 139622356395904 learning.py:507] global step 8355: loss = 2.8374 (1.224 sec/step)\n","I0830 18:52:52.992324 139622356395904 learning.py:507] global step 8356: loss = 2.7207 (1.254 sec/step)\n","I0830 18:52:54.194331 139622356395904 learning.py:507] global step 8357: loss = 2.8673 (1.200 sec/step)\n","I0830 18:52:55.422592 139622356395904 learning.py:507] global step 8358: loss = 2.8256 (1.227 sec/step)\n","I0830 18:52:56.633609 139622356395904 learning.py:507] global step 8359: loss = 2.2627 (1.209 sec/step)\n","I0830 18:52:57.870093 139622356395904 learning.py:507] global step 8360: loss = 3.9024 (1.234 sec/step)\n","I0830 18:52:59.088586 139622356395904 learning.py:507] global step 8361: loss = 2.9099 (1.217 sec/step)\n","I0830 18:53:00.319428 139622356395904 learning.py:507] global step 8362: loss = 2.8837 (1.229 sec/step)\n","I0830 18:53:01.549270 139622356395904 learning.py:507] global step 8363: loss = 3.3876 (1.227 sec/step)\n","I0830 18:53:02.757191 139622356395904 learning.py:507] global step 8364: loss = 2.7027 (1.206 sec/step)\n","I0830 18:53:04.014788 139622356395904 learning.py:507] global step 8365: loss = 2.8875 (1.256 sec/step)\n","I0830 18:53:05.210346 139622356395904 learning.py:507] global step 8366: loss = 2.9173 (1.194 sec/step)\n","I0830 18:53:06.442833 139622356395904 learning.py:507] global step 8367: loss = 2.1450 (1.231 sec/step)\n","I0830 18:53:07.667714 139622356395904 learning.py:507] global step 8368: loss = 2.5957 (1.223 sec/step)\n","I0830 18:53:08.857675 139622356395904 learning.py:507] global step 8369: loss = 3.7811 (1.188 sec/step)\n","I0830 18:53:10.075082 139622356395904 learning.py:507] global step 8370: loss = 2.5989 (1.215 sec/step)\n","I0830 18:53:11.287297 139622356395904 learning.py:507] global step 8371: loss = 3.7277 (1.210 sec/step)\n","I0830 18:53:12.511186 139622356395904 learning.py:507] global step 8372: loss = 2.0988 (1.222 sec/step)\n","I0830 18:53:13.733765 139622356395904 learning.py:507] global step 8373: loss = 2.9094 (1.220 sec/step)\n","I0830 18:53:14.976840 139622356395904 learning.py:507] global step 8374: loss = 3.1837 (1.241 sec/step)\n","I0830 18:53:16.151183 139622356395904 learning.py:507] global step 8375: loss = 2.2980 (1.172 sec/step)\n","I0830 18:53:17.389545 139622356395904 learning.py:507] global step 8376: loss = 3.3878 (1.237 sec/step)\n","I0830 18:53:18.643085 139622356395904 learning.py:507] global step 8377: loss = 2.6303 (1.252 sec/step)\n","I0830 18:53:19.871622 139622356395904 learning.py:507] global step 8378: loss = 2.6673 (1.227 sec/step)\n","I0830 18:53:21.081912 139622356395904 learning.py:507] global step 8379: loss = 2.8865 (1.208 sec/step)\n","I0830 18:53:22.310275 139622356395904 learning.py:507] global step 8380: loss = 2.7194 (1.226 sec/step)\n","I0830 18:53:23.539422 139622356395904 learning.py:507] global step 8381: loss = 2.8757 (1.227 sec/step)\n","I0830 18:53:24.755626 139622356395904 learning.py:507] global step 8382: loss = 2.4122 (1.214 sec/step)\n","I0830 18:53:25.967308 139622356395904 learning.py:507] global step 8383: loss = 2.9799 (1.210 sec/step)\n","I0830 18:53:27.176625 139622356395904 learning.py:507] global step 8384: loss = 2.3015 (1.207 sec/step)\n","I0830 18:53:28.399990 139622356395904 learning.py:507] global step 8385: loss = 2.2049 (1.222 sec/step)\n","I0830 18:53:29.641558 139622356395904 learning.py:507] global step 8386: loss = 2.1036 (1.240 sec/step)\n","I0830 18:53:30.864080 139622356395904 learning.py:507] global step 8387: loss = 2.1987 (1.220 sec/step)\n","I0830 18:53:32.094835 139622356395904 learning.py:507] global step 8388: loss = 2.8584 (1.229 sec/step)\n","I0830 18:53:33.312725 139622356395904 learning.py:507] global step 8389: loss = 3.0532 (1.216 sec/step)\n","I0830 18:53:34.510714 139622356395904 learning.py:507] global step 8390: loss = 2.5767 (1.196 sec/step)\n","I0830 18:53:35.755128 139622356395904 learning.py:507] global step 8391: loss = 3.1606 (1.243 sec/step)\n","I0830 18:53:36.968966 139622356395904 learning.py:507] global step 8392: loss = 3.0995 (1.212 sec/step)\n","I0830 18:53:38.227377 139622356395904 learning.py:507] global step 8393: loss = 2.6882 (1.257 sec/step)\n","I0830 18:53:39.483613 139622356395904 learning.py:507] global step 8394: loss = 2.6965 (1.254 sec/step)\n","I0830 18:53:40.670485 139622356395904 learning.py:507] global step 8395: loss = 3.1812 (1.185 sec/step)\n","I0830 18:53:41.921570 139622356395904 learning.py:507] global step 8396: loss = 3.1344 (1.249 sec/step)\n","I0830 18:53:43.106283 139622356395904 learning.py:507] global step 8397: loss = 2.2123 (1.183 sec/step)\n","I0830 18:53:44.325917 139622356395904 learning.py:507] global step 8398: loss = 2.2047 (1.218 sec/step)\n","I0830 18:53:45.525159 139622356395904 learning.py:507] global step 8399: loss = 2.6076 (1.197 sec/step)\n","I0830 18:53:46.735300 139622356395904 learning.py:507] global step 8400: loss = 2.6977 (1.205 sec/step)\n","I0830 18:53:47.993831 139622356395904 learning.py:507] global step 8401: loss = 1.9794 (1.257 sec/step)\n","I0830 18:53:49.212010 139622356395904 learning.py:507] global step 8402: loss = 2.8096 (1.216 sec/step)\n","I0830 18:53:50.468035 139622356395904 learning.py:507] global step 8403: loss = 2.9127 (1.254 sec/step)\n","I0830 18:53:51.696732 139622356395904 learning.py:507] global step 8404: loss = 2.9929 (1.227 sec/step)\n","I0830 18:53:52.928496 139622356395904 learning.py:507] global step 8405: loss = 2.8426 (1.230 sec/step)\n","I0830 18:53:54.162760 139622356395904 learning.py:507] global step 8406: loss = 2.2503 (1.232 sec/step)\n","I0830 18:53:55.368202 139622356395904 learning.py:507] global step 8407: loss = 2.2117 (1.204 sec/step)\n","I0830 18:53:56.598805 139622356395904 learning.py:507] global step 8408: loss = 2.0559 (1.229 sec/step)\n","I0830 18:53:57.787461 139622356395904 learning.py:507] global step 8409: loss = 2.0450 (1.187 sec/step)\n","I0830 18:53:58.993175 139622356395904 learning.py:507] global step 8410: loss = 3.0019 (1.204 sec/step)\n","I0830 18:54:00.242897 139622356395904 learning.py:507] global step 8411: loss = 3.2409 (1.248 sec/step)\n","I0830 18:54:02.002648 139619299985152 supervisor.py:1050] Recording summary at step 8411.\n","I0830 18:54:02.456308 139622356395904 learning.py:507] global step 8412: loss = 3.1133 (2.120 sec/step)\n","I0830 18:54:03.686431 139622356395904 learning.py:507] global step 8413: loss = 3.1625 (1.228 sec/step)\n","I0830 18:54:04.944506 139622356395904 learning.py:507] global step 8414: loss = 2.6326 (1.256 sec/step)\n","I0830 18:54:06.160184 139622356395904 learning.py:507] global step 8415: loss = 3.1281 (1.214 sec/step)\n","I0830 18:54:07.435486 139622356395904 learning.py:507] global step 8416: loss = 2.8358 (1.274 sec/step)\n","I0830 18:54:08.679182 139622356395904 learning.py:507] global step 8417: loss = 2.9381 (1.241 sec/step)\n","I0830 18:54:09.868188 139622356395904 learning.py:507] global step 8418: loss = 2.4056 (1.187 sec/step)\n","I0830 18:54:11.095370 139622356395904 learning.py:507] global step 8419: loss = 2.2128 (1.225 sec/step)\n","I0830 18:54:12.324752 139622356395904 learning.py:507] global step 8420: loss = 2.3958 (1.227 sec/step)\n","I0830 18:54:13.550389 139622356395904 learning.py:507] global step 8421: loss = 2.4107 (1.223 sec/step)\n","I0830 18:54:14.757539 139622356395904 learning.py:507] global step 8422: loss = 2.8170 (1.205 sec/step)\n","I0830 18:54:15.982878 139622356395904 learning.py:507] global step 8423: loss = 1.9392 (1.223 sec/step)\n","I0830 18:54:17.204084 139622356395904 learning.py:507] global step 8424: loss = 2.9654 (1.219 sec/step)\n","I0830 18:54:18.414447 139622356395904 learning.py:507] global step 8425: loss = 2.9557 (1.209 sec/step)\n","I0830 18:54:19.636764 139622356395904 learning.py:507] global step 8426: loss = 2.7352 (1.220 sec/step)\n","I0830 18:54:20.883589 139622356395904 learning.py:507] global step 8427: loss = 2.2107 (1.244 sec/step)\n","I0830 18:54:22.093141 139622356395904 learning.py:507] global step 8428: loss = 3.2395 (1.205 sec/step)\n","I0830 18:54:23.271636 139622356395904 learning.py:507] global step 8429: loss = 2.0919 (1.177 sec/step)\n","I0830 18:54:24.480181 139622356395904 learning.py:507] global step 8430: loss = 2.2636 (1.207 sec/step)\n","I0830 18:54:25.718218 139622356395904 learning.py:507] global step 8431: loss = 3.5446 (1.236 sec/step)\n","I0830 18:54:26.954490 139622356395904 learning.py:507] global step 8432: loss = 2.0847 (1.231 sec/step)\n","I0830 18:54:28.170839 139622356395904 learning.py:507] global step 8433: loss = 2.1870 (1.214 sec/step)\n","I0830 18:54:29.426958 139622356395904 learning.py:507] global step 8434: loss = 2.6136 (1.254 sec/step)\n","I0830 18:54:30.669711 139622356395904 learning.py:507] global step 8435: loss = 2.8228 (1.241 sec/step)\n","I0830 18:54:31.921846 139622356395904 learning.py:507] global step 8436: loss = 2.3591 (1.250 sec/step)\n","I0830 18:54:33.163407 139622356395904 learning.py:507] global step 8437: loss = 3.2828 (1.240 sec/step)\n","I0830 18:54:34.355490 139622356395904 learning.py:507] global step 8438: loss = 2.3106 (1.190 sec/step)\n","I0830 18:54:35.580137 139622356395904 learning.py:507] global step 8439: loss = 2.3478 (1.222 sec/step)\n","I0830 18:54:36.802213 139622356395904 learning.py:507] global step 8440: loss = 2.7092 (1.220 sec/step)\n","I0830 18:54:38.041610 139622356395904 learning.py:507] global step 8441: loss = 2.1633 (1.237 sec/step)\n","I0830 18:54:39.298306 139622356395904 learning.py:507] global step 8442: loss = 2.9858 (1.255 sec/step)\n","I0830 18:54:40.510568 139622356395904 learning.py:507] global step 8443: loss = 2.5433 (1.210 sec/step)\n","I0830 18:54:41.769990 139622356395904 learning.py:507] global step 8444: loss = 2.7300 (1.258 sec/step)\n","I0830 18:54:42.961174 139622356395904 learning.py:507] global step 8445: loss = 2.4447 (1.189 sec/step)\n","I0830 18:54:44.208602 139622356395904 learning.py:507] global step 8446: loss = 2.2712 (1.246 sec/step)\n","I0830 18:54:45.453317 139622356395904 learning.py:507] global step 8447: loss = 2.8519 (1.243 sec/step)\n","I0830 18:54:46.680151 139622356395904 learning.py:507] global step 8448: loss = 2.1860 (1.224 sec/step)\n","I0830 18:54:47.900650 139622356395904 learning.py:507] global step 8449: loss = 2.8296 (1.218 sec/step)\n","I0830 18:54:49.132611 139622356395904 learning.py:507] global step 8450: loss = 3.0089 (1.230 sec/step)\n","I0830 18:54:50.386536 139622356395904 learning.py:507] global step 8451: loss = 2.0176 (1.252 sec/step)\n","I0830 18:54:51.594624 139622356395904 learning.py:507] global step 8452: loss = 2.6878 (1.206 sec/step)\n","I0830 18:54:52.803639 139622356395904 learning.py:507] global step 8453: loss = 2.1237 (1.207 sec/step)\n","I0830 18:54:53.989595 139622356395904 learning.py:507] global step 8454: loss = 2.4435 (1.184 sec/step)\n","I0830 18:54:55.193161 139622356395904 learning.py:507] global step 8455: loss = 2.7967 (1.202 sec/step)\n","I0830 18:54:56.434817 139622356395904 learning.py:507] global step 8456: loss = 3.3082 (1.240 sec/step)\n","I0830 18:54:57.658815 139622356395904 learning.py:507] global step 8457: loss = 3.4985 (1.222 sec/step)\n","I0830 18:54:58.891916 139622356395904 learning.py:507] global step 8458: loss = 3.4807 (1.231 sec/step)\n","I0830 18:55:00.125181 139622356395904 learning.py:507] global step 8459: loss = 1.7663 (1.231 sec/step)\n","I0830 18:55:01.377659 139622356395904 learning.py:507] global step 8460: loss = 2.8705 (1.251 sec/step)\n","I0830 18:55:02.560431 139622356395904 learning.py:507] global step 8461: loss = 2.4916 (1.181 sec/step)\n","I0830 18:55:03.781373 139622356395904 learning.py:507] global step 8462: loss = 2.0208 (1.219 sec/step)\n","I0830 18:55:05.009365 139622356395904 learning.py:507] global step 8463: loss = 1.9663 (1.226 sec/step)\n","I0830 18:55:06.204383 139622356395904 learning.py:507] global step 8464: loss = 2.7565 (1.193 sec/step)\n","I0830 18:55:07.417801 139622356395904 learning.py:507] global step 8465: loss = 2.5209 (1.212 sec/step)\n","I0830 18:55:08.648165 139622356395904 learning.py:507] global step 8466: loss = 2.7243 (1.229 sec/step)\n","I0830 18:55:09.877501 139622356395904 learning.py:507] global step 8467: loss = 2.8498 (1.227 sec/step)\n","I0830 18:55:11.122117 139622356395904 learning.py:507] global step 8468: loss = 3.1020 (1.243 sec/step)\n","I0830 18:55:12.356547 139622356395904 learning.py:507] global step 8469: loss = 3.4476 (1.232 sec/step)\n","I0830 18:55:13.558414 139622356395904 learning.py:507] global step 8470: loss = 3.1610 (1.200 sec/step)\n","I0830 18:55:14.772956 139622356395904 learning.py:507] global step 8471: loss = 2.5296 (1.213 sec/step)\n","I0830 18:55:16.038122 139622356395904 learning.py:507] global step 8472: loss = 3.1579 (1.263 sec/step)\n","I0830 18:55:17.266938 139622356395904 learning.py:507] global step 8473: loss = 3.4297 (1.226 sec/step)\n","I0830 18:55:18.512296 139622356395904 learning.py:507] global step 8474: loss = 3.9834 (1.243 sec/step)\n","I0830 18:55:19.761524 139622356395904 learning.py:507] global step 8475: loss = 2.2577 (1.247 sec/step)\n","I0830 18:55:21.002243 139622356395904 learning.py:507] global step 8476: loss = 3.2168 (1.238 sec/step)\n","I0830 18:55:22.259122 139622356395904 learning.py:507] global step 8477: loss = 2.9608 (1.255 sec/step)\n","I0830 18:55:23.529804 139622356395904 learning.py:507] global step 8478: loss = 2.2721 (1.269 sec/step)\n","I0830 18:55:24.781635 139622356395904 learning.py:507] global step 8479: loss = 2.1994 (1.250 sec/step)\n","I0830 18:55:26.030532 139622356395904 learning.py:507] global step 8480: loss = 2.5095 (1.247 sec/step)\n","I0830 18:55:27.270510 139622356395904 learning.py:507] global step 8481: loss = 3.1692 (1.238 sec/step)\n","I0830 18:55:28.532259 139622356395904 learning.py:507] global step 8482: loss = 2.2246 (1.260 sec/step)\n","I0830 18:55:29.775982 139622356395904 learning.py:507] global step 8483: loss = 3.5640 (1.242 sec/step)\n","I0830 18:55:30.998575 139622356395904 learning.py:507] global step 8484: loss = 2.2296 (1.221 sec/step)\n","I0830 18:55:32.226027 139622356395904 learning.py:507] global step 8485: loss = 2.4012 (1.225 sec/step)\n","I0830 18:55:33.424733 139622356395904 learning.py:507] global step 8486: loss = 2.4449 (1.196 sec/step)\n","I0830 18:55:34.659972 139622356395904 learning.py:507] global step 8487: loss = 2.8270 (1.233 sec/step)\n","I0830 18:55:35.907136 139622356395904 learning.py:507] global step 8488: loss = 2.8638 (1.245 sec/step)\n","I0830 18:55:37.129263 139622356395904 learning.py:507] global step 8489: loss = 2.7417 (1.220 sec/step)\n","I0830 18:55:38.367229 139622356395904 learning.py:507] global step 8490: loss = 3.3653 (1.236 sec/step)\n","I0830 18:55:39.583003 139622356395904 learning.py:507] global step 8491: loss = 2.6005 (1.214 sec/step)\n","I0830 18:55:40.810001 139622356395904 learning.py:507] global step 8492: loss = 2.6253 (1.225 sec/step)\n","I0830 18:55:42.024861 139622356395904 learning.py:507] global step 8493: loss = 2.5871 (1.213 sec/step)\n","I0830 18:55:43.227186 139622356395904 learning.py:507] global step 8494: loss = 2.2932 (1.200 sec/step)\n","I0830 18:55:44.422843 139622356395904 learning.py:507] global step 8495: loss = 2.7616 (1.194 sec/step)\n","I0830 18:55:45.633725 139622356395904 learning.py:507] global step 8496: loss = 2.8296 (1.209 sec/step)\n","I0830 18:55:46.870925 139622356395904 learning.py:507] global step 8497: loss = 3.5341 (1.236 sec/step)\n","I0830 18:55:48.099706 139622356395904 learning.py:507] global step 8498: loss = 2.2170 (1.227 sec/step)\n","I0830 18:55:49.346289 139622356395904 learning.py:507] global step 8499: loss = 3.2728 (1.245 sec/step)\n","I0830 18:55:50.600619 139622356395904 learning.py:507] global step 8500: loss = 3.2749 (1.253 sec/step)\n","I0830 18:55:51.795487 139622356395904 learning.py:507] global step 8501: loss = 2.5882 (1.193 sec/step)\n","I0830 18:55:53.004738 139622356395904 learning.py:507] global step 8502: loss = 3.5183 (1.207 sec/step)\n","I0830 18:55:54.251031 139622356395904 learning.py:507] global step 8503: loss = 2.7803 (1.245 sec/step)\n","I0830 18:55:55.468608 139622356395904 learning.py:507] global step 8504: loss = 2.7027 (1.216 sec/step)\n","I0830 18:55:56.693532 139622356395904 learning.py:507] global step 8505: loss = 2.4241 (1.223 sec/step)\n","I0830 18:55:57.911745 139622356395904 learning.py:507] global step 8506: loss = 2.6869 (1.216 sec/step)\n","I0830 18:55:59.103406 139622356395904 learning.py:507] global step 8507: loss = 2.6016 (1.190 sec/step)\n","I0830 18:56:00.477758 139622356395904 learning.py:507] global step 8508: loss = 2.6256 (1.297 sec/step)\n","I0830 18:56:02.389436 139619299985152 supervisor.py:1050] Recording summary at step 8509.\n","I0830 18:56:02.412206 139622356395904 learning.py:507] global step 8509: loss = 2.2328 (1.933 sec/step)\n","I0830 18:56:03.648988 139622356395904 learning.py:507] global step 8510: loss = 2.5330 (1.235 sec/step)\n","I0830 18:56:04.866867 139622356395904 learning.py:507] global step 8511: loss = 2.9572 (1.216 sec/step)\n","I0830 18:56:06.115707 139622356395904 learning.py:507] global step 8512: loss = 2.2779 (1.247 sec/step)\n","I0830 18:56:07.374008 139622356395904 learning.py:507] global step 8513: loss = 2.3147 (1.256 sec/step)\n","I0830 18:56:08.623764 139622356395904 learning.py:507] global step 8514: loss = 2.4279 (1.248 sec/step)\n","I0830 18:56:09.861507 139622356395904 learning.py:507] global step 8515: loss = 2.9332 (1.236 sec/step)\n","I0830 18:56:11.061892 139622356395904 learning.py:507] global step 8516: loss = 2.7330 (1.198 sec/step)\n","I0830 18:56:12.299337 139622356395904 learning.py:507] global step 8517: loss = 2.3870 (1.233 sec/step)\n","I0830 18:56:13.534541 139622356395904 learning.py:507] global step 8518: loss = 2.3384 (1.233 sec/step)\n","I0830 18:56:14.772641 139622356395904 learning.py:507] global step 8519: loss = 2.3683 (1.236 sec/step)\n","I0830 18:56:16.019470 139622356395904 learning.py:507] global step 8520: loss = 3.3809 (1.245 sec/step)\n","I0830 18:56:17.271401 139622356395904 learning.py:507] global step 8521: loss = 3.6240 (1.250 sec/step)\n","I0830 18:56:18.502825 139622356395904 learning.py:507] global step 8522: loss = 1.9547 (1.229 sec/step)\n","I0830 18:56:19.743260 139622356395904 learning.py:507] global step 8523: loss = 1.8750 (1.238 sec/step)\n","I0830 18:56:21.034460 139622356395904 learning.py:507] global step 8524: loss = 3.4874 (1.289 sec/step)\n","I0830 18:56:22.295555 139622356395904 learning.py:507] global step 8525: loss = 3.0108 (1.259 sec/step)\n","I0830 18:56:23.523656 139622356395904 learning.py:507] global step 8526: loss = 2.4816 (1.226 sec/step)\n","I0830 18:56:24.752352 139622356395904 learning.py:507] global step 8527: loss = 2.0885 (1.227 sec/step)\n","I0830 18:56:26.009453 139622356395904 learning.py:507] global step 8528: loss = 3.7668 (1.255 sec/step)\n","I0830 18:56:27.238882 139622356395904 learning.py:507] global step 8529: loss = 2.2674 (1.228 sec/step)\n","I0830 18:56:28.459541 139622356395904 learning.py:507] global step 8530: loss = 4.3478 (1.219 sec/step)\n","I0830 18:56:29.682214 139622356395904 learning.py:507] global step 8531: loss = 2.4638 (1.221 sec/step)\n","I0830 18:56:30.903181 139622356395904 learning.py:507] global step 8532: loss = 2.3193 (1.219 sec/step)\n","I0830 18:56:32.079765 139622356395904 learning.py:507] global step 8533: loss = 2.8853 (1.175 sec/step)\n","I0830 18:56:33.275454 139622356395904 learning.py:507] global step 8534: loss = 3.1067 (1.194 sec/step)\n","I0830 18:56:34.515176 139622356395904 learning.py:507] global step 8535: loss = 2.7499 (1.238 sec/step)\n","I0830 18:56:35.798824 139622356395904 learning.py:507] global step 8536: loss = 2.8174 (1.282 sec/step)\n","I0830 18:56:37.032154 139622356395904 learning.py:507] global step 8537: loss = 2.5006 (1.231 sec/step)\n","I0830 18:56:38.280199 139622356395904 learning.py:507] global step 8538: loss = 3.3676 (1.246 sec/step)\n","I0830 18:56:39.500929 139622356395904 learning.py:507] global step 8539: loss = 3.6253 (1.219 sec/step)\n","I0830 18:56:40.737440 139622356395904 learning.py:507] global step 8540: loss = 2.3846 (1.235 sec/step)\n","I0830 18:56:41.943808 139622356395904 learning.py:507] global step 8541: loss = 4.0146 (1.204 sec/step)\n","I0830 18:56:43.189185 139622356395904 learning.py:507] global step 8542: loss = 3.1381 (1.243 sec/step)\n","I0830 18:56:44.421480 139622356395904 learning.py:507] global step 8543: loss = 2.7001 (1.230 sec/step)\n","I0830 18:56:45.632493 139622356395904 learning.py:507] global step 8544: loss = 2.6836 (1.209 sec/step)\n","I0830 18:56:46.891705 139622356395904 learning.py:507] global step 8545: loss = 2.4338 (1.257 sec/step)\n","I0830 18:56:48.129972 139622356395904 learning.py:507] global step 8546: loss = 3.2204 (1.236 sec/step)\n","I0830 18:56:49.365784 139622356395904 learning.py:507] global step 8547: loss = 4.4134 (1.234 sec/step)\n","I0830 18:56:50.608749 139622356395904 learning.py:507] global step 8548: loss = 2.1613 (1.240 sec/step)\n","I0830 18:56:51.800746 139622356395904 learning.py:507] global step 8549: loss = 2.7701 (1.190 sec/step)\n","I0830 18:56:53.045705 139622356395904 learning.py:507] global step 8550: loss = 2.2514 (1.242 sec/step)\n","I0830 18:56:54.268106 139622356395904 learning.py:507] global step 8551: loss = 4.0529 (1.220 sec/step)\n","I0830 18:56:55.511813 139622356395904 learning.py:507] global step 8552: loss = 3.8496 (1.242 sec/step)\n","I0830 18:56:56.711509 139622356395904 learning.py:507] global step 8553: loss = 3.9066 (1.198 sec/step)\n","I0830 18:56:57.942994 139622356395904 learning.py:507] global step 8554: loss = 3.1498 (1.230 sec/step)\n","I0830 18:56:59.177392 139622356395904 learning.py:507] global step 8555: loss = 2.6216 (1.232 sec/step)\n","I0830 18:57:00.432521 139622356395904 learning.py:507] global step 8556: loss = 2.8988 (1.253 sec/step)\n","I0830 18:57:01.671700 139622356395904 learning.py:507] global step 8557: loss = 3.7232 (1.237 sec/step)\n","I0830 18:57:02.898745 139622356395904 learning.py:507] global step 8558: loss = 2.9545 (1.225 sec/step)\n","I0830 18:57:04.105040 139622356395904 learning.py:507] global step 8559: loss = 2.6126 (1.204 sec/step)\n","I0830 18:57:05.322689 139622356395904 learning.py:507] global step 8560: loss = 4.5576 (1.216 sec/step)\n","I0830 18:57:06.554462 139622356395904 learning.py:507] global step 8561: loss = 3.2476 (1.230 sec/step)\n","I0830 18:57:07.781780 139622356395904 learning.py:507] global step 8562: loss = 2.5195 (1.225 sec/step)\n","I0830 18:57:08.994164 139622356395904 learning.py:507] global step 8563: loss = 2.6985 (1.210 sec/step)\n","I0830 18:57:10.246493 139622356395904 learning.py:507] global step 8564: loss = 4.2694 (1.250 sec/step)\n","I0830 18:57:11.490274 139622356395904 learning.py:507] global step 8565: loss = 2.3642 (1.242 sec/step)\n","I0830 18:57:12.683740 139622356395904 learning.py:507] global step 8566: loss = 1.9446 (1.191 sec/step)\n","I0830 18:57:13.887292 139622356395904 learning.py:507] global step 8567: loss = 2.2845 (1.201 sec/step)\n","I0830 18:57:15.093956 139622356395904 learning.py:507] global step 8568: loss = 2.8368 (1.205 sec/step)\n","I0830 18:57:16.339822 139622356395904 learning.py:507] global step 8569: loss = 2.8071 (1.244 sec/step)\n","I0830 18:57:17.544753 139622356395904 learning.py:507] global step 8570: loss = 2.4334 (1.203 sec/step)\n","I0830 18:57:18.783898 139622356395904 learning.py:507] global step 8571: loss = 3.0194 (1.237 sec/step)\n","I0830 18:57:20.023374 139622356395904 learning.py:507] global step 8572: loss = 2.2871 (1.238 sec/step)\n","I0830 18:57:21.243234 139622356395904 learning.py:507] global step 8573: loss = 2.5295 (1.218 sec/step)\n","I0830 18:57:22.461281 139622356395904 learning.py:507] global step 8574: loss = 2.4898 (1.216 sec/step)\n","I0830 18:57:23.676287 139622356395904 learning.py:507] global step 8575: loss = 2.7707 (1.213 sec/step)\n","I0830 18:57:24.895853 139622356395904 learning.py:507] global step 8576: loss = 4.0051 (1.218 sec/step)\n","I0830 18:57:26.097498 139622356395904 learning.py:507] global step 8577: loss = 3.2722 (1.200 sec/step)\n","I0830 18:57:27.335557 139622356395904 learning.py:507] global step 8578: loss = 4.4347 (1.236 sec/step)\n","I0830 18:57:28.526921 139622356395904 learning.py:507] global step 8579: loss = 2.4580 (1.189 sec/step)\n","I0830 18:57:29.760360 139622356395904 learning.py:507] global step 8580: loss = 2.6170 (1.230 sec/step)\n","I0830 18:57:30.979633 139622356395904 learning.py:507] global step 8581: loss = 2.2748 (1.218 sec/step)\n","I0830 18:57:32.213995 139622356395904 learning.py:507] global step 8582: loss = 2.3174 (1.233 sec/step)\n","I0830 18:57:33.464911 139622356395904 learning.py:507] global step 8583: loss = 2.4801 (1.249 sec/step)\n","I0830 18:57:34.728266 139622356395904 learning.py:507] global step 8584: loss = 2.2213 (1.262 sec/step)\n","I0830 18:57:35.965430 139622356395904 learning.py:507] global step 8585: loss = 2.8729 (1.235 sec/step)\n","I0830 18:57:37.177233 139622356395904 learning.py:507] global step 8586: loss = 2.3928 (1.210 sec/step)\n","I0830 18:57:38.419399 139622356395904 learning.py:507] global step 8587: loss = 2.9701 (1.240 sec/step)\n","I0830 18:57:39.645992 139622356395904 learning.py:507] global step 8588: loss = 3.0155 (1.225 sec/step)\n","I0830 18:57:40.893477 139622356395904 learning.py:507] global step 8589: loss = 3.1603 (1.246 sec/step)\n","I0830 18:57:42.135496 139622356395904 learning.py:507] global step 8590: loss = 3.3574 (1.240 sec/step)\n","I0830 18:57:43.366281 139622356395904 learning.py:507] global step 8591: loss = 2.1457 (1.229 sec/step)\n","I0830 18:57:44.602595 139622356395904 learning.py:507] global step 8592: loss = 2.2100 (1.235 sec/step)\n","I0830 18:57:45.837887 139622356395904 learning.py:507] global step 8593: loss = 2.6023 (1.233 sec/step)\n","I0830 18:57:47.090262 139622356395904 learning.py:507] global step 8594: loss = 1.9773 (1.250 sec/step)\n","I0830 18:57:48.302501 139622356395904 learning.py:507] global step 8595: loss = 2.5802 (1.210 sec/step)\n","I0830 18:57:49.514655 139622356395904 learning.py:507] global step 8596: loss = 3.0929 (1.210 sec/step)\n","I0830 18:57:50.722683 139622356395904 learning.py:507] global step 8597: loss = 2.2286 (1.206 sec/step)\n","I0830 18:57:51.945316 139622356395904 learning.py:507] global step 8598: loss = 3.2371 (1.220 sec/step)\n","I0830 18:57:53.158818 139622356395904 learning.py:507] global step 8599: loss = 2.6851 (1.212 sec/step)\n","I0830 18:57:54.356333 139622356395904 learning.py:507] global step 8600: loss = 2.6942 (1.196 sec/step)\n","I0830 18:57:55.544990 139622356395904 learning.py:507] global step 8601: loss = 2.7497 (1.187 sec/step)\n","I0830 18:57:56.752404 139622356395904 learning.py:507] global step 8602: loss = 2.9959 (1.206 sec/step)\n","I0830 18:57:57.986447 139622356395904 learning.py:507] global step 8603: loss = 2.2424 (1.232 sec/step)\n","I0830 18:57:59.200776 139622356395904 learning.py:507] global step 8604: loss = 2.5244 (1.212 sec/step)\n","I0830 18:58:00.800388 139622356395904 learning.py:507] global step 8605: loss = 2.5938 (1.502 sec/step)\n","I0830 18:58:02.585696 139619299985152 supervisor.py:1050] Recording summary at step 8606.\n","I0830 18:58:02.610893 139622356395904 learning.py:507] global step 8606: loss = 1.8691 (1.799 sec/step)\n","I0830 18:58:03.871235 139622356395904 learning.py:507] global step 8607: loss = 3.6554 (1.258 sec/step)\n","I0830 18:58:05.083032 139622356395904 learning.py:507] global step 8608: loss = 3.1401 (1.210 sec/step)\n","I0830 18:58:06.313844 139622356395904 learning.py:507] global step 8609: loss = 2.3405 (1.229 sec/step)\n","I0830 18:58:07.572463 139622356395904 learning.py:507] global step 8610: loss = 3.1259 (1.257 sec/step)\n","I0830 18:58:08.774752 139622356395904 learning.py:507] global step 8611: loss = 2.8724 (1.200 sec/step)\n","I0830 18:58:10.006161 139622356395904 learning.py:507] global step 8612: loss = 3.1509 (1.228 sec/step)\n","I0830 18:58:11.202954 139622356395904 learning.py:507] global step 8613: loss = 3.6319 (1.195 sec/step)\n","I0830 18:58:12.503167 139622356395904 learning.py:507] global step 8614: loss = 2.7301 (1.298 sec/step)\n","I0830 18:58:13.750877 139622356395904 learning.py:507] global step 8615: loss = 3.7838 (1.246 sec/step)\n","I0830 18:58:14.962432 139622356395904 learning.py:507] global step 8616: loss = 2.2468 (1.210 sec/step)\n","I0830 18:58:16.174188 139622356395904 learning.py:507] global step 8617: loss = 2.1130 (1.210 sec/step)\n","I0830 18:58:17.390318 139622356395904 learning.py:507] global step 8618: loss = 3.0089 (1.214 sec/step)\n","I0830 18:58:18.618315 139622356395904 learning.py:507] global step 8619: loss = 3.5734 (1.226 sec/step)\n","I0830 18:58:19.833464 139622356395904 learning.py:507] global step 8620: loss = 4.1094 (1.213 sec/step)\n","I0830 18:58:21.047578 139622356395904 learning.py:507] global step 8621: loss = 3.0885 (1.212 sec/step)\n","I0830 18:58:22.285235 139622356395904 learning.py:507] global step 8622: loss = 2.4889 (1.236 sec/step)\n","I0830 18:58:23.516921 139622356395904 learning.py:507] global step 8623: loss = 3.3864 (1.230 sec/step)\n","I0830 18:58:24.708892 139622356395904 learning.py:507] global step 8624: loss = 3.0614 (1.190 sec/step)\n","I0830 18:58:25.965180 139622356395904 learning.py:507] global step 8625: loss = 2.8995 (1.254 sec/step)\n","I0830 18:58:27.188272 139622356395904 learning.py:507] global step 8626: loss = 2.5011 (1.221 sec/step)\n","I0830 18:58:28.440926 139622356395904 learning.py:507] global step 8627: loss = 2.6708 (1.251 sec/step)\n","I0830 18:58:29.690777 139622356395904 learning.py:507] global step 8628: loss = 2.5158 (1.248 sec/step)\n","I0830 18:58:30.926200 139622356395904 learning.py:507] global step 8629: loss = 2.3172 (1.233 sec/step)\n","I0830 18:58:32.175282 139622356395904 learning.py:507] global step 8630: loss = 2.2484 (1.247 sec/step)\n","I0830 18:58:33.414228 139622356395904 learning.py:507] global step 8631: loss = 2.4975 (1.237 sec/step)\n","I0830 18:58:34.616237 139622356395904 learning.py:507] global step 8632: loss = 2.5632 (1.200 sec/step)\n","I0830 18:58:35.862250 139622356395904 learning.py:507] global step 8633: loss = 3.0021 (1.244 sec/step)\n","I0830 18:58:37.111707 139622356395904 learning.py:507] global step 8634: loss = 2.7299 (1.248 sec/step)\n","I0830 18:58:38.331902 139622356395904 learning.py:507] global step 8635: loss = 4.2278 (1.218 sec/step)\n","I0830 18:58:39.558988 139622356395904 learning.py:507] global step 8636: loss = 2.6672 (1.225 sec/step)\n","I0830 18:58:40.803986 139622356395904 learning.py:507] global step 8637: loss = 3.6738 (1.243 sec/step)\n","I0830 18:58:42.016070 139622356395904 learning.py:507] global step 8638: loss = 2.3954 (1.210 sec/step)\n","I0830 18:58:43.244650 139622356395904 learning.py:507] global step 8639: loss = 3.3115 (1.227 sec/step)\n","I0830 18:58:44.462897 139622356395904 learning.py:507] global step 8640: loss = 2.0632 (1.216 sec/step)\n","I0830 18:58:45.701000 139622356395904 learning.py:507] global step 8641: loss = 2.1597 (1.236 sec/step)\n","I0830 18:58:46.931231 139622356395904 learning.py:507] global step 8642: loss = 3.0415 (1.228 sec/step)\n","I0830 18:58:48.146310 139622356395904 learning.py:507] global step 8643: loss = 3.1816 (1.212 sec/step)\n","I0830 18:58:49.355121 139622356395904 learning.py:507] global step 8644: loss = 3.2981 (1.207 sec/step)\n","I0830 18:58:50.621477 139622356395904 learning.py:507] global step 8645: loss = 3.1008 (1.265 sec/step)\n","I0830 18:58:51.816540 139622356395904 learning.py:507] global step 8646: loss = 3.4678 (1.193 sec/step)\n","I0830 18:58:53.058489 139622356395904 learning.py:507] global step 8647: loss = 2.7241 (1.240 sec/step)\n","I0830 18:58:54.269596 139622356395904 learning.py:507] global step 8648: loss = 2.4045 (1.209 sec/step)\n","I0830 18:58:55.476680 139622356395904 learning.py:507] global step 8649: loss = 2.6621 (1.205 sec/step)\n","I0830 18:58:56.697723 139622356395904 learning.py:507] global step 8650: loss = 2.5889 (1.219 sec/step)\n","I0830 18:58:57.894625 139622356395904 learning.py:507] global step 8651: loss = 2.1602 (1.195 sec/step)\n","I0830 18:58:59.099497 139622356395904 learning.py:507] global step 8652: loss = 2.2581 (1.203 sec/step)\n","I0830 18:59:00.316843 139622356395904 learning.py:507] global step 8653: loss = 1.9598 (1.215 sec/step)\n","I0830 18:59:01.548607 139622356395904 learning.py:507] global step 8654: loss = 2.2690 (1.230 sec/step)\n","I0830 18:59:02.786515 139622356395904 learning.py:507] global step 8655: loss = 2.8955 (1.236 sec/step)\n","I0830 18:59:03.988595 139622356395904 learning.py:507] global step 8656: loss = 2.3186 (1.200 sec/step)\n","I0830 18:59:05.205753 139622356395904 learning.py:507] global step 8657: loss = 2.8005 (1.215 sec/step)\n","I0830 18:59:06.412922 139622356395904 learning.py:507] global step 8658: loss = 2.3128 (1.205 sec/step)\n","I0830 18:59:07.610379 139622356395904 learning.py:507] global step 8659: loss = 2.4651 (1.195 sec/step)\n","I0830 18:59:08.854758 139622356395904 learning.py:507] global step 8660: loss = 2.3809 (1.242 sec/step)\n","I0830 18:59:10.081274 139622356395904 learning.py:507] global step 8661: loss = 2.2844 (1.225 sec/step)\n","I0830 18:59:11.276253 139622356395904 learning.py:507] global step 8662: loss = 3.0577 (1.193 sec/step)\n","I0830 18:59:12.509915 139622356395904 learning.py:507] global step 8663: loss = 2.3757 (1.232 sec/step)\n","I0830 18:59:13.736370 139622356395904 learning.py:507] global step 8664: loss = 2.1372 (1.224 sec/step)\n","I0830 18:59:14.936162 139622356395904 learning.py:507] global step 8665: loss = 2.2131 (1.198 sec/step)\n","I0830 18:59:16.174280 139622356395904 learning.py:507] global step 8666: loss = 2.5334 (1.236 sec/step)\n","I0830 18:59:17.406428 139622356395904 learning.py:507] global step 8667: loss = 2.2287 (1.230 sec/step)\n","I0830 18:59:18.659640 139622356395904 learning.py:507] global step 8668: loss = 3.0135 (1.251 sec/step)\n","I0830 18:59:19.877670 139622356395904 learning.py:507] global step 8669: loss = 2.3761 (1.216 sec/step)\n","I0830 18:59:21.118677 139622356395904 learning.py:507] global step 8670: loss = 2.5731 (1.239 sec/step)\n","I0830 18:59:22.341656 139622356395904 learning.py:507] global step 8671: loss = 2.7380 (1.221 sec/step)\n","I0830 18:59:23.576497 139622356395904 learning.py:507] global step 8672: loss = 2.8262 (1.233 sec/step)\n","I0830 18:59:24.800350 139622356395904 learning.py:507] global step 8673: loss = 2.9079 (1.222 sec/step)\n","I0830 18:59:25.993877 139622356395904 learning.py:507] global step 8674: loss = 3.0966 (1.190 sec/step)\n","I0830 18:59:27.232233 139622356395904 learning.py:507] global step 8675: loss = 2.7064 (1.236 sec/step)\n","I0830 18:59:28.434816 139622356395904 learning.py:507] global step 8676: loss = 2.5322 (1.201 sec/step)\n","I0830 18:59:29.666654 139622356395904 learning.py:507] global step 8677: loss = 2.8360 (1.230 sec/step)\n","I0830 18:59:30.906500 139622356395904 learning.py:507] global step 8678: loss = 2.8398 (1.238 sec/step)\n","I0830 18:59:32.134259 139622356395904 learning.py:507] global step 8679: loss = 2.3395 (1.226 sec/step)\n","I0830 18:59:33.399194 139622356395904 learning.py:507] global step 8680: loss = 2.2590 (1.263 sec/step)\n","I0830 18:59:34.626381 139622356395904 learning.py:507] global step 8681: loss = 2.4325 (1.225 sec/step)\n","I0830 18:59:35.836997 139622356395904 learning.py:507] global step 8682: loss = 3.1168 (1.209 sec/step)\n","I0830 18:59:37.069610 139622356395904 learning.py:507] global step 8683: loss = 2.6867 (1.231 sec/step)\n","I0830 18:59:38.288970 139622356395904 learning.py:507] global step 8684: loss = 2.7004 (1.218 sec/step)\n","I0830 18:59:39.498512 139622356395904 learning.py:507] global step 8685: loss = 2.8190 (1.208 sec/step)\n","I0830 18:59:40.742489 139622356395904 learning.py:507] global step 8686: loss = 2.9673 (1.242 sec/step)\n","I0830 18:59:41.955748 139622356395904 learning.py:507] global step 8687: loss = 2.0890 (1.211 sec/step)\n","I0830 18:59:43.161499 139622356395904 learning.py:507] global step 8688: loss = 2.2150 (1.204 sec/step)\n","I0830 18:59:44.388716 139622356395904 learning.py:507] global step 8689: loss = 2.5089 (1.225 sec/step)\n","I0830 18:59:45.608620 139622356395904 learning.py:507] global step 8690: loss = 2.0956 (1.218 sec/step)\n","I0830 18:59:46.797605 139622356395904 learning.py:507] global step 8691: loss = 2.6226 (1.187 sec/step)\n","I0830 18:59:47.994972 139622356395904 learning.py:507] global step 8692: loss = 2.8727 (1.196 sec/step)\n","I0830 18:59:49.198724 139622356395904 learning.py:507] global step 8693: loss = 2.3722 (1.202 sec/step)\n","I0830 18:59:50.421590 139622356395904 learning.py:507] global step 8694: loss = 1.9763 (1.221 sec/step)\n","I0830 18:59:51.660134 139622356395904 learning.py:507] global step 8695: loss = 1.9418 (1.237 sec/step)\n","I0830 18:59:52.883697 139622356395904 learning.py:507] global step 8696: loss = 2.9494 (1.222 sec/step)\n","I0830 18:59:54.104167 139622356395904 learning.py:507] global step 8697: loss = 2.3309 (1.218 sec/step)\n","I0830 18:59:55.342876 139622356395904 learning.py:507] global step 8698: loss = 2.9888 (1.237 sec/step)\n","I0830 18:59:56.590641 139622356395904 learning.py:507] global step 8699: loss = 2.1154 (1.246 sec/step)\n","I0830 18:59:57.839320 139622356395904 learning.py:507] global step 8700: loss = 2.5981 (1.247 sec/step)\n","I0830 18:59:59.081041 139622356395904 learning.py:507] global step 8701: loss = 3.6079 (1.240 sec/step)\n","I0830 18:59:59.647270 139619316770560 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n","I0830 19:00:00.759247 139622356395904 learning.py:507] global step 8702: loss = 3.0134 (1.378 sec/step)\n","I0830 19:00:02.542251 139619299985152 supervisor.py:1050] Recording summary at step 8702.\n","I0830 19:00:03.129427 139622356395904 learning.py:507] global step 8703: loss = 2.4198 (2.283 sec/step)\n","I0830 19:00:04.912709 139622356395904 learning.py:507] global step 8704: loss = 2.6967 (1.767 sec/step)\n","I0830 19:00:06.304485 139622356395904 learning.py:507] global step 8705: loss = 2.9948 (1.389 sec/step)\n","I0830 19:00:07.518635 139622356395904 learning.py:507] global step 8706: loss = 2.9225 (1.212 sec/step)\n","I0830 19:00:08.727400 139622356395904 learning.py:507] global step 8707: loss = 2.0247 (1.207 sec/step)\n","I0830 19:00:09.949725 139622356395904 learning.py:507] global step 8708: loss = 2.4428 (1.221 sec/step)\n","I0830 19:00:11.195889 139622356395904 learning.py:507] global step 8709: loss = 2.3453 (1.244 sec/step)\n","I0830 19:00:12.415270 139622356395904 learning.py:507] global step 8710: loss = 2.8065 (1.218 sec/step)\n","I0830 19:00:13.629274 139622356395904 learning.py:507] global step 8711: loss = 4.3033 (1.212 sec/step)\n","I0830 19:00:14.824353 139622356395904 learning.py:507] global step 8712: loss = 2.9197 (1.193 sec/step)\n","I0830 19:00:16.068465 139622356395904 learning.py:507] global step 8713: loss = 2.9100 (1.242 sec/step)\n","I0830 19:00:17.291238 139622356395904 learning.py:507] global step 8714: loss = 2.7453 (1.221 sec/step)\n","I0830 19:00:18.566355 139622356395904 learning.py:507] global step 8715: loss = 2.3398 (1.272 sec/step)\n","I0830 19:00:19.779235 139622356395904 learning.py:507] global step 8716: loss = 2.0216 (1.210 sec/step)\n","I0830 19:00:21.024665 139622356395904 learning.py:507] global step 8717: loss = 2.8222 (1.243 sec/step)\n","I0830 19:00:22.260794 139622356395904 learning.py:507] global step 8718: loss = 3.5683 (1.234 sec/step)\n","I0830 19:00:23.512998 139622356395904 learning.py:507] global step 8719: loss = 3.5657 (1.250 sec/step)\n","I0830 19:00:24.737089 139622356395904 learning.py:507] global step 8720: loss = 2.7392 (1.222 sec/step)\n","I0830 19:00:25.974251 139622356395904 learning.py:507] global step 8721: loss = 2.1507 (1.235 sec/step)\n","I0830 19:00:27.214025 139622356395904 learning.py:507] global step 8722: loss = 2.5108 (1.238 sec/step)\n","I0830 19:00:28.448096 139622356395904 learning.py:507] global step 8723: loss = 2.1081 (1.232 sec/step)\n","I0830 19:00:29.653342 139622356395904 learning.py:507] global step 8724: loss = 2.4016 (1.203 sec/step)\n","I0830 19:00:30.883288 139622356395904 learning.py:507] global step 8725: loss = 2.3425 (1.225 sec/step)\n","I0830 19:00:32.101242 139622356395904 learning.py:507] global step 8726: loss = 2.7208 (1.216 sec/step)\n","I0830 19:00:33.317301 139622356395904 learning.py:507] global step 8727: loss = 2.2689 (1.214 sec/step)\n","I0830 19:00:34.562020 139622356395904 learning.py:507] global step 8728: loss = 1.7842 (1.243 sec/step)\n","I0830 19:00:35.806603 139622356395904 learning.py:507] global step 8729: loss = 2.6129 (1.243 sec/step)\n","I0830 19:00:37.044976 139622356395904 learning.py:507] global step 8730: loss = 1.8011 (1.236 sec/step)\n","I0830 19:00:38.285314 139622356395904 learning.py:507] global step 8731: loss = 2.4196 (1.238 sec/step)\n","I0830 19:00:39.495951 139622356395904 learning.py:507] global step 8732: loss = 2.6106 (1.208 sec/step)\n","I0830 19:00:40.712490 139622356395904 learning.py:507] global step 8733: loss = 2.4002 (1.215 sec/step)\n","I0830 19:00:41.966154 139622356395904 learning.py:507] global step 8734: loss = 1.9900 (1.251 sec/step)\n","I0830 19:00:43.195716 139622356395904 learning.py:507] global step 8735: loss = 3.5551 (1.228 sec/step)\n","I0830 19:00:44.438604 139622356395904 learning.py:507] global step 8736: loss = 3.4619 (1.241 sec/step)\n","I0830 19:00:45.676084 139622356395904 learning.py:507] global step 8737: loss = 2.5688 (1.236 sec/step)\n","I0830 19:00:46.904249 139622356395904 learning.py:507] global step 8738: loss = 3.5202 (1.226 sec/step)\n","I0830 19:00:48.137641 139622356395904 learning.py:507] global step 8739: loss = 2.3161 (1.231 sec/step)\n","I0830 19:00:49.394994 139622356395904 learning.py:507] global step 8740: loss = 2.8375 (1.256 sec/step)\n","I0830 19:00:50.627093 139622356395904 learning.py:507] global step 8741: loss = 2.2640 (1.230 sec/step)\n","I0830 19:00:51.821456 139622356395904 learning.py:507] global step 8742: loss = 2.1952 (1.192 sec/step)\n","I0830 19:00:53.051465 139622356395904 learning.py:507] global step 8743: loss = 2.2999 (1.228 sec/step)\n","I0830 19:00:54.310214 139622356395904 learning.py:507] global step 8744: loss = 2.8406 (1.257 sec/step)\n","I0830 19:00:55.548353 139622356395904 learning.py:507] global step 8745: loss = 2.3349 (1.236 sec/step)\n","I0830 19:00:56.770913 139622356395904 learning.py:507] global step 8746: loss = 2.1083 (1.221 sec/step)\n","I0830 19:00:57.975288 139622356395904 learning.py:507] global step 8747: loss = 2.0935 (1.202 sec/step)\n","I0830 19:00:59.188701 139622356395904 learning.py:507] global step 8748: loss = 2.9706 (1.212 sec/step)\n","I0830 19:01:00.380831 139622356395904 learning.py:507] global step 8749: loss = 3.4261 (1.190 sec/step)\n","I0830 19:01:01.588382 139622356395904 learning.py:507] global step 8750: loss = 2.3121 (1.206 sec/step)\n","I0830 19:01:02.846139 139622356395904 learning.py:507] global step 8751: loss = 2.2899 (1.256 sec/step)\n","I0830 19:01:04.056113 139622356395904 learning.py:507] global step 8752: loss = 2.1171 (1.208 sec/step)\n","I0830 19:01:05.254520 139622356395904 learning.py:507] global step 8753: loss = 2.8116 (1.197 sec/step)\n","I0830 19:01:06.468615 139622356395904 learning.py:507] global step 8754: loss = 1.8521 (1.212 sec/step)\n","I0830 19:01:07.655132 139622356395904 learning.py:507] global step 8755: loss = 2.5206 (1.185 sec/step)\n","I0830 19:01:08.896009 139622356395904 learning.py:507] global step 8756: loss = 2.5252 (1.239 sec/step)\n","I0830 19:01:10.111114 139622356395904 learning.py:507] global step 8757: loss = 3.7936 (1.213 sec/step)\n","I0830 19:01:11.361712 139622356395904 learning.py:507] global step 8758: loss = 2.3279 (1.249 sec/step)\n","I0830 19:01:12.567925 139622356395904 learning.py:507] global step 8759: loss = 2.5145 (1.204 sec/step)\n","I0830 19:01:13.769128 139622356395904 learning.py:507] global step 8760: loss = 2.3642 (1.199 sec/step)\n","I0830 19:01:15.016821 139622356395904 learning.py:507] global step 8761: loss = 2.2691 (1.246 sec/step)\n","I0830 19:01:16.240938 139622356395904 learning.py:507] global step 8762: loss = 2.8545 (1.222 sec/step)\n","I0830 19:01:17.458655 139622356395904 learning.py:507] global step 8763: loss = 4.0179 (1.216 sec/step)\n","I0830 19:01:18.691980 139622356395904 learning.py:507] global step 8764: loss = 4.1351 (1.231 sec/step)\n","I0830 19:01:19.878677 139622356395904 learning.py:507] global step 8765: loss = 3.2988 (1.185 sec/step)\n","I0830 19:01:21.137124 139622356395904 learning.py:507] global step 8766: loss = 2.4808 (1.256 sec/step)\n","I0830 19:01:22.360157 139622356395904 learning.py:507] global step 8767: loss = 2.6413 (1.221 sec/step)\n","I0830 19:01:23.623278 139622356395904 learning.py:507] global step 8768: loss = 2.3581 (1.261 sec/step)\n","I0830 19:01:24.849214 139622356395904 learning.py:507] global step 8769: loss = 3.9319 (1.224 sec/step)\n","I0830 19:01:26.086733 139622356395904 learning.py:507] global step 8770: loss = 2.0606 (1.236 sec/step)\n","I0830 19:01:27.313633 139622356395904 learning.py:507] global step 8771: loss = 3.0204 (1.225 sec/step)\n","I0830 19:01:28.562005 139622356395904 learning.py:507] global step 8772: loss = 3.0366 (1.247 sec/step)\n","I0830 19:01:29.796236 139622356395904 learning.py:507] global step 8773: loss = 2.2622 (1.232 sec/step)\n","I0830 19:01:31.048203 139622356395904 learning.py:507] global step 8774: loss = 2.3853 (1.250 sec/step)\n","I0830 19:01:32.261546 139622356395904 learning.py:507] global step 8775: loss = 2.8509 (1.211 sec/step)\n","I0830 19:01:33.506488 139622356395904 learning.py:507] global step 8776: loss = 2.3166 (1.241 sec/step)\n","I0830 19:01:34.755432 139622356395904 learning.py:507] global step 8777: loss = 2.5353 (1.247 sec/step)\n","I0830 19:01:35.998233 139622356395904 learning.py:507] global step 8778: loss = 2.4215 (1.241 sec/step)\n","I0830 19:01:37.251868 139622356395904 learning.py:507] global step 8779: loss = 2.4568 (1.252 sec/step)\n","I0830 19:01:38.437804 139622356395904 learning.py:507] global step 8780: loss = 2.1985 (1.184 sec/step)\n","I0830 19:01:39.689629 139622356395904 learning.py:507] global step 8781: loss = 2.2470 (1.250 sec/step)\n","I0830 19:01:40.939794 139622356395904 learning.py:507] global step 8782: loss = 2.6990 (1.248 sec/step)\n","I0830 19:01:42.200901 139622356395904 learning.py:507] global step 8783: loss = 2.3916 (1.259 sec/step)\n","I0830 19:01:43.405932 139622356395904 learning.py:507] global step 8784: loss = 2.2949 (1.203 sec/step)\n","I0830 19:01:44.619745 139622356395904 learning.py:507] global step 8785: loss = 2.3482 (1.212 sec/step)\n","I0830 19:01:45.866420 139622356395904 learning.py:507] global step 8786: loss = 2.1525 (1.245 sec/step)\n","I0830 19:01:47.117697 139622356395904 learning.py:507] global step 8787: loss = 2.3174 (1.249 sec/step)\n","I0830 19:01:48.381526 139622356395904 learning.py:507] global step 8788: loss = 3.2787 (1.262 sec/step)\n","I0830 19:01:49.620161 139622356395904 learning.py:507] global step 8789: loss = 2.5224 (1.237 sec/step)\n","I0830 19:01:50.856597 139622356395904 learning.py:507] global step 8790: loss = 3.2699 (1.235 sec/step)\n","I0830 19:01:52.116250 139622356395904 learning.py:507] global step 8791: loss = 2.5562 (1.258 sec/step)\n","I0830 19:01:53.341696 139622356395904 learning.py:507] global step 8792: loss = 2.7551 (1.224 sec/step)\n","I0830 19:01:54.631188 139622356395904 learning.py:507] global step 8793: loss = 2.4367 (1.288 sec/step)\n","I0830 19:01:55.837773 139622356395904 learning.py:507] global step 8794: loss = 2.2589 (1.204 sec/step)\n","I0830 19:01:57.110149 139622356395904 learning.py:507] global step 8795: loss = 3.1241 (1.270 sec/step)\n","I0830 19:01:58.353020 139622356395904 learning.py:507] global step 8796: loss = 2.4880 (1.241 sec/step)\n","I0830 19:01:59.583734 139622356395904 learning.py:507] global step 8797: loss = 2.7489 (1.229 sec/step)\n","I0830 19:02:01.445908 139619299985152 supervisor.py:1050] Recording summary at step 8797.\n","I0830 19:02:01.932202 139622356395904 learning.py:507] global step 8798: loss = 3.0572 (2.347 sec/step)\n","I0830 19:02:03.129509 139622356395904 learning.py:507] global step 8799: loss = 2.4492 (1.195 sec/step)\n","I0830 19:02:04.349784 139622356395904 learning.py:507] global step 8800: loss = 2.8190 (1.218 sec/step)\n","I0830 19:02:05.612182 139622356395904 learning.py:507] global step 8801: loss = 2.8035 (1.260 sec/step)\n","I0830 19:02:06.861797 139622356395904 learning.py:507] global step 8802: loss = 3.0570 (1.248 sec/step)\n","I0830 19:02:08.113152 139622356395904 learning.py:507] global step 8803: loss = 3.6868 (1.250 sec/step)\n","I0830 19:02:09.346898 139622356395904 learning.py:507] global step 8804: loss = 3.2200 (1.232 sec/step)\n","I0830 19:02:10.552571 139622356395904 learning.py:507] global step 8805: loss = 2.8238 (1.204 sec/step)\n","I0830 19:02:11.773246 139622356395904 learning.py:507] global step 8806: loss = 2.8354 (1.219 sec/step)\n","I0830 19:02:13.021021 139622356395904 learning.py:507] global step 8807: loss = 2.5046 (1.246 sec/step)\n","I0830 19:02:14.231549 139622356395904 learning.py:507] global step 8808: loss = 2.7534 (1.208 sec/step)\n","I0830 19:02:15.497693 139622356395904 learning.py:507] global step 8809: loss = 2.3166 (1.261 sec/step)\n","I0830 19:02:16.756525 139622356395904 learning.py:507] global step 8810: loss = 2.8232 (1.257 sec/step)\n","I0830 19:02:17.987008 139622356395904 learning.py:507] global step 8811: loss = 2.4800 (1.229 sec/step)\n","I0830 19:02:19.219853 139622356395904 learning.py:507] global step 8812: loss = 2.6735 (1.231 sec/step)\n","I0830 19:02:20.464101 139622356395904 learning.py:507] global step 8813: loss = 2.5163 (1.242 sec/step)\n","I0830 19:02:21.715966 139622356395904 learning.py:507] global step 8814: loss = 2.5096 (1.250 sec/step)\n","I0830 19:02:22.925944 139622356395904 learning.py:507] global step 8815: loss = 2.7391 (1.208 sec/step)\n","I0830 19:02:24.153947 139622356395904 learning.py:507] global step 8816: loss = 3.3619 (1.226 sec/step)\n","I0830 19:02:25.397758 139622356395904 learning.py:507] global step 8817: loss = 2.6930 (1.242 sec/step)\n","I0830 19:02:26.641280 139622356395904 learning.py:507] global step 8818: loss = 2.6819 (1.242 sec/step)\n","I0830 19:02:27.878160 139622356395904 learning.py:507] global step 8819: loss = 3.2560 (1.235 sec/step)\n","I0830 19:02:29.111242 139622356395904 learning.py:507] global step 8820: loss = 2.4131 (1.231 sec/step)\n","I0830 19:02:30.379369 139622356395904 learning.py:507] global step 8821: loss = 2.8681 (1.266 sec/step)\n","I0830 19:02:31.607791 139622356395904 learning.py:507] global step 8822: loss = 2.1434 (1.226 sec/step)\n","I0830 19:02:32.862586 139622356395904 learning.py:507] global step 8823: loss = 2.1076 (1.253 sec/step)\n","I0830 19:02:34.084851 139622356395904 learning.py:507] global step 8824: loss = 3.3376 (1.220 sec/step)\n","I0830 19:02:35.335819 139622356395904 learning.py:507] global step 8825: loss = 2.9957 (1.249 sec/step)\n","I0830 19:02:36.558343 139622356395904 learning.py:507] global step 8826: loss = 3.9082 (1.221 sec/step)\n","I0830 19:02:37.826727 139622356395904 learning.py:507] global step 8827: loss = 2.7730 (1.267 sec/step)\n","I0830 19:02:39.078166 139622356395904 learning.py:507] global step 8828: loss = 2.2148 (1.249 sec/step)\n","I0830 19:02:40.304497 139622356395904 learning.py:507] global step 8829: loss = 2.1198 (1.225 sec/step)\n","I0830 19:02:41.535969 139622356395904 learning.py:507] global step 8830: loss = 2.2600 (1.229 sec/step)\n","I0830 19:02:42.798927 139622356395904 learning.py:507] global step 8831: loss = 1.7438 (1.261 sec/step)\n","I0830 19:02:44.004444 139622356395904 learning.py:507] global step 8832: loss = 2.7630 (1.204 sec/step)\n","I0830 19:02:45.261541 139622356395904 learning.py:507] global step 8833: loss = 2.6274 (1.255 sec/step)\n","I0830 19:02:46.522444 139622356395904 learning.py:507] global step 8834: loss = 2.2343 (1.259 sec/step)\n","I0830 19:02:47.773529 139622356395904 learning.py:507] global step 8835: loss = 1.9668 (1.249 sec/step)\n","I0830 19:02:48.985443 139622356395904 learning.py:507] global step 8836: loss = 2.1474 (1.210 sec/step)\n","I0830 19:02:50.250233 139622356395904 learning.py:507] global step 8837: loss = 3.6149 (1.263 sec/step)\n","I0830 19:02:51.476720 139622356395904 learning.py:507] global step 8838: loss = 2.7892 (1.225 sec/step)\n","I0830 19:02:52.712637 139622356395904 learning.py:507] global step 8839: loss = 2.8675 (1.234 sec/step)\n","I0830 19:02:53.951707 139622356395904 learning.py:507] global step 8840: loss = 3.1337 (1.237 sec/step)\n","I0830 19:02:55.203685 139622356395904 learning.py:507] global step 8841: loss = 2.0628 (1.250 sec/step)\n","I0830 19:02:56.413187 139622356395904 learning.py:507] global step 8842: loss = 2.2341 (1.208 sec/step)\n","I0830 19:02:57.613752 139622356395904 learning.py:507] global step 8843: loss = 2.1105 (1.199 sec/step)\n","I0830 19:02:58.824682 139622356395904 learning.py:507] global step 8844: loss = 2.1809 (1.209 sec/step)\n","I0830 19:03:00.059379 139622356395904 learning.py:507] global step 8845: loss = 3.6003 (1.233 sec/step)\n","I0830 19:03:01.292164 139622356395904 learning.py:507] global step 8846: loss = 2.9113 (1.231 sec/step)\n","I0830 19:03:02.504713 139622356395904 learning.py:507] global step 8847: loss = 2.2827 (1.211 sec/step)\n","I0830 19:03:03.699979 139622356395904 learning.py:507] global step 8848: loss = 3.4356 (1.193 sec/step)\n","I0830 19:03:04.962410 139622356395904 learning.py:507] global step 8849: loss = 1.8977 (1.260 sec/step)\n","I0830 19:03:06.205650 139622356395904 learning.py:507] global step 8850: loss = 2.8842 (1.241 sec/step)\n","I0830 19:03:07.451757 139622356395904 learning.py:507] global step 8851: loss = 3.5432 (1.244 sec/step)\n","I0830 19:03:08.655502 139622356395904 learning.py:507] global step 8852: loss = 2.0850 (1.202 sec/step)\n","I0830 19:03:09.862835 139622356395904 learning.py:507] global step 8853: loss = 2.1947 (1.206 sec/step)\n","I0830 19:03:11.083040 139622356395904 learning.py:507] global step 8854: loss = 3.6707 (1.218 sec/step)\n","I0830 19:03:12.288946 139622356395904 learning.py:507] global step 8855: loss = 3.2457 (1.204 sec/step)\n","I0830 19:03:13.474437 139622356395904 learning.py:507] global step 8856: loss = 4.0662 (1.184 sec/step)\n","I0830 19:03:14.695517 139622356395904 learning.py:507] global step 8857: loss = 3.0956 (1.219 sec/step)\n","I0830 19:03:15.941863 139622356395904 learning.py:507] global step 8858: loss = 2.3123 (1.245 sec/step)\n","I0830 19:03:17.201883 139622356395904 learning.py:507] global step 8859: loss = 3.2832 (1.258 sec/step)\n","I0830 19:03:18.429480 139622356395904 learning.py:507] global step 8860: loss = 2.2648 (1.226 sec/step)\n","I0830 19:03:19.655740 139622356395904 learning.py:507] global step 8861: loss = 2.1307 (1.224 sec/step)\n","I0830 19:03:20.926919 139622356395904 learning.py:507] global step 8862: loss = 2.9350 (1.269 sec/step)\n","I0830 19:03:22.167316 139622356395904 learning.py:507] global step 8863: loss = 2.7013 (1.239 sec/step)\n","I0830 19:03:23.416315 139622356395904 learning.py:507] global step 8864: loss = 2.0152 (1.247 sec/step)\n","I0830 19:03:24.616077 139622356395904 learning.py:507] global step 8865: loss = 2.8329 (1.198 sec/step)\n","I0830 19:03:25.895716 139622356395904 learning.py:507] global step 8866: loss = 2.4776 (1.278 sec/step)\n","I0830 19:03:27.147957 139622356395904 learning.py:507] global step 8867: loss = 1.9882 (1.250 sec/step)\n","I0830 19:03:28.358219 139622356395904 learning.py:507] global step 8868: loss = 3.1367 (1.208 sec/step)\n","I0830 19:03:29.601230 139622356395904 learning.py:507] global step 8869: loss = 3.3808 (1.241 sec/step)\n","I0830 19:03:30.824911 139622356395904 learning.py:507] global step 8870: loss = 2.7975 (1.222 sec/step)\n","I0830 19:03:32.049420 139622356395904 learning.py:507] global step 8871: loss = 2.5450 (1.223 sec/step)\n","I0830 19:03:33.285630 139622356395904 learning.py:507] global step 8872: loss = 2.2847 (1.234 sec/step)\n","I0830 19:03:34.576182 139622356395904 learning.py:507] global step 8873: loss = 3.1252 (1.288 sec/step)\n","I0830 19:03:35.805830 139622356395904 learning.py:507] global step 8874: loss = 2.3756 (1.228 sec/step)\n","I0830 19:03:37.072810 139622356395904 learning.py:507] global step 8875: loss = 2.4177 (1.265 sec/step)\n","I0830 19:03:38.324331 139622356395904 learning.py:507] global step 8876: loss = 2.7188 (1.250 sec/step)\n","I0830 19:03:39.586958 139622356395904 learning.py:507] global step 8877: loss = 2.2428 (1.261 sec/step)\n","I0830 19:03:40.789556 139622356395904 learning.py:507] global step 8878: loss = 2.5972 (1.200 sec/step)\n","I0830 19:03:42.025574 139622356395904 learning.py:507] global step 8879: loss = 4.4608 (1.234 sec/step)\n","I0830 19:03:43.234244 139622356395904 learning.py:507] global step 8880: loss = 3.1654 (1.207 sec/step)\n","I0830 19:03:44.492884 139622356395904 learning.py:507] global step 8881: loss = 2.3767 (1.257 sec/step)\n","I0830 19:03:45.745863 139622356395904 learning.py:507] global step 8882: loss = 2.5691 (1.252 sec/step)\n","I0830 19:03:46.976114 139622356395904 learning.py:507] global step 8883: loss = 2.2599 (1.228 sec/step)\n","I0830 19:03:48.202452 139622356395904 learning.py:507] global step 8884: loss = 2.5061 (1.224 sec/step)\n","I0830 19:03:49.466430 139622356395904 learning.py:507] global step 8885: loss = 3.1720 (1.262 sec/step)\n","I0830 19:03:50.673992 139622356395904 learning.py:507] global step 8886: loss = 2.4644 (1.206 sec/step)\n","I0830 19:03:51.883204 139622356395904 learning.py:507] global step 8887: loss = 2.8326 (1.207 sec/step)\n","I0830 19:03:53.133024 139622356395904 learning.py:507] global step 8888: loss = 2.2819 (1.248 sec/step)\n","I0830 19:03:54.321469 139622356395904 learning.py:507] global step 8889: loss = 2.2348 (1.186 sec/step)\n","I0830 19:03:55.590245 139622356395904 learning.py:507] global step 8890: loss = 2.2849 (1.267 sec/step)\n","I0830 19:03:56.819805 139622356395904 learning.py:507] global step 8891: loss = 3.3522 (1.227 sec/step)\n","I0830 19:03:58.053225 139622356395904 learning.py:507] global step 8892: loss = 2.8037 (1.231 sec/step)\n","I0830 19:03:59.273514 139622356395904 learning.py:507] global step 8893: loss = 2.0676 (1.218 sec/step)\n","I0830 19:04:00.581839 139622356395904 learning.py:507] global step 8894: loss = 2.4952 (1.241 sec/step)\n","I0830 19:04:02.507468 139619299985152 supervisor.py:1050] Recording summary at step 8895.\n","I0830 19:04:02.530179 139622356395904 learning.py:507] global step 8895: loss = 2.8708 (1.943 sec/step)\n","I0830 19:04:03.714942 139622356395904 learning.py:507] global step 8896: loss = 2.8475 (1.183 sec/step)\n","I0830 19:04:04.945245 139622356395904 learning.py:507] global step 8897: loss = 2.3022 (1.228 sec/step)\n","I0830 19:04:06.229074 139622356395904 learning.py:507] global step 8898: loss = 2.4732 (1.282 sec/step)\n","I0830 19:04:07.440191 139622356395904 learning.py:507] global step 8899: loss = 3.1400 (1.209 sec/step)\n","I0830 19:04:08.658583 139622356395904 learning.py:507] global step 8900: loss = 2.7551 (1.216 sec/step)\n","I0830 19:04:09.906657 139622356395904 learning.py:507] global step 8901: loss = 2.1953 (1.246 sec/step)\n","I0830 19:04:11.091732 139622356395904 learning.py:507] global step 8902: loss = 3.2206 (1.183 sec/step)\n","I0830 19:04:12.300515 139622356395904 learning.py:507] global step 8903: loss = 2.4990 (1.207 sec/step)\n","I0830 19:04:13.556264 139622356395904 learning.py:507] global step 8904: loss = 2.5566 (1.254 sec/step)\n","I0830 19:04:14.789282 139622356395904 learning.py:507] global step 8905: loss = 2.2878 (1.231 sec/step)\n","I0830 19:04:15.995371 139622356395904 learning.py:507] global step 8906: loss = 2.2257 (1.204 sec/step)\n","I0830 19:04:17.195782 139622356395904 learning.py:507] global step 8907: loss = 1.9633 (1.198 sec/step)\n","I0830 19:04:18.413341 139622356395904 learning.py:507] global step 8908: loss = 4.0381 (1.216 sec/step)\n","I0830 19:04:19.643164 139622356395904 learning.py:507] global step 8909: loss = 2.4034 (1.228 sec/step)\n","I0830 19:04:20.875303 139622356395904 learning.py:507] global step 8910: loss = 2.6980 (1.230 sec/step)\n","I0830 19:04:22.100274 139622356395904 learning.py:507] global step 8911: loss = 2.4040 (1.223 sec/step)\n","I0830 19:04:23.297015 139622356395904 learning.py:507] global step 8912: loss = 2.8124 (1.195 sec/step)\n","I0830 19:04:24.523889 139622356395904 learning.py:507] global step 8913: loss = 2.7413 (1.225 sec/step)\n","I0830 19:04:25.745687 139622356395904 learning.py:507] global step 8914: loss = 1.9779 (1.220 sec/step)\n","I0830 19:04:27.000749 139622356395904 learning.py:507] global step 8915: loss = 2.6975 (1.253 sec/step)\n","I0830 19:04:28.201439 139622356395904 learning.py:507] global step 8916: loss = 3.3318 (1.199 sec/step)\n","I0830 19:04:29.387438 139622356395904 learning.py:507] global step 8917: loss = 2.5792 (1.184 sec/step)\n","I0830 19:04:30.580890 139622356395904 learning.py:507] global step 8918: loss = 2.2585 (1.192 sec/step)\n","I0830 19:04:31.814486 139622356395904 learning.py:507] global step 8919: loss = 2.5333 (1.232 sec/step)\n","I0830 19:04:33.011516 139622356395904 learning.py:507] global step 8920: loss = 2.3729 (1.195 sec/step)\n","I0830 19:04:34.227926 139622356395904 learning.py:507] global step 8921: loss = 3.1931 (1.214 sec/step)\n","I0830 19:04:35.478302 139622356395904 learning.py:507] global step 8922: loss = 2.0333 (1.245 sec/step)\n","I0830 19:04:36.703610 139622356395904 learning.py:507] global step 8923: loss = 2.5286 (1.223 sec/step)\n","I0830 19:04:37.874950 139622356395904 learning.py:507] global step 8924: loss = 2.4821 (1.170 sec/step)\n","I0830 19:04:39.082321 139622356395904 learning.py:507] global step 8925: loss = 2.4920 (1.205 sec/step)\n","I0830 19:04:40.326481 139622356395904 learning.py:507] global step 8926: loss = 2.7571 (1.242 sec/step)\n","I0830 19:04:41.564595 139622356395904 learning.py:507] global step 8927: loss = 2.2252 (1.236 sec/step)\n","I0830 19:04:42.830192 139622356395904 learning.py:507] global step 8928: loss = 1.9298 (1.264 sec/step)\n","I0830 19:04:44.069281 139622356395904 learning.py:507] global step 8929: loss = 2.7388 (1.237 sec/step)\n","I0830 19:04:45.321351 139622356395904 learning.py:507] global step 8930: loss = 2.6411 (1.250 sec/step)\n","I0830 19:04:46.538714 139622356395904 learning.py:507] global step 8931: loss = 2.2258 (1.215 sec/step)\n","I0830 19:04:47.771071 139622356395904 learning.py:507] global step 8932: loss = 2.5858 (1.230 sec/step)\n","I0830 19:04:48.992719 139622356395904 learning.py:507] global step 8933: loss = 2.1731 (1.219 sec/step)\n","I0830 19:04:50.205508 139622356395904 learning.py:507] global step 8934: loss = 2.0801 (1.211 sec/step)\n","I0830 19:04:51.441116 139622356395904 learning.py:507] global step 8935: loss = 3.9719 (1.234 sec/step)\n","I0830 19:04:52.683324 139622356395904 learning.py:507] global step 8936: loss = 2.4345 (1.241 sec/step)\n","I0830 19:04:53.958299 139622356395904 learning.py:507] global step 8937: loss = 2.4285 (1.273 sec/step)\n","I0830 19:04:55.194885 139622356395904 learning.py:507] global step 8938: loss = 2.4912 (1.235 sec/step)\n","I0830 19:04:56.382478 139622356395904 learning.py:507] global step 8939: loss = 2.7130 (1.186 sec/step)\n","I0830 19:04:57.647535 139622356395904 learning.py:507] global step 8940: loss = 2.4037 (1.263 sec/step)\n","I0830 19:04:58.838027 139622356395904 learning.py:507] global step 8941: loss = 2.9375 (1.189 sec/step)\n","I0830 19:05:00.061527 139622356395904 learning.py:507] global step 8942: loss = 2.4570 (1.222 sec/step)\n","I0830 19:05:01.286476 139622356395904 learning.py:507] global step 8943: loss = 2.1732 (1.223 sec/step)\n","I0830 19:05:02.504717 139622356395904 learning.py:507] global step 8944: loss = 2.8993 (1.216 sec/step)\n","I0830 19:05:03.777531 139622356395904 learning.py:507] global step 8945: loss = 2.4081 (1.271 sec/step)\n","I0830 19:05:04.966732 139622356395904 learning.py:507] global step 8946: loss = 2.5476 (1.187 sec/step)\n","I0830 19:05:06.162185 139622356395904 learning.py:507] global step 8947: loss = 2.1992 (1.194 sec/step)\n","I0830 19:05:07.369010 139622356395904 learning.py:507] global step 8948: loss = 3.1195 (1.205 sec/step)\n","I0830 19:05:08.582025 139622356395904 learning.py:507] global step 8949: loss = 3.1734 (1.211 sec/step)\n","I0830 19:05:09.776586 139622356395904 learning.py:507] global step 8950: loss = 2.1583 (1.193 sec/step)\n","I0830 19:05:11.003959 139622356395904 learning.py:507] global step 8951: loss = 2.4934 (1.225 sec/step)\n","I0830 19:05:12.214177 139622356395904 learning.py:507] global step 8952: loss = 2.2578 (1.208 sec/step)\n","I0830 19:05:13.430760 139622356395904 learning.py:507] global step 8953: loss = 3.3450 (1.215 sec/step)\n","I0830 19:05:14.617800 139622356395904 learning.py:507] global step 8954: loss = 2.5196 (1.185 sec/step)\n","I0830 19:05:15.846195 139622356395904 learning.py:507] global step 8955: loss = 2.9659 (1.226 sec/step)\n","I0830 19:05:17.065094 139622356395904 learning.py:507] global step 8956: loss = 2.3955 (1.217 sec/step)\n","I0830 19:05:18.289570 139622356395904 learning.py:507] global step 8957: loss = 2.8045 (1.223 sec/step)\n","I0830 19:05:19.538456 139622356395904 learning.py:507] global step 8958: loss = 3.1820 (1.247 sec/step)\n","I0830 19:05:20.744401 139622356395904 learning.py:507] global step 8959: loss = 2.3477 (1.204 sec/step)\n","I0830 19:05:21.955778 139622356395904 learning.py:507] global step 8960: loss = 2.7693 (1.210 sec/step)\n","I0830 19:05:23.168352 139622356395904 learning.py:507] global step 8961: loss = 2.2498 (1.211 sec/step)\n","I0830 19:05:24.393635 139622356395904 learning.py:507] global step 8962: loss = 4.1326 (1.224 sec/step)\n","I0830 19:05:25.567797 139622356395904 learning.py:507] global step 8963: loss = 3.1370 (1.172 sec/step)\n","I0830 19:05:26.798712 139622356395904 learning.py:507] global step 8964: loss = 3.0564 (1.229 sec/step)\n","I0830 19:05:28.018868 139622356395904 learning.py:507] global step 8965: loss = 2.9277 (1.218 sec/step)\n","I0830 19:05:29.277209 139622356395904 learning.py:507] global step 8966: loss = 2.3211 (1.256 sec/step)\n","I0830 19:05:30.533011 139622356395904 learning.py:507] global step 8967: loss = 2.6367 (1.254 sec/step)\n","I0830 19:05:31.773484 139622356395904 learning.py:507] global step 8968: loss = 2.4764 (1.239 sec/step)\n","I0830 19:05:32.976596 139622356395904 learning.py:507] global step 8969: loss = 2.4361 (1.201 sec/step)\n","I0830 19:05:34.228013 139622356395904 learning.py:507] global step 8970: loss = 2.5088 (1.250 sec/step)\n","I0830 19:05:35.470713 139622356395904 learning.py:507] global step 8971: loss = 2.3331 (1.241 sec/step)\n","I0830 19:05:36.704653 139622356395904 learning.py:507] global step 8972: loss = 2.8560 (1.232 sec/step)\n","I0830 19:05:37.956146 139622356395904 learning.py:507] global step 8973: loss = 2.6418 (1.249 sec/step)\n","I0830 19:05:39.152892 139622356395904 learning.py:507] global step 8974: loss = 2.7435 (1.195 sec/step)\n","I0830 19:05:40.359024 139622356395904 learning.py:507] global step 8975: loss = 2.7501 (1.204 sec/step)\n","I0830 19:05:41.607272 139622356395904 learning.py:507] global step 8976: loss = 3.5873 (1.246 sec/step)\n","I0830 19:05:42.847483 139622356395904 learning.py:507] global step 8977: loss = 2.7248 (1.238 sec/step)\n","I0830 19:05:44.094390 139622356395904 learning.py:507] global step 8978: loss = 2.6478 (1.245 sec/step)\n","I0830 19:05:45.336891 139622356395904 learning.py:507] global step 8979: loss = 1.7969 (1.240 sec/step)\n","I0830 19:05:46.602207 139622356395904 learning.py:507] global step 8980: loss = 2.3665 (1.263 sec/step)\n","I0830 19:05:47.801204 139622356395904 learning.py:507] global step 8981: loss = 2.7704 (1.196 sec/step)\n","I0830 19:05:48.996792 139622356395904 learning.py:507] global step 8982: loss = 2.5307 (1.194 sec/step)\n","I0830 19:05:50.185751 139622356395904 learning.py:507] global step 8983: loss = 2.3954 (1.187 sec/step)\n","I0830 19:05:51.386400 139622356395904 learning.py:507] global step 8984: loss = 2.8185 (1.199 sec/step)\n","I0830 19:05:52.573614 139622356395904 learning.py:507] global step 8985: loss = 3.5903 (1.185 sec/step)\n","I0830 19:05:53.773718 139622356395904 learning.py:507] global step 8986: loss = 2.5736 (1.198 sec/step)\n","I0830 19:05:55.002259 139622356395904 learning.py:507] global step 8987: loss = 1.9700 (1.227 sec/step)\n","I0830 19:05:56.226680 139622356395904 learning.py:507] global step 8988: loss = 2.0344 (1.223 sec/step)\n","I0830 19:05:57.446653 139622356395904 learning.py:507] global step 8989: loss = 2.2821 (1.218 sec/step)\n","I0830 19:05:58.711353 139622356395904 learning.py:507] global step 8990: loss = 3.7833 (1.263 sec/step)\n","I0830 19:05:59.960244 139622356395904 learning.py:507] global step 8991: loss = 2.2258 (1.231 sec/step)\n","I0830 19:06:02.015120 139622356395904 learning.py:507] global step 8992: loss = 2.2745 (1.850 sec/step)\n","I0830 19:06:02.266490 139619299985152 supervisor.py:1050] Recording summary at step 8992.\n","I0830 19:06:03.251261 139622356395904 learning.py:507] global step 8993: loss = 2.2748 (1.234 sec/step)\n","I0830 19:06:04.500291 139622356395904 learning.py:507] global step 8994: loss = 2.9155 (1.247 sec/step)\n","I0830 19:06:05.726347 139622356395904 learning.py:507] global step 8995: loss = 2.9241 (1.224 sec/step)\n","I0830 19:06:06.958216 139622356395904 learning.py:507] global step 8996: loss = 2.7054 (1.230 sec/step)\n","I0830 19:06:08.185747 139622356395904 learning.py:507] global step 8997: loss = 2.7955 (1.226 sec/step)\n","I0830 19:06:09.429738 139622356395904 learning.py:507] global step 8998: loss = 1.9155 (1.242 sec/step)\n","I0830 19:06:10.643598 139622356395904 learning.py:507] global step 8999: loss = 2.6184 (1.212 sec/step)\n","I0830 19:06:11.875170 139622356395904 learning.py:507] global step 9000: loss = 2.5350 (1.230 sec/step)\n","I0830 19:06:13.089979 139622356395904 learning.py:507] global step 9001: loss = 4.5636 (1.213 sec/step)\n","I0830 19:06:14.314970 139622356395904 learning.py:507] global step 9002: loss = 1.9507 (1.223 sec/step)\n","I0830 19:06:15.523583 139622356395904 learning.py:507] global step 9003: loss = 3.2209 (1.207 sec/step)\n","I0830 19:06:16.720501 139622356395904 learning.py:507] global step 9004: loss = 2.4808 (1.195 sec/step)\n","I0830 19:06:17.954236 139622356395904 learning.py:507] global step 9005: loss = 2.3878 (1.232 sec/step)\n","I0830 19:06:19.195960 139622356395904 learning.py:507] global step 9006: loss = 1.8654 (1.240 sec/step)\n","I0830 19:06:20.412971 139622356395904 learning.py:507] global step 9007: loss = 3.0288 (1.215 sec/step)\n","I0830 19:06:21.648975 139622356395904 learning.py:507] global step 9008: loss = 1.6473 (1.234 sec/step)\n","I0830 19:06:22.888263 139622356395904 learning.py:507] global step 9009: loss = 2.3752 (1.237 sec/step)\n","I0830 19:06:24.081362 139622356395904 learning.py:507] global step 9010: loss = 2.3522 (1.191 sec/step)\n","I0830 19:06:25.330475 139622356395904 learning.py:507] global step 9011: loss = 2.5008 (1.247 sec/step)\n","I0830 19:06:26.577276 139622356395904 learning.py:507] global step 9012: loss = 2.6664 (1.245 sec/step)\n","I0830 19:06:27.786040 139622356395904 learning.py:507] global step 9013: loss = 2.5564 (1.207 sec/step)\n","I0830 19:06:29.035985 139622356395904 learning.py:507] global step 9014: loss = 2.3729 (1.248 sec/step)\n","I0830 19:06:30.235233 139622356395904 learning.py:507] global step 9015: loss = 3.4964 (1.197 sec/step)\n","I0830 19:06:31.482961 139622356395904 learning.py:507] global step 9016: loss = 2.8577 (1.246 sec/step)\n","I0830 19:06:32.696722 139622356395904 learning.py:507] global step 9017: loss = 2.9472 (1.212 sec/step)\n","I0830 19:06:33.918857 139622356395904 learning.py:507] global step 9018: loss = 2.9852 (1.220 sec/step)\n","I0830 19:06:35.167829 139622356395904 learning.py:507] global step 9019: loss = 2.3270 (1.247 sec/step)\n","I0830 19:06:36.381931 139622356395904 learning.py:507] global step 9020: loss = 2.7696 (1.212 sec/step)\n","I0830 19:06:37.633586 139622356395904 learning.py:507] global step 9021: loss = 2.1062 (1.250 sec/step)\n","I0830 19:06:38.853681 139622356395904 learning.py:507] global step 9022: loss = 2.5734 (1.218 sec/step)\n","I0830 19:06:40.113588 139622356395904 learning.py:507] global step 9023: loss = 2.6191 (1.258 sec/step)\n","I0830 19:06:41.335510 139622356395904 learning.py:507] global step 9024: loss = 2.0704 (1.220 sec/step)\n","I0830 19:06:42.545841 139622356395904 learning.py:507] global step 9025: loss = 2.8321 (1.208 sec/step)\n","I0830 19:06:43.735983 139622356395904 learning.py:507] global step 9026: loss = 2.7528 (1.188 sec/step)\n","I0830 19:06:44.955681 139622356395904 learning.py:507] global step 9027: loss = 4.0676 (1.218 sec/step)\n","I0830 19:06:46.158191 139622356395904 learning.py:507] global step 9028: loss = 2.1769 (1.201 sec/step)\n","I0830 19:06:47.435488 139622356395904 learning.py:507] global step 9029: loss = 2.8307 (1.275 sec/step)\n","I0830 19:06:48.655547 139622356395904 learning.py:507] global step 9030: loss = 2.5381 (1.218 sec/step)\n","I0830 19:06:49.882329 139622356395904 learning.py:507] global step 9031: loss = 2.6742 (1.225 sec/step)\n","I0830 19:06:51.108360 139622356395904 learning.py:507] global step 9032: loss = 2.4359 (1.224 sec/step)\n","I0830 19:06:52.345163 139622356395904 learning.py:507] global step 9033: loss = 1.7863 (1.235 sec/step)\n","I0830 19:06:53.528873 139622356395904 learning.py:507] global step 9034: loss = 3.0888 (1.182 sec/step)\n","I0830 19:06:54.715845 139622356395904 learning.py:507] global step 9035: loss = 2.5941 (1.185 sec/step)\n","I0830 19:06:55.895838 139622356395904 learning.py:507] global step 9036: loss = 4.0055 (1.178 sec/step)\n","I0830 19:06:57.099383 139622356395904 learning.py:507] global step 9037: loss = 2.5518 (1.202 sec/step)\n","I0830 19:06:58.336022 139622356395904 learning.py:507] global step 9038: loss = 1.9531 (1.235 sec/step)\n","I0830 19:06:59.553956 139622356395904 learning.py:507] global step 9039: loss = 2.7014 (1.216 sec/step)\n","I0830 19:07:00.823919 139622356395904 learning.py:507] global step 9040: loss = 3.4295 (1.268 sec/step)\n","I0830 19:07:02.069596 139622356395904 learning.py:507] global step 9041: loss = 2.3136 (1.244 sec/step)\n","I0830 19:07:03.278764 139622356395904 learning.py:507] global step 9042: loss = 2.2687 (1.207 sec/step)\n","I0830 19:07:04.490233 139622356395904 learning.py:507] global step 9043: loss = 2.1234 (1.209 sec/step)\n","I0830 19:07:05.770390 139622356395904 learning.py:507] global step 9044: loss = 2.3879 (1.278 sec/step)\n","I0830 19:07:06.998621 139622356395904 learning.py:507] global step 9045: loss = 2.6379 (1.226 sec/step)\n","I0830 19:07:08.184999 139622356395904 learning.py:507] global step 9046: loss = 2.7504 (1.185 sec/step)\n","I0830 19:07:09.393473 139622356395904 learning.py:507] global step 9047: loss = 2.9979 (1.207 sec/step)\n","I0830 19:07:10.607225 139622356395904 learning.py:507] global step 9048: loss = 2.4371 (1.212 sec/step)\n","I0830 19:07:11.861339 139622356395904 learning.py:507] global step 9049: loss = 2.2099 (1.252 sec/step)\n","I0830 19:07:13.083093 139622356395904 learning.py:507] global step 9050: loss = 2.6942 (1.220 sec/step)\n","I0830 19:07:14.278790 139622356395904 learning.py:507] global step 9051: loss = 2.3452 (1.193 sec/step)\n","I0830 19:07:15.541026 139622356395904 learning.py:507] global step 9052: loss = 2.0385 (1.260 sec/step)\n","I0830 19:07:16.789295 139622356395904 learning.py:507] global step 9053: loss = 3.1181 (1.246 sec/step)\n","I0830 19:07:18.023816 139622356395904 learning.py:507] global step 9054: loss = 2.2254 (1.232 sec/step)\n","I0830 19:07:19.268371 139622356395904 learning.py:507] global step 9055: loss = 2.8063 (1.242 sec/step)\n","I0830 19:07:20.483731 139622356395904 learning.py:507] global step 9056: loss = 2.1700 (1.213 sec/step)\n","I0830 19:07:21.731543 139622356395904 learning.py:507] global step 9057: loss = 2.4454 (1.246 sec/step)\n","I0830 19:07:22.955887 139622356395904 learning.py:507] global step 9058: loss = 2.2504 (1.222 sec/step)\n","I0830 19:07:24.159838 139622356395904 learning.py:507] global step 9059: loss = 2.6527 (1.202 sec/step)\n","I0830 19:07:25.416568 139622356395904 learning.py:507] global step 9060: loss = 2.7951 (1.255 sec/step)\n","I0830 19:07:26.621094 139622356395904 learning.py:507] global step 9061: loss = 1.9623 (1.203 sec/step)\n","I0830 19:07:27.825755 139622356395904 learning.py:507] global step 9062: loss = 2.5000 (1.203 sec/step)\n","I0830 19:07:29.072498 139622356395904 learning.py:507] global step 9063: loss = 2.7278 (1.245 sec/step)\n","I0830 19:07:30.318700 139622356395904 learning.py:507] global step 9064: loss = 2.6185 (1.244 sec/step)\n","I0830 19:07:31.519373 139622356395904 learning.py:507] global step 9065: loss = 2.3908 (1.199 sec/step)\n","I0830 19:07:32.755298 139622356395904 learning.py:507] global step 9066: loss = 2.9419 (1.234 sec/step)\n","I0830 19:07:33.995384 139622356395904 learning.py:507] global step 9067: loss = 3.0125 (1.238 sec/step)\n","I0830 19:07:35.222520 139622356395904 learning.py:507] global step 9068: loss = 2.0171 (1.225 sec/step)\n","I0830 19:07:36.463747 139622356395904 learning.py:507] global step 9069: loss = 2.9237 (1.239 sec/step)\n","I0830 19:07:37.698168 139622356395904 learning.py:507] global step 9070: loss = 2.0933 (1.232 sec/step)\n","I0830 19:07:38.929100 139622356395904 learning.py:507] global step 9071: loss = 2.1717 (1.229 sec/step)\n","I0830 19:07:40.144587 139622356395904 learning.py:507] global step 9072: loss = 2.5779 (1.214 sec/step)\n","I0830 19:07:41.374226 139622356395904 learning.py:507] global step 9073: loss = 2.5223 (1.228 sec/step)\n","I0830 19:07:42.600899 139622356395904 learning.py:507] global step 9074: loss = 2.4357 (1.225 sec/step)\n","I0830 19:07:43.817405 139622356395904 learning.py:507] global step 9075: loss = 2.2551 (1.215 sec/step)\n","I0830 19:07:45.100993 139622356395904 learning.py:507] global step 9076: loss = 3.6733 (1.282 sec/step)\n","I0830 19:07:46.337977 139622356395904 learning.py:507] global step 9077: loss = 2.0308 (1.235 sec/step)\n","I0830 19:07:47.573746 139622356395904 learning.py:507] global step 9078: loss = 2.3892 (1.234 sec/step)\n","I0830 19:07:48.768743 139622356395904 learning.py:507] global step 9079: loss = 2.1488 (1.193 sec/step)\n","I0830 19:07:50.012654 139622356395904 learning.py:507] global step 9080: loss = 2.3545 (1.242 sec/step)\n","I0830 19:07:51.255408 139622356395904 learning.py:507] global step 9081: loss = 2.8522 (1.241 sec/step)\n","I0830 19:07:52.496438 139622356395904 learning.py:507] global step 9082: loss = 2.4803 (1.239 sec/step)\n","I0830 19:07:53.749942 139622356395904 learning.py:507] global step 9083: loss = 2.4980 (1.252 sec/step)\n","I0830 19:07:54.970236 139622356395904 learning.py:507] global step 9084: loss = 2.5772 (1.219 sec/step)\n","I0830 19:07:56.197961 139622356395904 learning.py:507] global step 9085: loss = 2.3128 (1.226 sec/step)\n","I0830 19:07:57.423272 139622356395904 learning.py:507] global step 9086: loss = 2.6664 (1.223 sec/step)\n","I0830 19:07:58.679310 139622356395904 learning.py:507] global step 9087: loss = 3.2258 (1.254 sec/step)\n","I0830 19:07:59.932907 139622356395904 learning.py:507] global step 9088: loss = 4.0786 (1.252 sec/step)\n","I0830 19:08:02.061720 139619299985152 supervisor.py:1050] Recording summary at step 9089.\n","I0830 19:08:02.095133 139622356395904 learning.py:507] global step 9089: loss = 2.4369 (2.079 sec/step)\n","I0830 19:08:03.353027 139622356395904 learning.py:507] global step 9090: loss = 2.5164 (1.256 sec/step)\n","I0830 19:08:04.574645 139622356395904 learning.py:507] global step 9091: loss = 3.5809 (1.220 sec/step)\n","I0830 19:08:05.804643 139622356395904 learning.py:507] global step 9092: loss = 2.7148 (1.228 sec/step)\n","I0830 19:08:07.070099 139622356395904 learning.py:507] global step 9093: loss = 2.8035 (1.264 sec/step)\n","I0830 19:08:08.252881 139622356395904 learning.py:507] global step 9094: loss = 2.5862 (1.181 sec/step)\n","I0830 19:08:09.464381 139622356395904 learning.py:507] global step 9095: loss = 2.7358 (1.210 sec/step)\n","I0830 19:08:10.725200 139622356395904 learning.py:507] global step 9096: loss = 1.9697 (1.259 sec/step)\n","I0830 19:08:11.944395 139622356395904 learning.py:507] global step 9097: loss = 3.2883 (1.217 sec/step)\n","I0830 19:08:13.186724 139622356395904 learning.py:507] global step 9098: loss = 2.7828 (1.241 sec/step)\n","I0830 19:08:14.411926 139622356395904 learning.py:507] global step 9099: loss = 2.0428 (1.223 sec/step)\n","I0830 19:08:15.660998 139622356395904 learning.py:507] global step 9100: loss = 2.7993 (1.247 sec/step)\n","I0830 19:08:16.892279 139622356395904 learning.py:507] global step 9101: loss = 2.4330 (1.229 sec/step)\n","I0830 19:08:18.098590 139622356395904 learning.py:507] global step 9102: loss = 2.9355 (1.204 sec/step)\n","I0830 19:08:19.319125 139622356395904 learning.py:507] global step 9103: loss = 2.4889 (1.219 sec/step)\n","I0830 19:08:20.542410 139622356395904 learning.py:507] global step 9104: loss = 3.0720 (1.221 sec/step)\n","I0830 19:08:21.733195 139622356395904 learning.py:507] global step 9105: loss = 2.1421 (1.189 sec/step)\n","I0830 19:08:22.974382 139622356395904 learning.py:507] global step 9106: loss = 2.9282 (1.240 sec/step)\n","I0830 19:08:24.233809 139622356395904 learning.py:507] global step 9107: loss = 2.1693 (1.258 sec/step)\n","I0830 19:08:25.449377 139622356395904 learning.py:507] global step 9108: loss = 2.2987 (1.213 sec/step)\n","I0830 19:08:26.709628 139622356395904 learning.py:507] global step 9109: loss = 2.2322 (1.258 sec/step)\n","I0830 19:08:27.942612 139622356395904 learning.py:507] global step 9110: loss = 2.9029 (1.231 sec/step)\n","I0830 19:08:29.180486 139622356395904 learning.py:507] global step 9111: loss = 2.5230 (1.236 sec/step)\n","I0830 19:08:30.394347 139622356395904 learning.py:507] global step 9112: loss = 3.0330 (1.212 sec/step)\n","I0830 19:08:31.636310 139622356395904 learning.py:507] global step 9113: loss = 2.7265 (1.240 sec/step)\n","I0830 19:08:32.893956 139622356395904 learning.py:507] global step 9114: loss = 2.9509 (1.255 sec/step)\n","I0830 19:08:34.138032 139622356395904 learning.py:507] global step 9115: loss = 2.9756 (1.242 sec/step)\n","I0830 19:08:35.378396 139622356395904 learning.py:507] global step 9116: loss = 2.0888 (1.239 sec/step)\n","I0830 19:08:36.602126 139622356395904 learning.py:507] global step 9117: loss = 3.3212 (1.222 sec/step)\n","I0830 19:08:37.830958 139622356395904 learning.py:507] global step 9118: loss = 2.0352 (1.227 sec/step)\n","I0830 19:08:39.064776 139622356395904 learning.py:507] global step 9119: loss = 2.6660 (1.232 sec/step)\n","I0830 19:08:40.267066 139622356395904 learning.py:507] global step 9120: loss = 2.8477 (1.200 sec/step)\n","I0830 19:08:41.531235 139622356395904 learning.py:507] global step 9121: loss = 3.2682 (1.262 sec/step)\n","I0830 19:08:42.750926 139622356395904 learning.py:507] global step 9122: loss = 2.6112 (1.218 sec/step)\n","I0830 19:08:43.952636 139622356395904 learning.py:507] global step 9123: loss = 2.3690 (1.200 sec/step)\n","I0830 19:08:45.262572 139622356395904 learning.py:507] global step 9124: loss = 3.1228 (1.308 sec/step)\n","I0830 19:08:46.497419 139622356395904 learning.py:507] global step 9125: loss = 2.9255 (1.233 sec/step)\n","I0830 19:08:47.706308 139622356395904 learning.py:507] global step 9126: loss = 2.8850 (1.207 sec/step)\n","I0830 19:08:48.908537 139622356395904 learning.py:507] global step 9127: loss = 2.3751 (1.200 sec/step)\n","I0830 19:08:50.130122 139622356395904 learning.py:507] global step 9128: loss = 2.2464 (1.220 sec/step)\n","I0830 19:08:51.332916 139622356395904 learning.py:507] global step 9129: loss = 2.3537 (1.201 sec/step)\n","I0830 19:08:52.542281 139622356395904 learning.py:507] global step 9130: loss = 1.9598 (1.208 sec/step)\n","I0830 19:08:53.757655 139622356395904 learning.py:507] global step 9131: loss = 2.1781 (1.214 sec/step)\n","I0830 19:08:54.971587 139622356395904 learning.py:507] global step 9132: loss = 2.3354 (1.212 sec/step)\n","I0830 19:08:56.187809 139622356395904 learning.py:507] global step 9133: loss = 3.7160 (1.214 sec/step)\n","I0830 19:08:57.423431 139622356395904 learning.py:507] global step 9134: loss = 2.2239 (1.234 sec/step)\n","I0830 19:08:58.702794 139622356395904 learning.py:507] global step 9135: loss = 2.1247 (1.277 sec/step)\n","I0830 19:08:59.933162 139622356395904 learning.py:507] global step 9136: loss = 3.1619 (1.228 sec/step)\n","I0830 19:09:01.171864 139622356395904 learning.py:507] global step 9137: loss = 3.7996 (1.237 sec/step)\n","I0830 19:09:02.407175 139622356395904 learning.py:507] global step 9138: loss = 2.3107 (1.233 sec/step)\n","I0830 19:09:03.610372 139622356395904 learning.py:507] global step 9139: loss = 2.8180 (1.201 sec/step)\n","I0830 19:09:04.857762 139622356395904 learning.py:507] global step 9140: loss = 3.3368 (1.245 sec/step)\n","I0830 19:09:06.069133 139622356395904 learning.py:507] global step 9141: loss = 2.5542 (1.210 sec/step)\n","I0830 19:09:07.271269 139622356395904 learning.py:507] global step 9142: loss = 2.4257 (1.200 sec/step)\n","I0830 19:09:08.506926 139622356395904 learning.py:507] global step 9143: loss = 2.5972 (1.234 sec/step)\n","I0830 19:09:09.753032 139622356395904 learning.py:507] global step 9144: loss = 2.9166 (1.244 sec/step)\n","I0830 19:09:10.951762 139622356395904 learning.py:507] global step 9145: loss = 3.0194 (1.197 sec/step)\n","I0830 19:09:12.170031 139622356395904 learning.py:507] global step 9146: loss = 2.4646 (1.216 sec/step)\n","I0830 19:09:13.409152 139622356395904 learning.py:507] global step 9147: loss = 2.3002 (1.237 sec/step)\n","I0830 19:09:14.654865 139622356395904 learning.py:507] global step 9148: loss = 2.0916 (1.244 sec/step)\n","I0830 19:09:15.857874 139622356395904 learning.py:507] global step 9149: loss = 3.4092 (1.201 sec/step)\n","I0830 19:09:17.080433 139622356395904 learning.py:507] global step 9150: loss = 2.3950 (1.221 sec/step)\n","I0830 19:09:18.320489 139622356395904 learning.py:507] global step 9151: loss = 2.4360 (1.238 sec/step)\n","I0830 19:09:19.527925 139622356395904 learning.py:507] global step 9152: loss = 2.8418 (1.206 sec/step)\n","I0830 19:09:20.769300 139622356395904 learning.py:507] global step 9153: loss = 2.3567 (1.240 sec/step)\n","I0830 19:09:21.970600 139622356395904 learning.py:507] global step 9154: loss = 2.4313 (1.199 sec/step)\n","I0830 19:09:23.197966 139622356395904 learning.py:507] global step 9155: loss = 2.0829 (1.225 sec/step)\n","I0830 19:09:24.424456 139622356395904 learning.py:507] global step 9156: loss = 4.4234 (1.225 sec/step)\n","I0830 19:09:25.650829 139622356395904 learning.py:507] global step 9157: loss = 2.2724 (1.224 sec/step)\n","I0830 19:09:26.893534 139622356395904 learning.py:507] global step 9158: loss = 2.3410 (1.241 sec/step)\n","I0830 19:09:28.108014 139622356395904 learning.py:507] global step 9159: loss = 2.4414 (1.212 sec/step)\n","I0830 19:09:29.346434 139622356395904 learning.py:507] global step 9160: loss = 3.5103 (1.236 sec/step)\n","I0830 19:09:30.512791 139622356395904 learning.py:507] global step 9161: loss = 2.3723 (1.164 sec/step)\n","I0830 19:09:31.766992 139622356395904 learning.py:507] global step 9162: loss = 2.1664 (1.252 sec/step)\n","I0830 19:09:32.978430 139622356395904 learning.py:507] global step 9163: loss = 3.3833 (1.209 sec/step)\n","I0830 19:09:34.233127 139622356395904 learning.py:507] global step 9164: loss = 2.4909 (1.253 sec/step)\n","I0830 19:09:35.441889 139622356395904 learning.py:507] global step 9165: loss = 2.1302 (1.207 sec/step)\n","I0830 19:09:36.653439 139622356395904 learning.py:507] global step 9166: loss = 3.2950 (1.210 sec/step)\n","I0830 19:09:37.891674 139622356395904 learning.py:507] global step 9167: loss = 2.4399 (1.236 sec/step)\n","I0830 19:09:39.102942 139622356395904 learning.py:507] global step 9168: loss = 2.5358 (1.209 sec/step)\n","I0830 19:09:40.308244 139622356395904 learning.py:507] global step 9169: loss = 2.6918 (1.203 sec/step)\n","I0830 19:09:41.525905 139622356395904 learning.py:507] global step 9170: loss = 2.6204 (1.216 sec/step)\n","I0830 19:09:42.755169 139622356395904 learning.py:507] global step 9171: loss = 2.1135 (1.227 sec/step)\n","I0830 19:09:43.999083 139622356395904 learning.py:507] global step 9172: loss = 2.7512 (1.242 sec/step)\n","I0830 19:09:45.265403 139622356395904 learning.py:507] global step 9173: loss = 2.0762 (1.264 sec/step)\n","I0830 19:09:46.484308 139622356395904 learning.py:507] global step 9174: loss = 2.4099 (1.217 sec/step)\n","I0830 19:09:47.737480 139622356395904 learning.py:507] global step 9175: loss = 2.5672 (1.251 sec/step)\n","I0830 19:09:48.970909 139622356395904 learning.py:507] global step 9176: loss = 2.9664 (1.231 sec/step)\n","I0830 19:09:50.181732 139622356395904 learning.py:507] global step 9177: loss = 2.3108 (1.209 sec/step)\n","I0830 19:09:51.371141 139622356395904 learning.py:507] global step 9178: loss = 2.6202 (1.188 sec/step)\n","I0830 19:09:52.575531 139622356395904 learning.py:507] global step 9179: loss = 1.9275 (1.203 sec/step)\n","I0830 19:09:53.767734 139622356395904 learning.py:507] global step 9180: loss = 2.0565 (1.190 sec/step)\n","I0830 19:09:55.010218 139622356395904 learning.py:507] global step 9181: loss = 2.4417 (1.241 sec/step)\n","I0830 19:09:56.261187 139622356395904 learning.py:507] global step 9182: loss = 2.4450 (1.248 sec/step)\n","I0830 19:09:57.465594 139622356395904 learning.py:507] global step 9183: loss = 3.0006 (1.200 sec/step)\n","I0830 19:09:58.685172 139622356395904 learning.py:507] global step 9184: loss = 1.9972 (1.218 sec/step)\n","I0830 19:09:59.646486 139619316770560 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n","I0830 19:10:00.235596 139622356395904 learning.py:507] global step 9185: loss = 2.4043 (1.317 sec/step)\n","I0830 19:10:03.505953 139619299985152 supervisor.py:1050] Recording summary at step 9186.\n","I0830 19:10:03.517912 139622356395904 learning.py:507] global step 9186: loss = 3.2103 (3.210 sec/step)\n","I0830 19:10:04.914637 139622356395904 learning.py:507] global step 9187: loss = 2.0933 (1.392 sec/step)\n","I0830 19:10:06.165487 139622356395904 learning.py:507] global step 9188: loss = 2.8556 (1.249 sec/step)\n","I0830 19:10:07.394548 139622356395904 learning.py:507] global step 9189: loss = 2.7048 (1.227 sec/step)\n","I0830 19:10:08.577460 139622356395904 learning.py:507] global step 9190: loss = 2.6411 (1.181 sec/step)\n","I0830 19:10:09.792343 139622356395904 learning.py:507] global step 9191: loss = 2.4196 (1.213 sec/step)\n","I0830 19:10:11.030509 139622356395904 learning.py:507] global step 9192: loss = 3.2619 (1.236 sec/step)\n","I0830 19:10:12.232538 139622356395904 learning.py:507] global step 9193: loss = 2.9740 (1.200 sec/step)\n","I0830 19:10:13.484375 139622356395904 learning.py:507] global step 9194: loss = 2.0824 (1.250 sec/step)\n","I0830 19:10:14.680148 139622356395904 learning.py:507] global step 9195: loss = 2.1502 (1.194 sec/step)\n","I0830 19:10:15.871918 139622356395904 learning.py:507] global step 9196: loss = 3.3289 (1.190 sec/step)\n","I0830 19:10:17.103259 139622356395904 learning.py:507] global step 9197: loss = 2.4260 (1.229 sec/step)\n","I0830 19:10:18.325724 139622356395904 learning.py:507] global step 9198: loss = 2.1451 (1.221 sec/step)\n","I0830 19:10:19.546351 139622356395904 learning.py:507] global step 9199: loss = 2.1017 (1.219 sec/step)\n","I0830 19:10:20.767823 139622356395904 learning.py:507] global step 9200: loss = 2.5295 (1.220 sec/step)\n","I0830 19:10:21.992995 139622356395904 learning.py:507] global step 9201: loss = 2.2785 (1.223 sec/step)\n","I0830 19:10:23.196583 139622356395904 learning.py:507] global step 9202: loss = 1.8063 (1.202 sec/step)\n","I0830 19:10:24.383390 139622356395904 learning.py:507] global step 9203: loss = 2.2414 (1.185 sec/step)\n","I0830 19:10:25.561785 139622356395904 learning.py:507] global step 9204: loss = 2.4511 (1.177 sec/step)\n","I0830 19:10:26.750880 139622356395904 learning.py:507] global step 9205: loss = 2.1049 (1.187 sec/step)\n","I0830 19:10:27.950154 139622356395904 learning.py:507] global step 9206: loss = 2.5510 (1.197 sec/step)\n","I0830 19:10:29.160843 139622356395904 learning.py:507] global step 9207: loss = 2.3418 (1.209 sec/step)\n","I0830 19:10:30.329570 139622356395904 learning.py:507] global step 9208: loss = 2.1675 (1.167 sec/step)\n","I0830 19:10:31.550179 139622356395904 learning.py:507] global step 9209: loss = 2.7182 (1.218 sec/step)\n","I0830 19:10:32.751535 139622356395904 learning.py:507] global step 9210: loss = 2.1896 (1.199 sec/step)\n","I0830 19:10:33.938271 139622356395904 learning.py:507] global step 9211: loss = 2.7951 (1.185 sec/step)\n","I0830 19:10:35.142800 139622356395904 learning.py:507] global step 9212: loss = 2.5284 (1.203 sec/step)\n","I0830 19:10:36.357141 139622356395904 learning.py:507] global step 9213: loss = 2.6144 (1.212 sec/step)\n","I0830 19:10:37.542255 139622356395904 learning.py:507] global step 9214: loss = 2.1077 (1.183 sec/step)\n","I0830 19:10:38.756681 139622356395904 learning.py:507] global step 9215: loss = 2.5037 (1.213 sec/step)\n","I0830 19:10:39.961000 139622356395904 learning.py:507] global step 9216: loss = 2.5637 (1.203 sec/step)\n","I0830 19:10:41.169632 139622356395904 learning.py:507] global step 9217: loss = 2.0732 (1.207 sec/step)\n","I0830 19:10:42.363368 139622356395904 learning.py:507] global step 9218: loss = 2.3479 (1.192 sec/step)\n","I0830 19:10:43.575231 139622356395904 learning.py:507] global step 9219: loss = 2.1176 (1.210 sec/step)\n","I0830 19:10:44.788481 139622356395904 learning.py:507] global step 9220: loss = 2.3004 (1.211 sec/step)\n","I0830 19:10:46.004021 139622356395904 learning.py:507] global step 9221: loss = 3.2425 (1.214 sec/step)\n","I0830 19:10:47.217767 139622356395904 learning.py:507] global step 9222: loss = 2.3277 (1.212 sec/step)\n","I0830 19:10:48.437571 139622356395904 learning.py:507] global step 9223: loss = 3.3716 (1.218 sec/step)\n","I0830 19:10:49.702596 139622356395904 learning.py:507] global step 9224: loss = 2.8440 (1.263 sec/step)\n","I0830 19:10:50.932404 139622356395904 learning.py:507] global step 9225: loss = 2.3134 (1.227 sec/step)\n","I0830 19:10:52.173147 139622356395904 learning.py:507] global step 9226: loss = 2.5084 (1.238 sec/step)\n","I0830 19:10:53.410792 139622356395904 learning.py:507] global step 9227: loss = 2.3458 (1.236 sec/step)\n","I0830 19:10:54.663944 139622356395904 learning.py:507] global step 9228: loss = 2.9352 (1.251 sec/step)\n","I0830 19:10:55.894877 139622356395904 learning.py:507] global step 9229: loss = 2.0075 (1.229 sec/step)\n","I0830 19:10:57.153647 139622356395904 learning.py:507] global step 9230: loss = 3.5416 (1.256 sec/step)\n","I0830 19:10:58.411680 139622356395904 learning.py:507] global step 9231: loss = 3.0218 (1.256 sec/step)\n","I0830 19:10:59.614419 139622356395904 learning.py:507] global step 9232: loss = 3.3326 (1.201 sec/step)\n","I0830 19:11:00.840007 139622356395904 learning.py:507] global step 9233: loss = 2.0942 (1.223 sec/step)\n","I0830 19:11:02.047987 139622356395904 learning.py:507] global step 9234: loss = 2.9687 (1.206 sec/step)\n","I0830 19:11:03.237985 139622356395904 learning.py:507] global step 9235: loss = 2.9635 (1.188 sec/step)\n","I0830 19:11:04.462620 139622356395904 learning.py:507] global step 9236: loss = 2.3602 (1.223 sec/step)\n","I0830 19:11:05.679346 139622356395904 learning.py:507] global step 9237: loss = 2.6634 (1.215 sec/step)\n","I0830 19:11:06.894515 139622356395904 learning.py:507] global step 9238: loss = 2.2816 (1.213 sec/step)\n","I0830 19:11:08.096938 139622356395904 learning.py:507] global step 9239: loss = 2.3897 (1.200 sec/step)\n","I0830 19:11:09.316159 139622356395904 learning.py:507] global step 9240: loss = 2.8445 (1.217 sec/step)\n","I0830 19:11:10.551384 139622356395904 learning.py:507] global step 9241: loss = 2.0652 (1.233 sec/step)\n","I0830 19:11:11.778265 139622356395904 learning.py:507] global step 9242: loss = 2.4844 (1.225 sec/step)\n","I0830 19:11:12.996554 139622356395904 learning.py:507] global step 9243: loss = 3.0228 (1.217 sec/step)\n","I0830 19:11:14.206701 139622356395904 learning.py:507] global step 9244: loss = 2.2833 (1.208 sec/step)\n","I0830 19:11:15.451796 139622356395904 learning.py:507] global step 9245: loss = 3.0994 (1.243 sec/step)\n","I0830 19:11:16.684937 139622356395904 learning.py:507] global step 9246: loss = 3.7006 (1.228 sec/step)\n","I0830 19:11:17.944318 139622356395904 learning.py:507] global step 9247: loss = 2.6427 (1.257 sec/step)\n","I0830 19:11:19.182390 139622356395904 learning.py:507] global step 9248: loss = 2.9582 (1.236 sec/step)\n","I0830 19:11:20.424473 139622356395904 learning.py:507] global step 9249: loss = 3.0591 (1.240 sec/step)\n","I0830 19:11:21.650010 139622356395904 learning.py:507] global step 9250: loss = 3.3958 (1.224 sec/step)\n","I0830 19:11:22.865518 139622356395904 learning.py:507] global step 9251: loss = 2.7220 (1.212 sec/step)\n","I0830 19:11:24.081762 139622356395904 learning.py:507] global step 9252: loss = 2.4914 (1.214 sec/step)\n","I0830 19:11:25.284268 139622356395904 learning.py:507] global step 9253: loss = 1.7584 (1.201 sec/step)\n","I0830 19:11:26.530240 139622356395904 learning.py:507] global step 9254: loss = 2.4132 (1.243 sec/step)\n","I0830 19:11:27.764639 139622356395904 learning.py:507] global step 9255: loss = 1.8727 (1.233 sec/step)\n","I0830 19:11:28.971447 139622356395904 learning.py:507] global step 9256: loss = 2.6462 (1.205 sec/step)\n","I0830 19:11:30.221289 139622356395904 learning.py:507] global step 9257: loss = 2.4236 (1.248 sec/step)\n","I0830 19:11:31.437738 139622356395904 learning.py:507] global step 9258: loss = 1.9766 (1.215 sec/step)\n","I0830 19:11:32.683894 139622356395904 learning.py:507] global step 9259: loss = 3.1542 (1.244 sec/step)\n","I0830 19:11:33.898717 139622356395904 learning.py:507] global step 9260: loss = 3.2154 (1.213 sec/step)\n","I0830 19:11:35.161737 139622356395904 learning.py:507] global step 9261: loss = 2.1273 (1.261 sec/step)\n","I0830 19:11:36.359013 139622356395904 learning.py:507] global step 9262: loss = 2.6069 (1.195 sec/step)\n","I0830 19:11:37.570988 139622356395904 learning.py:507] global step 9263: loss = 2.4665 (1.210 sec/step)\n","I0830 19:11:38.808487 139622356395904 learning.py:507] global step 9264: loss = 2.4266 (1.235 sec/step)\n","I0830 19:11:40.027562 139622356395904 learning.py:507] global step 9265: loss = 3.1461 (1.217 sec/step)\n","I0830 19:11:41.257789 139622356395904 learning.py:507] global step 9266: loss = 3.1875 (1.228 sec/step)\n","I0830 19:11:42.507768 139622356395904 learning.py:507] global step 9267: loss = 2.0943 (1.248 sec/step)\n","I0830 19:11:43.741990 139622356395904 learning.py:507] global step 9268: loss = 2.7055 (1.232 sec/step)\n","I0830 19:11:44.970416 139622356395904 learning.py:507] global step 9269: loss = 2.0627 (1.227 sec/step)\n","I0830 19:11:46.220762 139622356395904 learning.py:507] global step 9270: loss = 2.6789 (1.248 sec/step)\n","I0830 19:11:47.481385 139622356395904 learning.py:507] global step 9271: loss = 2.5023 (1.258 sec/step)\n","I0830 19:11:48.679903 139622356395904 learning.py:507] global step 9272: loss = 2.4989 (1.197 sec/step)\n","I0830 19:11:49.926223 139622356395904 learning.py:507] global step 9273: loss = 2.6595 (1.244 sec/step)\n","I0830 19:11:51.170495 139622356395904 learning.py:507] global step 9274: loss = 2.2380 (1.243 sec/step)\n","I0830 19:11:52.421206 139622356395904 learning.py:507] global step 9275: loss = 3.8919 (1.249 sec/step)\n","I0830 19:11:53.660322 139622356395904 learning.py:507] global step 9276: loss = 3.1494 (1.237 sec/step)\n","I0830 19:11:54.911549 139622356395904 learning.py:507] global step 9277: loss = 2.3245 (1.249 sec/step)\n","I0830 19:11:56.110696 139622356395904 learning.py:507] global step 9278: loss = 3.8690 (1.197 sec/step)\n","I0830 19:11:57.340754 139622356395904 learning.py:507] global step 9279: loss = 1.7891 (1.228 sec/step)\n","I0830 19:11:58.567186 139622356395904 learning.py:507] global step 9280: loss = 1.9316 (1.224 sec/step)\n","I0830 19:11:59.814991 139622356395904 learning.py:507] global step 9281: loss = 2.7629 (1.246 sec/step)\n","I0830 19:12:02.094345 139619299985152 supervisor.py:1050] Recording summary at step 9282.\n","I0830 19:12:02.128211 139622356395904 learning.py:507] global step 9282: loss = 3.1939 (2.238 sec/step)\n","I0830 19:12:03.330610 139622356395904 learning.py:507] global step 9283: loss = 2.8681 (1.200 sec/step)\n","I0830 19:12:04.516752 139622356395904 learning.py:507] global step 9284: loss = 2.6526 (1.184 sec/step)\n","I0830 19:12:05.768892 139622356395904 learning.py:507] global step 9285: loss = 2.5216 (1.250 sec/step)\n","I0830 19:12:07.015347 139622356395904 learning.py:507] global step 9286: loss = 2.6852 (1.245 sec/step)\n","I0830 19:12:08.226332 139622356395904 learning.py:507] global step 9287: loss = 2.5478 (1.209 sec/step)\n","I0830 19:12:09.457776 139622356395904 learning.py:507] global step 9288: loss = 3.2728 (1.229 sec/step)\n","I0830 19:12:10.667886 139622356395904 learning.py:507] global step 9289: loss = 2.4083 (1.208 sec/step)\n","I0830 19:12:11.884941 139622356395904 learning.py:507] global step 9290: loss = 3.5687 (1.215 sec/step)\n","I0830 19:12:13.077151 139622356395904 learning.py:507] global step 9291: loss = 2.3045 (1.190 sec/step)\n","I0830 19:12:14.299297 139622356395904 learning.py:507] global step 9292: loss = 2.3291 (1.220 sec/step)\n","I0830 19:12:15.529875 139622356395904 learning.py:507] global step 9293: loss = 2.5992 (1.228 sec/step)\n","I0830 19:12:16.753555 139622356395904 learning.py:507] global step 9294: loss = 3.2075 (1.222 sec/step)\n","I0830 19:12:17.977688 139622356395904 learning.py:507] global step 9295: loss = 3.2848 (1.222 sec/step)\n","I0830 19:12:19.191536 139622356395904 learning.py:507] global step 9296: loss = 2.2383 (1.212 sec/step)\n","I0830 19:12:20.386926 139622356395904 learning.py:507] global step 9297: loss = 2.1060 (1.193 sec/step)\n","I0830 19:12:21.580883 139622356395904 learning.py:507] global step 9298: loss = 2.6812 (1.192 sec/step)\n","I0830 19:12:22.782550 139622356395904 learning.py:507] global step 9299: loss = 2.9838 (1.200 sec/step)\n","I0830 19:12:23.991207 139622356395904 learning.py:507] global step 9300: loss = 2.4451 (1.207 sec/step)\n","I0830 19:12:25.242366 139622356395904 learning.py:507] global step 9301: loss = 2.8665 (1.249 sec/step)\n","I0830 19:12:26.439965 139622356395904 learning.py:507] global step 9302: loss = 2.4068 (1.196 sec/step)\n","I0830 19:12:27.648548 139622356395904 learning.py:507] global step 9303: loss = 3.4273 (1.206 sec/step)\n","I0830 19:12:28.853625 139622356395904 learning.py:507] global step 9304: loss = 2.6497 (1.203 sec/step)\n","I0830 19:12:30.041199 139622356395904 learning.py:507] global step 9305: loss = 2.4503 (1.186 sec/step)\n","I0830 19:12:31.255930 139622356395904 learning.py:507] global step 9306: loss = 2.7157 (1.213 sec/step)\n","I0830 19:12:32.464844 139622356395904 learning.py:507] global step 9307: loss = 2.4788 (1.207 sec/step)\n","I0830 19:12:33.675586 139622356395904 learning.py:507] global step 9308: loss = 4.3459 (1.208 sec/step)\n","I0830 19:12:34.889964 139622356395904 learning.py:507] global step 9309: loss = 1.9229 (1.212 sec/step)\n","I0830 19:12:36.111044 139622356395904 learning.py:507] global step 9310: loss = 3.1627 (1.219 sec/step)\n","I0830 19:12:37.338546 139622356395904 learning.py:507] global step 9311: loss = 2.6622 (1.226 sec/step)\n","I0830 19:12:38.579803 139622356395904 learning.py:507] global step 9312: loss = 2.4492 (1.239 sec/step)\n","I0830 19:12:39.810892 139622356395904 learning.py:507] global step 9313: loss = 2.0272 (1.229 sec/step)\n","I0830 19:12:41.014500 139622356395904 learning.py:507] global step 9314: loss = 2.2387 (1.202 sec/step)\n","I0830 19:12:42.228619 139622356395904 learning.py:507] global step 9315: loss = 2.6173 (1.212 sec/step)\n","I0830 19:12:43.453243 139622356395904 learning.py:507] global step 9316: loss = 2.0822 (1.223 sec/step)\n","I0830 19:12:44.673795 139622356395904 learning.py:507] global step 9317: loss = 3.1243 (1.219 sec/step)\n","I0830 19:12:45.895315 139622356395904 learning.py:507] global step 9318: loss = 2.6198 (1.219 sec/step)\n","I0830 19:12:47.130932 139622356395904 learning.py:507] global step 9319: loss = 2.5397 (1.234 sec/step)\n","I0830 19:12:48.395790 139622356395904 learning.py:507] global step 9320: loss = 2.0826 (1.263 sec/step)\n","I0830 19:12:49.592476 139622356395904 learning.py:507] global step 9321: loss = 3.3164 (1.194 sec/step)\n","I0830 19:12:50.815364 139622356395904 learning.py:507] global step 9322: loss = 3.0357 (1.221 sec/step)\n","I0830 19:12:52.033768 139622356395904 learning.py:507] global step 9323: loss = 2.3680 (1.217 sec/step)\n","I0830 19:12:53.266737 139622356395904 learning.py:507] global step 9324: loss = 1.9866 (1.231 sec/step)\n","I0830 19:12:54.475888 139622356395904 learning.py:507] global step 9325: loss = 2.1990 (1.207 sec/step)\n","I0830 19:12:55.691188 139622356395904 learning.py:507] global step 9326: loss = 2.4174 (1.212 sec/step)\n","I0830 19:12:56.921476 139622356395904 learning.py:507] global step 9327: loss = 2.1244 (1.228 sec/step)\n","I0830 19:12:58.134899 139622356395904 learning.py:507] global step 9328: loss = 2.7298 (1.212 sec/step)\n","I0830 19:12:59.371267 139622356395904 learning.py:507] global step 9329: loss = 2.3524 (1.234 sec/step)\n","I0830 19:13:00.576732 139622356395904 learning.py:507] global step 9330: loss = 3.4177 (1.203 sec/step)\n","I0830 19:13:01.783770 139622356395904 learning.py:507] global step 9331: loss = 2.3816 (1.205 sec/step)\n","I0830 19:13:03.044584 139622356395904 learning.py:507] global step 9332: loss = 2.4608 (1.259 sec/step)\n","I0830 19:13:04.254275 139622356395904 learning.py:507] global step 9333: loss = 3.9464 (1.208 sec/step)\n","I0830 19:13:05.467713 139622356395904 learning.py:507] global step 9334: loss = 2.0687 (1.212 sec/step)\n","I0830 19:13:06.690166 139622356395904 learning.py:507] global step 9335: loss = 2.5880 (1.221 sec/step)\n","I0830 19:13:07.911752 139622356395904 learning.py:507] global step 9336: loss = 3.2099 (1.219 sec/step)\n","I0830 19:13:09.123413 139622356395904 learning.py:507] global step 9337: loss = 2.2280 (1.209 sec/step)\n","I0830 19:13:10.352159 139622356395904 learning.py:507] global step 9338: loss = 3.1914 (1.227 sec/step)\n","I0830 19:13:11.589466 139622356395904 learning.py:507] global step 9339: loss = 2.3511 (1.235 sec/step)\n","I0830 19:13:12.844890 139622356395904 learning.py:507] global step 9340: loss = 3.4707 (1.254 sec/step)\n","I0830 19:13:14.083584 139622356395904 learning.py:507] global step 9341: loss = 3.3628 (1.236 sec/step)\n","I0830 19:13:15.234594 139622356395904 learning.py:507] global step 9342: loss = 3.7385 (1.149 sec/step)\n","I0830 19:13:16.460486 139622356395904 learning.py:507] global step 9343: loss = 3.0179 (1.224 sec/step)\n","I0830 19:13:17.652719 139622356395904 learning.py:507] global step 9344: loss = 2.3801 (1.190 sec/step)\n","I0830 19:13:18.870411 139622356395904 learning.py:507] global step 9345: loss = 2.7986 (1.216 sec/step)\n","I0830 19:13:20.116040 139622356395904 learning.py:507] global step 9346: loss = 3.4315 (1.244 sec/step)\n","I0830 19:13:21.340930 139622356395904 learning.py:507] global step 9347: loss = 2.1282 (1.223 sec/step)\n","I0830 19:13:22.577820 139622356395904 learning.py:507] global step 9348: loss = 3.0748 (1.235 sec/step)\n","I0830 19:13:23.783408 139622356395904 learning.py:507] global step 9349: loss = 2.9500 (1.204 sec/step)\n","I0830 19:13:24.998589 139622356395904 learning.py:507] global step 9350: loss = 4.9206 (1.213 sec/step)\n","I0830 19:13:26.221249 139622356395904 learning.py:507] global step 9351: loss = 2.7353 (1.221 sec/step)\n","I0830 19:13:27.465981 139622356395904 learning.py:507] global step 9352: loss = 2.5604 (1.243 sec/step)\n","I0830 19:13:28.691582 139622356395904 learning.py:507] global step 9353: loss = 2.6645 (1.223 sec/step)\n","I0830 19:13:29.916817 139622356395904 learning.py:507] global step 9354: loss = 2.3263 (1.223 sec/step)\n","I0830 19:13:31.147920 139622356395904 learning.py:507] global step 9355: loss = 2.2560 (1.229 sec/step)\n","I0830 19:13:32.404139 139622356395904 learning.py:507] global step 9356: loss = 2.4468 (1.254 sec/step)\n","I0830 19:13:33.639219 139622356395904 learning.py:507] global step 9357: loss = 3.0899 (1.233 sec/step)\n","I0830 19:13:34.846323 139622356395904 learning.py:507] global step 9358: loss = 2.6932 (1.205 sec/step)\n","I0830 19:13:36.121868 139622356395904 learning.py:507] global step 9359: loss = 2.3570 (1.273 sec/step)\n","I0830 19:13:37.345862 139622356395904 learning.py:507] global step 9360: loss = 2.1074 (1.222 sec/step)\n","I0830 19:13:38.519759 139622356395904 learning.py:507] global step 9361: loss = 3.5347 (1.172 sec/step)\n","I0830 19:13:39.731020 139622356395904 learning.py:507] global step 9362: loss = 2.1229 (1.209 sec/step)\n","I0830 19:13:40.961817 139622356395904 learning.py:507] global step 9363: loss = 2.7211 (1.229 sec/step)\n","I0830 19:13:42.158295 139622356395904 learning.py:507] global step 9364: loss = 2.1156 (1.195 sec/step)\n","I0830 19:13:43.341436 139622356395904 learning.py:507] global step 9365: loss = 2.6789 (1.181 sec/step)\n","I0830 19:13:44.525283 139622356395904 learning.py:507] global step 9366: loss = 3.8171 (1.182 sec/step)\n","I0830 19:13:45.745705 139622356395904 learning.py:507] global step 9367: loss = 3.1720 (1.219 sec/step)\n","I0830 19:13:46.950886 139622356395904 learning.py:507] global step 9368: loss = 2.5565 (1.203 sec/step)\n","I0830 19:13:48.166910 139622356395904 learning.py:507] global step 9369: loss = 3.4903 (1.214 sec/step)\n","I0830 19:13:49.375401 139622356395904 learning.py:507] global step 9370: loss = 2.6576 (1.207 sec/step)\n","I0830 19:13:50.574004 139622356395904 learning.py:507] global step 9371: loss = 2.4655 (1.197 sec/step)\n","I0830 19:13:51.760087 139622356395904 learning.py:507] global step 9372: loss = 2.3860 (1.184 sec/step)\n","I0830 19:13:52.978113 139622356395904 learning.py:507] global step 9373: loss = 3.0588 (1.216 sec/step)\n","I0830 19:13:54.171771 139622356395904 learning.py:507] global step 9374: loss = 2.4439 (1.192 sec/step)\n","I0830 19:13:55.392328 139622356395904 learning.py:507] global step 9375: loss = 2.3863 (1.219 sec/step)\n","I0830 19:13:56.600688 139622356395904 learning.py:507] global step 9376: loss = 3.4499 (1.207 sec/step)\n","I0830 19:13:57.823968 139622356395904 learning.py:507] global step 9377: loss = 2.2175 (1.222 sec/step)\n","I0830 19:13:59.032226 139622356395904 learning.py:507] global step 9378: loss = 1.9729 (1.206 sec/step)\n","I0830 19:14:00.271568 139622356395904 learning.py:507] global step 9379: loss = 2.1370 (1.228 sec/step)\n","I0830 19:14:02.161796 139619299985152 supervisor.py:1050] Recording summary at step 9380.\n","I0830 19:14:02.191283 139622356395904 learning.py:507] global step 9380: loss = 3.5554 (1.913 sec/step)\n","I0830 19:14:03.413673 139622356395904 learning.py:507] global step 9381: loss = 2.5427 (1.220 sec/step)\n","I0830 19:14:04.612357 139622356395904 learning.py:507] global step 9382: loss = 2.8030 (1.197 sec/step)\n","I0830 19:14:05.855197 139622356395904 learning.py:507] global step 9383: loss = 2.2047 (1.241 sec/step)\n","I0830 19:14:07.058754 139622356395904 learning.py:507] global step 9384: loss = 2.0124 (1.202 sec/step)\n","I0830 19:14:08.296041 139622356395904 learning.py:507] global step 9385: loss = 2.6675 (1.235 sec/step)\n","I0830 19:14:09.522795 139622356395904 learning.py:507] global step 9386: loss = 2.8663 (1.224 sec/step)\n","I0830 19:14:10.733041 139622356395904 learning.py:507] global step 9387: loss = 3.4181 (1.209 sec/step)\n","I0830 19:14:11.956164 139622356395904 learning.py:507] global step 9388: loss = 1.8464 (1.221 sec/step)\n","I0830 19:14:13.207270 139622356395904 learning.py:507] global step 9389: loss = 3.2521 (1.249 sec/step)\n","I0830 19:14:14.437633 139622356395904 learning.py:507] global step 9390: loss = 2.8528 (1.229 sec/step)\n","I0830 19:14:15.687321 139622356395904 learning.py:507] global step 9391: loss = 4.3077 (1.248 sec/step)\n","I0830 19:14:16.868390 139622356395904 learning.py:507] global step 9392: loss = 2.4121 (1.179 sec/step)\n","I0830 19:14:18.079693 139622356395904 learning.py:507] global step 9393: loss = 2.3867 (1.210 sec/step)\n","I0830 19:14:19.281952 139622356395904 learning.py:507] global step 9394: loss = 3.8412 (1.200 sec/step)\n","I0830 19:14:20.475220 139622356395904 learning.py:507] global step 9395: loss = 1.9713 (1.191 sec/step)\n","I0830 19:14:21.696950 139622356395904 learning.py:507] global step 9396: loss = 1.9769 (1.220 sec/step)\n","I0830 19:14:22.916567 139622356395904 learning.py:507] global step 9397: loss = 2.8461 (1.218 sec/step)\n","I0830 19:14:24.159074 139622356395904 learning.py:507] global step 9398: loss = 2.5479 (1.241 sec/step)\n","I0830 19:14:25.390548 139622356395904 learning.py:507] global step 9399: loss = 2.5810 (1.230 sec/step)\n","I0830 19:14:26.615242 139622356395904 learning.py:507] global step 9400: loss = 3.8932 (1.223 sec/step)\n","I0830 19:14:27.831525 139622356395904 learning.py:507] global step 9401: loss = 3.4214 (1.215 sec/step)\n","I0830 19:14:29.045748 139622356395904 learning.py:507] global step 9402: loss = 3.8589 (1.212 sec/step)\n","I0830 19:14:30.272581 139622356395904 learning.py:507] global step 9403: loss = 2.9256 (1.225 sec/step)\n","I0830 19:14:31.489333 139622356395904 learning.py:507] global step 9404: loss = 4.2641 (1.215 sec/step)\n","I0830 19:14:32.646503 139622356395904 learning.py:507] global step 9405: loss = 2.1682 (1.155 sec/step)\n","I0830 19:14:33.854358 139622356395904 learning.py:507] global step 9406: loss = 2.5364 (1.205 sec/step)\n","I0830 19:14:35.049306 139622356395904 learning.py:507] global step 9407: loss = 2.9887 (1.193 sec/step)\n","I0830 19:14:36.218087 139622356395904 learning.py:507] global step 9408: loss = 3.3818 (1.167 sec/step)\n","I0830 19:14:37.386242 139622356395904 learning.py:507] global step 9409: loss = 2.5154 (1.167 sec/step)\n","I0830 19:14:38.565403 139622356395904 learning.py:507] global step 9410: loss = 1.8769 (1.178 sec/step)\n","I0830 19:14:39.766703 139622356395904 learning.py:507] global step 9411: loss = 3.1138 (1.200 sec/step)\n","I0830 19:14:40.944702 139622356395904 learning.py:507] global step 9412: loss = 2.4652 (1.176 sec/step)\n","I0830 19:14:42.163366 139622356395904 learning.py:507] global step 9413: loss = 2.2414 (1.217 sec/step)\n","I0830 19:14:43.375242 139622356395904 learning.py:507] global step 9414: loss = 2.3250 (1.210 sec/step)\n","I0830 19:14:44.594180 139622356395904 learning.py:507] global step 9415: loss = 3.6832 (1.217 sec/step)\n","I0830 19:14:45.786449 139622356395904 learning.py:507] global step 9416: loss = 2.4679 (1.190 sec/step)\n","I0830 19:14:47.015372 139622356395904 learning.py:507] global step 9417: loss = 3.1966 (1.227 sec/step)\n","I0830 19:14:48.191168 139622356395904 learning.py:507] global step 9418: loss = 2.2831 (1.174 sec/step)\n","I0830 19:14:49.387989 139622356395904 learning.py:507] global step 9419: loss = 2.7227 (1.195 sec/step)\n","I0830 19:14:50.592876 139622356395904 learning.py:507] global step 9420: loss = 2.0115 (1.203 sec/step)\n","I0830 19:14:51.824745 139622356395904 learning.py:507] global step 9421: loss = 2.0031 (1.230 sec/step)\n","I0830 19:14:53.017529 139622356395904 learning.py:507] global step 9422: loss = 2.1969 (1.191 sec/step)\n","I0830 19:14:54.278784 139622356395904 learning.py:507] global step 9423: loss = 2.7595 (1.259 sec/step)\n","I0830 19:14:55.495663 139622356395904 learning.py:507] global step 9424: loss = 2.8288 (1.215 sec/step)\n","I0830 19:14:56.769661 139622356395904 learning.py:507] global step 9425: loss = 3.1317 (1.272 sec/step)\n","I0830 19:14:58.014192 139622356395904 learning.py:507] global step 9426: loss = 2.4934 (1.243 sec/step)\n","I0830 19:14:59.199757 139622356395904 learning.py:507] global step 9427: loss = 2.3644 (1.184 sec/step)\n","I0830 19:15:00.420505 139622356395904 learning.py:507] global step 9428: loss = 2.0398 (1.219 sec/step)\n","I0830 19:15:01.600227 139622356395904 learning.py:507] global step 9429: loss = 2.8997 (1.178 sec/step)\n","I0830 19:15:02.782937 139622356395904 learning.py:507] global step 9430: loss = 2.4583 (1.181 sec/step)\n","I0830 19:15:04.015373 139622356395904 learning.py:507] global step 9431: loss = 2.4793 (1.231 sec/step)\n","I0830 19:15:05.198570 139622356395904 learning.py:507] global step 9432: loss = 2.5907 (1.182 sec/step)\n","I0830 19:15:06.418412 139622356395904 learning.py:507] global step 9433: loss = 2.6072 (1.218 sec/step)\n","I0830 19:15:07.647921 139622356395904 learning.py:507] global step 9434: loss = 3.3639 (1.228 sec/step)\n","I0830 19:15:08.852376 139622356395904 learning.py:507] global step 9435: loss = 3.1163 (1.202 sec/step)\n","I0830 19:15:10.048002 139622356395904 learning.py:507] global step 9436: loss = 2.6839 (1.193 sec/step)\n","I0830 19:15:11.278191 139622356395904 learning.py:507] global step 9437: loss = 2.0010 (1.228 sec/step)\n","I0830 19:15:12.523706 139622356395904 learning.py:507] global step 9438: loss = 1.9975 (1.244 sec/step)\n","I0830 19:15:13.782742 139622356395904 learning.py:507] global step 9439: loss = 2.3521 (1.257 sec/step)\n","I0830 19:15:15.012632 139622356395904 learning.py:507] global step 9440: loss = 2.4806 (1.228 sec/step)\n","I0830 19:15:16.243179 139622356395904 learning.py:507] global step 9441: loss = 2.4564 (1.229 sec/step)\n","I0830 19:15:17.474788 139622356395904 learning.py:507] global step 9442: loss = 3.1265 (1.230 sec/step)\n","I0830 19:15:18.731596 139622356395904 learning.py:507] global step 9443: loss = 2.3133 (1.255 sec/step)\n","I0830 19:15:19.973257 139622356395904 learning.py:507] global step 9444: loss = 2.8470 (1.239 sec/step)\n","I0830 19:15:21.208329 139622356395904 learning.py:507] global step 9445: loss = 2.4451 (1.233 sec/step)\n","I0830 19:15:22.436199 139622356395904 learning.py:507] global step 9446: loss = 1.8439 (1.226 sec/step)\n","I0830 19:15:23.632422 139622356395904 learning.py:507] global step 9447: loss = 2.1309 (1.194 sec/step)\n","I0830 19:15:24.871268 139622356395904 learning.py:507] global step 9448: loss = 3.7045 (1.237 sec/step)\n","I0830 19:15:26.105246 139622356395904 learning.py:507] global step 9449: loss = 2.7746 (1.232 sec/step)\n","I0830 19:15:27.318649 139622356395904 learning.py:507] global step 9450: loss = 2.1270 (1.211 sec/step)\n","I0830 19:15:28.551133 139622356395904 learning.py:507] global step 9451: loss = 2.4443 (1.231 sec/step)\n","I0830 19:15:29.746514 139622356395904 learning.py:507] global step 9452: loss = 2.4279 (1.194 sec/step)\n","I0830 19:15:30.993183 139622356395904 learning.py:507] global step 9453: loss = 3.2175 (1.245 sec/step)\n","I0830 19:15:32.202009 139622356395904 learning.py:507] global step 9454: loss = 2.6037 (1.207 sec/step)\n","I0830 19:15:33.416664 139622356395904 learning.py:507] global step 9455: loss = 2.1418 (1.213 sec/step)\n","I0830 19:15:34.642720 139622356395904 learning.py:507] global step 9456: loss = 2.5981 (1.224 sec/step)\n","I0830 19:15:35.875163 139622356395904 learning.py:507] global step 9457: loss = 1.7189 (1.231 sec/step)\n","I0830 19:15:37.129328 139622356395904 learning.py:507] global step 9458: loss = 2.3390 (1.252 sec/step)\n","I0830 19:15:38.349921 139622356395904 learning.py:507] global step 9459: loss = 2.8225 (1.219 sec/step)\n","I0830 19:15:39.621575 139622356395904 learning.py:507] global step 9460: loss = 2.5317 (1.270 sec/step)\n","I0830 19:15:40.841845 139622356395904 learning.py:507] global step 9461: loss = 2.5706 (1.218 sec/step)\n","I0830 19:15:42.049177 139622356395904 learning.py:507] global step 9462: loss = 1.8961 (1.205 sec/step)\n","I0830 19:15:43.266117 139622356395904 learning.py:507] global step 9463: loss = 2.3892 (1.215 sec/step)\n","I0830 19:15:44.488776 139622356395904 learning.py:507] global step 9464: loss = 2.0891 (1.221 sec/step)\n","I0830 19:15:45.722809 139622356395904 learning.py:507] global step 9465: loss = 2.4448 (1.232 sec/step)\n","I0830 19:15:46.954293 139622356395904 learning.py:507] global step 9466: loss = 2.3540 (1.229 sec/step)\n","I0830 19:15:48.228753 139622356395904 learning.py:507] global step 9467: loss = 1.7691 (1.272 sec/step)\n","I0830 19:15:49.457194 139622356395904 learning.py:507] global step 9468: loss = 4.1749 (1.227 sec/step)\n","I0830 19:15:50.685398 139622356395904 learning.py:507] global step 9469: loss = 3.2857 (1.226 sec/step)\n","I0830 19:15:51.901005 139622356395904 learning.py:507] global step 9470: loss = 2.6550 (1.214 sec/step)\n","I0830 19:15:53.135148 139622356395904 learning.py:507] global step 9471: loss = 1.7964 (1.232 sec/step)\n","I0830 19:15:54.376289 139622356395904 learning.py:507] global step 9472: loss = 2.5110 (1.239 sec/step)\n","I0830 19:15:55.621194 139622356395904 learning.py:507] global step 9473: loss = 3.2601 (1.243 sec/step)\n","I0830 19:15:56.831888 139622356395904 learning.py:507] global step 9474: loss = 3.0374 (1.209 sec/step)\n","I0830 19:15:58.080976 139622356395904 learning.py:507] global step 9475: loss = 2.0127 (1.247 sec/step)\n","I0830 19:15:59.284101 139622356395904 learning.py:507] global step 9476: loss = 2.9511 (1.202 sec/step)\n","I0830 19:16:00.754081 139622356395904 learning.py:507] global step 9477: loss = 2.8327 (1.462 sec/step)\n","I0830 19:16:02.617390 139619299985152 supervisor.py:1050] Recording summary at step 9478.\n","I0830 19:16:02.636270 139622356395904 learning.py:507] global step 9478: loss = 2.6861 (1.874 sec/step)\n","I0830 19:16:03.866369 139622356395904 learning.py:507] global step 9479: loss = 2.1413 (1.228 sec/step)\n","I0830 19:16:05.124886 139622356395904 learning.py:507] global step 9480: loss = 2.6988 (1.256 sec/step)\n","I0830 19:16:06.359086 139622356395904 learning.py:507] global step 9481: loss = 3.3854 (1.232 sec/step)\n","I0830 19:16:07.640283 139622356395904 learning.py:507] global step 9482: loss = 2.6131 (1.279 sec/step)\n","I0830 19:16:08.855472 139622356395904 learning.py:507] global step 9483: loss = 2.1239 (1.213 sec/step)\n","I0830 19:16:10.069522 139622356395904 learning.py:507] global step 9484: loss = 2.7206 (1.212 sec/step)\n","I0830 19:16:11.295678 139622356395904 learning.py:507] global step 9485: loss = 2.0631 (1.224 sec/step)\n","I0830 19:16:12.529652 139622356395904 learning.py:507] global step 9486: loss = 2.3035 (1.232 sec/step)\n","I0830 19:16:13.756614 139622356395904 learning.py:507] global step 9487: loss = 2.2788 (1.225 sec/step)\n","I0830 19:16:14.990476 139622356395904 learning.py:507] global step 9488: loss = 1.8323 (1.232 sec/step)\n","I0830 19:16:16.231194 139622356395904 learning.py:507] global step 9489: loss = 3.3400 (1.239 sec/step)\n","I0830 19:16:17.451804 139622356395904 learning.py:507] global step 9490: loss = 2.8268 (1.219 sec/step)\n","I0830 19:16:18.684579 139622356395904 learning.py:507] global step 9491: loss = 2.5013 (1.231 sec/step)\n","I0830 19:16:19.943695 139622356395904 learning.py:507] global step 9492: loss = 2.2189 (1.257 sec/step)\n","I0830 19:16:21.149521 139622356395904 learning.py:507] global step 9493: loss = 2.5353 (1.204 sec/step)\n","I0830 19:16:22.355132 139622356395904 learning.py:507] global step 9494: loss = 2.4044 (1.204 sec/step)\n","I0830 19:16:23.562557 139622356395904 learning.py:507] global step 9495: loss = 2.1823 (1.206 sec/step)\n","I0830 19:16:24.815001 139622356395904 learning.py:507] global step 9496: loss = 2.4319 (1.251 sec/step)\n","I0830 19:16:26.080943 139622356395904 learning.py:507] global step 9497: loss = 3.1085 (1.264 sec/step)\n","I0830 19:16:27.298828 139622356395904 learning.py:507] global step 9498: loss = 2.9214 (1.216 sec/step)\n","I0830 19:16:28.540166 139622356395904 learning.py:507] global step 9499: loss = 2.1507 (1.239 sec/step)\n","I0830 19:16:29.809191 139622356395904 learning.py:507] global step 9500: loss = 2.7398 (1.267 sec/step)\n","I0830 19:16:31.063554 139622356395904 learning.py:507] global step 9501: loss = 2.8141 (1.252 sec/step)\n","I0830 19:16:32.285377 139622356395904 learning.py:507] global step 9502: loss = 2.3446 (1.220 sec/step)\n","I0830 19:16:33.515550 139622356395904 learning.py:507] global step 9503: loss = 2.1839 (1.228 sec/step)\n","I0830 19:16:34.760436 139622356395904 learning.py:507] global step 9504: loss = 2.5034 (1.243 sec/step)\n","I0830 19:16:35.958192 139622356395904 learning.py:507] global step 9505: loss = 2.0763 (1.196 sec/step)\n","I0830 19:16:37.234813 139622356395904 learning.py:507] global step 9506: loss = 2.2750 (1.275 sec/step)\n","I0830 19:16:38.461271 139622356395904 learning.py:507] global step 9507: loss = 2.8558 (1.224 sec/step)\n","I0830 19:16:39.689184 139622356395904 learning.py:507] global step 9508: loss = 2.3056 (1.226 sec/step)\n","I0830 19:16:40.909410 139622356395904 learning.py:507] global step 9509: loss = 1.9437 (1.218 sec/step)\n","I0830 19:16:42.130117 139622356395904 learning.py:507] global step 9510: loss = 2.6878 (1.218 sec/step)\n","I0830 19:16:43.379366 139622356395904 learning.py:507] global step 9511: loss = 3.3486 (1.247 sec/step)\n","I0830 19:16:44.592518 139622356395904 learning.py:507] global step 9512: loss = 2.4533 (1.211 sec/step)\n","I0830 19:16:45.830178 139622356395904 learning.py:507] global step 9513: loss = 3.9767 (1.236 sec/step)\n","I0830 19:16:47.079447 139622356395904 learning.py:507] global step 9514: loss = 3.8185 (1.247 sec/step)\n","I0830 19:16:48.302124 139622356395904 learning.py:507] global step 9515: loss = 2.0533 (1.220 sec/step)\n","I0830 19:16:49.557341 139622356395904 learning.py:507] global step 9516: loss = 2.4515 (1.253 sec/step)\n","I0830 19:16:50.793014 139622356395904 learning.py:507] global step 9517: loss = 2.6729 (1.234 sec/step)\n","I0830 19:16:52.016532 139622356395904 learning.py:507] global step 9518: loss = 3.1835 (1.219 sec/step)\n","I0830 19:16:53.244724 139622356395904 learning.py:507] global step 9519: loss = 2.5804 (1.226 sec/step)\n","I0830 19:16:54.464387 139622356395904 learning.py:507] global step 9520: loss = 1.9873 (1.217 sec/step)\n","I0830 19:16:55.705299 139622356395904 learning.py:507] global step 9521: loss = 2.1890 (1.239 sec/step)\n","I0830 19:16:56.971896 139622356395904 learning.py:507] global step 9522: loss = 2.6047 (1.265 sec/step)\n","I0830 19:16:58.222969 139622356395904 learning.py:507] global step 9523: loss = 1.9124 (1.249 sec/step)\n","I0830 19:16:59.430140 139622356395904 learning.py:507] global step 9524: loss = 1.8751 (1.205 sec/step)\n","I0830 19:17:00.646466 139622356395904 learning.py:507] global step 9525: loss = 3.7880 (1.214 sec/step)\n","I0830 19:17:01.893363 139622356395904 learning.py:507] global step 9526: loss = 2.7625 (1.245 sec/step)\n","I0830 19:17:03.135729 139622356395904 learning.py:507] global step 9527: loss = 2.2552 (1.240 sec/step)\n","I0830 19:17:04.361042 139622356395904 learning.py:507] global step 9528: loss = 3.2898 (1.224 sec/step)\n","I0830 19:17:05.593572 139622356395904 learning.py:507] global step 9529: loss = 2.1705 (1.231 sec/step)\n","I0830 19:17:06.820016 139622356395904 learning.py:507] global step 9530: loss = 3.1194 (1.225 sec/step)\n","I0830 19:17:08.077599 139622356395904 learning.py:507] global step 9531: loss = 2.7184 (1.256 sec/step)\n","I0830 19:17:09.300364 139622356395904 learning.py:507] global step 9532: loss = 1.8819 (1.221 sec/step)\n","I0830 19:17:10.550382 139622356395904 learning.py:507] global step 9533: loss = 2.9645 (1.248 sec/step)\n","I0830 19:17:11.767490 139622356395904 learning.py:507] global step 9534: loss = 2.6436 (1.215 sec/step)\n","I0830 19:17:12.989521 139622356395904 learning.py:507] global step 9535: loss = 3.1474 (1.220 sec/step)\n","I0830 19:17:14.215601 139622356395904 learning.py:507] global step 9536: loss = 2.0165 (1.224 sec/step)\n","I0830 19:17:15.459846 139622356395904 learning.py:507] global step 9537: loss = 2.6698 (1.242 sec/step)\n","I0830 19:17:16.670773 139622356395904 learning.py:507] global step 9538: loss = 2.7407 (1.209 sec/step)\n","I0830 19:17:17.917854 139622356395904 learning.py:507] global step 9539: loss = 2.4955 (1.245 sec/step)\n","I0830 19:17:19.111798 139622356395904 learning.py:507] global step 9540: loss = 3.6185 (1.192 sec/step)\n","I0830 19:17:20.371402 139622356395904 learning.py:507] global step 9541: loss = 2.0611 (1.258 sec/step)\n","I0830 19:17:21.622599 139622356395904 learning.py:507] global step 9542: loss = 3.3516 (1.249 sec/step)\n","I0830 19:17:22.852858 139622356395904 learning.py:507] global step 9543: loss = 2.2999 (1.228 sec/step)\n","I0830 19:17:24.054933 139622356395904 learning.py:507] global step 9544: loss = 2.7426 (1.200 sec/step)\n","I0830 19:17:25.282238 139622356395904 learning.py:507] global step 9545: loss = 2.5595 (1.225 sec/step)\n","I0830 19:17:26.507778 139622356395904 learning.py:507] global step 9546: loss = 2.5766 (1.224 sec/step)\n","I0830 19:17:27.719485 139622356395904 learning.py:507] global step 9547: loss = 3.0365 (1.210 sec/step)\n","I0830 19:17:28.941503 139622356395904 learning.py:507] global step 9548: loss = 2.5595 (1.220 sec/step)\n","I0830 19:17:30.175126 139622356395904 learning.py:507] global step 9549: loss = 2.4627 (1.232 sec/step)\n","I0830 19:17:31.372346 139622356395904 learning.py:507] global step 9550: loss = 2.6491 (1.195 sec/step)\n","I0830 19:17:32.611344 139622356395904 learning.py:507] global step 9551: loss = 2.4012 (1.237 sec/step)\n","I0830 19:17:33.825893 139622356395904 learning.py:507] global step 9552: loss = 1.9867 (1.213 sec/step)\n","I0830 19:17:35.091299 139622356395904 learning.py:507] global step 9553: loss = 2.5103 (1.263 sec/step)\n","I0830 19:17:36.323767 139622356395904 learning.py:507] global step 9554: loss = 2.3953 (1.230 sec/step)\n","I0830 19:17:37.519366 139622356395904 learning.py:507] global step 9555: loss = 3.4242 (1.194 sec/step)\n","I0830 19:17:38.769574 139622356395904 learning.py:507] global step 9556: loss = 2.8233 (1.249 sec/step)\n","I0830 19:17:40.023137 139622356395904 learning.py:507] global step 9557: loss = 2.5400 (1.252 sec/step)\n","I0830 19:17:41.277951 139622356395904 learning.py:507] global step 9558: loss = 2.2418 (1.253 sec/step)\n","I0830 19:17:42.517993 139622356395904 learning.py:507] global step 9559: loss = 2.9148 (1.238 sec/step)\n","I0830 19:17:43.721458 139622356395904 learning.py:507] global step 9560: loss = 2.2286 (1.202 sec/step)\n","I0830 19:17:44.914313 139622356395904 learning.py:507] global step 9561: loss = 3.6876 (1.191 sec/step)\n","I0830 19:17:46.180749 139622356395904 learning.py:507] global step 9562: loss = 2.1502 (1.264 sec/step)\n","I0830 19:17:47.441024 139622356395904 learning.py:507] global step 9563: loss = 2.3016 (1.258 sec/step)\n","I0830 19:17:48.702787 139622356395904 learning.py:507] global step 9564: loss = 3.6505 (1.260 sec/step)\n","I0830 19:17:49.989312 139622356395904 learning.py:507] global step 9565: loss = 2.7322 (1.283 sec/step)\n","I0830 19:17:51.219188 139622356395904 learning.py:507] global step 9566: loss = 2.0129 (1.228 sec/step)\n","I0830 19:17:52.456914 139622356395904 learning.py:507] global step 9567: loss = 2.8014 (1.236 sec/step)\n","I0830 19:17:53.723578 139622356395904 learning.py:507] global step 9568: loss = 2.5650 (1.265 sec/step)\n","I0830 19:17:54.918950 139622356395904 learning.py:507] global step 9569: loss = 2.7019 (1.193 sec/step)\n","I0830 19:17:56.135017 139622356395904 learning.py:507] global step 9570: loss = 2.1236 (1.214 sec/step)\n","I0830 19:17:57.364296 139622356395904 learning.py:507] global step 9571: loss = 2.9388 (1.227 sec/step)\n","I0830 19:17:58.560090 139622356395904 learning.py:507] global step 9572: loss = 2.9311 (1.194 sec/step)\n","I0830 19:17:59.799475 139622356395904 learning.py:507] global step 9573: loss = 2.4164 (1.238 sec/step)\n","I0830 19:18:01.557184 139619299985152 supervisor.py:1050] Recording summary at step 9573.\n","I0830 19:18:02.017354 139622356395904 learning.py:507] global step 9574: loss = 2.2952 (2.207 sec/step)\n","I0830 19:18:03.259276 139622356395904 learning.py:507] global step 9575: loss = 2.0772 (1.240 sec/step)\n","I0830 19:18:04.483006 139622356395904 learning.py:507] global step 9576: loss = 2.6493 (1.222 sec/step)\n","I0830 19:18:05.695574 139622356395904 learning.py:507] global step 9577: loss = 2.6705 (1.210 sec/step)\n","I0830 19:18:06.922645 139622356395904 learning.py:507] global step 9578: loss = 2.0148 (1.225 sec/step)\n","I0830 19:18:08.147547 139622356395904 learning.py:507] global step 9579: loss = 3.0092 (1.223 sec/step)\n","I0830 19:18:09.384447 139622356395904 learning.py:507] global step 9580: loss = 2.0150 (1.235 sec/step)\n","I0830 19:18:10.616597 139622356395904 learning.py:507] global step 9581: loss = 2.9176 (1.230 sec/step)\n","I0830 19:18:11.858227 139622356395904 learning.py:507] global step 9582: loss = 2.1569 (1.240 sec/step)\n","I0830 19:18:13.069235 139622356395904 learning.py:507] global step 9583: loss = 3.3027 (1.209 sec/step)\n","I0830 19:18:14.305303 139622356395904 learning.py:507] global step 9584: loss = 2.3999 (1.233 sec/step)\n","I0830 19:18:15.527460 139622356395904 learning.py:507] global step 9585: loss = 3.3876 (1.220 sec/step)\n","I0830 19:18:16.758373 139622356395904 learning.py:507] global step 9586: loss = 2.4784 (1.229 sec/step)\n","I0830 19:18:17.997157 139622356395904 learning.py:507] global step 9587: loss = 2.1753 (1.237 sec/step)\n","I0830 19:18:19.197173 139622356395904 learning.py:507] global step 9588: loss = 2.6066 (1.198 sec/step)\n","I0830 19:18:20.431306 139622356395904 learning.py:507] global step 9589: loss = 3.1548 (1.232 sec/step)\n","I0830 19:18:21.678491 139622356395904 learning.py:507] global step 9590: loss = 2.3788 (1.245 sec/step)\n","I0830 19:18:22.904636 139622356395904 learning.py:507] global step 9591: loss = 2.5956 (1.224 sec/step)\n","I0830 19:18:24.115615 139622356395904 learning.py:507] global step 9592: loss = 2.9667 (1.209 sec/step)\n","I0830 19:18:25.367948 139622356395904 learning.py:507] global step 9593: loss = 3.4970 (1.250 sec/step)\n","I0830 19:18:26.589814 139622356395904 learning.py:507] global step 9594: loss = 3.2180 (1.220 sec/step)\n","I0830 19:18:27.815318 139622356395904 learning.py:507] global step 9595: loss = 1.9547 (1.224 sec/step)\n","I0830 19:18:29.054627 139622356395904 learning.py:507] global step 9596: loss = 2.4382 (1.237 sec/step)\n","I0830 19:18:30.264720 139622356395904 learning.py:507] global step 9597: loss = 3.0616 (1.208 sec/step)\n","I0830 19:18:31.474456 139622356395904 learning.py:507] global step 9598: loss = 2.4198 (1.208 sec/step)\n","I0830 19:18:32.709194 139622356395904 learning.py:507] global step 9599: loss = 3.3878 (1.233 sec/step)\n","I0830 19:18:33.920222 139622356395904 learning.py:507] global step 9600: loss = 2.3221 (1.209 sec/step)\n","I0830 19:18:35.174388 139622356395904 learning.py:507] global step 9601: loss = 2.8654 (1.252 sec/step)\n","I0830 19:18:36.416849 139622356395904 learning.py:507] global step 9602: loss = 2.5842 (1.240 sec/step)\n","I0830 19:18:37.655695 139622356395904 learning.py:507] global step 9603: loss = 3.0115 (1.237 sec/step)\n","I0830 19:18:38.884671 139622356395904 learning.py:507] global step 9604: loss = 3.9958 (1.227 sec/step)\n","I0830 19:18:40.144411 139622356395904 learning.py:507] global step 9605: loss = 2.3858 (1.258 sec/step)\n","I0830 19:18:41.386505 139622356395904 learning.py:507] global step 9606: loss = 2.1185 (1.239 sec/step)\n","I0830 19:18:42.600413 139622356395904 learning.py:507] global step 9607: loss = 3.7821 (1.211 sec/step)\n","I0830 19:18:43.855317 139622356395904 learning.py:507] global step 9608: loss = 2.6411 (1.253 sec/step)\n","I0830 19:18:45.100980 139622356395904 learning.py:507] global step 9609: loss = 2.0122 (1.244 sec/step)\n","I0830 19:18:46.368026 139622356395904 learning.py:507] global step 9610: loss = 2.0539 (1.265 sec/step)\n","I0830 19:18:47.624910 139622356395904 learning.py:507] global step 9611: loss = 3.5600 (1.255 sec/step)\n","I0830 19:18:48.870968 139622356395904 learning.py:507] global step 9612: loss = 2.3411 (1.244 sec/step)\n","I0830 19:18:50.106222 139622356395904 learning.py:507] global step 9613: loss = 2.2066 (1.233 sec/step)\n","I0830 19:18:51.342380 139622356395904 learning.py:507] global step 9614: loss = 2.7191 (1.234 sec/step)\n","I0830 19:18:52.585694 139622356395904 learning.py:507] global step 9615: loss = 3.2360 (1.241 sec/step)\n","I0830 19:18:53.825863 139622356395904 learning.py:507] global step 9616: loss = 3.3509 (1.238 sec/step)\n","I0830 19:18:55.076021 139622356395904 learning.py:507] global step 9617: loss = 2.9849 (1.248 sec/step)\n","I0830 19:18:56.304722 139622356395904 learning.py:507] global step 9618: loss = 2.5782 (1.227 sec/step)\n","I0830 19:18:57.567620 139622356395904 learning.py:507] global step 9619: loss = 2.3075 (1.261 sec/step)\n","I0830 19:18:58.816440 139622356395904 learning.py:507] global step 9620: loss = 1.7513 (1.247 sec/step)\n","I0830 19:19:00.036328 139622356395904 learning.py:507] global step 9621: loss = 2.6588 (1.218 sec/step)\n","I0830 19:19:01.287355 139622356395904 learning.py:507] global step 9622: loss = 2.4926 (1.249 sec/step)\n","I0830 19:19:02.544782 139622356395904 learning.py:507] global step 9623: loss = 2.5932 (1.255 sec/step)\n","I0830 19:19:03.834568 139622356395904 learning.py:507] global step 9624: loss = 3.6477 (1.288 sec/step)\n","I0830 19:19:05.060838 139622356395904 learning.py:507] global step 9625: loss = 2.1667 (1.224 sec/step)\n","I0830 19:19:06.307970 139622356395904 learning.py:507] global step 9626: loss = 2.1594 (1.245 sec/step)\n","I0830 19:19:07.532942 139622356395904 learning.py:507] global step 9627: loss = 2.8180 (1.223 sec/step)\n","I0830 19:19:08.770109 139622356395904 learning.py:507] global step 9628: loss = 2.6204 (1.235 sec/step)\n","I0830 19:19:10.001217 139622356395904 learning.py:507] global step 9629: loss = 1.9569 (1.229 sec/step)\n","I0830 19:19:11.279623 139622356395904 learning.py:507] global step 9630: loss = 2.8196 (1.276 sec/step)\n","I0830 19:19:12.493594 139622356395904 learning.py:507] global step 9631: loss = 2.2944 (1.210 sec/step)\n","I0830 19:19:13.702214 139622356395904 learning.py:507] global step 9632: loss = 2.9202 (1.206 sec/step)\n","I0830 19:19:14.962781 139622356395904 learning.py:507] global step 9633: loss = 2.4455 (1.259 sec/step)\n","I0830 19:19:16.192968 139622356395904 learning.py:507] global step 9634: loss = 2.3000 (1.228 sec/step)\n","I0830 19:19:17.407705 139622356395904 learning.py:507] global step 9635: loss = 2.3952 (1.213 sec/step)\n","I0830 19:19:18.663220 139622356395904 learning.py:507] global step 9636: loss = 2.4818 (1.254 sec/step)\n","I0830 19:19:19.904841 139622356395904 learning.py:507] global step 9637: loss = 2.1355 (1.240 sec/step)\n","I0830 19:19:21.116294 139622356395904 learning.py:507] global step 9638: loss = 3.0980 (1.210 sec/step)\n","I0830 19:19:22.386821 139622356395904 learning.py:507] global step 9639: loss = 3.3001 (1.269 sec/step)\n","I0830 19:19:23.654039 139622356395904 learning.py:507] global step 9640: loss = 2.3668 (1.264 sec/step)\n","I0830 19:19:24.857928 139622356395904 learning.py:507] global step 9641: loss = 2.8541 (1.200 sec/step)\n","I0830 19:19:26.097640 139622356395904 learning.py:507] global step 9642: loss = 2.9249 (1.238 sec/step)\n","I0830 19:19:27.331590 139622356395904 learning.py:507] global step 9643: loss = 2.5423 (1.232 sec/step)\n","I0830 19:19:28.591746 139622356395904 learning.py:507] global step 9644: loss = 2.2967 (1.258 sec/step)\n","I0830 19:19:29.814343 139622356395904 learning.py:507] global step 9645: loss = 2.3759 (1.221 sec/step)\n","I0830 19:19:31.066523 139622356395904 learning.py:507] global step 9646: loss = 2.1768 (1.250 sec/step)\n","I0830 19:19:32.322997 139622356395904 learning.py:507] global step 9647: loss = 3.2717 (1.254 sec/step)\n","I0830 19:19:33.566140 139622356395904 learning.py:507] global step 9648: loss = 2.9760 (1.241 sec/step)\n","I0830 19:19:34.816623 139622356395904 learning.py:507] global step 9649: loss = 2.7804 (1.248 sec/step)\n","I0830 19:19:36.073358 139622356395904 learning.py:507] global step 9650: loss = 2.2662 (1.255 sec/step)\n","I0830 19:19:37.308175 139622356395904 learning.py:507] global step 9651: loss = 2.5156 (1.233 sec/step)\n","I0830 19:19:38.538184 139622356395904 learning.py:507] global step 9652: loss = 2.6977 (1.228 sec/step)\n","I0830 19:19:39.781220 139622356395904 learning.py:507] global step 9653: loss = 2.7653 (1.241 sec/step)\n","I0830 19:19:41.003531 139622356395904 learning.py:507] global step 9654: loss = 3.7068 (1.220 sec/step)\n","I0830 19:19:42.203761 139622356395904 learning.py:507] global step 9655: loss = 3.6310 (1.198 sec/step)\n","I0830 19:19:43.423829 139622356395904 learning.py:507] global step 9656: loss = 3.2460 (1.218 sec/step)\n","I0830 19:19:44.633806 139622356395904 learning.py:507] global step 9657: loss = 2.5670 (1.208 sec/step)\n","I0830 19:19:45.887383 139622356395904 learning.py:507] global step 9658: loss = 2.3663 (1.252 sec/step)\n","I0830 19:19:47.105403 139622356395904 learning.py:507] global step 9659: loss = 2.8241 (1.216 sec/step)\n","I0830 19:19:48.339034 139622356395904 learning.py:507] global step 9660: loss = 1.8409 (1.232 sec/step)\n","I0830 19:19:49.570475 139622356395904 learning.py:507] global step 9661: loss = 2.9632 (1.230 sec/step)\n","I0830 19:19:50.785417 139622356395904 learning.py:507] global step 9662: loss = 2.0334 (1.213 sec/step)\n","I0830 19:19:51.999001 139622356395904 learning.py:507] global step 9663: loss = 2.7225 (1.212 sec/step)\n","I0830 19:19:53.244374 139622356395904 learning.py:507] global step 9664: loss = 3.3225 (1.244 sec/step)\n","I0830 19:19:54.468112 139622356395904 learning.py:507] global step 9665: loss = 2.4336 (1.222 sec/step)\n","I0830 19:19:55.677336 139622356395904 learning.py:507] global step 9666: loss = 2.7053 (1.207 sec/step)\n","I0830 19:19:56.896372 139622356395904 learning.py:507] global step 9667: loss = 2.5729 (1.217 sec/step)\n","I0830 19:19:58.123393 139622356395904 learning.py:507] global step 9668: loss = 2.2269 (1.225 sec/step)\n","I0830 19:19:59.350864 139622356395904 learning.py:507] global step 9669: loss = 2.4795 (1.226 sec/step)\n","I0830 19:19:59.645684 139619316770560 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n","I0830 19:20:02.312127 139622356395904 learning.py:507] global step 9670: loss = 5.2322 (2.259 sec/step)\n","I0830 19:20:03.073109 139619299985152 supervisor.py:1050] Recording summary at step 9670.\n","I0830 19:20:04.177353 139622356395904 learning.py:507] global step 9671: loss = 2.6104 (1.727 sec/step)\n","I0830 19:20:05.604661 139622356395904 learning.py:507] global step 9672: loss = 2.4712 (1.421 sec/step)\n","I0830 19:20:06.850572 139622356395904 learning.py:507] global step 9673: loss = 2.4395 (1.244 sec/step)\n","I0830 19:20:08.095369 139622356395904 learning.py:507] global step 9674: loss = 2.5230 (1.243 sec/step)\n","I0830 19:20:09.329675 139622356395904 learning.py:507] global step 9675: loss = 2.6676 (1.232 sec/step)\n","I0830 19:20:10.541575 139622356395904 learning.py:507] global step 9676: loss = 2.1024 (1.210 sec/step)\n","I0830 19:20:11.791129 139622356395904 learning.py:507] global step 9677: loss = 2.6154 (1.248 sec/step)\n","I0830 19:20:13.027863 139622356395904 learning.py:507] global step 9678: loss = 2.3996 (1.235 sec/step)\n","I0830 19:20:14.253227 139622356395904 learning.py:507] global step 9679: loss = 2.0393 (1.223 sec/step)\n","I0830 19:20:15.509031 139622356395904 learning.py:507] global step 9680: loss = 2.1766 (1.254 sec/step)\n","I0830 19:20:16.724162 139622356395904 learning.py:507] global step 9681: loss = 2.8384 (1.213 sec/step)\n","I0830 19:20:17.965255 139622356395904 learning.py:507] global step 9682: loss = 2.7711 (1.239 sec/step)\n","I0830 19:20:19.226483 139622356395904 learning.py:507] global step 9683: loss = 2.1740 (1.259 sec/step)\n","I0830 19:20:20.465702 139622356395904 learning.py:507] global step 9684: loss = 2.2941 (1.237 sec/step)\n","I0830 19:20:21.710770 139622356395904 learning.py:507] global step 9685: loss = 2.8265 (1.243 sec/step)\n","I0830 19:20:22.949601 139622356395904 learning.py:507] global step 9686: loss = 3.1988 (1.237 sec/step)\n","I0830 19:20:24.206771 139622356395904 learning.py:507] global step 9687: loss = 2.7941 (1.255 sec/step)\n","I0830 19:20:25.451289 139622356395904 learning.py:507] global step 9688: loss = 3.8762 (1.243 sec/step)\n","I0830 19:20:26.685810 139622356395904 learning.py:507] global step 9689: loss = 2.9331 (1.233 sec/step)\n","I0830 19:20:27.963000 139622356395904 learning.py:507] global step 9690: loss = 1.8917 (1.275 sec/step)\n","I0830 19:20:29.190086 139622356395904 learning.py:507] global step 9691: loss = 2.2884 (1.225 sec/step)\n","I0830 19:20:30.451576 139622356395904 learning.py:507] global step 9692: loss = 2.4302 (1.259 sec/step)\n","I0830 19:20:31.662391 139622356395904 learning.py:507] global step 9693: loss = 1.9041 (1.209 sec/step)\n","I0830 19:20:32.844712 139622356395904 learning.py:507] global step 9694: loss = 2.2612 (1.180 sec/step)\n","I0830 19:20:34.009320 139622356395904 learning.py:507] global step 9695: loss = 3.5410 (1.163 sec/step)\n","I0830 19:20:35.239232 139622356395904 learning.py:507] global step 9696: loss = 3.2973 (1.228 sec/step)\n","I0830 19:20:36.453372 139622356395904 learning.py:507] global step 9697: loss = 2.2456 (1.212 sec/step)\n","I0830 19:20:37.669490 139622356395904 learning.py:507] global step 9698: loss = 2.6023 (1.214 sec/step)\n","I0830 19:20:38.902885 139622356395904 learning.py:507] global step 9699: loss = 3.1950 (1.231 sec/step)\n","I0830 19:20:40.146252 139622356395904 learning.py:507] global step 9700: loss = 1.7891 (1.241 sec/step)\n","I0830 19:20:41.362658 139622356395904 learning.py:507] global step 9701: loss = 2.2378 (1.214 sec/step)\n","I0830 19:20:42.595781 139622356395904 learning.py:507] global step 9702: loss = 2.7773 (1.231 sec/step)\n","I0830 19:20:43.799955 139622356395904 learning.py:507] global step 9703: loss = 2.5787 (1.202 sec/step)\n","I0830 19:20:45.001461 139622356395904 learning.py:507] global step 9704: loss = 2.8768 (1.200 sec/step)\n","I0830 19:20:46.182877 139622356395904 learning.py:507] global step 9705: loss = 2.5288 (1.179 sec/step)\n","I0830 19:20:47.382219 139622356395904 learning.py:507] global step 9706: loss = 2.7413 (1.197 sec/step)\n","I0830 19:20:48.583519 139622356395904 learning.py:507] global step 9707: loss = 2.0732 (1.199 sec/step)\n","I0830 19:20:49.784185 139622356395904 learning.py:507] global step 9708: loss = 2.6471 (1.199 sec/step)\n","I0830 19:20:51.055230 139622356395904 learning.py:507] global step 9709: loss = 2.5710 (1.269 sec/step)\n","I0830 19:20:52.261558 139622356395904 learning.py:507] global step 9710: loss = 2.1916 (1.204 sec/step)\n","I0830 19:20:53.500556 139622356395904 learning.py:507] global step 9711: loss = 2.2295 (1.237 sec/step)\n","I0830 19:20:54.696743 139622356395904 learning.py:507] global step 9712: loss = 1.8401 (1.194 sec/step)\n","I0830 19:20:55.931448 139622356395904 learning.py:507] global step 9713: loss = 2.4950 (1.233 sec/step)\n","I0830 19:20:57.152904 139622356395904 learning.py:507] global step 9714: loss = 2.2651 (1.220 sec/step)\n","I0830 19:20:58.355411 139622356395904 learning.py:507] global step 9715: loss = 2.8651 (1.200 sec/step)\n","I0830 19:20:59.563641 139622356395904 learning.py:507] global step 9716: loss = 2.1090 (1.206 sec/step)\n","I0830 19:21:00.791824 139622356395904 learning.py:507] global step 9717: loss = 2.4345 (1.226 sec/step)\n","I0830 19:21:02.001951 139622356395904 learning.py:507] global step 9718: loss = 2.2168 (1.208 sec/step)\n","I0830 19:21:03.215543 139622356395904 learning.py:507] global step 9719: loss = 2.3708 (1.212 sec/step)\n","I0830 19:21:04.467207 139622356395904 learning.py:507] global step 9720: loss = 2.1745 (1.250 sec/step)\n","I0830 19:21:05.687394 139622356395904 learning.py:507] global step 9721: loss = 2.5308 (1.218 sec/step)\n","I0830 19:21:06.915542 139622356395904 learning.py:507] global step 9722: loss = 3.1440 (1.226 sec/step)\n","I0830 19:21:08.135374 139622356395904 learning.py:507] global step 9723: loss = 3.7858 (1.218 sec/step)\n","I0830 19:21:09.367258 139622356395904 learning.py:507] global step 9724: loss = 2.4668 (1.230 sec/step)\n","I0830 19:21:10.578831 139622356395904 learning.py:507] global step 9725: loss = 3.0078 (1.210 sec/step)\n","I0830 19:21:11.801464 139622356395904 learning.py:507] global step 9726: loss = 2.4556 (1.221 sec/step)\n","I0830 19:21:13.001299 139622356395904 learning.py:507] global step 9727: loss = 3.6913 (1.198 sec/step)\n","I0830 19:21:14.159734 139622356395904 learning.py:507] global step 9728: loss = 2.4997 (1.157 sec/step)\n","I0830 19:21:15.377488 139622356395904 learning.py:507] global step 9729: loss = 2.4839 (1.216 sec/step)\n","I0830 19:21:16.589556 139622356395904 learning.py:507] global step 9730: loss = 2.6255 (1.210 sec/step)\n","I0830 19:21:17.840140 139622356395904 learning.py:507] global step 9731: loss = 2.5092 (1.248 sec/step)\n","I0830 19:21:19.048824 139622356395904 learning.py:507] global step 9732: loss = 1.9423 (1.207 sec/step)\n","I0830 19:21:20.247186 139622356395904 learning.py:507] global step 9733: loss = 3.6158 (1.196 sec/step)\n","I0830 19:21:21.491937 139622356395904 learning.py:507] global step 9734: loss = 3.1658 (1.243 sec/step)\n","I0830 19:21:22.700187 139622356395904 learning.py:507] global step 9735: loss = 2.2875 (1.206 sec/step)\n","I0830 19:21:23.966551 139622356395904 learning.py:507] global step 9736: loss = 2.5640 (1.264 sec/step)\n","I0830 19:21:25.206823 139622356395904 learning.py:507] global step 9737: loss = 2.4886 (1.238 sec/step)\n","I0830 19:21:26.457394 139622356395904 learning.py:507] global step 9738: loss = 2.4856 (1.247 sec/step)\n","I0830 19:21:27.687683 139622356395904 learning.py:507] global step 9739: loss = 3.3540 (1.224 sec/step)\n","I0830 19:21:28.905425 139622356395904 learning.py:507] global step 9740: loss = 2.4225 (1.216 sec/step)\n","I0830 19:21:30.132072 139622356395904 learning.py:507] global step 9741: loss = 2.3988 (1.225 sec/step)\n","I0830 19:21:31.386857 139622356395904 learning.py:507] global step 9742: loss = 2.7509 (1.253 sec/step)\n","I0830 19:21:32.603232 139622356395904 learning.py:507] global step 9743: loss = 3.3102 (1.214 sec/step)\n","I0830 19:21:33.862973 139622356395904 learning.py:507] global step 9744: loss = 2.2818 (1.258 sec/step)\n","I0830 19:21:35.085472 139622356395904 learning.py:507] global step 9745: loss = 3.1808 (1.221 sec/step)\n","I0830 19:21:36.355689 139622356395904 learning.py:507] global step 9746: loss = 2.0356 (1.268 sec/step)\n","I0830 19:21:37.577478 139622356395904 learning.py:507] global step 9747: loss = 2.8998 (1.220 sec/step)\n","I0830 19:21:38.803892 139622356395904 learning.py:507] global step 9748: loss = 1.9041 (1.225 sec/step)\n","I0830 19:21:40.052463 139622356395904 learning.py:507] global step 9749: loss = 2.7320 (1.247 sec/step)\n","I0830 19:21:41.312860 139622356395904 learning.py:507] global step 9750: loss = 2.0387 (1.258 sec/step)\n","I0830 19:21:42.532245 139622356395904 learning.py:507] global step 9751: loss = 1.9721 (1.218 sec/step)\n","I0830 19:21:43.780320 139622356395904 learning.py:507] global step 9752: loss = 2.5527 (1.246 sec/step)\n","I0830 19:21:45.050694 139622356395904 learning.py:507] global step 9753: loss = 2.7951 (1.268 sec/step)\n","I0830 19:21:46.293971 139622356395904 learning.py:507] global step 9754: loss = 2.3024 (1.241 sec/step)\n","I0830 19:21:47.532586 139622356395904 learning.py:507] global step 9755: loss = 2.7716 (1.237 sec/step)\n","I0830 19:21:48.764630 139622356395904 learning.py:507] global step 9756: loss = 2.3064 (1.230 sec/step)\n","I0830 19:21:50.016026 139622356395904 learning.py:507] global step 9757: loss = 3.5839 (1.249 sec/step)\n","I0830 19:21:51.246355 139622356395904 learning.py:507] global step 9758: loss = 1.9359 (1.228 sec/step)\n","I0830 19:21:52.480917 139622356395904 learning.py:507] global step 9759: loss = 2.1138 (1.233 sec/step)\n","I0830 19:21:53.670781 139622356395904 learning.py:507] global step 9760: loss = 2.9126 (1.188 sec/step)\n","I0830 19:21:54.909918 139622356395904 learning.py:507] global step 9761: loss = 2.9162 (1.237 sec/step)\n","I0830 19:21:56.142544 139622356395904 learning.py:507] global step 9762: loss = 3.0025 (1.231 sec/step)\n","I0830 19:21:57.369136 139622356395904 learning.py:507] global step 9763: loss = 2.0683 (1.225 sec/step)\n","I0830 19:21:58.617367 139622356395904 learning.py:507] global step 9764: loss = 3.1851 (1.247 sec/step)\n","I0830 19:21:59.980634 139622356395904 learning.py:507] global step 9765: loss = 2.1702 (1.279 sec/step)\n","I0830 19:22:02.215486 139619299985152 supervisor.py:1050] Recording summary at step 9766.\n","I0830 19:22:02.242084 139622356395904 learning.py:507] global step 9766: loss = 1.9028 (2.259 sec/step)\n","I0830 19:22:03.455939 139622356395904 learning.py:507] global step 9767: loss = 2.0091 (1.212 sec/step)\n","I0830 19:22:04.692879 139622356395904 learning.py:507] global step 9768: loss = 2.3672 (1.234 sec/step)\n","I0830 19:22:05.952599 139622356395904 learning.py:507] global step 9769: loss = 2.6990 (1.256 sec/step)\n","I0830 19:22:07.179853 139622356395904 learning.py:507] global step 9770: loss = 2.1143 (1.225 sec/step)\n","I0830 19:22:08.419017 139622356395904 learning.py:507] global step 9771: loss = 2.3060 (1.237 sec/step)\n","I0830 19:22:09.636259 139622356395904 learning.py:507] global step 9772: loss = 2.4292 (1.215 sec/step)\n","I0830 19:22:10.911212 139622356395904 learning.py:507] global step 9773: loss = 2.3168 (1.273 sec/step)\n","I0830 19:22:12.121040 139622356395904 learning.py:507] global step 9774: loss = 2.1737 (1.208 sec/step)\n","I0830 19:22:13.331780 139622356395904 learning.py:507] global step 9775: loss = 2.5401 (1.209 sec/step)\n","I0830 19:22:14.592128 139622356395904 learning.py:507] global step 9776: loss = 2.4708 (1.258 sec/step)\n","I0830 19:22:15.817924 139622356395904 learning.py:507] global step 9777: loss = 2.7146 (1.224 sec/step)\n","I0830 19:22:17.037169 139622356395904 learning.py:507] global step 9778: loss = 2.4542 (1.217 sec/step)\n","I0830 19:22:18.279727 139622356395904 learning.py:507] global step 9779: loss = 2.6011 (1.240 sec/step)\n","I0830 19:22:19.516370 139622356395904 learning.py:507] global step 9780: loss = 2.6422 (1.235 sec/step)\n","I0830 19:22:20.773920 139622356395904 learning.py:507] global step 9781: loss = 2.8913 (1.256 sec/step)\n","I0830 19:22:22.021340 139622356395904 learning.py:507] global step 9782: loss = 2.7970 (1.245 sec/step)\n","I0830 19:22:23.276966 139622356395904 learning.py:507] global step 9783: loss = 2.8479 (1.253 sec/step)\n","I0830 19:22:24.534650 139622356395904 learning.py:507] global step 9784: loss = 2.8600 (1.256 sec/step)\n","I0830 19:22:25.740993 139622356395904 learning.py:507] global step 9785: loss = 2.9957 (1.204 sec/step)\n","I0830 19:22:26.946122 139622356395904 learning.py:507] global step 9786: loss = 2.7902 (1.203 sec/step)\n","I0830 19:22:28.205126 139622356395904 learning.py:507] global step 9787: loss = 2.3003 (1.257 sec/step)\n","I0830 19:22:29.470315 139622356395904 learning.py:507] global step 9788: loss = 2.1429 (1.263 sec/step)\n","I0830 19:22:30.713369 139622356395904 learning.py:507] global step 9789: loss = 2.3506 (1.238 sec/step)\n","I0830 19:22:31.914513 139622356395904 learning.py:507] global step 9790: loss = 2.0954 (1.199 sec/step)\n","I0830 19:22:33.190179 139622356395904 learning.py:507] global step 9791: loss = 2.3514 (1.273 sec/step)\n","I0830 19:22:34.420297 139622356395904 learning.py:507] global step 9792: loss = 2.4043 (1.228 sec/step)\n","I0830 19:22:35.645893 139622356395904 learning.py:507] global step 9793: loss = 2.2331 (1.224 sec/step)\n","I0830 19:22:36.865530 139622356395904 learning.py:507] global step 9794: loss = 2.1520 (1.218 sec/step)\n","I0830 19:22:38.107727 139622356395904 learning.py:507] global step 9795: loss = 2.5214 (1.240 sec/step)\n","I0830 19:22:39.361114 139622356395904 learning.py:507] global step 9796: loss = 2.7460 (1.251 sec/step)\n","I0830 19:22:40.574671 139622356395904 learning.py:507] global step 9797: loss = 2.6061 (1.212 sec/step)\n","I0830 19:22:41.803979 139622356395904 learning.py:507] global step 9798: loss = 2.0648 (1.227 sec/step)\n","I0830 19:22:43.048137 139622356395904 learning.py:507] global step 9799: loss = 2.4296 (1.242 sec/step)\n","I0830 19:22:44.290682 139622356395904 learning.py:507] global step 9800: loss = 2.6765 (1.241 sec/step)\n","I0830 19:22:45.530168 139622356395904 learning.py:507] global step 9801: loss = 2.0123 (1.237 sec/step)\n","I0830 19:22:46.747904 139622356395904 learning.py:507] global step 9802: loss = 2.0175 (1.216 sec/step)\n","I0830 19:22:47.971909 139622356395904 learning.py:507] global step 9803: loss = 2.8051 (1.222 sec/step)\n","I0830 19:22:49.208878 139622356395904 learning.py:507] global step 9804: loss = 1.9981 (1.235 sec/step)\n","I0830 19:22:50.430614 139622356395904 learning.py:507] global step 9805: loss = 3.2118 (1.220 sec/step)\n","I0830 19:22:51.659398 139622356395904 learning.py:507] global step 9806: loss = 2.5985 (1.227 sec/step)\n","I0830 19:22:52.868143 139622356395904 learning.py:507] global step 9807: loss = 2.3072 (1.207 sec/step)\n","I0830 19:22:54.099570 139622356395904 learning.py:507] global step 9808: loss = 2.4934 (1.230 sec/step)\n","I0830 19:22:55.356855 139622356395904 learning.py:507] global step 9809: loss = 2.9514 (1.255 sec/step)\n","I0830 19:22:56.628828 139622356395904 learning.py:507] global step 9810: loss = 1.8071 (1.270 sec/step)\n","I0830 19:22:57.879420 139622356395904 learning.py:507] global step 9811: loss = 2.4355 (1.249 sec/step)\n","I0830 19:22:59.105352 139622356395904 learning.py:507] global step 9812: loss = 2.5598 (1.224 sec/step)\n","I0830 19:23:00.317992 139622356395904 learning.py:507] global step 9813: loss = 2.1991 (1.210 sec/step)\n","I0830 19:23:01.573661 139622356395904 learning.py:507] global step 9814: loss = 2.6785 (1.253 sec/step)\n","I0830 19:23:02.817278 139622356395904 learning.py:507] global step 9815: loss = 3.4763 (1.242 sec/step)\n","I0830 19:23:04.038337 139622356395904 learning.py:507] global step 9816: loss = 2.8593 (1.219 sec/step)\n","I0830 19:23:05.253587 139622356395904 learning.py:507] global step 9817: loss = 1.9355 (1.213 sec/step)\n","I0830 19:23:06.515396 139622356395904 learning.py:507] global step 9818: loss = 2.6205 (1.260 sec/step)\n","I0830 19:23:07.764017 139622356395904 learning.py:507] global step 9819: loss = 2.3706 (1.247 sec/step)\n","I0830 19:23:08.947370 139622356395904 learning.py:507] global step 9820: loss = 2.8054 (1.182 sec/step)\n","I0830 19:23:10.183432 139622356395904 learning.py:507] global step 9821: loss = 2.1607 (1.234 sec/step)\n","I0830 19:23:11.400895 139622356395904 learning.py:507] global step 9822: loss = 4.6959 (1.216 sec/step)\n","I0830 19:23:12.657344 139622356395904 learning.py:507] global step 9823: loss = 2.7679 (1.254 sec/step)\n","I0830 19:23:13.905655 139622356395904 learning.py:507] global step 9824: loss = 2.0656 (1.246 sec/step)\n","I0830 19:23:15.130226 139622356395904 learning.py:507] global step 9825: loss = 3.2294 (1.222 sec/step)\n","I0830 19:23:16.375123 139622356395904 learning.py:507] global step 9826: loss = 2.8108 (1.243 sec/step)\n","I0830 19:23:17.638209 139622356395904 learning.py:507] global step 9827: loss = 3.0037 (1.261 sec/step)\n","I0830 19:23:18.879764 139622356395904 learning.py:507] global step 9828: loss = 2.8806 (1.240 sec/step)\n","I0830 19:23:20.098439 139622356395904 learning.py:507] global step 9829: loss = 3.0425 (1.217 sec/step)\n","I0830 19:23:21.293033 139622356395904 learning.py:507] global step 9830: loss = 2.5366 (1.193 sec/step)\n","I0830 19:23:22.544482 139622356395904 learning.py:507] global step 9831: loss = 3.1998 (1.250 sec/step)\n","I0830 19:23:23.773793 139622356395904 learning.py:507] global step 9832: loss = 2.6390 (1.227 sec/step)\n","I0830 19:23:24.976500 139622356395904 learning.py:507] global step 9833: loss = 3.1577 (1.201 sec/step)\n","I0830 19:23:26.217974 139622356395904 learning.py:507] global step 9834: loss = 2.4713 (1.240 sec/step)\n","I0830 19:23:27.459742 139622356395904 learning.py:507] global step 9835: loss = 2.5754 (1.240 sec/step)\n","I0830 19:23:28.711013 139622356395904 learning.py:507] global step 9836: loss = 2.4866 (1.249 sec/step)\n","I0830 19:23:29.955943 139622356395904 learning.py:507] global step 9837: loss = 2.2297 (1.243 sec/step)\n","I0830 19:23:31.195533 139622356395904 learning.py:507] global step 9838: loss = 2.8038 (1.237 sec/step)\n","I0830 19:23:32.430937 139622356395904 learning.py:507] global step 9839: loss = 2.7932 (1.233 sec/step)\n","I0830 19:23:33.662856 139622356395904 learning.py:507] global step 9840: loss = 2.1923 (1.230 sec/step)\n","I0830 19:23:34.877311 139622356395904 learning.py:507] global step 9841: loss = 3.3241 (1.213 sec/step)\n","I0830 19:23:36.103711 139622356395904 learning.py:507] global step 9842: loss = 2.8931 (1.224 sec/step)\n","I0830 19:23:37.332705 139622356395904 learning.py:507] global step 9843: loss = 2.0835 (1.227 sec/step)\n","I0830 19:23:38.581525 139622356395904 learning.py:507] global step 9844: loss = 2.6858 (1.247 sec/step)\n","I0830 19:23:39.824436 139622356395904 learning.py:507] global step 9845: loss = 2.5531 (1.241 sec/step)\n","I0830 19:23:41.055708 139622356395904 learning.py:507] global step 9846: loss = 2.2727 (1.229 sec/step)\n","I0830 19:23:42.280781 139622356395904 learning.py:507] global step 9847: loss = 2.7429 (1.223 sec/step)\n","I0830 19:23:43.511731 139622356395904 learning.py:507] global step 9848: loss = 2.0952 (1.229 sec/step)\n","I0830 19:23:44.762778 139622356395904 learning.py:507] global step 9849: loss = 2.2066 (1.249 sec/step)\n","I0830 19:23:46.031514 139622356395904 learning.py:507] global step 9850: loss = 2.3598 (1.267 sec/step)\n","I0830 19:23:47.256372 139622356395904 learning.py:507] global step 9851: loss = 2.4906 (1.223 sec/step)\n","I0830 19:23:48.500824 139622356395904 learning.py:507] global step 9852: loss = 1.7535 (1.242 sec/step)\n","I0830 19:23:49.767467 139622356395904 learning.py:507] global step 9853: loss = 2.3493 (1.264 sec/step)\n","I0830 19:23:51.029388 139622356395904 learning.py:507] global step 9854: loss = 2.4078 (1.260 sec/step)\n","I0830 19:23:52.237197 139622356395904 learning.py:507] global step 9855: loss = 2.7796 (1.206 sec/step)\n","I0830 19:23:53.454412 139622356395904 learning.py:507] global step 9856: loss = 2.1805 (1.215 sec/step)\n","I0830 19:23:54.657348 139622356395904 learning.py:507] global step 9857: loss = 2.4602 (1.201 sec/step)\n","I0830 19:23:55.891884 139622356395904 learning.py:507] global step 9858: loss = 4.6085 (1.233 sec/step)\n","I0830 19:23:57.113365 139622356395904 learning.py:507] global step 9859: loss = 2.2951 (1.220 sec/step)\n","I0830 19:23:58.329645 139622356395904 learning.py:507] global step 9860: loss = 2.0197 (1.214 sec/step)\n","I0830 19:23:59.568450 139622356395904 learning.py:507] global step 9861: loss = 1.9662 (1.237 sec/step)\n","I0830 19:24:01.361397 139622356395904 learning.py:507] global step 9862: loss = 2.1608 (1.650 sec/step)\n","I0830 19:24:02.524017 139619299985152 supervisor.py:1050] Recording summary at step 9862.\n","I0830 19:24:03.011139 139622356395904 learning.py:507] global step 9863: loss = 2.3490 (1.642 sec/step)\n","I0830 19:24:04.233586 139622356395904 learning.py:507] global step 9864: loss = 1.9583 (1.221 sec/step)\n","I0830 19:24:05.471193 139622356395904 learning.py:507] global step 9865: loss = 2.9147 (1.236 sec/step)\n","I0830 19:24:06.714368 139622356395904 learning.py:507] global step 9866: loss = 2.4330 (1.241 sec/step)\n","I0830 19:24:07.914334 139622356395904 learning.py:507] global step 9867: loss = 2.5436 (1.198 sec/step)\n","I0830 19:24:09.156415 139622356395904 learning.py:507] global step 9868: loss = 2.7446 (1.240 sec/step)\n","I0830 19:24:10.366353 139622356395904 learning.py:507] global step 9869: loss = 3.0358 (1.208 sec/step)\n","I0830 19:24:11.628500 139622356395904 learning.py:507] global step 9870: loss = 2.6631 (1.260 sec/step)\n","I0830 19:24:12.873763 139622356395904 learning.py:507] global step 9871: loss = 1.9546 (1.244 sec/step)\n","I0830 19:24:14.144814 139622356395904 learning.py:507] global step 9872: loss = 2.9571 (1.269 sec/step)\n","I0830 19:24:15.435575 139622356395904 learning.py:507] global step 9873: loss = 1.8010 (1.289 sec/step)\n","I0830 19:24:16.700230 139622356395904 learning.py:507] global step 9874: loss = 2.4252 (1.263 sec/step)\n","I0830 19:24:17.937938 139622356395904 learning.py:507] global step 9875: loss = 3.1717 (1.236 sec/step)\n","I0830 19:24:19.148299 139622356395904 learning.py:507] global step 9876: loss = 2.0243 (1.208 sec/step)\n","I0830 19:24:20.355819 139622356395904 learning.py:507] global step 9877: loss = 2.4645 (1.205 sec/step)\n","I0830 19:24:21.578644 139622356395904 learning.py:507] global step 9878: loss = 2.2406 (1.221 sec/step)\n","I0830 19:24:22.798294 139622356395904 learning.py:507] global step 9879: loss = 2.0549 (1.218 sec/step)\n","I0830 19:24:24.006574 139622356395904 learning.py:507] global step 9880: loss = 2.8109 (1.206 sec/step)\n","I0830 19:24:25.241806 139622356395904 learning.py:507] global step 9881: loss = 2.0267 (1.233 sec/step)\n","I0830 19:24:26.463038 139622356395904 learning.py:507] global step 9882: loss = 2.4222 (1.219 sec/step)\n","I0830 19:24:27.713537 139622356395904 learning.py:507] global step 9883: loss = 3.1439 (1.248 sec/step)\n","I0830 19:24:28.949712 139622356395904 learning.py:507] global step 9884: loss = 2.1614 (1.234 sec/step)\n","I0830 19:24:30.193278 139622356395904 learning.py:507] global step 9885: loss = 2.5387 (1.242 sec/step)\n","I0830 19:24:31.432987 139622356395904 learning.py:507] global step 9886: loss = 2.7726 (1.238 sec/step)\n","I0830 19:24:32.644637 139622356395904 learning.py:507] global step 9887: loss = 2.2303 (1.210 sec/step)\n","I0830 19:24:33.882853 139622356395904 learning.py:507] global step 9888: loss = 3.2490 (1.236 sec/step)\n","I0830 19:24:35.149877 139622356395904 learning.py:507] global step 9889: loss = 3.1541 (1.265 sec/step)\n","I0830 19:24:36.367770 139622356395904 learning.py:507] global step 9890: loss = 2.8356 (1.216 sec/step)\n","I0830 19:24:37.624507 139622356395904 learning.py:507] global step 9891: loss = 2.2380 (1.255 sec/step)\n","I0830 19:24:38.899876 139622356395904 learning.py:507] global step 9892: loss = 3.3777 (1.270 sec/step)\n","I0830 19:24:40.103951 139622356395904 learning.py:507] global step 9893: loss = 2.4324 (1.202 sec/step)\n","I0830 19:24:41.368269 139622356395904 learning.py:507] global step 9894: loss = 3.2723 (1.262 sec/step)\n","I0830 19:24:42.642536 139622356395904 learning.py:507] global step 9895: loss = 2.5284 (1.272 sec/step)\n","I0830 19:24:43.871753 139622356395904 learning.py:507] global step 9896: loss = 3.0614 (1.227 sec/step)\n","I0830 19:24:45.063842 139622356395904 learning.py:507] global step 9897: loss = 3.6135 (1.190 sec/step)\n","I0830 19:24:46.314749 139622356395904 learning.py:507] global step 9898: loss = 2.6670 (1.249 sec/step)\n","I0830 19:24:47.543620 139622356395904 learning.py:507] global step 9899: loss = 1.8434 (1.227 sec/step)\n","I0830 19:24:48.796972 139622356395904 learning.py:507] global step 9900: loss = 2.3425 (1.251 sec/step)\n","I0830 19:24:50.026321 139622356395904 learning.py:507] global step 9901: loss = 2.3784 (1.228 sec/step)\n","I0830 19:24:51.285004 139622356395904 learning.py:507] global step 9902: loss = 2.6800 (1.257 sec/step)\n","I0830 19:24:52.545272 139622356395904 learning.py:507] global step 9903: loss = 2.0648 (1.258 sec/step)\n","I0830 19:24:53.791336 139622356395904 learning.py:507] global step 9904: loss = 2.4135 (1.244 sec/step)\n","I0830 19:24:55.002584 139622356395904 learning.py:507] global step 9905: loss = 2.6584 (1.209 sec/step)\n","I0830 19:24:56.224340 139622356395904 learning.py:507] global step 9906: loss = 3.7148 (1.220 sec/step)\n","I0830 19:24:57.457040 139622356395904 learning.py:507] global step 9907: loss = 2.0733 (1.231 sec/step)\n","I0830 19:24:58.703620 139622356395904 learning.py:507] global step 9908: loss = 2.1083 (1.245 sec/step)\n","I0830 19:24:59.957564 139622356395904 learning.py:507] global step 9909: loss = 2.8679 (1.252 sec/step)\n","I0830 19:25:01.207602 139622356395904 learning.py:507] global step 9910: loss = 2.8717 (1.248 sec/step)\n","I0830 19:25:02.434865 139622356395904 learning.py:507] global step 9911: loss = 2.0342 (1.225 sec/step)\n","I0830 19:25:03.688395 139622356395904 learning.py:507] global step 9912: loss = 2.5902 (1.252 sec/step)\n","I0830 19:25:04.931959 139622356395904 learning.py:507] global step 9913: loss = 2.5070 (1.242 sec/step)\n","I0830 19:25:06.157076 139622356395904 learning.py:507] global step 9914: loss = 3.3560 (1.223 sec/step)\n","I0830 19:25:07.423930 139622356395904 learning.py:507] global step 9915: loss = 2.4559 (1.265 sec/step)\n","I0830 19:25:08.669261 139622356395904 learning.py:507] global step 9916: loss = 2.5761 (1.243 sec/step)\n","I0830 19:25:09.902222 139622356395904 learning.py:507] global step 9917: loss = 1.9854 (1.231 sec/step)\n","I0830 19:25:11.126366 139622356395904 learning.py:507] global step 9918: loss = 2.6447 (1.222 sec/step)\n","I0830 19:25:12.358423 139622356395904 learning.py:507] global step 9919: loss = 2.3103 (1.230 sec/step)\n","I0830 19:25:13.629388 139622356395904 learning.py:507] global step 9920: loss = 2.9071 (1.267 sec/step)\n","I0830 19:25:14.828347 139622356395904 learning.py:507] global step 9921: loss = 2.3343 (1.196 sec/step)\n","I0830 19:25:16.041770 139622356395904 learning.py:507] global step 9922: loss = 3.6639 (1.212 sec/step)\n","I0830 19:25:17.282934 139622356395904 learning.py:507] global step 9923: loss = 2.8248 (1.239 sec/step)\n","I0830 19:25:18.524591 139622356395904 learning.py:507] global step 9924: loss = 2.9014 (1.240 sec/step)\n","I0830 19:25:19.738558 139622356395904 learning.py:507] global step 9925: loss = 2.9130 (1.212 sec/step)\n","I0830 19:25:20.971446 139622356395904 learning.py:507] global step 9926: loss = 2.0184 (1.231 sec/step)\n","I0830 19:25:22.171533 139622356395904 learning.py:507] global step 9927: loss = 2.3942 (1.198 sec/step)\n","I0830 19:25:23.381740 139622356395904 learning.py:507] global step 9928: loss = 2.6024 (1.208 sec/step)\n","I0830 19:25:24.595740 139622356395904 learning.py:507] global step 9929: loss = 1.8989 (1.212 sec/step)\n","I0830 19:25:25.800837 139622356395904 learning.py:507] global step 9930: loss = 2.7003 (1.203 sec/step)\n","I0830 19:25:27.053314 139622356395904 learning.py:507] global step 9931: loss = 3.1793 (1.250 sec/step)\n","I0830 19:25:28.311219 139622356395904 learning.py:507] global step 9932: loss = 2.9598 (1.256 sec/step)\n","I0830 19:25:29.526422 139622356395904 learning.py:507] global step 9933: loss = 2.2575 (1.214 sec/step)\n","I0830 19:25:30.768981 139622356395904 learning.py:507] global step 9934: loss = 1.9949 (1.241 sec/step)\n","I0830 19:25:31.985673 139622356395904 learning.py:507] global step 9935: loss = 3.2570 (1.215 sec/step)\n","I0830 19:25:33.214431 139622356395904 learning.py:507] global step 9936: loss = 2.5056 (1.227 sec/step)\n","I0830 19:25:34.499653 139622356395904 learning.py:507] global step 9937: loss = 2.8699 (1.283 sec/step)\n","I0830 19:25:35.723363 139622356395904 learning.py:507] global step 9938: loss = 2.6819 (1.222 sec/step)\n","I0830 19:25:36.950975 139622356395904 learning.py:507] global step 9939: loss = 2.5410 (1.226 sec/step)\n","I0830 19:25:38.184271 139622356395904 learning.py:507] global step 9940: loss = 3.3762 (1.231 sec/step)\n","I0830 19:25:39.408531 139622356395904 learning.py:507] global step 9941: loss = 2.2550 (1.222 sec/step)\n","I0830 19:25:40.616841 139622356395904 learning.py:507] global step 9942: loss = 2.7575 (1.206 sec/step)\n","I0830 19:25:41.858932 139622356395904 learning.py:507] global step 9943: loss = 2.6369 (1.240 sec/step)\n","I0830 19:25:43.077500 139622356395904 learning.py:507] global step 9944: loss = 1.9075 (1.216 sec/step)\n","I0830 19:25:44.331127 139622356395904 learning.py:507] global step 9945: loss = 2.2919 (1.252 sec/step)\n","I0830 19:25:45.565613 139622356395904 learning.py:507] global step 9946: loss = 2.3145 (1.233 sec/step)\n","I0830 19:25:46.801187 139622356395904 learning.py:507] global step 9947: loss = 3.6909 (1.234 sec/step)\n","I0830 19:25:48.014555 139622356395904 learning.py:507] global step 9948: loss = 2.9357 (1.211 sec/step)\n","I0830 19:25:49.231373 139622356395904 learning.py:507] global step 9949: loss = 2.1395 (1.215 sec/step)\n","I0830 19:25:50.470110 139622356395904 learning.py:507] global step 9950: loss = 2.3295 (1.237 sec/step)\n","I0830 19:25:51.696636 139622356395904 learning.py:507] global step 9951: loss = 2.3447 (1.225 sec/step)\n","I0830 19:25:52.949997 139622356395904 learning.py:507] global step 9952: loss = 1.9757 (1.252 sec/step)\n","I0830 19:25:54.181654 139622356395904 learning.py:507] global step 9953: loss = 1.9128 (1.230 sec/step)\n","I0830 19:25:55.424482 139622356395904 learning.py:507] global step 9954: loss = 2.9397 (1.241 sec/step)\n","I0830 19:25:56.645305 139622356395904 learning.py:507] global step 9955: loss = 1.9458 (1.219 sec/step)\n","I0830 19:25:57.882396 139622356395904 learning.py:507] global step 9956: loss = 2.4633 (1.235 sec/step)\n","I0830 19:25:59.142143 139622356395904 learning.py:507] global step 9957: loss = 2.1925 (1.258 sec/step)\n","I0830 19:26:00.572971 139622356395904 learning.py:507] global step 9958: loss = 2.6320 (1.360 sec/step)\n","I0830 19:26:02.582712 139619299985152 supervisor.py:1050] Recording summary at step 9959.\n","I0830 19:26:02.626794 139622356395904 learning.py:507] global step 9959: loss = 2.1744 (2.052 sec/step)\n","I0830 19:26:03.864500 139622356395904 learning.py:507] global step 9960: loss = 3.4131 (1.236 sec/step)\n","I0830 19:26:05.105199 139622356395904 learning.py:507] global step 9961: loss = 2.8500 (1.239 sec/step)\n","I0830 19:26:06.299088 139622356395904 learning.py:507] global step 9962: loss = 1.9191 (1.189 sec/step)\n","I0830 19:26:07.511415 139622356395904 learning.py:507] global step 9963: loss = 2.9618 (1.210 sec/step)\n","I0830 19:26:08.728798 139622356395904 learning.py:507] global step 9964: loss = 2.4203 (1.215 sec/step)\n","I0830 19:26:09.952576 139622356395904 learning.py:507] global step 9965: loss = 2.0044 (1.222 sec/step)\n","I0830 19:26:11.188668 139622356395904 learning.py:507] global step 9966: loss = 2.4781 (1.234 sec/step)\n","I0830 19:26:12.431992 139622356395904 learning.py:507] global step 9967: loss = 2.3058 (1.242 sec/step)\n","I0830 19:26:13.650696 139622356395904 learning.py:507] global step 9968: loss = 2.0481 (1.217 sec/step)\n","I0830 19:26:14.862330 139622356395904 learning.py:507] global step 9969: loss = 2.1691 (1.206 sec/step)\n","I0830 19:26:16.119772 139622356395904 learning.py:507] global step 9970: loss = 2.6200 (1.256 sec/step)\n","I0830 19:26:17.359486 139622356395904 learning.py:507] global step 9971: loss = 2.2384 (1.238 sec/step)\n","I0830 19:26:18.576902 139622356395904 learning.py:507] global step 9972: loss = 2.0467 (1.215 sec/step)\n","I0830 19:26:19.808614 139622356395904 learning.py:507] global step 9973: loss = 2.4365 (1.230 sec/step)\n","I0830 19:26:21.015038 139622356395904 learning.py:507] global step 9974: loss = 2.2791 (1.204 sec/step)\n","I0830 19:26:22.248828 139622356395904 learning.py:507] global step 9975: loss = 1.6901 (1.232 sec/step)\n","I0830 19:26:23.480420 139622356395904 learning.py:507] global step 9976: loss = 2.8772 (1.230 sec/step)\n","I0830 19:26:24.734083 139622356395904 learning.py:507] global step 9977: loss = 2.4632 (1.252 sec/step)\n","I0830 19:26:25.973855 139622356395904 learning.py:507] global step 9978: loss = 2.6073 (1.238 sec/step)\n","I0830 19:26:27.214942 139622356395904 learning.py:507] global step 9979: loss = 2.1680 (1.239 sec/step)\n","I0830 19:26:28.480441 139622356395904 learning.py:507] global step 9980: loss = 2.1657 (1.263 sec/step)\n","I0830 19:26:29.743691 139622356395904 learning.py:507] global step 9981: loss = 2.6682 (1.261 sec/step)\n","I0830 19:26:30.956374 139622356395904 learning.py:507] global step 9982: loss = 2.6526 (1.210 sec/step)\n","I0830 19:26:32.187891 139622356395904 learning.py:507] global step 9983: loss = 2.0709 (1.230 sec/step)\n","I0830 19:26:33.397696 139622356395904 learning.py:507] global step 9984: loss = 3.2021 (1.208 sec/step)\n","I0830 19:26:34.616209 139622356395904 learning.py:507] global step 9985: loss = 2.2632 (1.216 sec/step)\n","I0830 19:26:35.868234 139622356395904 learning.py:507] global step 9986: loss = 2.0301 (1.250 sec/step)\n","I0830 19:26:37.138912 139622356395904 learning.py:507] global step 9987: loss = 2.6109 (1.269 sec/step)\n","I0830 19:26:38.342150 139622356395904 learning.py:507] global step 9988: loss = 2.2720 (1.201 sec/step)\n","I0830 19:26:39.537320 139622356395904 learning.py:507] global step 9989: loss = 2.1511 (1.193 sec/step)\n","I0830 19:26:40.748653 139622356395904 learning.py:507] global step 9990: loss = 2.1706 (1.209 sec/step)\n","I0830 19:26:41.965040 139622356395904 learning.py:507] global step 9991: loss = 1.8296 (1.214 sec/step)\n","I0830 19:26:43.173410 139622356395904 learning.py:507] global step 9992: loss = 2.3449 (1.207 sec/step)\n","I0830 19:26:44.395601 139622356395904 learning.py:507] global step 9993: loss = 3.2132 (1.220 sec/step)\n","I0830 19:26:45.629348 139622356395904 learning.py:507] global step 9994: loss = 2.5747 (1.232 sec/step)\n","I0830 19:26:46.839337 139622356395904 learning.py:507] global step 9995: loss = 2.6785 (1.208 sec/step)\n","I0830 19:26:48.053848 139622356395904 learning.py:507] global step 9996: loss = 2.3191 (1.213 sec/step)\n","I0830 19:26:49.284697 139622356395904 learning.py:507] global step 9997: loss = 2.1008 (1.229 sec/step)\n","I0830 19:26:50.482139 139622356395904 learning.py:507] global step 9998: loss = 2.3571 (1.196 sec/step)\n","I0830 19:26:51.714398 139622356395904 learning.py:507] global step 9999: loss = 3.1429 (1.231 sec/step)\n","I0830 19:26:52.942342 139622356395904 learning.py:507] global step 10000: loss = 3.1804 (1.226 sec/step)\n","I0830 19:26:52.943267 139622356395904 learning.py:777] Stopping Training.\n","I0830 19:26:52.943437 139622356395904 learning.py:785] Finished training! Saving model to disk.\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/summary/writer/writer.py:386: UserWarning: Attempting to use a closed FileWriter. The operation will be a noop unless the FileWriter is explicitly reopened.\n","  warnings.warn(\"Attempting to use a closed FileWriter. \"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"opLdQpcyy7KG","colab_type":"code","outputId":"d660c9e4-6105-4ffb-896e-85cfa2a7cd09","executionInfo":{"status":"ok","timestamp":1566300705004,"user_tz":-330,"elapsed":1255,"user":{"displayName":"Kapil Bansal","photoUrl":"https://lh6.googleusercontent.com/-e1ccxSHV7oc/AAAAAAAAAAI/AAAAAAAAABE/wGCXdBwsmhQ/s64/photo.jpg","userId":"08400742664448331672"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/root\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DuvuwPdStk6j","colab_type":"code","outputId":"42abfeda-720d-41c8-bca4-47d161642a83","executionInfo":{"status":"ok","timestamp":1562592037440,"user_tz":-330,"elapsed":75108,"user":{"displayName":"vishal sharma","photoUrl":"https://lh5.googleusercontent.com/-1owwVkjENpM/AAAAAAAAAAI/AAAAAAAADys/clXU8JIWDhg/s64/photo.jpg","userId":"03634247771647957761"}},"colab":{"base_uri":"https://localhost:8080/","height":425}},"source":["!zip -r file.zip /root/models/research/object_detection/training"],"execution_count":0,"outputs":[{"output_type":"stream","text":["  adding: root/models/research/object_detection/training/ (stored 0%)\n","  adding: root/models/research/object_detection/training/ssd_inception_v2_coco.config (deflated 66%)\n","  adding: root/models/research/object_detection/training/model.ckpt-13440.index (deflated 73%)\n","  adding: root/models/research/object_detection/training/graph.pbtxt (deflated 97%)\n","  adding: root/models/research/object_detection/training/checkpoint (deflated 72%)\n","  adding: root/models/research/object_detection/training/model.ckpt-14708.meta (deflated 94%)\n","  adding: root/models/research/object_detection/training/model.ckpt-13440.data-00000-of-00001 (deflated 7%)\n","  adding: root/models/research/object_detection/training/model.ckpt-13865.index (deflated 73%)\n","  adding: root/models/research/object_detection/training/model.ckpt-13865.data-00000-of-00001 (deflated 7%)\n","  adding: root/models/research/object_detection/training/model.ckpt-15131.data-00000-of-00001 (deflated 7%)\n","  adding: root/models/research/object_detection/training/model.ckpt-15131.index (deflated 73%)\n","  adding: root/models/research/object_detection/training/model.ckpt-13440.meta (deflated 94%)\n","  adding: root/models/research/object_detection/training/model.ckpt-14292.index (deflated 73%)\n","  adding: root/models/research/object_detection/training/model.ckpt-14708.data-00000-of-00001 (deflated 7%)\n","  adding: root/models/research/object_detection/training/model.ckpt-14708.index (deflated 73%)\n","  adding: root/models/research/object_detection/training/events.out.tfevents.1562582904.1d237a02e233 (deflated 89%)\n","  adding: root/models/research/object_detection/training/model.ckpt-14292.meta (deflated 94%)\n","  adding: root/models/research/object_detection/training/model.ckpt-15131.meta (deflated 94%)\n","  adding: root/models/research/object_detection/training/model.ckpt-13865.meta (deflated 94%)\n","  adding: root/models/research/object_detection/training/model.ckpt-14292.data-00000-of-00001 (deflated 7%)\n","  adding: root/models/research/object_detection/training/labelmap.pbtxt (deflated 49%)\n","  adding: root/models/research/object_detection/training/.ipynb_checkpoints/ (stored 0%)\n","  adding: root/models/research/object_detection/training/events.out.tfevents.1562564733.1d237a02e233 (deflated 87%)\n","  adding: root/models/research/object_detection/training/pipeline.config (deflated 66%)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"83osrMiCvsG1","colab_type":"code","outputId":"322dff36-f51b-4733-b0f3-116c9b7ac88f","executionInfo":{"status":"ok","timestamp":1562592216072,"user_tz":-330,"elapsed":157007,"user":{"displayName":"vishal sharma","photoUrl":"https://lh5.googleusercontent.com/-1owwVkjENpM/AAAAAAAAAAI/AAAAAAAADys/clXU8JIWDhg/s64/photo.jpg","userId":"03634247771647957761"}},"colab":{"base_uri":"https://localhost:8080/","height":442}},"source":["from google.colab import files\n","files.download('file.zip')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["----------------------------------------\n","Exception happened during processing of request from ('::ffff:127.0.0.1', 35446, 0, 0)\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n","    self.process_request(request, client_address)\n","  File \"/usr/lib/python3.6/socketserver.py\", line 351, in process_request\n","    self.finish_request(request, client_address)\n","  File \"/usr/lib/python3.6/socketserver.py\", line 364, in finish_request\n","    self.RequestHandlerClass(request, client_address, self)\n","  File \"/usr/lib/python3.6/socketserver.py\", line 724, in __init__\n","    self.handle()\n","  File \"/usr/lib/python3.6/http/server.py\", line 418, in handle\n","    self.handle_one_request()\n","  File \"/usr/lib/python3.6/http/server.py\", line 406, in handle_one_request\n","    method()\n","  File \"/usr/lib/python3.6/http/server.py\", line 639, in do_GET\n","    self.copyfile(f, self.wfile)\n","  File \"/usr/lib/python3.6/http/server.py\", line 800, in copyfile\n","    shutil.copyfileobj(source, outputfile)\n","  File \"/usr/lib/python3.6/shutil.py\", line 82, in copyfileobj\n","    fdst.write(buf)\n","  File \"/usr/lib/python3.6/socketserver.py\", line 803, in write\n","    self._sock.sendall(b)\n","ConnectionResetError: [Errno 104] Connection reset by peer\n","----------------------------------------\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"JOeFYGsSPG2s","colab_type":"code","outputId":"1e99e1e4-1ce1-4c7c-9154-2f49a1932d67","executionInfo":{"status":"ok","timestamp":1567193868007,"user_tz":-330,"elapsed":25333,"user":{"displayName":"Ravi Pawar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBtV9qOP0yVjG8p6Z6OsNi6vPiuKPc2AKMnCJqR=s64","userId":"00073904877215451927"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#Exporting Inference Graph\n","%cd ~/models/research/object_detection\n","!rm -rf inference_graph\n","!mkdir inference_graph\n","!python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssd_inception_v2_coco.config --trained_checkpoint_prefix training/model.ckpt-9669 --output_directory inference_graph "],"execution_count":31,"outputs":[{"output_type":"stream","text":["/root/models/research/object_detection\n","WARNING: Logging before flag parsing goes to stderr.\n","W0830 19:37:32.114604 139977685026688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/slim-0.1-py3.6.egg/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","W0830 19:37:32.163542 139977685026688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/slim-0.1-py3.6.egg/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n","\n","W0830 19:37:32.179861 139977685026688 deprecation_wrapper.py:119] From export_inference_graph.py:162: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n","\n","W0830 19:37:32.180586 139977685026688 deprecation_wrapper.py:119] From export_inference_graph.py:145: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","W0830 19:37:32.187111 139977685026688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:381: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n","\n","W0830 19:37:32.187322 139977685026688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:113: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0830 19:37:32.228631 139977685026688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2660: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n","\n","W0830 19:37:32.265201 139977685026688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/meta_architectures/ssd_meta_arch.py:575: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","W0830 19:37:32.266997 139977685026688 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","W0830 19:37:36.751518 139977685026688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/anchor_generator.py:154: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n","\n","W0830 19:37:36.770363 139977685026688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n","\n","I0830 19:37:36.770508 139977685026688 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n","I0830 19:37:36.824436 139977685026688 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n","I0830 19:37:36.879337 139977685026688 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n","I0830 19:37:36.933683 139977685026688 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n","I0830 19:37:36.994672 139977685026688 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n","I0830 19:37:37.056447 139977685026688 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n","W0830 19:37:37.549253 139977685026688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/post_processing.py:567: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","W0830 19:37:38.005416 139977685026688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:260: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n","\n","W0830 19:37:38.005765 139977685026688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:362: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please switch to tf.train.get_or_create_global_step\n","W0830 19:37:38.009949 139977685026688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:518: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n","Instructions for updating:\n","Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n","W0830 19:37:38.011391 139977685026688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n","169 ops no flops stats due to incomplete shapes.\n","Parsing Inputs...\n","Incomplete shape.\n","\n","=========================Options=============================\n","-max_depth                  10000\n","-min_bytes                  0\n","-min_peak_bytes             0\n","-min_residual_bytes         0\n","-min_output_bytes           0\n","-min_micros                 0\n","-min_accelerator_micros     0\n","-min_cpu_micros             0\n","-min_params                 0\n","-min_float_ops              0\n","-min_occurrence             0\n","-step                       -1\n","-order_by                   name\n","-account_type_regexes       _trainable_variables\n","-start_name_regexes         .*\n","-trim_name_regexes          .*BatchNorm.*\n","-show_name_regexes          .*\n","-hide_name_regexes          \n","-account_displayed_op_only  true\n","-select                     params\n","-output                     stdout:\n","\n","==================Model Analysis Report======================\n","Incomplete shape.\n","\n","Doc:\n","scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n","param: Number of parameters (in the Variable).\n","\n","Profile:\n","node name | # parameters\n","_TFProfRoot (--/13.30m params)\n","  BoxPredictor_0 (--/108.89k params)\n","    BoxPredictor_0/BoxEncodingPredictor (--/62.22k params)\n","      BoxPredictor_0/BoxEncodingPredictor/biases (12, 12/12 params)\n","      BoxPredictor_0/BoxEncodingPredictor/weights (3x3x576x12, 62.21k/62.21k params)\n","    BoxPredictor_0/ClassPredictor (--/46.66k params)\n","      BoxPredictor_0/ClassPredictor/biases (9, 9/9 params)\n","      BoxPredictor_0/ClassPredictor/weights (3x3x576x9, 46.66k/46.66k params)\n","  BoxPredictor_1 (--/387.11k params)\n","    BoxPredictor_1/BoxEncodingPredictor (--/221.21k params)\n","      BoxPredictor_1/BoxEncodingPredictor/biases (24, 24/24 params)\n","      BoxPredictor_1/BoxEncodingPredictor/weights (3x3x1024x24, 221.18k/221.18k params)\n","    BoxPredictor_1/ClassPredictor (--/165.91k params)\n","      BoxPredictor_1/ClassPredictor/biases (18, 18/18 params)\n","      BoxPredictor_1/ClassPredictor/weights (3x3x1024x18, 165.89k/165.89k params)\n","  BoxPredictor_2 (--/193.58k params)\n","    BoxPredictor_2/BoxEncodingPredictor (--/110.62k params)\n","      BoxPredictor_2/BoxEncodingPredictor/biases (24, 24/24 params)\n","      BoxPredictor_2/BoxEncodingPredictor/weights (3x3x512x24, 110.59k/110.59k params)\n","    BoxPredictor_2/ClassPredictor (--/82.96k params)\n","      BoxPredictor_2/ClassPredictor/biases (18, 18/18 params)\n","      BoxPredictor_2/ClassPredictor/weights (3x3x512x18, 82.94k/82.94k params)\n","  BoxPredictor_3 (--/96.81k params)\n","    BoxPredictor_3/BoxEncodingPredictor (--/55.32k params)\n","      BoxPredictor_3/BoxEncodingPredictor/biases (24, 24/24 params)\n","      BoxPredictor_3/BoxEncodingPredictor/weights (3x3x256x24, 55.30k/55.30k params)\n","    BoxPredictor_3/ClassPredictor (--/41.49k params)\n","      BoxPredictor_3/ClassPredictor/biases (18, 18/18 params)\n","      BoxPredictor_3/ClassPredictor/weights (3x3x256x18, 41.47k/41.47k params)\n","  BoxPredictor_4 (--/96.81k params)\n","    BoxPredictor_4/BoxEncodingPredictor (--/55.32k params)\n","      BoxPredictor_4/BoxEncodingPredictor/biases (24, 24/24 params)\n","      BoxPredictor_4/BoxEncodingPredictor/weights (3x3x256x24, 55.30k/55.30k params)\n","    BoxPredictor_4/ClassPredictor (--/41.49k params)\n","      BoxPredictor_4/ClassPredictor/biases (18, 18/18 params)\n","      BoxPredictor_4/ClassPredictor/weights (3x3x256x18, 41.47k/41.47k params)\n","  BoxPredictor_5 (--/48.43k params)\n","    BoxPredictor_5/BoxEncodingPredictor (--/27.67k params)\n","      BoxPredictor_5/BoxEncodingPredictor/biases (24, 24/24 params)\n","      BoxPredictor_5/BoxEncodingPredictor/weights (3x3x128x24, 27.65k/27.65k params)\n","    BoxPredictor_5/ClassPredictor (--/20.75k params)\n","      BoxPredictor_5/ClassPredictor/biases (18, 18/18 params)\n","      BoxPredictor_5/ClassPredictor/weights (3x3x128x18, 20.74k/20.74k params)\n","  FeatureExtractor (--/12.36m params)\n","    FeatureExtractor/InceptionV2 (--/12.36m params)\n","      FeatureExtractor/InceptionV2/Conv2d_1a_7x7 (--/2.71k params)\n","        FeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm (--/0 params)\n","        FeatureExtractor/InceptionV2/Conv2d_1a_7x7/depthwise_weights (7x7x3x8, 1.18k/1.18k params)\n","        FeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights (1x1x24x64, 1.54k/1.54k params)\n","      FeatureExtractor/InceptionV2/Conv2d_2b_1x1 (--/4.10k params)\n","        FeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm (--/0 params)\n","        FeatureExtractor/InceptionV2/Conv2d_2b_1x1/weights (1x1x64x64, 4.10k/4.10k params)\n","      FeatureExtractor/InceptionV2/Conv2d_2c_3x3 (--/110.59k params)\n","        FeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm (--/0 params)\n","        FeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights (3x3x64x192, 110.59k/110.59k params)\n","      FeatureExtractor/InceptionV2/Mixed_3b (--/218.11k params)\n","        FeatureExtractor/InceptionV2/Mixed_3b/Branch_0 (--/12.29k params)\n","          FeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1 (--/12.29k params)\n","            FeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/weights (1x1x192x64, 12.29k/12.29k params)\n","        FeatureExtractor/InceptionV2/Mixed_3b/Branch_1 (--/49.15k params)\n","          FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1 (--/12.29k params)\n","            FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/weights (1x1x192x64, 12.29k/12.29k params)\n","          FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3 (--/36.86k params)\n","            FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/weights (3x3x64x64, 36.86k/36.86k params)\n","        FeatureExtractor/InceptionV2/Mixed_3b/Branch_2 (--/150.53k params)\n","          FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1 (--/12.29k params)\n","            FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/weights (1x1x192x64, 12.29k/12.29k params)\n","          FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3 (--/55.30k params)\n","            FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n","          FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3 (--/82.94k params)\n","            FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/weights (3x3x96x96, 82.94k/82.94k params)\n","        FeatureExtractor/InceptionV2/Mixed_3b/Branch_3 (--/6.14k params)\n","          FeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1 (--/6.14k params)\n","            FeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/weights (1x1x192x32, 6.14k/6.14k params)\n","      FeatureExtractor/InceptionV2/Mixed_3c (--/259.07k params)\n","        FeatureExtractor/InceptionV2/Mixed_3c/Branch_0 (--/16.38k params)\n","          FeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1 (--/16.38k params)\n","            FeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/weights (1x1x256x64, 16.38k/16.38k params)\n","        FeatureExtractor/InceptionV2/Mixed_3c/Branch_1 (--/71.68k params)\n","          FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1 (--/16.38k params)\n","            FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/weights (1x1x256x64, 16.38k/16.38k params)\n","          FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3 (--/55.30k params)\n","            FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n","        FeatureExtractor/InceptionV2/Mixed_3c/Branch_2 (--/154.62k params)\n","          FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1 (--/16.38k params)\n","            FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/weights (1x1x256x64, 16.38k/16.38k params)\n","          FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3 (--/55.30k params)\n","            FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n","          FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3 (--/82.94k params)\n","            FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/weights (3x3x96x96, 82.94k/82.94k params)\n","        FeatureExtractor/InceptionV2/Mixed_3c/Branch_3 (--/16.38k params)\n","          FeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1 (--/16.38k params)\n","            FeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/weights (1x1x256x64, 16.38k/16.38k params)\n","      FeatureExtractor/InceptionV2/Mixed_4a (--/384.00k params)\n","        FeatureExtractor/InceptionV2/Mixed_4a/Branch_0 (--/225.28k params)\n","          FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1 (--/40.96k params)\n","            FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/weights (1x1x320x128, 40.96k/40.96k params)\n","          FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3 (--/184.32k params)\n","            FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/weights (3x3x128x160, 184.32k/184.32k params)\n","        FeatureExtractor/InceptionV2/Mixed_4a/Branch_1 (--/158.72k params)\n","          FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1 (--/20.48k params)\n","            FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/weights (1x1x320x64, 20.48k/20.48k params)\n","          FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3 (--/55.30k params)\n","            FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n","          FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3 (--/82.94k params)\n","            FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/weights (3x3x96x96, 82.94k/82.94k params)\n","      FeatureExtractor/InceptionV2/Mixed_4b (--/608.26k params)\n","        FeatureExtractor/InceptionV2/Mixed_4b/Branch_0 (--/129.02k params)\n","          FeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1 (--/129.02k params)\n","            FeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/weights (1x1x576x224, 129.02k/129.02k params)\n","        FeatureExtractor/InceptionV2/Mixed_4b/Branch_1 (--/92.16k params)\n","          FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1 (--/36.86k params)\n","            FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/weights (1x1x576x64, 36.86k/36.86k params)\n","          FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3 (--/55.30k params)\n","            FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n","        FeatureExtractor/InceptionV2/Mixed_4b/Branch_2 (--/313.34k params)\n","          FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1 (--/55.30k params)\n","            FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n","          FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3 (--/110.59k params)\n","            FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/weights (3x3x96x128, 110.59k/110.59k params)\n","          FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3 (--/147.46k params)\n","            FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/weights (3x3x128x128, 147.46k/147.46k params)\n","        FeatureExtractor/InceptionV2/Mixed_4b/Branch_3 (--/73.73k params)\n","          FeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1 (--/73.73k params)\n","            FeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n","      FeatureExtractor/InceptionV2/Mixed_4c (--/663.55k params)\n","        FeatureExtractor/InceptionV2/Mixed_4c/Branch_0 (--/110.59k params)\n","          FeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1 (--/110.59k params)\n","            FeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/weights (1x1x576x192, 110.59k/110.59k params)\n","        FeatureExtractor/InceptionV2/Mixed_4c/Branch_1 (--/165.89k params)\n","          FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1 (--/55.30k params)\n","            FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n","          FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3 (--/110.59k params)\n","            FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/weights (3x3x96x128, 110.59k/110.59k params)\n","        FeatureExtractor/InceptionV2/Mixed_4c/Branch_2 (--/313.34k params)\n","          FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1 (--/55.30k params)\n","            FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n","          FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3 (--/110.59k params)\n","            FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/weights (3x3x96x128, 110.59k/110.59k params)\n","          FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3 (--/147.46k params)\n","            FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/weights (3x3x128x128, 147.46k/147.46k params)\n","        FeatureExtractor/InceptionV2/Mixed_4c/Branch_3 (--/73.73k params)\n","          FeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1 (--/73.73k params)\n","            FeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n","      FeatureExtractor/InceptionV2/Mixed_4d (--/893.95k params)\n","        FeatureExtractor/InceptionV2/Mixed_4d/Branch_0 (--/92.16k params)\n","          FeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1 (--/92.16k params)\n","            FeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/weights (1x1x576x160, 92.16k/92.16k params)\n","        FeatureExtractor/InceptionV2/Mixed_4d/Branch_1 (--/258.05k params)\n","          FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1 (--/73.73k params)\n","            FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n","          FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3 (--/184.32k params)\n","            FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/weights (3x3x128x160, 184.32k/184.32k params)\n","        FeatureExtractor/InceptionV2/Mixed_4d/Branch_2 (--/488.45k params)\n","          FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1 (--/73.73k params)\n","            FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n","          FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3 (--/184.32k params)\n","            FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/weights (3x3x128x160, 184.32k/184.32k params)\n","          FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3 (--/230.40k params)\n","            FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/weights (3x3x160x160, 230.40k/230.40k params)\n","        FeatureExtractor/InceptionV2/Mixed_4d/Branch_3 (--/55.30k params)\n","          FeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1 (--/55.30k params)\n","            FeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n","      FeatureExtractor/InceptionV2/Mixed_4e (--/1.11m params)\n","        FeatureExtractor/InceptionV2/Mixed_4e/Branch_0 (--/55.30k params)\n","          FeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1 (--/55.30k params)\n","            FeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n","        FeatureExtractor/InceptionV2/Mixed_4e/Branch_1 (--/294.91k params)\n","          FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1 (--/73.73k params)\n","            FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n","          FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3 (--/221.18k params)\n","            FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/weights (3x3x128x192, 221.18k/221.18k params)\n","        FeatureExtractor/InceptionV2/Mixed_4e/Branch_2 (--/700.42k params)\n","          FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1 (--/92.16k params)\n","            FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/weights (1x1x576x160, 92.16k/92.16k params)\n","          FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3 (--/276.48k params)\n","            FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/weights (3x3x160x192, 276.48k/276.48k params)\n","          FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3 (--/331.78k params)\n","            FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/weights (3x3x192x192, 331.78k/331.78k params)\n","        FeatureExtractor/InceptionV2/Mixed_4e/Branch_3 (--/55.30k params)\n","          FeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1 (--/55.30k params)\n","            FeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n","      FeatureExtractor/InceptionV2/Mixed_5a (--/1.44m params)\n","        FeatureExtractor/InceptionV2/Mixed_5a/Branch_0 (--/294.91k params)\n","          FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1 (--/73.73k params)\n","            FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n","          FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3 (--/221.18k params)\n","            FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/weights (3x3x128x192, 221.18k/221.18k params)\n","        FeatureExtractor/InceptionV2/Mixed_5a/Branch_1 (--/1.14m params)\n","          FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1 (--/110.59k params)\n","            FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/weights (1x1x576x192, 110.59k/110.59k params)\n","          FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3 (--/442.37k params)\n","            FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/weights (3x3x192x256, 442.37k/442.37k params)\n","          FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3 (--/589.82k params)\n","            FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/weights (3x3x256x256, 589.82k/589.82k params)\n","      FeatureExtractor/InceptionV2/Mixed_5b (--/2.18m params)\n","        FeatureExtractor/InceptionV2/Mixed_5b/Branch_0 (--/360.45k params)\n","          FeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1 (--/360.45k params)\n","            FeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights (1x1x1024x352, 360.45k/360.45k params)\n","        FeatureExtractor/InceptionV2/Mixed_5b/Branch_1 (--/749.57k params)\n","          FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1 (--/196.61k params)\n","            FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights (1x1x1024x192, 196.61k/196.61k params)\n","          FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3 (--/552.96k params)\n","            FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/weights (3x3x192x320, 552.96k/552.96k params)\n","        FeatureExtractor/InceptionV2/Mixed_5b/Branch_2 (--/937.98k params)\n","          FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1 (--/163.84k params)\n","            FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights (1x1x1024x160, 163.84k/163.84k params)\n","          FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3 (--/322.56k params)\n","            FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights (3x3x160x224, 322.56k/322.56k params)\n","          FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3 (--/451.58k params)\n","            FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights (3x3x224x224, 451.58k/451.58k params)\n","        FeatureExtractor/InceptionV2/Mixed_5b/Branch_3 (--/131.07k params)\n","          FeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1 (--/131.07k params)\n","            FeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights (1x1x1024x128, 131.07k/131.07k params)\n","      FeatureExtractor/InceptionV2/Mixed_5c (--/2.28m params)\n","        FeatureExtractor/InceptionV2/Mixed_5c/Branch_0 (--/360.45k params)\n","          FeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1 (--/360.45k params)\n","            FeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights (1x1x1024x352, 360.45k/360.45k params)\n","        FeatureExtractor/InceptionV2/Mixed_5c/Branch_1 (--/749.57k params)\n","          FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1 (--/196.61k params)\n","            FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/weights (1x1x1024x192, 196.61k/196.61k params)\n","          FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3 (--/552.96k params)\n","            FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/weights (3x3x192x320, 552.96k/552.96k params)\n","        FeatureExtractor/InceptionV2/Mixed_5c/Branch_2 (--/1.04m params)\n","          FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1 (--/196.61k params)\n","            FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights (1x1x1024x192, 196.61k/196.61k params)\n","          FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3 (--/387.07k params)\n","            FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights (3x3x192x224, 387.07k/387.07k params)\n","          FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3 (--/451.58k params)\n","            FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights (3x3x224x224, 451.58k/451.58k params)\n","        FeatureExtractor/InceptionV2/Mixed_5c/Branch_3 (--/131.07k params)\n","          FeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1 (--/131.07k params)\n","            FeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            FeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights (1x1x1024x128, 131.07k/131.07k params)\n","      FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_2_1x1_256 (--/262.14k params)\n","        FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_2_1x1_256/BatchNorm (--/0 params)\n","        FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_2_1x1_256/weights (1x1x1024x256, 262.14k/262.14k params)\n","      FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_3_1x1_128 (--/65.54k params)\n","        FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_3_1x1_128/BatchNorm (--/0 params)\n","        FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_3_1x1_128/weights (1x1x512x128, 65.54k/65.54k params)\n","      FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_4_1x1_128 (--/32.77k params)\n","        FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_4_1x1_128/BatchNorm (--/0 params)\n","        FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_4_1x1_128/weights (1x1x256x128, 32.77k/32.77k params)\n","      FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_5_1x1_64 (--/16.38k params)\n","        FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_5_1x1_64/BatchNorm (--/0 params)\n","        FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_5_1x1_64/weights (1x1x256x64, 16.38k/16.38k params)\n","      FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_2_3x3_s2_512 (--/1.18m params)\n","        FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_2_3x3_s2_512/BatchNorm (--/0 params)\n","        FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_2_3x3_s2_512/weights (3x3x256x512, 1.18m/1.18m params)\n","      FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_3_3x3_s2_256 (--/294.91k params)\n","        FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_3_3x3_s2_256/BatchNorm (--/0 params)\n","        FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_3_3x3_s2_256/weights (3x3x128x256, 294.91k/294.91k params)\n","      FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_4_3x3_s2_256 (--/294.91k params)\n","        FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_4_3x3_s2_256/BatchNorm (--/0 params)\n","        FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_4_3x3_s2_256/weights (3x3x128x256, 294.91k/294.91k params)\n","      FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_5_3x3_s2_128 (--/73.73k params)\n","        FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_5_3x3_s2_128/BatchNorm (--/0 params)\n","        FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_5_3x3_s2_128/weights (3x3x64x128, 73.73k/73.73k params)\n","\n","======================End of Report==========================\n","169 ops no flops stats due to incomplete shapes.\n","Parsing Inputs...\n","Incomplete shape.\n","\n","=========================Options=============================\n","-max_depth                  10000\n","-min_bytes                  0\n","-min_peak_bytes             0\n","-min_residual_bytes         0\n","-min_output_bytes           0\n","-min_micros                 0\n","-min_accelerator_micros     0\n","-min_cpu_micros             0\n","-min_params                 0\n","-min_float_ops              1\n","-min_occurrence             0\n","-step                       -1\n","-order_by                   float_ops\n","-account_type_regexes       .*\n","-start_name_regexes         .*\n","-trim_name_regexes          .*BatchNorm.*,.*Initializer.*,.*Regularizer.*,.*BiasAdd.*\n","-show_name_regexes          .*\n","-hide_name_regexes          \n","-account_displayed_op_only  true\n","-select                     float_ops\n","-output                     stdout:\n","\n","==================Model Analysis Report======================\n","Incomplete shape.\n","\n","Doc:\n","scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n","flops: Number of float operations. Note: Please read the implementation for the math behind it.\n","\n","Profile:\n","node name | # float_ops\n","_TFProfRoot (--/46.53k flops)\n","  MultipleGridAnchorGenerator/mul_19 (6.14k/6.14k flops)\n","  MultipleGridAnchorGenerator/add_2 (6.14k/6.14k flops)\n","  MultipleGridAnchorGenerator/mul_20 (6.14k/6.14k flops)\n","  MultipleGridAnchorGenerator/sub (6.14k/6.14k flops)\n","  MultipleGridAnchorGenerator/add_5 (3.07k/3.07k flops)\n","  MultipleGridAnchorGenerator/mul_28 (3.07k/3.07k flops)\n","  MultipleGridAnchorGenerator/mul_27 (3.07k/3.07k flops)\n","  MultipleGridAnchorGenerator/mul_21 (3.07k/3.07k flops)\n","  MultipleGridAnchorGenerator/sub_1 (3.07k/3.07k flops)\n","  MultipleGridAnchorGenerator/mul_29 (1.54k/1.54k flops)\n","  MultipleGridAnchorGenerator/sub_2 (768/768 flops)\n","  MultipleGridAnchorGenerator/mul_35 (768/768 flops)\n","  MultipleGridAnchorGenerator/mul_36 (768/768 flops)\n","  MultipleGridAnchorGenerator/add_8 (768/768 flops)\n","  MultipleGridAnchorGenerator/mul_37 (384/384 flops)\n","  MultipleGridAnchorGenerator/sub_3 (192/192 flops)\n","  MultipleGridAnchorGenerator/mul_44 (192/192 flops)\n","  MultipleGridAnchorGenerator/add_11 (192/192 flops)\n","  MultipleGridAnchorGenerator/mul_43 (192/192 flops)\n","  MultipleGridAnchorGenerator/mul_45 (96/96 flops)\n","  MultipleGridAnchorGenerator/add_14 (48/48 flops)\n","  MultipleGridAnchorGenerator/mul_51 (48/48 flops)\n","  MultipleGridAnchorGenerator/mul_52 (48/48 flops)\n","  MultipleGridAnchorGenerator/sub_4 (48/48 flops)\n","  MultipleGridAnchorGenerator/mul_17 (32/32 flops)\n","  MultipleGridAnchorGenerator/mul_18 (32/32 flops)\n","  MultipleGridAnchorGenerator/add (32/32 flops)\n","  MultipleGridAnchorGenerator/add_1 (32/32 flops)\n","  MultipleGridAnchorGenerator/mul_53 (24/24 flops)\n","  MultipleGridAnchorGenerator/add_4 (16/16 flops)\n","  MultipleGridAnchorGenerator/add_3 (16/16 flops)\n","  MultipleGridAnchorGenerator/mul_26 (16/16 flops)\n","  MultipleGridAnchorGenerator/mul_25 (16/16 flops)\n","  MultipleGridAnchorGenerator/mul_60 (12/12 flops)\n","  MultipleGridAnchorGenerator/sub_5 (12/12 flops)\n","  MultipleGridAnchorGenerator/mul_59 (12/12 flops)\n","  MultipleGridAnchorGenerator/add_17 (12/12 flops)\n","  MultipleGridAnchorGenerator/add_6 (8/8 flops)\n","  MultipleGridAnchorGenerator/add_7 (8/8 flops)\n","  MultipleGridAnchorGenerator/mul_34 (8/8 flops)\n","  MultipleGridAnchorGenerator/mul_33 (8/8 flops)\n","  MultipleGridAnchorGenerator/mul_32 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_46 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_47 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_48 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_40 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_54 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_39 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_38 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_55 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_31 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_24 (6/6 flops)\n","  MultipleGridAnchorGenerator/truediv_19 (6/6 flops)\n","  MultipleGridAnchorGenerator/truediv_18 (6/6 flops)\n","  MultipleGridAnchorGenerator/truediv_17 (6/6 flops)\n","  MultipleGridAnchorGenerator/truediv_16 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_22 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_23 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_30 (6/6 flops)\n","  MultipleGridAnchorGenerator/truediv_15 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_61 (6/6 flops)\n","  MultipleGridAnchorGenerator/mul_56 (6/6 flops)\n","  MultipleGridAnchorGenerator/add_9 (4/4 flops)\n","  MultipleGridAnchorGenerator/mul_41 (4/4 flops)\n","  MultipleGridAnchorGenerator/mul_42 (4/4 flops)\n","  MultipleGridAnchorGenerator/add_10 (4/4 flops)\n","  MultipleGridAnchorGenerator/truediv_14 (3/3 flops)\n","  MultipleGridAnchorGenerator/mul_15 (3/3 flops)\n","  MultipleGridAnchorGenerator/mul_14 (3/3 flops)\n","  MultipleGridAnchorGenerator/mul_16 (3/3 flops)\n","  MultipleGridAnchorGenerator/mul_49 (2/2 flops)\n","  MultipleGridAnchorGenerator/mul_50 (2/2 flops)\n","  MultipleGridAnchorGenerator/add_12 (2/2 flops)\n","  MultipleGridAnchorGenerator/add_13 (2/2 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n","  Preprocessor/map/while/Less (1/1 flops)\n","  Preprocessor/map/while/Less_1 (1/1 flops)\n","  Preprocessor/map/while/add (1/1 flops)\n","  Preprocessor/map/while/add_1 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/ones/Less (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_1 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_2 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_2 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_19 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_18 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_9 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/add (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/add_1 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n","  MultipleGridAnchorGenerator/add_23 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_7 (1/1 flops)\n","  MultipleGridAnchorGenerator/add_18 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_6 (1/1 flops)\n","  MultipleGridAnchorGenerator/Minimum (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_58 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_57 (1/1 flops)\n","  MultipleGridAnchorGenerator/add_19 (1/1 flops)\n","  MultipleGridAnchorGenerator/add_20 (1/1 flops)\n","  MultipleGridAnchorGenerator/add_21 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_5 (1/1 flops)\n","  MultipleGridAnchorGenerator/add_22 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_8 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_4 (1/1 flops)\n","  MultipleGridAnchorGenerator/assert_equal/Equal (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_3 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_1 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_10 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_11 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_12 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_13 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_2 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_3 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_9 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_8 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_7 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_6 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_5 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_4 (1/1 flops)\n","  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_2 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_13 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_12 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_11 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_10 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv_1 (1/1 flops)\n","  MultipleGridAnchorGenerator/truediv (1/1 flops)\n","  MultipleGridAnchorGenerator/add_15 (1/1 flops)\n","  MultipleGridAnchorGenerator/add_16 (1/1 flops)\n","  MultipleGridAnchorGenerator/mul_9 (1/1 flops)\n","\n","======================End of Report==========================\n","W0830 19:37:39.526221 139977685026688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:411: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n","\n","2019-08-30 19:37:40.906027: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n","2019-08-30 19:37:40.924438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 19:37:40.925200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n","pciBusID: 0000:00:04.0\n","2019-08-30 19:37:40.925490: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2019-08-30 19:37:40.926796: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2019-08-30 19:37:40.927993: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2019-08-30 19:37:40.928414: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2019-08-30 19:37:40.929935: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2019-08-30 19:37:40.931007: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2019-08-30 19:37:40.934446: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2019-08-30 19:37:40.934574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 19:37:40.935365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 19:37:40.936032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2019-08-30 19:37:40.941363: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n","2019-08-30 19:37:40.941627: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29db480 executing computations on platform Host. Devices:\n","2019-08-30 19:37:40.941662: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n","2019-08-30 19:37:40.995253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 19:37:40.996203: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xb1eb180 executing computations on platform CUDA. Devices:\n","2019-08-30 19:37:40.996232: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n","2019-08-30 19:37:40.996400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 19:37:40.997119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n","pciBusID: 0000:00:04.0\n","2019-08-30 19:37:40.997178: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2019-08-30 19:37:40.997211: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2019-08-30 19:37:40.997235: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2019-08-30 19:37:40.997262: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2019-08-30 19:37:40.997285: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2019-08-30 19:37:40.997308: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2019-08-30 19:37:40.997332: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2019-08-30 19:37:40.997422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 19:37:40.998182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 19:37:40.998834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2019-08-30 19:37:40.998891: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2019-08-30 19:37:41.000410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2019-08-30 19:37:41.000454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2019-08-30 19:37:41.000467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2019-08-30 19:37:41.000603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 19:37:41.001332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 19:37:41.001964: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2019-08-30 19:37:41.002019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n","W0830 19:37:41.002914 139977685026688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","I0830 19:37:41.004244 139977685026688 saver.py:1280] Restoring parameters from training/model.ckpt-9669\n","2019-08-30 19:37:43.991532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 19:37:43.992283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n","pciBusID: 0000:00:04.0\n","2019-08-30 19:37:43.992355: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2019-08-30 19:37:43.992385: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2019-08-30 19:37:43.992408: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2019-08-30 19:37:43.992431: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2019-08-30 19:37:43.992458: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2019-08-30 19:37:43.992480: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2019-08-30 19:37:43.992503: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2019-08-30 19:37:43.992594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 19:37:43.993319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 19:37:43.994104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2019-08-30 19:37:43.994145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2019-08-30 19:37:43.994158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2019-08-30 19:37:43.994169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2019-08-30 19:37:43.994320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 19:37:43.995143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 19:37:43.995817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n","I0830 19:37:43.997275 139977685026688 saver.py:1280] Restoring parameters from training/model.ckpt-9669\n","W0830 19:37:44.789318 139977685026688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n","W0830 19:37:44.789603 139977685026688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.extract_sub_graph`\n","I0830 19:37:45.218438 139977685026688 graph_util_impl.py:311] Froze 410 variables.\n","I0830 19:37:45.373485 139977685026688 graph_util_impl.py:364] Converted 410 variables to const ops.\n","2019-08-30 19:37:45.616282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 19:37:45.617031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n","pciBusID: 0000:00:04.0\n","2019-08-30 19:37:45.617168: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2019-08-30 19:37:45.617215: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2019-08-30 19:37:45.617266: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2019-08-30 19:37:45.617310: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2019-08-30 19:37:45.617357: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2019-08-30 19:37:45.617400: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2019-08-30 19:37:45.617445: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2019-08-30 19:37:45.617587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 19:37:45.618409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 19:37:45.619192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2019-08-30 19:37:45.619244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2019-08-30 19:37:45.619265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2019-08-30 19:37:45.619297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2019-08-30 19:37:45.619439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 19:37:45.620250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-08-30 19:37:45.621001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n","W0830 19:37:46.440658 139977685026688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:288: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n","\n","W0830 19:37:46.441210 139977685026688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:291: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n","W0830 19:37:46.441758 139977685026688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:297: The name tf.saved_model.signature_def_utils.build_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.build_signature_def instead.\n","\n","W0830 19:37:46.441903 139977685026688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:300: The name tf.saved_model.signature_constants.PREDICT_METHOD_NAME is deprecated. Please use tf.saved_model.PREDICT_METHOD_NAME instead.\n","\n","W0830 19:37:46.442153 139977685026688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:305: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n","\n","W0830 19:37:46.442336 139977685026688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:307: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n","\n","I0830 19:37:46.442626 139977685026688 builder_impl.py:636] No assets to save.\n","I0830 19:37:46.442729 139977685026688 builder_impl.py:456] No assets to write.\n","I0830 19:37:46.941359 139977685026688 builder_impl.py:421] SavedModel written to: inference_graph/saved_model/saved_model.pb\n","W0830 19:37:46.994828 139977685026688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:188: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","I0830 19:37:46.995069 139977685026688 config_util.py:190] Writing pipeline config file to inference_graph/pipeline.config\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2qQaexuwblo8","colab":{}},"source":["%cd test_images\n","from google.colab import files\n","\n","uploaded = files.upload()\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n_w6-iKoEk0X","colab_type":"code","outputId":"30b9aaa2-ce63-4e2e-af85-c698c80de127","executionInfo":{"status":"ok","timestamp":1567195422399,"user_tz":-330,"elapsed":5019,"user":{"displayName":"Ravi Pawar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBtV9qOP0yVjG8p6Z6OsNi6vPiuKPc2AKMnCJqR=s64","userId":"00073904877215451927"}},"colab":{"base_uri":"https://localhost:8080/","height":626}},"source":["%cd ~/models/research/object_detection\n","import numpy as np\n","import os\n","import six.moves.urllib as urllib\n","import sys\n","import tarfile\n","import tensorflow as tf\n","import zipfile\n","\n","from collections import defaultdict\n","from io import StringIO\n","from matplotlib import pyplot as plt\n","from PIL import Image\n","\n","# This is needed since the notebook is stored in the object_detection folder.\n","sys.path.append(\"..\")\n","from object_detection.utils import ops as utils_ops\n","\n","#if tf.__version__ < '1.4.0':\n"," # raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')\n","  \n","\n","  \n","  \n","# This is needed to display the images.\n","%matplotlib inline\n","\n","\n","\n","\n","from utils import label_map_util\n","\n","from utils import visualization_utils as vis_util\n","\n","\n","MODEL_NAME = 'inference_graph'\n","CWD_PATH = os.getcwd()\n","\n","\n","# What model to download.\n","# Path to frozen detection graph. This is the actual model that is used for the object detection.\n","PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')\n","\n","# List of the strings that is used to add correct label for each box.\n","PATH_TO_LABELS = os.path.join(CWD_PATH,'training','labelmap.pbtxt')\n","\n","NUM_CLASSES = 2\n","\n","\n","\n","\n","detection_graph = tf.Graph()\n","with detection_graph.as_default():\n","  od_graph_def = tf.GraphDef()\n","  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n","    serialized_graph = fid.read()\n","    od_graph_def.ParseFromString(serialized_graph)\n","    tf.import_graph_def(od_graph_def, name='')\n","    \n","    \n","    \n","    \n","label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n","categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n","category_index = label_map_util.create_category_index(categories)\n","\n","\n","\n","\n","def load_image_into_numpy_array(image):\n","  (im_width, im_height) = image.size\n","  return np.array(image.getdata()).reshape(\n","      (im_height, im_width, 3)).astype(np.uint8)\n","\n","\n","\n","\n","# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n","PATH_TO_TEST_IMAGES_DIR = os.getcwd()\n","TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR,'/root/models/research/object_detection/images/test/design2 (44).jpg') ]\n","#TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, '.jpg'.format(i)) for i in range(1, 2) ]\n","# Size, in inches, of the output images.\n","IMAGE_SIZE = (10, 20)\n","\n","\n","\n","\n","def run_inference_for_single_image(image, graph):\n","  with graph.as_default():\n","    with tf.Session() as sess:\n","      # Get handles to input and output tensors\n","      ops = tf.get_default_graph().get_operations()\n","      all_tensor_names = {output.name for op in ops for output in op.outputs}\n","      tensor_dict = {}\n","      for key in [\n","          'num_detections', 'detection_boxes', 'detection_scores',\n","          'detection_classes', 'detection_masks'\n","      ]:\n","        tensor_name = key + ':0'\n","        if tensor_name in all_tensor_names:\n","          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n","              tensor_name)\n","      if 'detection_masks' in tensor_dict:\n","        # The following processing is only for single image\n","        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n","        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n","        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n","        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n","        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n","        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n","        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n","            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n","        detection_masks_reframed = tf.cast(\n","            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n","        # Follow the convention by adding back the batch dimension\n","        tensor_dict['detection_masks'] = tf.expand_dims(\n","            detection_masks_reframed, 0)\n","      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n","\n","      # Run inference\n","      output_dict = sess.run(tensor_dict,\n","                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n","\n","      # all outputs are float32 numpy arrays, so convert types as appropriate\n","      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n","      output_dict['detection_classes'] = output_dict[\n","          'detection_classes'][0].astype(np.uint8)\n","      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n","      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n","      if 'detection_masks' in output_dict:\n","        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n","  return output_dict\n","\n","\n","\n","\n","for image_path in TEST_IMAGE_PATHS:\n","  image = Image.open(image_path)\n","  # the array based representation of the image will be used later in order to prepare the\n","  # result image with boxes and labels on it.\n","  image_np = load_image_into_numpy_array(image)\n","  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n","  image_np_expanded = np.expand_dims(image_np, axis=0)\n","  # Actual detection.\n","  output_dict = run_inference_for_single_image(image_np, detection_graph)\n","  # Visualization of the results of a detection.\n","  vis_util.visualize_boxes_and_labels_on_image_array(\n","      image_np,\n","      output_dict['detection_boxes'],\n","      output_dict['detection_classes'],\n","      output_dict['detection_scores'],\n","      category_index,\n","      instance_masks=output_dict.get('detection_masks'),\n","      use_normalized_coordinates=True,\n","      line_thickness=4)\n","  plt.figure(figsize=IMAGE_SIZE)\n","  plt.imshow(image_np)"],"execution_count":41,"outputs":[{"output_type":"stream","text":["/root/models/research/object_detection\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAlkAAAJQCAYAAAC993GPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB9okkduRZ1zAAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 720x1440 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"g2CnwjoSEn2M","colab_type":"code","colab":{}},"source":["import numpy as np\n","import os\n","import six.moves.urllib as urllib\n","import sys\n","import tarfile\n","import tensorflow as tf\n","import zipfile\n","import uuid\n","import numpy as np\n","import argparse\n","import imutils\n","import cv2\n","from datetime import datetime\n","from collections import defaultdict\n","from io import StringIO\n","from matplotlib import pyplot as plt\n","from PIL import Image\n","import json\n","import cv2\n","import glob\n","from lxml import etree\n","import xml.etree.cElementTree as ET\n","from json2xml import json2xml, readfromurl, readfromstring, readfromjson\n","\n","sys.path.append(\"..\")\n","\n","from utils import label_map_util\n","\n","from utils import visualization_utils as vis_util\n","from utils import visualization_utils_1 as vis_util_1\n","from utils import visualization_utils_2 as vis_util_2\n","from utils import visualization_utils_3 as vis_util_3\n","\n","MODEL_NAME_1 = 'inference_graph_12'\n","\n","# Path to frozen detection graph. This is the actual model that is used for the object detection.\n","PATH_TO_CKPT_1 = MODEL_NAME_1 + '/frozen_inference_graph.pb'\n","\n","# List of the strings that is used to add correct label for each box.\n","PATH_TO_LABELS_1 = os.path.join('inference_graph_12', 'labelmap.pbtxt')\n","\n","MODEL_NAME_2 = 'inference_graph_new'\n","\n","# Path to frozen detection graph. This is the actual model that is used for the object detection.\n","PATH_TO_CKPT_2 = MODEL_NAME_2 + '/frozen_inference_graph.pb'\n","\n","# List of the strings that is used to add correct label for each box.\n","PATH_TO_LABELS_2 = os.path.join('inference_graph_14', 'labelmap.pbtxt')\n","\n","MODEL_NAME_3 = 'inference_graph_13'\n","\n","# Path to frozen detection graph. This is the actual model that is used for the object detection.\n","PATH_TO_CKPT_3 = MODEL_NAME_3 + '/frozen_inference_graph.pb'\n","\n","# List of the strings that is used to add correct label for each box.\n","PATH_TO_LABELS_3 = os.path.join('inference_graph_13', 'labelmap.pbtxt')\n","\n","# -- doors only\n","\n","MODEL_NAME_4 = 'inference_graph_new'\n","\n","# Path to frozen detection graph. This is the actual model that is used for the object detection.\n","PATH_TO_CKPT_4 = MODEL_NAME_4 + '/frozen_inference_graph_door.pb'\n","\n","# List of the strings that is used to add correct label for each box.\n","PATH_TO_LABELS_4 = os.path.join('inference_graph_14', 'labelmap.pbtxt')\n","\n","all_rooms = []\n","all_windows = []\n","all_doors = []\n","all_walls = []\n","\n","NUM_CLASSES = 90\n","image = cv2.imread(r'C:\\Users\\tidyquant\\Desktop\\000013.jpg')\n","\n","\n","def correlation(x1, y1, x2,\n","                y2, x, y):\n","    if (x > x1 and x < x2 and\n","            y > y1 and y < y2):\n","        return True\n","    else:\n","        return False\n","\n","\n","def process_1(image):\n","    detection_graph = tf.Graph()\n","    with detection_graph.as_default():\n","        od_graph_def = tf.GraphDef()\n","        with tf.gfile.GFile(PATH_TO_CKPT_1, 'rb') as fid:\n","            serialized_graph = fid.read()\n","            od_graph_def.ParseFromString(serialized_graph)\n","            tf.import_graph_def(od_graph_def, name='')\n","\n","    label_map = label_map_util.load_labelmap(PATH_TO_LABELS_1)\n","    categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES,\n","                                                                use_display_name=True)\n","\n","    category_index = label_map_util.create_category_index(categories)\n","\n","    with detection_graph.as_default():\n","        with tf.Session(graph=detection_graph) as sess:\n","\n","            image_np = image\n","            '''image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2GRAY)\n","\n","            cv2.imshow('fff',image_np)\n","            cv2.imwrite('f.jpg',image_np)\n","            cv2.waitKey(0)\n","            #frame=image_np.copy()'''\n","            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n","            image_np_expanded = np.expand_dims(image_np, axis=0)\n","            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n","            # Each box represents a part of the image where a particular object was detected.\n","            boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n","            # Each score represent how level of confidence for each of the objects.\n","            # Score is shown on the result image, together with the class label.\n","            scores = detection_graph.get_tensor_by_name('detection_scores:0')\n","            classes = detection_graph.get_tensor_by_name('detection_classes:0')\n","            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n","            # Actual detection.\n","\n","            (boxes, scores, classes, num_detections) = sess.run(\n","                [boxes, scores, classes, num_detections],\n","                feed_dict={image_tensor: image_np_expanded})\n","            print(classes)\n","\n","            # Visualization of the results of a detection.\n","            image, box, name = vis_util.visualize_boxes_and_labels_on_image_array(\n","                image_np,\n","                np.squeeze(boxes),\n","                np.squeeze(classes).astype(np.int32),\n","                np.squeeze(scores),\n","                category_index,\n","\n","                # skip_scores=True,\n","                # skip_labels=True,\n","\n","                use_normalized_coordinates=True,\n","                line_thickness=1)\n","\n","            doors = []\n","            windows = []\n","            walls = []\n","            rooms = []\n","            height, width, channels = image_np.shape\n","            for index, value in enumerate(classes[0]):\n","                if scores[0, index] > 0.0:\n","                    if ((category_index.get(value)).get(\"name\") == 'door'):\n","                        print(name)\n","                        object_dict = {}\n","                        object_dict[\"door\"] = {}\n","                        object_dict[\"door\"][\"assignedname\"] = {}\n","                        object_dict[\"door\"][\"coordinates\"] = {}\n","                        object_dict[\"door\"][\"ActualCoordinates\"] = {}\n","                        object_dict[\"door\"][\"index\"] = (category_index.get(value)).get(\"id\")\n","                        object_dict[\"door\"][\"name\"] = (category_index.get(value)).get(\"name\")\n","                        object_dict[\"door\"][\"confidence\"] = str(scores[0, index])\n","                        object_dict[\"door\"][\"assignedname\"] = name[index]\n","                        ymin = boxes[0][index][0] * height\n","                        xmin = boxes[0][index][1] * width\n","                        ymax = boxes[0][index][2] * height\n","                        xmax = boxes[0][index][3] * width\n","                        x = (((xmin) + (xmax)) / 2)\n","                        y = (((ymin) + (ymax)) / 2)\n","                        # (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n","                        #   ymin * im_height, ymax * im_height)\n","                        # print(left, right, top, bottom)\n","                        object_dict[\"door\"][\"coordinates\"][\"ymin\"] = ymin\n","                        object_dict[\"door\"][\"coordinates\"][\"xmin\"] = xmin\n","                        object_dict[\"door\"][\"coordinates\"][\"ymax\"] = ymax\n","                        object_dict[\"door\"][\"coordinates\"][\"xmax\"] = xmax\n","                        object_dict[\"door\"][\"ActualCoordinates\"][\"x\"] = x\n","                        object_dict[\"door\"][\"ActualCoordinates\"][\"y\"] = y\n","                        doors.append(object_dict)\n","\n","            # cv2.rectangle(image_np, (round(right), round(bottom)), (round(left), round(top)), (0, 255, 0), 5)'''\n","\n","            image_np = imutils.resize(image_np, width=600, height=600)\n","            cv2.imshow('Recognized Image', image_np)\n","\n","            cv2.waitKey(000)\n","\n","            process_4(image, doors)\n","\n","\n","def process_4(image, doors):\n","    detection_graph = tf.Graph()\n","    with detection_graph.as_default():\n","        od_graph_def = tf.GraphDef()\n","        with tf.gfile.GFile(PATH_TO_CKPT_4, 'rb') as fid:\n","            serialized_graph = fid.read()\n","            od_graph_def.ParseFromString(serialized_graph)\n","            tf.import_graph_def(od_graph_def, name='')\n","\n","    label_map = label_map_util.load_labelmap(PATH_TO_LABELS_4)\n","    categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES,\n","                                                                use_display_name=True)\n","\n","    category_index = label_map_util.create_category_index(categories)\n","\n","    with detection_graph.as_default():\n","        with tf.Session(graph=detection_graph) as sess:\n","\n","            image_np = image\n","            '''image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2GRAY)\n","\n","            cv2.imshow('fff',image_np)\n","            cv2.imwrite('f.jpg',image_np)\n","            cv2.waitKey(0)\n","            #frame=image_np.copy()'''\n","            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n","            image_np_expanded = np.expand_dims(image_np, axis=0)\n","            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n","            # Each box represents a part of the image where a particular object was detected.\n","            boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n","            # Each score represent how level of confidence for each of the objects.\n","            # Score is shown on the result image, together with the class label.\n","            scores = detection_graph.get_tensor_by_name('detection_scores:0')\n","            classes = detection_graph.get_tensor_by_name('detection_classes:0')\n","            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n","\n","            # Actual detection.\n","\n","            (boxes, scores, classes, num_detections) = sess.run(\n","                [boxes, scores, classes, num_detections],\n","                feed_dict={image_tensor: image_np_expanded})\n","            image, box, name = vis_util_3.visualize_boxes_and_labels_on_image_array(\n","                image_np,\n","                np.squeeze(boxes),\n","                np.squeeze(classes).astype(np.int32),\n","                np.squeeze(scores),\n","                category_index,\n","\n","                # skip_scores=True,\n","                # skip_labels=True,\n","\n","                use_normalized_coordinates=True,\n","                line_thickness=1)\n","            print(classes)\n","            doors_2 = []\n","            windows = []\n","            walls = []\n","            rooms = []\n","            corr_dorrs = []\n","            height, width, channels = image_np.shape\n","            for index, value in enumerate(classes[0]):\n","                if scores[0, index] > 0.7:\n","                    if ((category_index.get(value)).get(\"name\") == 'door'):\n","                        object_dict = {}\n","                        object_dict[\"door\"] = {}\n","                        object_dict[\"door\"][\"assignedname\"] = {}\n","                        object_dict[\"door\"][\"coordinates\"] = {}\n","                        object_dict[\"door\"][\"ActualCoordinates\"] = {}\n","                        object_dict[\"door\"][\"index\"] = (category_index.get(value)).get(\"id\")\n","                        object_dict[\"door\"][\"name\"] = (category_index.get(value)).get(\"name\")\n","                        object_dict[\"door\"][\"confidence\"] = str(scores[0, index])\n","                        object_dict[\"door\"][\"assignedname\"] = name[index]\n","                        ymin = boxes[0][index][0] * height\n","                        xmin = boxes[0][index][1] * width\n","                        ymax = boxes[0][index][2] * height\n","                        xmax = boxes[0][index][3] * width\n","                        x = (((xmin) + (xmax)) / 2)\n","                        y = (((ymin) + (ymax)) / 2)\n","                        # (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n","                        #   ymin * im_height, ymax * im_height)\n","                        # print(left, right, top, bottom)\n","                        object_dict[\"door\"][\"coordinates\"][\"ymin\"] = ymin\n","                        object_dict[\"door\"][\"coordinates\"][\"xmin\"] = xmin\n","                        object_dict[\"door\"][\"coordinates\"][\"ymax\"] = ymax\n","                        object_dict[\"door\"][\"coordinates\"][\"xmax\"] = xmax\n","                        object_dict[\"door\"][\"ActualCoordinates\"][\"x\"] = x\n","                        object_dict[\"door\"][\"ActualCoordinates\"][\"y\"] = y\n","                        doors_2.append(object_dict)\n","\n","            for door in doors:\n","                for doorss in doors_2:\n","                    ymin = boxes[0][index][0] * height\n","                    xmin = boxes[0][index][1] * width\n","                    ymax = boxes[0][index][2] * height\n","                    xmax = boxes[0][index][3] * width\n","                    x = (((xmin) + (xmax)) / 2)\n","                    y = (((ymin) + (ymax)) / 2)\n","                    res = correlation(door['door'][\"coordinates\"][\"xmin\"], door['door'][\"coordinates\"][\"ymin\"],\n","                                      door['door'][\"coordinates\"][\"xmax\"], door['door'][\"coordinates\"][\"ymax\"],\n","                                      doorss['door']['ActualCoordinates']['x'],\n","                                      doorss['door']['ActualCoordinates']['y'])\n","                    print('result', res)\n","                    if ((res == True)):\n","                        # if(scores[0, index] > 0.7):\n","                        # vis_utils_3\n","                        # print('boxes',scores[0, index])\n","                        # cv2.rectangle(image_np, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (255, 0, 0), 7)\n","                        doors_2.remove(doorss)\n","            for doorsss in doors_2:\n","\n","                if (float(doorsss[\"door\"][\"confidence\"]) > 0.7):\n","                    '''cv2.rectangle(image_np, (\n","                        int(doorsss[\"door\"][\"coordinates\"][\"xmin\"]), int(doorsss[\"door\"][\"coordinates\"][\"ymin\"])), (\n","                                      int(doorsss[\"door\"][\"coordinates\"][\"xmax\"]),\n","                                      int(doorsss[\"door\"][\"coordinates\"][\"ymax\"])), (255, 0, 0), 7)'''\n","                    score = round(float(doorsss[\"door\"][\"confidence\"]) * 100)\n","                    '''cv2.putText(image_np, doorsss[\"door\"][\"assignedname\"] + ':' + str(score) + '%', (\n","                        int(doorsss[\"door\"][\"coordinates\"][\"xmin\"]), int(doorsss[\"door\"][\"coordinates\"][\"ymin\"])),\n","                                cv2.FONT_HERSHEY_SIMPLEX, 0.95, (255, 0, 0), 2, cv2.LINE_AA)'''\n","\n","            image_np = imutils.resize(image_np, width=600, height=600)\n","            cv2.imshow('Recognized Image', image_np)\n","            cv2.waitKey(000)\n","            corr_dorrs.append(doors)\n","            corr_dorrs.append(doors_2)\n","            print('gg', corr_dorrs)\n","\n","            process_2(image, corr_dorrs)\n","\n","\n","def process_2(image, corr_dorrs):\n","    detection_graph = tf.Graph()\n","    with detection_graph.as_default():\n","        od_graph_def = tf.GraphDef()\n","        with tf.gfile.GFile(PATH_TO_CKPT_2, 'rb') as fid:\n","            serialized_graph = fid.read()\n","            od_graph_def.ParseFromString(serialized_graph)\n","            tf.import_graph_def(od_graph_def, name='')\n","\n","    label_map = label_map_util.load_labelmap(PATH_TO_LABELS_2)\n","    categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES,\n","                                                                use_display_name=True)\n","\n","    category_index = label_map_util.create_category_index(categories)\n","\n","    with detection_graph.as_default():\n","        with tf.Session(graph=detection_graph) as sess:\n","\n","            image_np = image\n","            '''image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2GRAY)\n","\n","                 cv2.imshow('fff',image_np)\n","                 cv2.imwrite('f.jpg',image_np)\n","                 cv2.waitKey(0)\n","                 #frame=image_np.copy()'''\n","            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n","            image_np_expanded = np.expand_dims(image_np, axis=0)\n","            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n","            # Each box represents a part of the image where a particular object was detected.\n","            boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n","            # Each score represent how level of confidence for each of the objects.\n","            # Score is shown on the result image, together with the class label.\n","            scores = detection_graph.get_tensor_by_name('detection_scores:0')\n","            classes = detection_graph.get_tensor_by_name('detection_classes:0')\n","            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n","            # Actual detection.\n","\n","            (boxes, scores, classes, num_detections) = sess.run(\n","                [boxes, scores, classes, num_detections],\n","                feed_dict={image_tensor: image_np_expanded})\n","            print(classes)\n","\n","            # Visualization of the results of a detection.\n","            image, box, name = vis_util_1.visualize_boxes_and_labels_on_image_array(\n","                image_np,\n","                np.squeeze(boxes),\n","                np.squeeze(classes).astype(np.int32),\n","                np.squeeze(scores),\n","                category_index,\n","\n","                # skip_scores=True,\n","                # skip_labels=True,\n","\n","                use_normalized_coordinates=True,\n","                line_thickness=1)\n","            print('corrdooors', corr_dorrs)\n","            doors_2 = []\n","            windows = []\n","            walls = []\n","            rooms = []\n","            corr_dorrs_2 = []\n","            height, width, channels = image_np.shape\n","            try:\n","              for index, value in enumerate(classes[0]):\n","                print('name',((category_index.get(value)).get(\"name\")))\n","                if scores[0, index] > 0.3:\n","                    if ((category_index.get(value)).get(\"name\") == 'wall'):\n","                        wall = {}\n","                        wall[\"wall\"] = {}\n","                        wall[\"wall\"][\"coordinates\"] = {}\n","                        wall[\"wall\"][\"assignedname\"] = {}\n","                        wall[\"wall\"][\"ActualCoordinates\"] = {}\n","                        wall[\"wall\"][\"index\"] = (category_index.get(value)).get(\"id\")\n","                        wall[\"wall\"][\"name\"] = (category_index.get(value)).get(\"name\")\n","                        wall[\"wall\"][\"confidence\"] = str(scores[0, index])\n","                        wall[\"wall\"][\"assignedname\"] = name[index]\n","                        ymin = boxes[0][index][0] * height\n","                        xmin = boxes[0][index][1] * width\n","                        ymax = boxes[0][index][2] * height\n","                        xmax = boxes[0][index][3] * width\n","                        x = (((xmin) + (xmax)) / 2)\n","                        y = (((ymin) + (ymax)) / 2)\n","                        # (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n","                        #   ymin * im_height, ymax * im_height)\n","                        # print(left, right, top, bottom)\n","                        wall[\"wall\"][\"coordinates\"][\"ymin\"] = ymin\n","                        wall[\"wall\"][\"coordinates\"][\"xmin\"] = xmin\n","                        wall[\"wall\"][\"coordinates\"][\"ymax\"] = ymax\n","                        wall[\"wall\"][\"coordinates\"][\"xmax\"] = xmax\n","                        wall[\"wall\"][\"ActualCoordinates\"][\"x\"] = x\n","                        wall[\"wall\"][\"ActualCoordinates\"][\"y\"] = y\n","                        walls.append(wall)\n","                    if ((category_index.get(value)).get(\"name\") == 'room'):\n","                        print((category_index.get(value)).get(\"name\"))\n","                        room = {}\n","                        room[\"room\"] = {}\n","                        room[\"room\"][\"coordinates\"] = {}\n","                        room[\"room\"][\"assignedname\"] = {}\n","                        room[\"room\"][\"ActualCoordinates\"] = {}\n","                        room[\"room\"][\"index\"] = (category_index.get(value)).get(\"id\")\n","                        room[\"room\"][\"name\"] = (category_index.get(value)).get(\"name\")\n","                        room[\"room\"][\"confidence\"] = str(scores[0, index])\n","                        room[\"room\"][\"assignedname\"] = name[index]\n","                        ymin = boxes[0][index][0] * height\n","                        xmin = boxes[0][index][1] * width\n","                        ymax = boxes[0][index][2] * height\n","                        xmax = boxes[0][index][3] * width\n","                        # cv2.rectangle(image_np, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 255, 0), 5)\n","                        x = (((xmin) + (xmax)) / 2)\n","                        y = (((ymin) + (ymax)) / 2)\n","                        # (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n","                        #   ymin * im_height, ymax * im_height)\n","                        # print(left, right, top, bottom)\n","                        room[\"room\"][\"coordinates\"][\"ymin\"] = ymin\n","                        room[\"room\"][\"coordinates\"][\"xmin\"] = xmin\n","                        room[\"room\"][\"coordinates\"][\"ymax\"] = ymax\n","                        room[\"room\"][\"coordinates\"][\"xmax\"] = xmax\n","                        room[\"room\"][\"ActualCoordinates\"][\"x\"] = x\n","                        room[\"room\"][\"ActualCoordinates\"][\"y\"] = y\n","\n","                        rooms.append(room)\n","\n","                    if ((category_index.get(value)).get(\"name\") == 'door'):\n","                        object_dict = {}\n","                        object_dict[\"door\"] = {}\n","                        object_dict[\"door\"][\"assignedname\"] = {}\n","                        object_dict[\"door\"][\"coordinates\"] = {}\n","                        object_dict[\"door\"][\"ActualCoordinates\"] = {}\n","                        object_dict[\"door\"][\"index\"] = (category_index.get(value)).get(\"id\")\n","                        object_dict[\"door\"][\"name\"] = (category_index.get(value)).get(\"name\")\n","                        object_dict[\"door\"][\"confidence\"] = str(scores[0, index])\n","                        object_dict[\"door\"][\"assignedname\"] = name[index]\n","                        ymin = boxes[0][index][0] * height\n","                        xmin = boxes[0][index][1] * width\n","                        ymax = boxes[0][index][2] * height\n","                        xmax = boxes[0][index][3] * width\n","                        x = (((xmin) + (xmax)) / 2)\n","                        y = (((ymin) + (ymax)) / 2)\n","                        # (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n","                        #   ymin * im_height, ymax * im_height)\n","                        # print(left, right, top, bottom)\n","                        object_dict[\"door\"][\"coordinates\"][\"ymin\"] = ymin\n","                        object_dict[\"door\"][\"coordinates\"][\"xmin\"] = xmin\n","                        object_dict[\"door\"][\"coordinates\"][\"ymax\"] = ymax\n","                        object_dict[\"door\"][\"coordinates\"][\"xmax\"] = xmax\n","                        object_dict[\"door\"][\"ActualCoordinates\"][\"x\"] = x\n","                        object_dict[\"door\"][\"ActualCoordinates\"][\"y\"] = y\n","                        doors_2.append(object_dict)\n","            except:\n","                print('no data')\n","            for door in corr_dorrs:\n","                for d in door:\n","                    for doors in doors_2:\n","                        print('ddd', door)\n","                        ymin = boxes[0][index][0] * height\n","                        xmin = boxes[0][index][1] * width\n","                        ymax = boxes[0][index][2] * height\n","                        xmax = boxes[0][index][3] * width\n","                        x = (((xmin) + (xmax)) / 2)\n","                        y = (((ymin) + (ymax)) / 2)\n","                        res = correlation(d['door'][\"coordinates\"][\"xmin\"], d['door'][\"coordinates\"][\"ymin\"],\n","                                          d['door'][\"coordinates\"][\"xmax\"], d['door'][\"coordinates\"][\"ymax\"],\n","                                          doors['door']['ActualCoordinates']['x'],\n","                                          doors['door']['ActualCoordinates']['y'])\n","                        print('result', res)\n","                        if ((res == True)):\n","                            # if(scores[0, index] > 0.7):\n","                            # vis_utils_3\n","                            # print('boxes',scores[0, index])\n","                            # cv2.rectangle(image_np, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (255, 0, 0), 7)\n","                            doors_2.remove(doors)\n","            for doors in doors_2:\n","                print('doorfull', doors)\n","                if (float(doors[\"door\"][\"confidence\"]) > 0.3):\n","                    '''cv2.rectangle(image_np,\n","                                  (\n","                                      int(doors[\"door\"][\"coordinates\"][\"xmin\"]),\n","                                      int(doors[\"door\"][\"coordinates\"][\"ymin\"])),\n","                                  (\n","                                      int(doors[\"door\"][\"coordinates\"][\"xmax\"]),\n","                                      int(doors[\"door\"][\"coordinates\"][\"ymax\"])),\n","                                  (255, 0, 0), 7)'''\n","                    score = round(float(doors[\"door\"][\"confidence\"]) * 100)\n","                    '''cv2.putText(image_np, doors[\"door\"][\"assignedname\"] + ':' + str(score) + '%',\n","                                (int(doors[\"door\"][\"coordinates\"][\"xmin\"]), int(doors[\"door\"][\"coordinates\"][\"ymin\"])),\n","                                cv2.FONT_HERSHEY_SIMPLEX, 0.95, (255, 0, 0), 2, cv2.LINE_AA)'''\n","\n","            image_np = imutils.resize(image_np, width=600, height=600)\n","            cv2.imshow('Recognized Image', image_np)\n","            cv2.waitKey(000)\n","            corr_dorrs_2.append(corr_dorrs)\n","            corr_dorrs_2.append(doors_2)\n","            all_rooms.append(rooms)\n","            print('dddd', all_rooms)\n","            all_walls.append(walls)\n","            process_3(image, corr_dorrs_2)\n","\n","\n","def process_3(image, corr_dorrs_2):\n","    detection_graph = tf.Graph()\n","    with detection_graph.as_default():\n","        od_graph_def = tf.GraphDef()\n","        with tf.gfile.GFile(PATH_TO_CKPT_3, 'rb') as fid:\n","            serialized_graph = fid.read()\n","            od_graph_def.ParseFromString(serialized_graph)\n","            tf.import_graph_def(od_graph_def, name='')\n","\n","    label_map = label_map_util.load_labelmap(PATH_TO_LABELS_3)\n","    categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES,\n","                                                                use_display_name=True)\n","\n","    category_index = label_map_util.create_category_index(categories)\n","\n","    with detection_graph.as_default():\n","        with tf.Session(graph=detection_graph) as sess:\n","\n","            image_np = image\n","            '''image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2GRAY)\n","\n","            cv2.imshow('fff',image_np)\n","            cv2.imwrite('f.jpg',image_np)\n","            cv2.waitKey(0)\n","            #frame=image_np.copy()'''\n","            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n","            image_np_expanded = np.expand_dims(image_np, axis=0)\n","            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n","            # Each box represents a part of the image where a particular object was detected.\n","            boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n","            # Each score represent how level of confidence for each of the objects.\n","            # Score is shown on the result image, together with the class label.\n","            scores = detection_graph.get_tensor_by_name('detection_scores:0')\n","            classes = detection_graph.get_tensor_by_name('detection_classes:0')\n","            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n","            # Actual detection.\n","\n","            (boxes, scores, classes, num_detections) = sess.run(\n","                [boxes, scores, classes, num_detections],\n","                feed_dict={image_tensor: image_np_expanded})\n","            print(classes)\n","\n","            # Visualization of the results of a detection.\n","            image, box, name = vis_util_2.visualize_boxes_and_labels_on_image_array(\n","                image_np,\n","                np.squeeze(boxes),\n","                np.squeeze(classes).astype(np.int32),\n","                np.squeeze(scores),\n","                category_index,\n","\n","                # skip_scores=True,\n","                # skip_labels=True,\n","\n","                use_normalized_coordinates=True,\n","                line_thickness=1)\n","            doors_2 = []\n","            windows = []\n","            walls = []\n","            rooms = []\n","            corr_dorrs_3 = []\n","            height, width, channels = image_np.shape\n","            for index, value in enumerate(classes[0]):\n","                if scores[0, index] > 0.0:\n","                    if ((category_index.get(value)).get(\"name\") == 'wall'):\n","                        wall = {}\n","                        wall[\"wall\"] = {}\n","                        wall[\"wall\"][\"coordinates\"] = {}\n","                        wall[\"wall\"][\"assignedname\"] = {}\n","                        wall[\"wall\"][\"ActualCoordinates\"] = {}\n","                        wall[\"wall\"][\"index\"] = (category_index.get(value)).get(\"id\")\n","                        wall[\"wall\"][\"name\"] = (category_index.get(value)).get(\"name\")\n","                        wall[\"wall\"][\"confidence\"] = str(scores[0, index])\n","                        wall[\"wall\"][\"assignedname\"] = name[index]\n","                        ymin = boxes[0][index][0] * height\n","                        xmin = boxes[0][index][1] * width\n","                        ymax = boxes[0][index][2] * height\n","                        xmax = boxes[0][index][3] * width\n","                        x = (((xmin) + (xmax)) / 2)\n","                        y = (((ymin) + (ymax)) / 2)\n","                        # (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n","                        #   ymin * im_height, ymax * im_height)\n","                        # print(left, right, top, bottom)\n","                        wall[\"wall\"][\"coordinates\"][\"ymin\"] = ymin\n","                        wall[\"wall\"][\"coordinates\"][\"xmin\"] = xmin\n","                        wall[\"wall\"][\"coordinates\"][\"ymax\"] = ymax\n","                        wall[\"wall\"][\"coordinates\"][\"xmax\"] = xmax\n","                        wall[\"wall\"][\"ActualCoordinates\"][\"x\"] = x\n","                        wall[\"wall\"][\"ActualCoordinates\"][\"y\"] = y\n","                        walls.append(wall)\n","                    if ((category_index.get(value)).get(\"name\") == 'window'):\n","                        window = {}\n","                        window[\"window\"] = {}\n","                        window[\"window\"][\"coordinates\"] = {}\n","                        window[\"window\"][\"assignedname\"] = {}\n","                        window[\"window\"][\"ActualCoordinates\"] = {}\n","                        window[\"window\"][\"index\"] = (category_index.get(value)).get(\"id\")\n","                        window[\"window\"][\"name\"] = (category_index.get(value)).get(\"name\")\n","                        window[\"window\"][\"confidence\"] = str(scores[0, index])\n","                        window[\"window\"][\"assignedname\"] = name[index]\n","\n","                        ymin = boxes[0][index][0] * height\n","                        xmin = boxes[0][index][1] * width\n","                        ymax = boxes[0][index][2] * height\n","                        xmax = boxes[0][index][3] * width\n","                        x = (((xmin) + (xmax)) / 2)\n","                        y = (((ymin) + (ymax)) / 2)\n","                        # (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n","                        #   ymin * im_height, ymax * im_height)\n","                        # print(left, right, top, bottom)\n","                        window[\"window\"][\"coordinates\"][\"ymin\"] = ymin\n","                        window[\"window\"][\"coordinates\"][\"xmin\"] = xmin\n","                        window[\"window\"][\"coordinates\"][\"ymax\"] = ymax\n","                        window[\"window\"][\"coordinates\"][\"xmax\"] = xmax\n","                        window[\"window\"][\"ActualCoordinates\"][\"x\"] = x\n","                        window[\"window\"][\"ActualCoordinates\"][\"y\"] = y\n","\n","                        windows.append(window)\n","                    if ((category_index.get(value)).get(\"name\") == 'door'):\n","                        object_dict = {}\n","                        object_dict[\"door\"] = {}\n","                        object_dict[\"door\"][\"assignedname\"] = {}\n","                        object_dict[\"door\"][\"coordinates\"] = {}\n","                        object_dict[\"door\"][\"ActualCoordinates\"] = {}\n","                        object_dict[\"door\"][\"index\"] = (category_index.get(value)).get(\"id\")\n","                        object_dict[\"door\"][\"name\"] = (category_index.get(value)).get(\"name\")\n","                        object_dict[\"door\"][\"confidence\"] = str(scores[0, index])\n","                        object_dict[\"door\"][\"assignedname\"] = name[index]\n","                        ymin = boxes[0][index][0] * height\n","                        xmin = boxes[0][index][1] * width\n","                        ymax = boxes[0][index][2] * height\n","                        xmax = boxes[0][index][3] * width\n","                        x = (((xmin) + (xmax)) / 2)\n","                        y = (((ymin) + (ymax)) / 2)\n","                        # (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n","                        #   ymin * im_height, ymax * im_height)\n","                        # print(left, right, top, bottom)\n","                        object_dict[\"door\"][\"coordinates\"][\"ymin\"] = ymin\n","                        object_dict[\"door\"][\"coordinates\"][\"xmin\"] = xmin\n","                        object_dict[\"door\"][\"coordinates\"][\"ymax\"] = ymax\n","                        object_dict[\"door\"][\"coordinates\"][\"xmax\"] = xmax\n","                        object_dict[\"door\"][\"ActualCoordinates\"][\"x\"] = x\n","                        object_dict[\"door\"][\"ActualCoordinates\"][\"y\"] = y\n","                        doors_2.append(object_dict)\n","            print('pre',corr_dorrs_2)\n","            print('ddd', doors_2)\n","            for door in corr_dorrs_2:\n","                for s in door:\n","                    for d in s:\n","\n","                        for doors in doors_2:\n","\n","                            ymin = boxes[0][index][0] * height\n","                            xmin = boxes[0][index][1] * width\n","                            ymax = boxes[0][index][2] * height\n","                            xmax = boxes[0][index][3] * width\n","                            x = (((xmin) + (xmax)) / 2)\n","                            y = (((ymin) + (ymax)) / 2)\n","                            res = correlation(d['door'][\"coordinates\"][\"xmin\"], d['door'][\"coordinates\"][\"ymin\"],\n","                                              d['door'][\"coordinates\"][\"xmax\"], d['door'][\"coordinates\"][\"ymax\"],\n","                                              doors['door']['ActualCoordinates']['x'],\n","                                              doors['door']['ActualCoordinates']['y'])\n","                            print('result', res)\n","                            if ((res == True)):\n","                                # if(scores[0, index] > 0.7):\n","                                # vis_utils_3\n","                                #print('boxes',scores[0, index])\n","                                # cv2.rectangle(image_np, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (255, 0, 0), 7)\n","                                doors_2.remove(doors)\n","            print('doorfull', doors_2)\n","            for doors in doors_2:\n","\n","                if (float(doors[\"door\"][\"confidence\"]) > 0.0):\n","                    '''cv2.rectangle(image_np,\n","                                  (\n","                                      int(doors[\"door\"][\"coordinates\"][\"xmin\"]),\n","                                      int(doors[\"door\"][\"coordinates\"][\"ymin\"])),\n","                                  (\n","                                      int(doors[\"door\"][\"coordinates\"][\"xmax\"]),\n","                                      int(doors[\"door\"][\"coordinates\"][\"ymax\"])),\n","                                  (255, 0, 0), 7)'''\n","                    score = round(float(doors[\"door\"][\"confidence\"]) * 100)\n","                    '''cv2.putText(image_np, doors[\"door\"][\"assignedname\"] + ':' + str(score) + '%',\n","                                (int(doors[\"door\"][\"coordinates\"][\"xmin\"]), int(doors[\"door\"][\"coordinates\"][\"ymin\"])),\n","                                cv2.FONT_HERSHEY_SIMPLEX, 0.95, (255, 0, 0), 2, cv2.LINE_AA)'''\n","\n","            image_np = imutils.resize(image_np, width=600, height=600)\n","            cv2.imshow('Recognized Image', image_np)\n","            cv2.waitKey(000)\n","            all_doors.append(corr_dorrs_2)\n","            all_doors.append(doors_2)\n","            all_windows.append(windows)\n","            all_walls.append(walls)\n","\n","            process_5()\n","\n","\n","def process_5():\n","    objdoor1 = []\n","    objdoor1.append(all_doors)\n","    print('addd',all_doors)\n","    obj1 = json.dumps(objdoor1)\n","    print('fffh', obj1)\n","    data1 = readfromstring(obj1)\n","    fg = json2xml.Json2xml(data1, wrapper=\"doors\", indent=8).to_xml()\n","    objwindow2 = []\n","    objwindow2.append(all_windows)\n","    obj2 = json.dumps(objwindow2)\n","    data2 = readfromstring(obj2)\n","    fgg = json2xml.Json2xml(data2, wrapper=\"windows\", indent=8).to_xml()\n","    objroom = []\n","    objwall = []\n","\n","    objwall.append(all_walls)\n","    objroom.append(all_rooms)\n","    obj3 = json.dumps(objwall)\n","    obj4 = json.dumps(objroom)\n","    data3 = readfromstring(obj3)\n","    data4 = readfromstring(obj4)\n","    fggg = json2xml.Json2xml(data3, wrapper=\"walls\", indent=8).to_xml()\n","    fgggg = json2xml.Json2xml(data4, wrapper=\"rooms\", indent=8).to_xml()\n","    corrooms = []\n","    corrooms = all_rooms.copy()\n","\n","    corrdoors = []\n","    corrdoors = all_doors.copy()\n","\n","    corrwindows = []\n","    corrwindows = all_windows.copy()\n","    corrwalls = []\n","    corrwalls = all_walls.copy()\n","    roomcorrelations = []\n","    roomcorrelation = {}\n","    roomcorrelation[\"doors\"] = {}\n","    roomcorrelation[\"windows\"] = {}\n","    roomcorrelation[\"walls\"] = {}\n","    print('cr', corrdoors)\n","    print('dr', corrwalls)\n","    a = 0\n","    try:\n","      for index_1 in corrooms:\n","        for index in index_1:\n","            for door_3 in corrdoors:\n","\n","                for door_2 in door_3:\n","                    for door_1 in door_2:\n","                        for door in door_1:\n","\n","\n","                            res = correlation(index['room'][\"coordinates\"][\"xmin\"],\n","                                              index['room'][\"coordinates\"][\"ymin\"],\n","                                              index['room'][\"coordinates\"][\"xmax\"],\n","                                              index['room'][\"coordinates\"][\"ymax\"],\n","                                              door['door']['ActualCoordinates']['x'],\n","                                              door['door']['ActualCoordinates']['y'])\n","                            if (res == True):\n","                                roomcorrelation[\"doors\"][a] = {}\n","                                roomcorrelation[\"doors\"][a][\"name\"] = door['door'][\"assignedname\"]\n","                                roomcorrelation[\"doors\"][a][\"proomid\"] = index['room'][\"assignedname\"]\n","                                # roomcorrelation[\"doors\"][\"proomid\"]=index['room'][\"assignedname\"]\n","\n","                                roomcorrelations.append(roomcorrelation)\n","                                door_1.remove(door)\n","                                a = a + 1\n","    except:\n","        print('ggg')\n","    a = 0\n","    for index in corrooms:\n","        for index in index_1:\n","            for door_1 in corrwindows:\n","                for door in door_1:\n","                    res = correlation(index['room'][\"coordinates\"][\"xmin\"], index['room'][\"coordinates\"][\"ymin\"],\n","                                      index['room'][\"coordinates\"][\"xmax\"], index['room'][\"coordinates\"][\"ymax\"],\n","                                      door['window']['ActualCoordinates']['x'],\n","                                      door['window']['ActualCoordinates']['y'])\n","                    if (res == True):\n","                        roomcorrelation[\"windows\"][a] = {}\n","                        roomcorrelation[\"windows\"][a][\"name\"] = door['window'][\"assignedname\"]\n","                        roomcorrelation[\"windows\"][a][\"proomid\"] = index['room'][\"assignedname\"]\n","                        # roomcorrelation[\"doors\"][\"proomid\"]=index['room'][\"assignedname\"]\n","\n","                        # corrdoors.remove(door)\n","                        roomcorrelations.append(roomcorrelation)\n","                        door_1.remove(door)\n","                        a = a + 1\n","    a = 0\n","    for index_1 in corrooms:\n","        for index in index_1:\n","            for door_1 in corrwalls:\n","                for door in door_1:\n","                    res = correlation(index['room'][\"coordinates\"][\"xmin\"], index['room'][\"coordinates\"][\"ymin\"],\n","                                      index['room'][\"coordinates\"][\"xmax\"], index['room'][\"coordinates\"][\"ymax\"],\n","                                      door['wall']['ActualCoordinates']['x'], door['wall']['ActualCoordinates']['y'])\n","                    if (res == True):\n","                        roomcorrelation[\"walls\"][a] = {}\n","                        roomcorrelation[\"walls\"][a][\"name\"] = door['wall'][\"assignedname\"]\n","                        roomcorrelation[\"walls\"][a][\"proomid\"] = index['room'][\"assignedname\"]\n","                        # roomcorrelation[\"doors\"][\"proomid\"]=index['room'][\"assignedname\"]\n","\n","                        # corrdoors.remove(door)\n","                        roomcorrelations.append(roomcorrelation)\n","                        door_1.remove(door)\n","                        a = a + 1\n","\n","\n","\n","\n","    objcorrelation = []\n","\n","\n","\n","\n","    objcorrelation.append(roomcorrelation)\n","\n","\n","\n","    obj5 = json.dumps(objcorrelation)\n","\n","\n","\n","    data5 = readfromstring(obj5)\n","\n","\n","\n","\n","    fggggg = json2xml.Json2xml(data5, wrapper=\"correlations\", indent=8).to_xml()\n","    root = ET.Element(\"Floorplan\")\n","    tree = ET.ElementTree(root)\n","    tree.write(\"demofile3.xml\")\n","\n","    # XML strings to etree\n","    addressbook_root = etree.fromstring(fg)\n","    note_root = etree.fromstring(fgg)\n","    wall_root = etree.fromstring(fggg)\n","    room_root = etree.fromstring(fgggg)\n","    correlation_root = etree.fromstring(fggggg)\n","    newbook_root = etree.parse('demofile3.xml').getroot()\n","    # append the note\n","    newbook_root.append(addressbook_root)\n","    newbook_root.append(note_root)\n","    newbook_root.append(wall_root)\n","    newbook_root.append(room_root)\n","    newbook_root.append(correlation_root)\n","    # print the new addressbook XML document\n","\n","    f = open(\"demofile3.xml\", \"wb\")\n","    f.write(etree.tostring(newbook_root))\n","    f.close()\n","process_1(image)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Y6By6UREo_u","colab_type":"code","colab":{}},"source":["cd root\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fu506yuREsIW","colab_type":"code","colab":{}},"source":["ls"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TN5BtmIwEy5Y","colab_type":"code","colab":{}},"source":["#Generate root password\n","import random, string\n","password = ''.join(random.choice(string.ascii_letters + string.digits) for i in range(20))\n","\n","#Download ngrok\n","! wget -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","! unzip -qq -n ngrok-stable-linux-amd64.zip\n","#Setup sshd\n","! apt-get install -qq -o=Dpkg::Use-Pty=0 openssh-server pwgen > /dev/null\n","#Set root password\n","! echo root:$password | chpasswd\n","! mkdir -p /var/run/sshd\n","! echo \"PermitRootLogin yes\" >> /etc/ssh/sshd_config\n","! echo \"PasswordAuthentication yes\" >> /etc/ssh/sshd_config\n","! echo \"LD_LIBRARY_PATH=/usr/lib64-nvidia\" >> /root/.bashrc\n","! echo \"export LD_LIBRARY_PATH\" >> /root/.bashrc\n","\n","#Run sshd\n","get_ipython().system_raw('/usr/sbin/sshd -D &')\n","\n","#Ask token\n","print(\"Copy authtoken from https://dashboard.ngrok.com/auth\")\n","import getpass\n","authtoken = getpass.getpass()\n","\n","#Create tunnel\n","get_ipython().system_raw('./ngrok authtoken $authtoken && ./ngrok tcp 22 &')\n","#Print root password\n","print(\"Root password: {}\".format(password))\n","#Get public address\n","! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":0,"outputs":[]}]}